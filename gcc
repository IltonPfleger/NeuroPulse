Layer.c:7:34: missed: statement clobbers memory: _3 = aligned_alloc (64, _2);
Layer.c:8:35: missed: statement clobbers memory: _6 = aligned_alloc (64, _5);
Layer.c:9:34: missed: statement clobbers memory: _7 = aligned_alloc (64, _5);
Layer.c:14:19: missed: statement clobbers memory: _8 = PULSE_GetActivationFunctionPtr (activation_function_18(D));
Layer.c:19:9: note: ***** Analysis succeeded with vector mode V8SI
Layer.c:19:9: note: SLPing BB part
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x35d77c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: op template: MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: 	stmt 0 MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: 	stmt 1 MEM <unsigned int> [(struct PULSE_Layer *)&<retval> + 4B] = optimization_type_17(D);
Layer.c:19:9: note: 	children 0x35d7848
Layer.c:19:9: note: node (external) 0x35d7848 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ type_16(D), optimization_type_17(D) }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x35d78c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: op template: <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: 	stmt 0 <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: 	stmt 1 <retval>.n_outputs = n_outputs_12(D);
Layer.c:19:9: note: 	children 0x35d7948
Layer.c:19:9: note: node (external) 0x35d7948 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ n_inputs_11(D), n_outputs_12(D) }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x35d7a48 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: <retval>.inputs = _3;
Layer.c:19:9: note: 	stmt 0 <retval>.inputs = _3;
Layer.c:19:9: note: 	stmt 1 <retval>.outputs = _6;
Layer.c:19:9: note: 	children 0x35d7ac8
Layer.c:19:9: note: node (external) 0x35d7ac8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: 	stmt 0 _3 = aligned_alloc (64, _2);
Layer.c:19:9: note: 	stmt 1 _6 = aligned_alloc (64, _5);
Layer.c:19:9: note: 	children 0x35d7b48 0x35d7bc8
Layer.c:19:9: note: node (constant) 0x35d7b48 (max_nunits=1, refcnt=1)
Layer.c:19:9: note: 	{ 64, 64 }
Layer.c:19:9: note: node 0x35d7bc8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: patt_40 = n_inputs_11(D) w* 4;
Layer.c:19:9: note: 	stmt 0 patt_40 = n_inputs_11(D) w* 4;
Layer.c:19:9: note: 	stmt 1 patt_41 = n_outputs_12(D) w* 4;
Layer.c:19:9: note: 	children 0x35d7c48 0x35d7cc8
Layer.c:19:9: note: node (external) 0x35d7c48 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ n_inputs_11(D), n_outputs_12(D) }
Layer.c:19:9: note: node (constant) 0x35d7cc8 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ 4, 4 }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x35d7dc8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Layer.c:19:9: note: op template: <retval>.feed = feed_20(D);
Layer.c:19:9: note: 	stmt 0 <retval>.feed = feed_20(D);
Layer.c:19:9: note: 	stmt 1 <retval>.back = back_21(D);
Layer.c:19:9: note: 	stmt 2 <retval>.fix = fix_22(D);
Layer.c:19:9: note: 	stmt 3 <retval>.destroy = destroy_23(D);
Layer.c:19:9: note: 	children 0x35d7e48
Layer.c:19:9: note: node (external) 0x35d7e48 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Layer.c:19:9: note: 	{ feed_20(D), back_21(D), fix_22(D), destroy_23(D) }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 64
  Scalar cost: 64
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x35d7f48 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: <retval>.parent = 0B;
Layer.c:19:9: note: 	stmt 0 <retval>.parent = 0B;
Layer.c:19:9: note: 	stmt 1 <retval>.child = 0B;
Layer.c:19:9: note: 	children 0x35d7fc8
Layer.c:19:9: note: node (constant) 0x35d7fc8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: 	{ 0B, 0B }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Layer.c:19:9: note: Basic block will be vectorized using SLP
Layer.c:19:9: optimized: basic block part vectorized using 8 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x35d77c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: op template: MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: 	stmt 0 MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: 	stmt 1 MEM <unsigned int> [(struct PULSE_Layer *)&<retval> + 4B] = optimization_type_17(D);
Layer.c:19:9: note: 	children 0x35d7848
Layer.c:19:9: note: node (external) 0x35d7848 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ type_16(D), optimization_type_17(D) }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: vect_is_simple_use: operand optimization_type_17(D), type of def: external
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: MEM <unsigned int> [(struct PULSE_Layer *)&<retval>]
Layer.c:19:9: note: created &MEM <unsigned int> [(struct PULSE_Layer *)&<retval>]
Layer.c:19:9: note: add new stmt: MEM <vector(2) unsigned int> [(struct PULSE_Layer *)&<retval>] = _42;
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: optimized: basic block part vectorized using 8 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x35d78c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: op template: <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: 	stmt 0 <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: 	stmt 1 <retval>.n_outputs = n_outputs_12(D);
Layer.c:19:9: note: 	children 0x35d7948
Layer.c:19:9: note: node (external) 0x35d7948 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ n_inputs_11(D), n_outputs_12(D) }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: vect_is_simple_use: operand n_outputs_12(D), type of def: external
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: <retval>.n_inputs
Layer.c:19:9: note: created &<retval>.n_inputs
Layer.c:19:9: note: add new stmt: MEM <vector(2) unsigned int> [(unsigned int *)&<retval> + 96B] = _44;
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: optimized: basic block part vectorized using 16 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x35d7a48 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: <retval>.inputs = _3;
Layer.c:19:9: note: 	stmt 0 <retval>.inputs = _3;
Layer.c:19:9: note: 	stmt 1 <retval>.outputs = _6;
Layer.c:19:9: note: 	children 0x35d7ac8
Layer.c:19:9: note: node (external) 0x35d7ac8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: 	stmt 0 _3 = aligned_alloc (64, _2);
Layer.c:19:9: note: 	stmt 1 _6 = aligned_alloc (64, _5);
Layer.c:19:9: note: 	children 0x35d7b48 0x35d7bc8
Layer.c:19:9: note: node (constant) 0x35d7b48 (max_nunits=1, refcnt=1)
Layer.c:19:9: note: 	{ 64, 64 }
Layer.c:19:9: note: node 0x35d7bc8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: patt_40 = n_inputs_11(D) w* 4;
Layer.c:19:9: note: 	stmt 0 patt_40 = n_inputs_11(D) w* 4;
Layer.c:19:9: note: 	stmt 1 patt_41 = n_outputs_12(D) w* 4;
Layer.c:19:9: note: 	children 0x35d7c48 0x35d7cc8
Layer.c:19:9: note: node (external) 0x35d7c48 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ n_inputs_11(D), n_outputs_12(D) }
Layer.c:19:9: note: node (constant) 0x35d7cc8 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ 4, 4 }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: <retval>.inputs = _3;
Layer.c:19:9: note: vect_is_simple_use: operand aligned_alloc (64, _5), type of def: internal
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.inputs
Layer.c:19:9: note: created &<retval>.inputs
Layer.c:19:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval> + 8B] = _48;
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: optimized: basic block part vectorized using 32 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x35d7dc8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Layer.c:19:9: note: op template: <retval>.feed = feed_20(D);
Layer.c:19:9: note: 	stmt 0 <retval>.feed = feed_20(D);
Layer.c:19:9: note: 	stmt 1 <retval>.back = back_21(D);
Layer.c:19:9: note: 	stmt 2 <retval>.fix = fix_22(D);
Layer.c:19:9: note: 	stmt 3 <retval>.destroy = destroy_23(D);
Layer.c:19:9: note: 	children 0x35d7e48
Layer.c:19:9: note: node (external) 0x35d7e48 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Layer.c:19:9: note: 	{ feed_20(D), back_21(D), fix_22(D), destroy_23(D) }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: <retval>.feed = feed_20(D);
Layer.c:19:9: note: vect_is_simple_use: operand back_21(D), type of def: external
Layer.c:19:9: note: vect_is_simple_use: operand fix_22(D), type of def: external
Layer.c:19:9: note: vect_is_simple_use: operand destroy_23(D), type of def: external
Layer.c:19:9: note: conflicting alias set types.
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(4) long unsigned int  vectorizing a pointer ref: <retval>.feed
Layer.c:19:9: note: created &<retval>.feed
Layer.c:19:9: note: add new stmt: MEM <vector(4) long unsigned int> [(void *)&<retval> + 32B] = _54;
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: optimized: basic block part vectorized using 16 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x35d7f48 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: <retval>.parent = 0B;
Layer.c:19:9: note: 	stmt 0 <retval>.parent = 0B;
Layer.c:19:9: note: 	stmt 1 <retval>.child = 0B;
Layer.c:19:9: note: 	children 0x35d7fc8
Layer.c:19:9: note: node (constant) 0x35d7fc8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: 	{ 0B, 0B }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: <retval>.parent = 0B;
Layer.c:19:9: note: vect_is_simple_use: operand 0B, type of def: constant
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.parent
Layer.c:19:9: note: created &<retval>.parent
Layer.c:19:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer * *)&<retval> + 72B] = { 0, 0 };
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: note: ***** The result for vector mode V32QI would be the same
Layer.c:24:2: missed: statement clobbers memory: free (_1);
Layer.c:25:2: missed: statement clobbers memory: free (_2);
Layer.c:26:2: missed: statement clobbers memory: free (_3);
Layer.c:27:1: note: ***** Analysis failed with vector mode V4DI
Layer.c:27:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _30 (pretmp_91, _50, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:20: missed: not vectorized: no vectype for stmt: _21 = *_20;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:145:19: missed: couldn't vectorize loop
Dense.c:145:19: missed: not vectorized: number of iterations cannot be computed.
Dense.c:136:19: missed: couldn't vectorize loop
Dense.c:136:19: missed: not vectorized: number of iterations cannot be computed.
Dense.c:129:13: note: vectorized 0 loops in function.
Dense.c:153:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:153:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:98:28: missed: couldn't vectorize loop
Dense.c:98:28: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:101:21: missed: couldn't vectorize loop
Dense.c:101:21: missed: not vectorized: number of iterations cannot be computed.
Dense.c:115:28: missed: couldn't vectorize loop
Dense.c:115:28: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:118:21: missed: couldn't vectorize loop
Dense.c:118:21: missed: not vectorized: number of iterations cannot be computed.
Dense.c:84:19: missed: couldn't vectorize loop
Dense.c:84:19: missed: not vectorized: number of iterations cannot be computed.
Dense.c:78:13: note: vectorized 0 loops in function.
Dense.c:81:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:103:17: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:103:17: note: SLPing BB part
Dense.c:120:17: note: Costing subgraph: 
Dense.c:120:17: note: node 0x8010d88 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:120:17: note: op template: _49 = (sizetype) j_137;
Dense.c:120:17: note: 	stmt 0 _49 = (sizetype) j_137;
Dense.c:120:17: note: 	stmt 1 _209 = (sizetype) wi_136;
Dense.c:120:17: note: 	children 0x8010e88
Dense.c:120:17: note: node (external) 0x8010e88 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:120:17: note: 	{ j_137, wi_136 }
Dense.c:120:17: note: Cost model analysis: 
Dense.c:120:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:120:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:120:17: missed: not vectorized: vectorization is not profitable.
Dense.c:103:17: note: Costing subgraph: 
Dense.c:103:17: note: node 0x8010f88 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:103:17: note: op template: _24 = (sizetype) j_134;
Dense.c:103:17: note: 	stmt 0 _24 = (sizetype) j_134;
Dense.c:103:17: note: 	stmt 1 _229 = (sizetype) wi_133;
Dense.c:103:17: note: 	children 0x8011088
Dense.c:103:17: note: node (external) 0x8011088 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:103:17: note: 	{ j_134, wi_133 }
Dense.c:103:17: note: Cost model analysis: 
Dense.c:103:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:103:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:103:17: missed: not vectorized: vectorization is not profitable.
Dense.c:103:17: note: ***** The result for vector mode V32QI would be the same
Dense.c:103:17: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:103:17: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:103:17: note: SLPing BB part
Dense.c:120:17: note: Costing subgraph: 
Dense.c:120:17: note: node 0x8010f88 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:120:17: note: op template: _49 = (sizetype) j_137;
Dense.c:120:17: note: 	stmt 0 _49 = (sizetype) j_137;
Dense.c:120:17: note: 	stmt 1 _209 = (sizetype) wi_136;
Dense.c:120:17: note: 	children 0x8010d88
Dense.c:120:17: note: node (external) 0x8010d88 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:120:17: note: 	{ j_137, wi_136 }
Dense.c:120:17: note: Cost model analysis: 
Dense.c:120:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:120:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:120:17: missed: not vectorized: vectorization is not profitable.
Dense.c:103:17: note: Costing subgraph: 
Dense.c:103:17: note: node 0x8011108 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:103:17: note: op template: _24 = (sizetype) j_134;
Dense.c:103:17: note: 	stmt 0 _24 = (sizetype) j_134;
Dense.c:103:17: note: 	stmt 1 _229 = (sizetype) wi_133;
Dense.c:103:17: note: 	children 0x8011008
Dense.c:103:17: note: node (external) 0x8011008 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:103:17: note: 	{ j_134, wi_133 }
Dense.c:103:17: note: Cost model analysis: 
Dense.c:103:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:103:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:103:17: missed: not vectorized: vectorization is not profitable.
Dense.c:103:17: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:98:55: note: ***** Analysis failed with vector mode V8QI
Dense.c:98:55: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:98:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:58:27: missed: couldn't vectorize loop
Dense.c:58:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:61:20: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:905:10: missed: not vectorized: no vectype for stmt: _41 = MEM[(__m256_u * {ref-all})_6];
 scalar_type: __m256_u
Dense.c:61:20: note: ***** Analysis failed with vector mode VOID
Dense.c:54:13: note: vectorized 0 loops in function.
Dense.c:54:13: missed: splitting region at dont-vectorize loop 3 entry at bb18
Dense.c:63:14: note: ***** Analysis failed with vector mode V4DI
Dense.c:63:14: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:63:14: missed: splitting region at dominance boundary bb8
Dense.c:63:14: note: ***** Analysis failed with vector mode VOID
Dense.c:63:14: missed: splitting region at loop 1 exit at bb16
Dense.c:58:27: note: ***** Analysis failed with vector mode V8SF
Dense.c:58:27: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SF
Dense.c:58:27: missed: splitting region at dominance boundary bb10
Dense.c:58:27: note: ***** Analysis failed with vector mode VOID
Dense.c:74:2: missed: statement clobbers memory: _27 (pretmp_153, _61, 0);
Dense.c:75:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:75:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:160:2: missed: statement clobbers memory: free (_1);
Dense.c:161:2: missed: statement clobbers memory: free (_2);
Dense.c:162:2: missed: statement clobbers memory: free (_3);
Dense.c:163:2: missed: statement clobbers memory: free (_4);
Dense.c:164:2: missed: statement clobbers memory: free (dense_7);
Dense.c:165:2: missed: statement clobbers memory: PULSE_DestroyLayer (this_6(D));
Dense.c:166:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:166:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:180:19: missed: couldn't vectorize loop
Dense.c:180:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:170:13: note: vectorized 0 loops in function.
Dense.c:172:47: missed: statement clobbers memory: dense_35 = malloc (40);
Dense.c:174:36: missed: statement clobbers memory: _5 = aligned_alloc (64, _4);
Dense.c:175:38: missed: statement clobbers memory: _6 = aligned_alloc (64, _4);
Dense.c:176:36: missed: statement clobbers memory: _8 = aligned_alloc (64, _7);
Dense.c:177:35: missed: statement clobbers memory: _9 = aligned_alloc (64, _7);
Dense.c:178:36: missed: statement clobbers memory: _10 = aligned_alloc (64, _7);
Dense.c:181:39: missed: statement clobbers memory: _11 = rand ();
Dense.c:181:73: missed: statement clobbers memory: _63 = sqrt (_18);
Dense.c:198:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:199:4: missed: statement clobbers memory: exit (1);
Dense.c:191:12: missed: statement clobbers memory: layer = PULSE_CreateLayer (n_inputs.3_29, n_outputs.2_28, 1, activation_function_51(D), _SIMD_FeedDense, _SIMD_BackDense, _SIMD_FixDense, _DestroyDense, 1); [return slot optimization]
Dense.c:187:12: missed: statement clobbers memory: layer = PULSE_CreateLayer (n_inputs.1_27, n_outputs.0_26, 1, activation_function_51(D), _FeedDense, _BackDense, _FixDense, _DestroyDense, 0); [return slot optimization]
Dense.c:205:9: missed: not vectorized: more than one data ref in stmt: <retval> = layer;
Dense.c:205:9: note: ***** Analysis failed with vector mode V4DI
Dense.c:205:9: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Activations.c:17:19: optimized: loop vectorized using 32 byte vectors
Activations.c:17:19: optimized: loop vectorized using 16 byte vectors
Activations.c:17:19: optimized: loop vectorized using 32 byte vectors
Activations.c:17:19: optimized: loop vectorized using 16 byte vectors
Activations.c:14:13: note: vectorized 2 loops in function.
Activations.c:14:13: note: ***** Analysis failed with vector mode V8SF
Activations.c:14:13: note: ***** The result for vector mode V32QI would be the same
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V16QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V16QI
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V8QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V8QI
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V4QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V4QI
Activations.c:28:19: missed: couldn't vectorize loop
Activations.c:28:19: missed: not vectorized: unsupported control flow in loop.
Activations.c:28:19: optimized: loop vectorized using 32 byte vectors
Activations.c:28:19: optimized: loop vectorized using 16 byte vectors
Activations.c:25:13: note: vectorized 1 loops in function.
Activations.c:25:13: note: ***** Analysis failed with vector mode V8SF
Activations.c:25:13: note: ***** The result for vector mode V32QI would be the same
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V16QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V16QI
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V8QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V8QI
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V4QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V4QI
Activations.c:38:1: note: ***** Analysis failed with vector mode VOID
Activations.c:5:19: missed: couldn't vectorize loop
Activations.c:9:21: missed: statement clobbers memory: _36 = expf (_18);
Activations.c:5:19: missed: couldn't vectorize loop
Activations.c:9:21: missed: statement clobbers memory: _12 = expf (_11);
Activations.c:3:13: note: vectorized 0 loops in function.
Activations.c:9:21: missed: statement clobbers memory: _12 = expf (_11);
Activations.c:9:21: missed: statement clobbers memory: _36 = expf (_18);
Activations.c:11:1: note: ***** Analysis failed with vector mode V8SF
Activations.c:11:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SF
Activations.c:44:2: note: ***** Analysis failed with vector mode VOID
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:4:18: note: vectorized 0 loops in function.
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_4, inputs_15, _3);
PULSE.c:8:2: missed: statement clobbers memory: _5 (layer_17);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_12, pretmp_26, _9);
PULSE.c:13:28: note: ***** Analysis failed with vector mode V8SI
PULSE.c:13:28: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:22:2: missed: statement clobbers memory: _1 (layer_10(D));
PULSE.c:22:2: missed: statement clobbers memory: _14 (_2);
PULSE.c:22:2: missed: statement clobbers memory: _21 (_15);
PULSE.c:22:2: missed: statement clobbers memory: _28 (_22);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_29);
PULSE.c:26:3: missed: statement clobbers memory: memset (_34, 0, _32);
PULSE.c:26:3: missed: statement clobbers memory: memset (_27, 0, _25);
PULSE.c:26:3: missed: statement clobbers memory: memset (_20, 0, _18);
PULSE.c:26:3: missed: statement clobbers memory: memset (_7, 0, _5);
PULSE.c:28:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:28:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:32:6: note: vectorized 1 loops in function.
PULSE.c:34:8: missed: statement clobbers memory: _1 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_2);
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _9 = rand ();
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8SI
PULSE.c:32:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:61:16: missed: couldn't vectorize loop
PULSE.c:61:16: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:64:17: missed: couldn't vectorize loop
PULSE.c:64:17: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:73:19: missed: couldn't vectorize loop
PULSE.c:73:19: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _64 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:53:22: missed: couldn't vectorize loop
PULSE.c:53:22: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:47:6: note: vectorized 1 loops in function.
PULSE.c:48:1: missed: statement clobbers memory: saved_stack.7_35 = __builtin_stack_save ();
PULSE.c:49:46: missed: statement clobbers memory: _1 = PULSE_GetLossFunctionPtr (loss_function_36(D));
PULSE.c:60:10: missed: statement clobbers memory: random.4_42 = __builtin_alloca_with_align (_4, 32);
PULSE.c:34:8: missed: statement clobbers memory: _56 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_57);
PULSE.c:39:29: missed: statement clobbers memory: _64 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _67 = rand ();
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_93, inputs_88, _92);
PULSE.c:8:2: missed: statement clobbers memory: _94 (layer_89);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_100, _99, _98);
PULSE.c:67:11: missed: statement clobbers memory: loss_51 = PULSE_GetLoss_38 (_17, _16, _12, _11);
PULSE.c:22:2: missed: statement clobbers memory: _81 (output_30);
PULSE.c:22:2: missed: statement clobbers memory: _103 (_82);
PULSE.c:22:2: missed: statement clobbers memory: _110 (_104);
PULSE.c:22:2: missed: statement clobbers memory: _117 (_111);
PULSE.c:22:2: missed: statement clobbers memory: _124 (_118);
PULSE.c:22:2: missed: statement clobbers memory: _131 (_125);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_132);
PULSE.c:26:3: missed: statement clobbers memory: memset (_137, 0, _135);
PULSE.c:26:3: missed: statement clobbers memory: memset (_130, 0, _128);
PULSE.c:26:3: missed: statement clobbers memory: memset (_123, 0, _121);
PULSE.c:26:3: missed: statement clobbers memory: memset (_116, 0, _114);
PULSE.c:26:3: missed: statement clobbers memory: memset (_109, 0, _107);
PULSE.c:26:3: missed: statement clobbers memory: memset (_87, 0, _85);
PULSE.c:75:6: missed: statement clobbers memory: _21 (current_183, args);
PULSE.c:80:4: missed: statement clobbers memory: printf ("Epoch: %d Item: %d Loss: %.10f\r", i_181, j_182, _22);
PULSE.c:47:6: missed: statement clobbers memory: __builtin_stack_restore (saved_stack.7_35);
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4DI
PULSE.c:47:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:89:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:89:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:94:8: missed: couldn't vectorize loop
PULSE.c:94:8: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:91:6: note: vectorized 0 loops in function.
PULSE.c:96:3: missed: statement clobbers memory: _1 (current_10);
PULSE.c:99:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:99:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Loss.c:15:19: optimized: loop vectorized using 32 byte vectors
Loss.c:15:19: optimized:  loop versioned for vectorization because of possible aliasing
Loss.c:15:19: optimized: loop vectorized using 16 byte vectors
Loss.c:12:23: note: vectorized 1 loops in function.
Loss.c:12:23: note: ***** Analysis failed with vector mode V8SF
Loss.c:12:23: note: ***** The result for vector mode V32QI would be the same
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V16QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V16QI
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V8QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V8QI
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V4QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V4QI
Loss.c:7:19: missed: couldn't vectorize loop
Loss.c:8:38: missed: not vectorized: unsupported use in stmt.
Loss.c:4:23: note: vectorized 0 loops in function.
Loss.c:9:13: note: ***** Analysis failed with vector mode V8SF
Loss.c:9:13: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SF
Loss.c:23:2: note: ***** Analysis failed with vector mode VOID
Loss.c:15:19: optimized: loop vectorized using 32 byte vectors
Loss.c:15:19: optimized:  loop versioned for vectorization because of possible aliasing
Loss.c:15:19: optimized: loop vectorized using 16 byte vectors
Loss.c:12:23: note: vectorized 1 loops in function.
Loss.c:12:23: note: ***** Analysis failed with vector mode V8SF
Loss.c:12:23: note: ***** The result for vector mode V32QI would be the same
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V16QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V16QI
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V8QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V8QI
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V4QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V4QI
Loss.c:7:19: missed: couldn't vectorize loop
Loss.c:8:38: missed: not vectorized: unsupported use in stmt.
Loss.c:4:23: note: vectorized 0 loops in function.
Loss.c:9:13: note: ***** Analysis failed with vector mode V8SF
Loss.c:9:13: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SF
Loss.c:23:2: note: ***** Analysis failed with vector mode VOID
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _30 (pretmp_91, _50, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:20: missed: not vectorized: no vectype for stmt: _21 = *_20;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:145:19: missed: couldn't vectorize loop
Dense.c:145:19: missed: not vectorized: number of iterations cannot be computed.
Dense.c:136:19: missed: couldn't vectorize loop
Dense.c:136:19: missed: not vectorized: number of iterations cannot be computed.
Dense.c:129:13: note: vectorized 0 loops in function.
Dense.c:153:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:153:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:98:28: missed: couldn't vectorize loop
Dense.c:98:28: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:101:21: missed: couldn't vectorize loop
Dense.c:101:21: missed: not vectorized: number of iterations cannot be computed.
Dense.c:115:28: missed: couldn't vectorize loop
Dense.c:115:28: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:118:21: missed: couldn't vectorize loop
Dense.c:118:21: missed: not vectorized: number of iterations cannot be computed.
Dense.c:84:19: missed: couldn't vectorize loop
Dense.c:84:19: missed: not vectorized: number of iterations cannot be computed.
Dense.c:78:13: note: vectorized 0 loops in function.
Dense.c:81:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:103:17: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:103:17: note: SLPing BB part
Dense.c:120:17: note: Costing subgraph: 
Dense.c:120:17: note: node 0x6e94798 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:120:17: note: op template: _49 = (sizetype) j_137;
Dense.c:120:17: note: 	stmt 0 _49 = (sizetype) j_137;
Dense.c:120:17: note: 	stmt 1 _209 = (sizetype) wi_136;
Dense.c:120:17: note: 	children 0x6e94898
Dense.c:120:17: note: node (external) 0x6e94898 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:120:17: note: 	{ j_137, wi_136 }
Dense.c:120:17: note: Cost model analysis: 
Dense.c:120:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:120:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:120:17: missed: not vectorized: vectorization is not profitable.
Dense.c:103:17: note: Costing subgraph: 
Dense.c:103:17: note: node 0x6e94998 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:103:17: note: op template: _24 = (sizetype) j_134;
Dense.c:103:17: note: 	stmt 0 _24 = (sizetype) j_134;
Dense.c:103:17: note: 	stmt 1 _229 = (sizetype) wi_133;
Dense.c:103:17: note: 	children 0x6e94a98
Dense.c:103:17: note: node (external) 0x6e94a98 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:103:17: note: 	{ j_134, wi_133 }
Dense.c:103:17: note: Cost model analysis: 
Dense.c:103:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:103:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:103:17: missed: not vectorized: vectorization is not profitable.
Dense.c:103:17: note: ***** The result for vector mode V32QI would be the same
Dense.c:103:17: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:103:17: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:103:17: note: SLPing BB part
Dense.c:120:17: note: Costing subgraph: 
Dense.c:120:17: note: node 0x6e94998 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:120:17: note: op template: _49 = (sizetype) j_137;
Dense.c:120:17: note: 	stmt 0 _49 = (sizetype) j_137;
Dense.c:120:17: note: 	stmt 1 _209 = (sizetype) wi_136;
Dense.c:120:17: note: 	children 0x6e94798
Dense.c:120:17: note: node (external) 0x6e94798 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:120:17: note: 	{ j_137, wi_136 }
Dense.c:120:17: note: Cost model analysis: 
Dense.c:120:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:120:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:120:17: missed: not vectorized: vectorization is not profitable.
Dense.c:103:17: note: Costing subgraph: 
Dense.c:103:17: note: node 0x6e94b18 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:103:17: note: op template: _24 = (sizetype) j_134;
Dense.c:103:17: note: 	stmt 0 _24 = (sizetype) j_134;
Dense.c:103:17: note: 	stmt 1 _229 = (sizetype) wi_133;
Dense.c:103:17: note: 	children 0x6e94a18
Dense.c:103:17: note: node (external) 0x6e94a18 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:103:17: note: 	{ j_134, wi_133 }
Dense.c:103:17: note: Cost model analysis: 
Dense.c:103:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:103:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:103:17: missed: not vectorized: vectorization is not profitable.
Dense.c:103:17: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:98:55: note: ***** Analysis failed with vector mode V8QI
Dense.c:98:55: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:98:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:58:27: missed: couldn't vectorize loop
Dense.c:58:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:61:20: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:905:10: missed: not vectorized: no vectype for stmt: _41 = MEM[(__m256_u * {ref-all})_6];
 scalar_type: __m256_u
Dense.c:61:20: note: ***** Analysis failed with vector mode VOID
Dense.c:54:13: note: vectorized 0 loops in function.
Dense.c:54:13: missed: splitting region at dont-vectorize loop 3 entry at bb18
Dense.c:63:14: note: ***** Analysis failed with vector mode V4DI
Dense.c:63:14: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:63:14: missed: splitting region at dominance boundary bb8
Dense.c:63:14: note: ***** Analysis failed with vector mode VOID
Dense.c:63:14: missed: splitting region at loop 1 exit at bb16
Dense.c:58:27: note: ***** Analysis failed with vector mode V8SF
Dense.c:58:27: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SF
Dense.c:58:27: missed: splitting region at dominance boundary bb10
Dense.c:58:27: note: ***** Analysis failed with vector mode VOID
Dense.c:74:2: missed: statement clobbers memory: _27 (pretmp_153, _61, 0);
Dense.c:75:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:75:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:160:2: missed: statement clobbers memory: free (_1);
Dense.c:161:2: missed: statement clobbers memory: free (_2);
Dense.c:162:2: missed: statement clobbers memory: free (_3);
Dense.c:163:2: missed: statement clobbers memory: free (_4);
Dense.c:164:2: missed: statement clobbers memory: free (dense_7);
Dense.c:165:2: missed: statement clobbers memory: PULSE_DestroyLayer (this_6(D));
Dense.c:166:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:166:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:180:19: missed: couldn't vectorize loop
Dense.c:180:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:170:13: note: vectorized 0 loops in function.
Dense.c:172:47: missed: statement clobbers memory: dense_35 = malloc (40);
Dense.c:174:36: missed: statement clobbers memory: _5 = aligned_alloc (64, _4);
Dense.c:175:38: missed: statement clobbers memory: _6 = aligned_alloc (64, _4);
Dense.c:176:36: missed: statement clobbers memory: _8 = aligned_alloc (64, _7);
Dense.c:177:35: missed: statement clobbers memory: _9 = aligned_alloc (64, _7);
Dense.c:178:36: missed: statement clobbers memory: _10 = aligned_alloc (64, _7);
Dense.c:181:39: missed: statement clobbers memory: _11 = rand ();
Dense.c:181:73: missed: statement clobbers memory: _63 = sqrt (_18);
Dense.c:193:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:194:4: missed: statement clobbers memory: exit (1);
Dense.c:190:4: missed: statement clobbers memory: layer = PULSE_CreateLayer (n_inputs.3_29, n_outputs.2_28, 1, activation_function_51(D), _SIMD_FeedDense, _SIMD_BackDense, _SIMD_FixDense, _DestroyDense, 1); [return slot optimization]
Dense.c:187:12: missed: statement clobbers memory: layer = PULSE_CreateLayer (n_inputs.1_27, n_outputs.0_26, 1, activation_function_51(D), _FeedDense, _BackDense, _FixDense, _DestroyDense, 0); [return slot optimization]
Dense.c:200:9: missed: not vectorized: more than one data ref in stmt: <retval> = layer;
Dense.c:200:9: note: ***** Analysis failed with vector mode V4DI
Dense.c:200:9: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _30 (pretmp_91, _50, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:20: missed: not vectorized: no vectype for stmt: _21 = *_20;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:145:19: missed: couldn't vectorize loop
Dense.c:145:19: missed: not vectorized: number of iterations cannot be computed.
Dense.c:136:19: missed: couldn't vectorize loop
Dense.c:136:19: missed: not vectorized: number of iterations cannot be computed.
Dense.c:129:13: note: vectorized 0 loops in function.
Dense.c:153:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:153:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:98:28: missed: couldn't vectorize loop
Dense.c:98:28: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:101:21: missed: couldn't vectorize loop
Dense.c:101:21: missed: not vectorized: number of iterations cannot be computed.
Dense.c:115:28: missed: couldn't vectorize loop
Dense.c:115:28: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:118:21: missed: couldn't vectorize loop
Dense.c:118:21: missed: not vectorized: number of iterations cannot be computed.
Dense.c:84:19: missed: couldn't vectorize loop
Dense.c:84:19: missed: not vectorized: number of iterations cannot be computed.
Dense.c:78:13: note: vectorized 0 loops in function.
Dense.c:81:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:103:17: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:103:17: note: SLPing BB part
Dense.c:120:17: note: Costing subgraph: 
Dense.c:120:17: note: node 0x857a0a8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:120:17: note: op template: _49 = (sizetype) j_137;
Dense.c:120:17: note: 	stmt 0 _49 = (sizetype) j_137;
Dense.c:120:17: note: 	stmt 1 _209 = (sizetype) wi_136;
Dense.c:120:17: note: 	children 0x857a1a8
Dense.c:120:17: note: node (external) 0x857a1a8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:120:17: note: 	{ j_137, wi_136 }
Dense.c:120:17: note: Cost model analysis: 
Dense.c:120:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:120:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:120:17: missed: not vectorized: vectorization is not profitable.
Dense.c:103:17: note: Costing subgraph: 
Dense.c:103:17: note: node 0x857a2a8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:103:17: note: op template: _24 = (sizetype) j_134;
Dense.c:103:17: note: 	stmt 0 _24 = (sizetype) j_134;
Dense.c:103:17: note: 	stmt 1 _229 = (sizetype) wi_133;
Dense.c:103:17: note: 	children 0x857a3a8
Dense.c:103:17: note: node (external) 0x857a3a8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:103:17: note: 	{ j_134, wi_133 }
Dense.c:103:17: note: Cost model analysis: 
Dense.c:103:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:103:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:103:17: missed: not vectorized: vectorization is not profitable.
Dense.c:103:17: note: ***** The result for vector mode V32QI would be the same
Dense.c:103:17: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:103:17: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:103:17: note: SLPing BB part
Dense.c:120:17: note: Costing subgraph: 
Dense.c:120:17: note: node 0x857a2a8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:120:17: note: op template: _49 = (sizetype) j_137;
Dense.c:120:17: note: 	stmt 0 _49 = (sizetype) j_137;
Dense.c:120:17: note: 	stmt 1 _209 = (sizetype) wi_136;
Dense.c:120:17: note: 	children 0x857a0a8
Dense.c:120:17: note: node (external) 0x857a0a8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:120:17: note: 	{ j_137, wi_136 }
Dense.c:120:17: note: Cost model analysis: 
Dense.c:120:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:120:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:120:17: missed: not vectorized: vectorization is not profitable.
Dense.c:103:17: note: Costing subgraph: 
Dense.c:103:17: note: node 0x857a428 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:103:17: note: op template: _24 = (sizetype) j_134;
Dense.c:103:17: note: 	stmt 0 _24 = (sizetype) j_134;
Dense.c:103:17: note: 	stmt 1 _229 = (sizetype) wi_133;
Dense.c:103:17: note: 	children 0x857a328
Dense.c:103:17: note: node (external) 0x857a328 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:103:17: note: 	{ j_134, wi_133 }
Dense.c:103:17: note: Cost model analysis: 
Dense.c:103:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:103:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:103:17: missed: not vectorized: vectorization is not profitable.
Dense.c:103:17: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:98:55: note: ***** Analysis failed with vector mode V8QI
Dense.c:98:55: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:98:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:58:27: optimized: loop vectorized using 32 byte vectors
Dense.c:58:27: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:58:27: optimized: loop vectorized using 16 byte vectors
Dense.c:54:13: note: vectorized 1 loops in function.
Dense.c:74:2: missed: statement clobbers memory: _11 (pretmp_42, _31, 0);
Dense.c:54:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:54:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:160:2: missed: statement clobbers memory: free (_1);
Dense.c:161:2: missed: statement clobbers memory: free (_2);
Dense.c:162:2: missed: statement clobbers memory: free (_3);
Dense.c:163:2: missed: statement clobbers memory: free (_4);
Dense.c:164:2: missed: statement clobbers memory: free (dense_7);
Dense.c:165:2: missed: statement clobbers memory: PULSE_DestroyLayer (this_6(D));
Dense.c:166:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:166:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:180:19: missed: couldn't vectorize loop
Dense.c:180:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:170:13: note: vectorized 0 loops in function.
Dense.c:172:47: missed: statement clobbers memory: dense_35 = malloc (40);
Dense.c:174:36: missed: statement clobbers memory: _5 = aligned_alloc (64, _4);
Dense.c:175:38: missed: statement clobbers memory: _6 = aligned_alloc (64, _4);
Dense.c:176:36: missed: statement clobbers memory: _8 = aligned_alloc (64, _7);
Dense.c:177:35: missed: statement clobbers memory: _9 = aligned_alloc (64, _7);
Dense.c:178:36: missed: statement clobbers memory: _10 = aligned_alloc (64, _7);
Dense.c:181:39: missed: statement clobbers memory: _11 = rand ();
Dense.c:181:73: missed: statement clobbers memory: _63 = sqrt (_18);
Dense.c:193:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:194:4: missed: statement clobbers memory: exit (1);
Dense.c:190:4: missed: statement clobbers memory: layer = PULSE_CreateLayer (n_inputs.3_29, n_outputs.2_28, 1, activation_function_51(D), _SIMD_FeedDense, _SIMD_BackDense, _SIMD_FixDense, _DestroyDense, 1); [return slot optimization]
Dense.c:187:12: missed: statement clobbers memory: layer = PULSE_CreateLayer (n_inputs.1_27, n_outputs.0_26, 1, activation_function_51(D), _FeedDense, _BackDense, _FixDense, _DestroyDense, 0); [return slot optimization]
Dense.c:200:9: missed: not vectorized: more than one data ref in stmt: <retval> = layer;
Dense.c:200:9: note: ***** Analysis failed with vector mode V4DI
Dense.c:200:9: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Layer.c:7:34: missed: statement clobbers memory: _3 = aligned_alloc (64, _2);
Layer.c:8:35: missed: statement clobbers memory: _6 = aligned_alloc (64, _5);
Layer.c:9:34: missed: statement clobbers memory: _7 = aligned_alloc (64, _5);
Layer.c:14:19: missed: statement clobbers memory: _8 = PULSE_GetActivationFunctionPtr (activation_function_18(D));
Layer.c:19:9: note: ***** Analysis succeeded with vector mode V8SI
Layer.c:19:9: note: SLPing BB part
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x4a547c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: op template: MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: 	stmt 0 MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: 	stmt 1 MEM <unsigned int> [(struct PULSE_Layer *)&<retval> + 4B] = optimization_type_17(D);
Layer.c:19:9: note: 	children 0x4a54848
Layer.c:19:9: note: node (external) 0x4a54848 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ type_16(D), optimization_type_17(D) }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x4a548c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: op template: <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: 	stmt 0 <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: 	stmt 1 <retval>.n_outputs = n_outputs_12(D);
Layer.c:19:9: note: 	children 0x4a54948
Layer.c:19:9: note: node (external) 0x4a54948 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ n_inputs_11(D), n_outputs_12(D) }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x4a54a48 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: <retval>.inputs = _3;
Layer.c:19:9: note: 	stmt 0 <retval>.inputs = _3;
Layer.c:19:9: note: 	stmt 1 <retval>.outputs = _6;
Layer.c:19:9: note: 	children 0x4a54ac8
Layer.c:19:9: note: node (external) 0x4a54ac8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: 	stmt 0 _3 = aligned_alloc (64, _2);
Layer.c:19:9: note: 	stmt 1 _6 = aligned_alloc (64, _5);
Layer.c:19:9: note: 	children 0x4a54b48 0x4a54bc8
Layer.c:19:9: note: node (constant) 0x4a54b48 (max_nunits=1, refcnt=1)
Layer.c:19:9: note: 	{ 64, 64 }
Layer.c:19:9: note: node 0x4a54bc8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: patt_40 = n_inputs_11(D) w* 4;
Layer.c:19:9: note: 	stmt 0 patt_40 = n_inputs_11(D) w* 4;
Layer.c:19:9: note: 	stmt 1 patt_41 = n_outputs_12(D) w* 4;
Layer.c:19:9: note: 	children 0x4a54c48 0x4a54cc8
Layer.c:19:9: note: node (external) 0x4a54c48 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ n_inputs_11(D), n_outputs_12(D) }
Layer.c:19:9: note: node (constant) 0x4a54cc8 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ 4, 4 }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x4a54dc8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Layer.c:19:9: note: op template: <retval>.feed = feed_20(D);
Layer.c:19:9: note: 	stmt 0 <retval>.feed = feed_20(D);
Layer.c:19:9: note: 	stmt 1 <retval>.back = back_21(D);
Layer.c:19:9: note: 	stmt 2 <retval>.fix = fix_22(D);
Layer.c:19:9: note: 	stmt 3 <retval>.destroy = destroy_23(D);
Layer.c:19:9: note: 	children 0x4a54e48
Layer.c:19:9: note: node (external) 0x4a54e48 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Layer.c:19:9: note: 	{ feed_20(D), back_21(D), fix_22(D), destroy_23(D) }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 64
  Scalar cost: 64
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x4a54f48 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: <retval>.parent = 0B;
Layer.c:19:9: note: 	stmt 0 <retval>.parent = 0B;
Layer.c:19:9: note: 	stmt 1 <retval>.child = 0B;
Layer.c:19:9: note: 	children 0x4a54fc8
Layer.c:19:9: note: node (constant) 0x4a54fc8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: 	{ 0B, 0B }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Layer.c:19:9: note: Basic block will be vectorized using SLP
Layer.c:19:9: optimized: basic block part vectorized using 8 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x4a547c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: op template: MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: 	stmt 0 MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: 	stmt 1 MEM <unsigned int> [(struct PULSE_Layer *)&<retval> + 4B] = optimization_type_17(D);
Layer.c:19:9: note: 	children 0x4a54848
Layer.c:19:9: note: node (external) 0x4a54848 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ type_16(D), optimization_type_17(D) }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: vect_is_simple_use: operand optimization_type_17(D), type of def: external
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: MEM <unsigned int> [(struct PULSE_Layer *)&<retval>]
Layer.c:19:9: note: created &MEM <unsigned int> [(struct PULSE_Layer *)&<retval>]
Layer.c:19:9: note: add new stmt: MEM <vector(2) unsigned int> [(struct PULSE_Layer *)&<retval>] = _42;
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: optimized: basic block part vectorized using 8 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x4a548c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: op template: <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: 	stmt 0 <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: 	stmt 1 <retval>.n_outputs = n_outputs_12(D);
Layer.c:19:9: note: 	children 0x4a54948
Layer.c:19:9: note: node (external) 0x4a54948 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ n_inputs_11(D), n_outputs_12(D) }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: vect_is_simple_use: operand n_outputs_12(D), type of def: external
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: <retval>.n_inputs
Layer.c:19:9: note: created &<retval>.n_inputs
Layer.c:19:9: note: add new stmt: MEM <vector(2) unsigned int> [(unsigned int *)&<retval> + 96B] = _44;
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: optimized: basic block part vectorized using 16 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x4a54a48 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: <retval>.inputs = _3;
Layer.c:19:9: note: 	stmt 0 <retval>.inputs = _3;
Layer.c:19:9: note: 	stmt 1 <retval>.outputs = _6;
Layer.c:19:9: note: 	children 0x4a54ac8
Layer.c:19:9: note: node (external) 0x4a54ac8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: 	stmt 0 _3 = aligned_alloc (64, _2);
Layer.c:19:9: note: 	stmt 1 _6 = aligned_alloc (64, _5);
Layer.c:19:9: note: 	children 0x4a54b48 0x4a54bc8
Layer.c:19:9: note: node (constant) 0x4a54b48 (max_nunits=1, refcnt=1)
Layer.c:19:9: note: 	{ 64, 64 }
Layer.c:19:9: note: node 0x4a54bc8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: patt_40 = n_inputs_11(D) w* 4;
Layer.c:19:9: note: 	stmt 0 patt_40 = n_inputs_11(D) w* 4;
Layer.c:19:9: note: 	stmt 1 patt_41 = n_outputs_12(D) w* 4;
Layer.c:19:9: note: 	children 0x4a54c48 0x4a54cc8
Layer.c:19:9: note: node (external) 0x4a54c48 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ n_inputs_11(D), n_outputs_12(D) }
Layer.c:19:9: note: node (constant) 0x4a54cc8 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ 4, 4 }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: <retval>.inputs = _3;
Layer.c:19:9: note: vect_is_simple_use: operand aligned_alloc (64, _5), type of def: internal
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.inputs
Layer.c:19:9: note: created &<retval>.inputs
Layer.c:19:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval> + 8B] = _48;
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: optimized: basic block part vectorized using 32 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x4a54dc8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Layer.c:19:9: note: op template: <retval>.feed = feed_20(D);
Layer.c:19:9: note: 	stmt 0 <retval>.feed = feed_20(D);
Layer.c:19:9: note: 	stmt 1 <retval>.back = back_21(D);
Layer.c:19:9: note: 	stmt 2 <retval>.fix = fix_22(D);
Layer.c:19:9: note: 	stmt 3 <retval>.destroy = destroy_23(D);
Layer.c:19:9: note: 	children 0x4a54e48
Layer.c:19:9: note: node (external) 0x4a54e48 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Layer.c:19:9: note: 	{ feed_20(D), back_21(D), fix_22(D), destroy_23(D) }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: <retval>.feed = feed_20(D);
Layer.c:19:9: note: vect_is_simple_use: operand back_21(D), type of def: external
Layer.c:19:9: note: vect_is_simple_use: operand fix_22(D), type of def: external
Layer.c:19:9: note: vect_is_simple_use: operand destroy_23(D), type of def: external
Layer.c:19:9: note: conflicting alias set types.
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(4) long unsigned int  vectorizing a pointer ref: <retval>.feed
Layer.c:19:9: note: created &<retval>.feed
Layer.c:19:9: note: add new stmt: MEM <vector(4) long unsigned int> [(void *)&<retval> + 32B] = _54;
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: optimized: basic block part vectorized using 16 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x4a54f48 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: <retval>.parent = 0B;
Layer.c:19:9: note: 	stmt 0 <retval>.parent = 0B;
Layer.c:19:9: note: 	stmt 1 <retval>.child = 0B;
Layer.c:19:9: note: 	children 0x4a54fc8
Layer.c:19:9: note: node (constant) 0x4a54fc8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: 	{ 0B, 0B }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: <retval>.parent = 0B;
Layer.c:19:9: note: vect_is_simple_use: operand 0B, type of def: constant
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.parent
Layer.c:19:9: note: created &<retval>.parent
Layer.c:19:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer * *)&<retval> + 72B] = { 0, 0 };
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: note: ***** The result for vector mode V32QI would be the same
Layer.c:24:2: missed: statement clobbers memory: free (_1);
Layer.c:25:2: missed: statement clobbers memory: free (_2);
Layer.c:26:2: missed: statement clobbers memory: free (_3);
Layer.c:27:1: note: ***** Analysis failed with vector mode V4DI
Layer.c:27:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _30 (pretmp_91, _50, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:20: missed: not vectorized: no vectype for stmt: _21 = *_20;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:54:13: note: vectorized 0 loops in function.
Dense.c:74:2: missed: statement clobbers memory: _3 (_4, _11, 0);
Dense.c:75:1: note: ***** Analysis failed with vector mode V8SI
Dense.c:75:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
Dense.c:145:19: missed: couldn't vectorize loop
Dense.c:145:19: missed: not vectorized: number of iterations cannot be computed.
Dense.c:136:19: missed: couldn't vectorize loop
Dense.c:136:19: missed: not vectorized: number of iterations cannot be computed.
Dense.c:129:13: note: vectorized 0 loops in function.
Dense.c:153:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:153:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:98:28: missed: couldn't vectorize loop
Dense.c:98:28: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:101:21: missed: couldn't vectorize loop
Dense.c:101:21: missed: not vectorized: number of iterations cannot be computed.
Dense.c:115:28: missed: couldn't vectorize loop
Dense.c:115:28: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:118:21: missed: couldn't vectorize loop
Dense.c:118:21: missed: not vectorized: number of iterations cannot be computed.
Dense.c:84:19: missed: couldn't vectorize loop
Dense.c:84:19: missed: not vectorized: number of iterations cannot be computed.
Dense.c:78:13: note: vectorized 0 loops in function.
Dense.c:81:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:103:17: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:103:17: note: SLPing BB part
Dense.c:120:17: note: Costing subgraph: 
Dense.c:120:17: note: node 0x873b5e8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:120:17: note: op template: _49 = (sizetype) j_137;
Dense.c:120:17: note: 	stmt 0 _49 = (sizetype) j_137;
Dense.c:120:17: note: 	stmt 1 _209 = (sizetype) wi_136;
Dense.c:120:17: note: 	children 0x873b6e8
Dense.c:120:17: note: node (external) 0x873b6e8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:120:17: note: 	{ j_137, wi_136 }
Dense.c:120:17: note: Cost model analysis: 
Dense.c:120:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:120:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:120:17: missed: not vectorized: vectorization is not profitable.
Dense.c:103:17: note: Costing subgraph: 
Dense.c:103:17: note: node 0x873b7e8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:103:17: note: op template: _24 = (sizetype) j_134;
Dense.c:103:17: note: 	stmt 0 _24 = (sizetype) j_134;
Dense.c:103:17: note: 	stmt 1 _229 = (sizetype) wi_133;
Dense.c:103:17: note: 	children 0x873b8e8
Dense.c:103:17: note: node (external) 0x873b8e8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:103:17: note: 	{ j_134, wi_133 }
Dense.c:103:17: note: Cost model analysis: 
Dense.c:103:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:103:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:103:17: missed: not vectorized: vectorization is not profitable.
Dense.c:103:17: note: ***** The result for vector mode V32QI would be the same
Dense.c:103:17: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:103:17: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:103:17: note: SLPing BB part
Dense.c:120:17: note: Costing subgraph: 
Dense.c:120:17: note: node 0x873b7e8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:120:17: note: op template: _49 = (sizetype) j_137;
Dense.c:120:17: note: 	stmt 0 _49 = (sizetype) j_137;
Dense.c:120:17: note: 	stmt 1 _209 = (sizetype) wi_136;
Dense.c:120:17: note: 	children 0x873b5e8
Dense.c:120:17: note: node (external) 0x873b5e8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:120:17: note: 	{ j_137, wi_136 }
Dense.c:120:17: note: Cost model analysis: 
Dense.c:120:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:120:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:120:17: missed: not vectorized: vectorization is not profitable.
Dense.c:103:17: note: Costing subgraph: 
Dense.c:103:17: note: node 0x873b968 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:103:17: note: op template: _24 = (sizetype) j_134;
Dense.c:103:17: note: 	stmt 0 _24 = (sizetype) j_134;
Dense.c:103:17: note: 	stmt 1 _229 = (sizetype) wi_133;
Dense.c:103:17: note: 	children 0x873b868
Dense.c:103:17: note: node (external) 0x873b868 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:103:17: note: 	{ j_134, wi_133 }
Dense.c:103:17: note: Cost model analysis: 
Dense.c:103:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:103:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:103:17: missed: not vectorized: vectorization is not profitable.
Dense.c:103:17: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:98:55: note: ***** Analysis failed with vector mode V8QI
Dense.c:98:55: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:98:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:160:2: missed: statement clobbers memory: free (_1);
Dense.c:161:2: missed: statement clobbers memory: free (_2);
Dense.c:162:2: missed: statement clobbers memory: free (_3);
Dense.c:163:2: missed: statement clobbers memory: free (_4);
Dense.c:164:2: missed: statement clobbers memory: free (dense_7);
Dense.c:165:2: missed: statement clobbers memory: PULSE_DestroyLayer (this_6(D));
Dense.c:166:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:166:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:180:19: missed: couldn't vectorize loop
Dense.c:180:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:170:13: note: vectorized 0 loops in function.
Dense.c:172:47: missed: statement clobbers memory: dense_35 = malloc (40);
Dense.c:174:36: missed: statement clobbers memory: _5 = aligned_alloc (64, _4);
Dense.c:175:38: missed: statement clobbers memory: _6 = aligned_alloc (64, _4);
Dense.c:176:36: missed: statement clobbers memory: _8 = aligned_alloc (64, _7);
Dense.c:177:35: missed: statement clobbers memory: _9 = aligned_alloc (64, _7);
Dense.c:178:36: missed: statement clobbers memory: _10 = aligned_alloc (64, _7);
Dense.c:181:39: missed: statement clobbers memory: _11 = rand ();
Dense.c:181:73: missed: statement clobbers memory: _63 = sqrt (_18);
Dense.c:193:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:194:4: missed: statement clobbers memory: exit (1);
Dense.c:190:4: missed: statement clobbers memory: layer = PULSE_CreateLayer (n_inputs.3_29, n_outputs.2_28, 1, activation_function_51(D), _SIMD_FeedDense, _SIMD_BackDense, _SIMD_FixDense, _DestroyDense, 1); [return slot optimization]
Dense.c:187:12: missed: statement clobbers memory: layer = PULSE_CreateLayer (n_inputs.1_27, n_outputs.0_26, 1, activation_function_51(D), _FeedDense, _BackDense, _FixDense, _DestroyDense, 0); [return slot optimization]
Dense.c:200:9: missed: not vectorized: more than one data ref in stmt: <retval> = layer;
Dense.c:200:9: note: ***** Analysis failed with vector mode V4DI
Dense.c:200:9: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Activations.c:17:19: optimized: loop vectorized using 32 byte vectors
Activations.c:17:19: optimized: loop vectorized using 16 byte vectors
Activations.c:17:19: optimized: loop vectorized using 32 byte vectors
Activations.c:17:19: optimized: loop vectorized using 16 byte vectors
Activations.c:14:13: note: vectorized 2 loops in function.
Activations.c:14:13: note: ***** Analysis failed with vector mode V8SF
Activations.c:14:13: note: ***** The result for vector mode V32QI would be the same
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V16QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V16QI
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V8QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V8QI
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V4QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V4QI
Activations.c:28:19: missed: couldn't vectorize loop
Activations.c:28:19: missed: not vectorized: unsupported control flow in loop.
Activations.c:28:19: optimized: loop vectorized using 32 byte vectors
Activations.c:28:19: optimized: loop vectorized using 16 byte vectors
Activations.c:25:13: note: vectorized 1 loops in function.
Activations.c:25:13: note: ***** Analysis failed with vector mode V8SF
Activations.c:25:13: note: ***** The result for vector mode V32QI would be the same
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V16QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V16QI
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V8QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V8QI
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V4QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V4QI
Activations.c:38:1: note: ***** Analysis failed with vector mode VOID
Activations.c:5:19: missed: couldn't vectorize loop
Activations.c:9:21: missed: statement clobbers memory: _36 = expf (_18);
Activations.c:5:19: missed: couldn't vectorize loop
Activations.c:9:21: missed: statement clobbers memory: _12 = expf (_11);
Activations.c:3:13: note: vectorized 0 loops in function.
Activations.c:9:21: missed: statement clobbers memory: _12 = expf (_11);
Activations.c:9:21: missed: statement clobbers memory: _36 = expf (_18);
Activations.c:11:1: note: ***** Analysis failed with vector mode V8SF
Activations.c:11:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SF
Activations.c:44:2: note: ***** Analysis failed with vector mode VOID
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:4:18: note: vectorized 0 loops in function.
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_4, inputs_15, _3);
PULSE.c:8:2: missed: statement clobbers memory: _5 (layer_17);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_12, pretmp_26, _9);
PULSE.c:13:28: note: ***** Analysis failed with vector mode V8SI
PULSE.c:13:28: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:22:2: missed: statement clobbers memory: _1 (layer_10(D));
PULSE.c:22:2: missed: statement clobbers memory: _14 (_2);
PULSE.c:22:2: missed: statement clobbers memory: _21 (_15);
PULSE.c:22:2: missed: statement clobbers memory: _28 (_22);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_29);
PULSE.c:26:3: missed: statement clobbers memory: memset (_34, 0, _32);
PULSE.c:26:3: missed: statement clobbers memory: memset (_27, 0, _25);
PULSE.c:26:3: missed: statement clobbers memory: memset (_20, 0, _18);
PULSE.c:26:3: missed: statement clobbers memory: memset (_7, 0, _5);
PULSE.c:28:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:28:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:32:6: note: vectorized 1 loops in function.
PULSE.c:34:8: missed: statement clobbers memory: _1 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_2);
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _9 = rand ();
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8SI
PULSE.c:32:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:61:16: missed: couldn't vectorize loop
PULSE.c:61:16: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:64:17: missed: couldn't vectorize loop
PULSE.c:64:17: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:73:19: missed: couldn't vectorize loop
PULSE.c:73:19: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _64 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:53:22: missed: couldn't vectorize loop
PULSE.c:53:22: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:47:6: note: vectorized 1 loops in function.
PULSE.c:48:1: missed: statement clobbers memory: saved_stack.7_35 = __builtin_stack_save ();
PULSE.c:49:46: missed: statement clobbers memory: _1 = PULSE_GetLossFunctionPtr (loss_function_36(D));
PULSE.c:60:10: missed: statement clobbers memory: random.4_42 = __builtin_alloca_with_align (_4, 32);
PULSE.c:34:8: missed: statement clobbers memory: _56 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_57);
PULSE.c:39:29: missed: statement clobbers memory: _64 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _67 = rand ();
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_93, inputs_88, _92);
PULSE.c:8:2: missed: statement clobbers memory: _94 (layer_89);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_100, _99, _98);
PULSE.c:67:11: missed: statement clobbers memory: loss_51 = PULSE_GetLoss_38 (_17, _16, _12, _11);
PULSE.c:22:2: missed: statement clobbers memory: _81 (output_30);
PULSE.c:22:2: missed: statement clobbers memory: _103 (_82);
PULSE.c:22:2: missed: statement clobbers memory: _110 (_104);
PULSE.c:22:2: missed: statement clobbers memory: _117 (_111);
PULSE.c:22:2: missed: statement clobbers memory: _124 (_118);
PULSE.c:22:2: missed: statement clobbers memory: _131 (_125);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_132);
PULSE.c:26:3: missed: statement clobbers memory: memset (_137, 0, _135);
PULSE.c:26:3: missed: statement clobbers memory: memset (_130, 0, _128);
PULSE.c:26:3: missed: statement clobbers memory: memset (_123, 0, _121);
PULSE.c:26:3: missed: statement clobbers memory: memset (_116, 0, _114);
PULSE.c:26:3: missed: statement clobbers memory: memset (_109, 0, _107);
PULSE.c:26:3: missed: statement clobbers memory: memset (_87, 0, _85);
PULSE.c:75:6: missed: statement clobbers memory: _21 (current_183, args);
PULSE.c:80:4: missed: statement clobbers memory: printf ("Epoch: %d Item: %d Loss: %.10f\r", i_181, j_182, _22);
PULSE.c:47:6: missed: statement clobbers memory: __builtin_stack_restore (saved_stack.7_35);
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4DI
PULSE.c:47:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:89:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:89:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:94:8: missed: couldn't vectorize loop
PULSE.c:94:8: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:91:6: note: vectorized 0 loops in function.
PULSE.c:96:3: missed: statement clobbers memory: _1 (current_10);
PULSE.c:99:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:99:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Loss.c:15:19: optimized: loop vectorized using 32 byte vectors
Loss.c:15:19: optimized:  loop versioned for vectorization because of possible aliasing
Loss.c:15:19: optimized: loop vectorized using 16 byte vectors
Loss.c:12:23: note: vectorized 1 loops in function.
Loss.c:12:23: note: ***** Analysis failed with vector mode V8SF
Loss.c:12:23: note: ***** The result for vector mode V32QI would be the same
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V16QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V16QI
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V8QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V8QI
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V4QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V4QI
Loss.c:7:19: missed: couldn't vectorize loop
Loss.c:8:38: missed: not vectorized: unsupported use in stmt.
Loss.c:4:23: note: vectorized 0 loops in function.
Loss.c:9:13: note: ***** Analysis failed with vector mode V8SF
Loss.c:9:13: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SF
Loss.c:23:2: note: ***** Analysis failed with vector mode VOID
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _30 (pretmp_91, _50, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:20: missed: not vectorized: no vectype for stmt: _21 = *_20;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:145:19: missed: couldn't vectorize loop
Dense.c:145:19: missed: not vectorized: number of iterations cannot be computed.
Dense.c:136:19: missed: couldn't vectorize loop
Dense.c:136:19: missed: not vectorized: number of iterations cannot be computed.
Dense.c:129:13: note: vectorized 0 loops in function.
Dense.c:153:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:153:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:98:28: missed: couldn't vectorize loop
Dense.c:98:28: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:101:21: missed: couldn't vectorize loop
Dense.c:101:21: missed: not vectorized: number of iterations cannot be computed.
Dense.c:115:28: missed: couldn't vectorize loop
Dense.c:115:28: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:118:21: missed: couldn't vectorize loop
Dense.c:118:21: missed: not vectorized: number of iterations cannot be computed.
Dense.c:84:19: missed: couldn't vectorize loop
Dense.c:84:19: missed: not vectorized: number of iterations cannot be computed.
Dense.c:78:13: note: vectorized 0 loops in function.
Dense.c:81:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:103:17: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:103:17: note: SLPing BB part
Dense.c:120:17: note: Costing subgraph: 
Dense.c:120:17: note: node 0x7df9388 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:120:17: note: op template: _49 = (sizetype) j_137;
Dense.c:120:17: note: 	stmt 0 _49 = (sizetype) j_137;
Dense.c:120:17: note: 	stmt 1 _209 = (sizetype) wi_136;
Dense.c:120:17: note: 	children 0x7df9488
Dense.c:120:17: note: node (external) 0x7df9488 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:120:17: note: 	{ j_137, wi_136 }
Dense.c:120:17: note: Cost model analysis: 
Dense.c:120:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:120:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:120:17: missed: not vectorized: vectorization is not profitable.
Dense.c:103:17: note: Costing subgraph: 
Dense.c:103:17: note: node 0x7df9588 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:103:17: note: op template: _24 = (sizetype) j_134;
Dense.c:103:17: note: 	stmt 0 _24 = (sizetype) j_134;
Dense.c:103:17: note: 	stmt 1 _229 = (sizetype) wi_133;
Dense.c:103:17: note: 	children 0x7df9688
Dense.c:103:17: note: node (external) 0x7df9688 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:103:17: note: 	{ j_134, wi_133 }
Dense.c:103:17: note: Cost model analysis: 
Dense.c:103:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:103:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:103:17: missed: not vectorized: vectorization is not profitable.
Dense.c:103:17: note: ***** The result for vector mode V32QI would be the same
Dense.c:103:17: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:103:17: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:103:17: note: SLPing BB part
Dense.c:120:17: note: Costing subgraph: 
Dense.c:120:17: note: node 0x7df9588 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:120:17: note: op template: _49 = (sizetype) j_137;
Dense.c:120:17: note: 	stmt 0 _49 = (sizetype) j_137;
Dense.c:120:17: note: 	stmt 1 _209 = (sizetype) wi_136;
Dense.c:120:17: note: 	children 0x7df9388
Dense.c:120:17: note: node (external) 0x7df9388 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:120:17: note: 	{ j_137, wi_136 }
Dense.c:120:17: note: Cost model analysis: 
Dense.c:120:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:120:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:120:17: missed: not vectorized: vectorization is not profitable.
Dense.c:103:17: note: Costing subgraph: 
Dense.c:103:17: note: node 0x7df9708 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:103:17: note: op template: _24 = (sizetype) j_134;
Dense.c:103:17: note: 	stmt 0 _24 = (sizetype) j_134;
Dense.c:103:17: note: 	stmt 1 _229 = (sizetype) wi_133;
Dense.c:103:17: note: 	children 0x7df9608
Dense.c:103:17: note: node (external) 0x7df9608 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:103:17: note: 	{ j_134, wi_133 }
Dense.c:103:17: note: Cost model analysis: 
Dense.c:103:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:103:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:103:17: missed: not vectorized: vectorization is not profitable.
Dense.c:103:17: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:98:55: note: ***** Analysis failed with vector mode V8QI
Dense.c:98:55: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:98:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:58:27: missed: couldn't vectorize loop
Dense.c:58:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:61:20: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:905:10: missed: not vectorized: no vectype for stmt: _41 = MEM[(__m256_u * {ref-all})_6];
 scalar_type: __m256_u
Dense.c:61:20: note: ***** Analysis failed with vector mode VOID
Dense.c:54:13: note: vectorized 0 loops in function.
Dense.c:54:13: missed: splitting region at dont-vectorize loop 3 entry at bb18
Dense.c:63:14: note: ***** Analysis failed with vector mode V4DI
Dense.c:63:14: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:63:14: missed: splitting region at dominance boundary bb8
Dense.c:63:14: note: ***** Analysis failed with vector mode VOID
Dense.c:63:14: missed: splitting region at loop 1 exit at bb16
Dense.c:58:27: note: ***** Analysis failed with vector mode V8SF
Dense.c:58:27: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SF
Dense.c:58:27: missed: splitting region at dominance boundary bb10
Dense.c:58:27: note: ***** Analysis failed with vector mode VOID
Dense.c:74:2: missed: statement clobbers memory: _27 (pretmp_153, _61, 0);
Dense.c:75:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:75:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:160:2: missed: statement clobbers memory: free (_1);
Dense.c:161:2: missed: statement clobbers memory: free (_2);
Dense.c:162:2: missed: statement clobbers memory: free (_3);
Dense.c:163:2: missed: statement clobbers memory: free (_4);
Dense.c:164:2: missed: statement clobbers memory: free (dense_7);
Dense.c:165:2: missed: statement clobbers memory: PULSE_DestroyLayer (this_6(D));
Dense.c:166:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:166:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:180:19: missed: couldn't vectorize loop
Dense.c:180:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:170:13: note: vectorized 0 loops in function.
Dense.c:172:47: missed: statement clobbers memory: dense_35 = malloc (40);
Dense.c:174:36: missed: statement clobbers memory: _5 = aligned_alloc (64, _4);
Dense.c:175:38: missed: statement clobbers memory: _6 = aligned_alloc (64, _4);
Dense.c:176:36: missed: statement clobbers memory: _8 = aligned_alloc (64, _7);
Dense.c:177:35: missed: statement clobbers memory: _9 = aligned_alloc (64, _7);
Dense.c:178:36: missed: statement clobbers memory: _10 = aligned_alloc (64, _7);
Dense.c:181:39: missed: statement clobbers memory: _11 = rand ();
Dense.c:181:73: missed: statement clobbers memory: _63 = sqrt (_18);
Dense.c:193:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:194:4: missed: statement clobbers memory: exit (1);
Dense.c:190:4: missed: statement clobbers memory: layer = PULSE_CreateLayer (n_inputs.3_29, n_outputs.2_28, 1, activation_function_51(D), _SIMD_FeedDense, _SIMD_BackDense, _SIMD_FixDense, _DestroyDense, 1); [return slot optimization]
Dense.c:187:12: missed: statement clobbers memory: layer = PULSE_CreateLayer (n_inputs.1_27, n_outputs.0_26, 1, activation_function_51(D), _FeedDense, _BackDense, _FixDense, _DestroyDense, 0); [return slot optimization]
Dense.c:200:9: missed: not vectorized: more than one data ref in stmt: <retval> = layer;
Dense.c:200:9: note: ***** Analysis failed with vector mode V4DI
Dense.c:200:9: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _30 (pretmp_91, _50, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:20: missed: not vectorized: no vectype for stmt: _21 = *_20;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:147:19: missed: couldn't vectorize loop
Dense.c:147:19: missed: not vectorized: number of iterations cannot be computed.
Dense.c:138:19: missed: couldn't vectorize loop
Dense.c:138:19: missed: not vectorized: number of iterations cannot be computed.
Dense.c:131:13: note: vectorized 0 loops in function.
Dense.c:155:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:155:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:100:28: missed: couldn't vectorize loop
Dense.c:100:28: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:103:21: missed: couldn't vectorize loop
Dense.c:103:21: missed: not vectorized: number of iterations cannot be computed.
Dense.c:117:28: missed: couldn't vectorize loop
Dense.c:117:28: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:120:21: missed: couldn't vectorize loop
Dense.c:120:21: missed: not vectorized: number of iterations cannot be computed.
Dense.c:86:19: missed: couldn't vectorize loop
Dense.c:86:19: missed: not vectorized: number of iterations cannot be computed.
Dense.c:80:13: note: vectorized 0 loops in function.
Dense.c:83:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:105:17: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:105:17: note: SLPing BB part
Dense.c:122:17: note: Costing subgraph: 
Dense.c:122:17: note: node 0x767bc18 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:122:17: note: op template: _49 = (sizetype) j_137;
Dense.c:122:17: note: 	stmt 0 _49 = (sizetype) j_137;
Dense.c:122:17: note: 	stmt 1 _209 = (sizetype) wi_136;
Dense.c:122:17: note: 	children 0x767bd18
Dense.c:122:17: note: node (external) 0x767bd18 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:122:17: note: 	{ j_137, wi_136 }
Dense.c:122:17: note: Cost model analysis: 
Dense.c:122:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:122:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:122:17: missed: not vectorized: vectorization is not profitable.
Dense.c:105:17: note: Costing subgraph: 
Dense.c:105:17: note: node 0x767be18 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:105:17: note: op template: _24 = (sizetype) j_134;
Dense.c:105:17: note: 	stmt 0 _24 = (sizetype) j_134;
Dense.c:105:17: note: 	stmt 1 _229 = (sizetype) wi_133;
Dense.c:105:17: note: 	children 0x767bf18
Dense.c:105:17: note: node (external) 0x767bf18 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:105:17: note: 	{ j_134, wi_133 }
Dense.c:105:17: note: Cost model analysis: 
Dense.c:105:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:105:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:105:17: missed: not vectorized: vectorization is not profitable.
Dense.c:105:17: note: ***** The result for vector mode V32QI would be the same
Dense.c:105:17: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:105:17: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:105:17: note: SLPing BB part
Dense.c:122:17: note: Costing subgraph: 
Dense.c:122:17: note: node 0x767be18 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:122:17: note: op template: _49 = (sizetype) j_137;
Dense.c:122:17: note: 	stmt 0 _49 = (sizetype) j_137;
Dense.c:122:17: note: 	stmt 1 _209 = (sizetype) wi_136;
Dense.c:122:17: note: 	children 0x767bc18
Dense.c:122:17: note: node (external) 0x767bc18 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:122:17: note: 	{ j_137, wi_136 }
Dense.c:122:17: note: Cost model analysis: 
Dense.c:122:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:122:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:122:17: missed: not vectorized: vectorization is not profitable.
Dense.c:105:17: note: Costing subgraph: 
Dense.c:105:17: note: node 0x767bf98 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:105:17: note: op template: _24 = (sizetype) j_134;
Dense.c:105:17: note: 	stmt 0 _24 = (sizetype) j_134;
Dense.c:105:17: note: 	stmt 1 _229 = (sizetype) wi_133;
Dense.c:105:17: note: 	children 0x767be98
Dense.c:105:17: note: node (external) 0x767be98 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:105:17: note: 	{ j_134, wi_133 }
Dense.c:105:17: note: Cost model analysis: 
Dense.c:105:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:105:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:105:17: missed: not vectorized: vectorization is not profitable.
Dense.c:105:17: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:100:55: note: ***** Analysis failed with vector mode V8QI
Dense.c:100:55: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:100:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:59:27: missed: couldn't vectorize loop
Dense.c:72:19: missed: not vectorized: no vectype for stmt: _200 = dense_41->weights;
 scalar_type: float *
Dense.c:62:7: missed: couldn't vectorize loop
Dense.c:62:7: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:54:13: note: vectorized 0 loops in function.
Dense.c:76:2: missed: statement clobbers memory: _31 (prephitmp_215, _166, 0);
Dense.c:77:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:77:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:162:2: missed: statement clobbers memory: free (_1);
Dense.c:163:2: missed: statement clobbers memory: free (_2);
Dense.c:164:2: missed: statement clobbers memory: free (_3);
Dense.c:165:2: missed: statement clobbers memory: free (_4);
Dense.c:166:2: missed: statement clobbers memory: free (dense_7);
Dense.c:167:2: missed: statement clobbers memory: PULSE_DestroyLayer (this_6(D));
Dense.c:168:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:168:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:182:19: missed: couldn't vectorize loop
Dense.c:182:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:172:13: note: vectorized 0 loops in function.
Dense.c:174:47: missed: statement clobbers memory: dense_35 = malloc (40);
Dense.c:176:36: missed: statement clobbers memory: _5 = aligned_alloc (64, _4);
Dense.c:177:38: missed: statement clobbers memory: _6 = aligned_alloc (64, _4);
Dense.c:178:36: missed: statement clobbers memory: _8 = aligned_alloc (64, _7);
Dense.c:179:35: missed: statement clobbers memory: _9 = aligned_alloc (64, _7);
Dense.c:180:36: missed: statement clobbers memory: _10 = aligned_alloc (64, _7);
Dense.c:183:39: missed: statement clobbers memory: _11 = rand ();
Dense.c:183:73: missed: statement clobbers memory: _63 = sqrt (_18);
Dense.c:195:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:196:4: missed: statement clobbers memory: exit (1);
Dense.c:192:4: missed: statement clobbers memory: layer = PULSE_CreateLayer (n_inputs.3_29, n_outputs.2_28, 1, activation_function_51(D), _SIMD_FeedDense, _SIMD_BackDense, _SIMD_FixDense, _DestroyDense, 1); [return slot optimization]
Dense.c:189:12: missed: statement clobbers memory: layer = PULSE_CreateLayer (n_inputs.1_27, n_outputs.0_26, 1, activation_function_51(D), _FeedDense, _BackDense, _FixDense, _DestroyDense, 0); [return slot optimization]
Dense.c:202:9: missed: not vectorized: more than one data ref in stmt: <retval> = layer;
Dense.c:202:9: note: ***** Analysis failed with vector mode V4DI
Dense.c:202:9: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _30 (pretmp_91, _50, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:20: missed: not vectorized: no vectype for stmt: _21 = *_20;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:148:19: missed: couldn't vectorize loop
Dense.c:148:19: missed: not vectorized: number of iterations cannot be computed.
Dense.c:139:19: missed: couldn't vectorize loop
Dense.c:139:19: missed: not vectorized: number of iterations cannot be computed.
Dense.c:132:13: note: vectorized 0 loops in function.
Dense.c:156:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:156:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:101:28: missed: couldn't vectorize loop
Dense.c:101:28: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:104:21: missed: couldn't vectorize loop
Dense.c:104:21: missed: not vectorized: number of iterations cannot be computed.
Dense.c:118:28: missed: couldn't vectorize loop
Dense.c:118:28: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:121:21: missed: couldn't vectorize loop
Dense.c:121:21: missed: not vectorized: number of iterations cannot be computed.
Dense.c:87:19: missed: couldn't vectorize loop
Dense.c:87:19: missed: not vectorized: number of iterations cannot be computed.
Dense.c:81:13: note: vectorized 0 loops in function.
Dense.c:84:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:106:17: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:106:17: note: SLPing BB part
Dense.c:123:17: note: Costing subgraph: 
Dense.c:123:17: note: node 0x7515148 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:123:17: note: op template: _49 = (sizetype) j_137;
Dense.c:123:17: note: 	stmt 0 _49 = (sizetype) j_137;
Dense.c:123:17: note: 	stmt 1 _209 = (sizetype) wi_136;
Dense.c:123:17: note: 	children 0x7515248
Dense.c:123:17: note: node (external) 0x7515248 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:123:17: note: 	{ j_137, wi_136 }
Dense.c:123:17: note: Cost model analysis: 
Dense.c:123:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:123:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:123:17: missed: not vectorized: vectorization is not profitable.
Dense.c:106:17: note: Costing subgraph: 
Dense.c:106:17: note: node 0x7515348 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:106:17: note: op template: _24 = (sizetype) j_134;
Dense.c:106:17: note: 	stmt 0 _24 = (sizetype) j_134;
Dense.c:106:17: note: 	stmt 1 _229 = (sizetype) wi_133;
Dense.c:106:17: note: 	children 0x7515448
Dense.c:106:17: note: node (external) 0x7515448 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:106:17: note: 	{ j_134, wi_133 }
Dense.c:106:17: note: Cost model analysis: 
Dense.c:106:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:106:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:106:17: missed: not vectorized: vectorization is not profitable.
Dense.c:106:17: note: ***** The result for vector mode V32QI would be the same
Dense.c:106:17: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:106:17: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:106:17: note: SLPing BB part
Dense.c:123:17: note: Costing subgraph: 
Dense.c:123:17: note: node 0x7515348 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:123:17: note: op template: _49 = (sizetype) j_137;
Dense.c:123:17: note: 	stmt 0 _49 = (sizetype) j_137;
Dense.c:123:17: note: 	stmt 1 _209 = (sizetype) wi_136;
Dense.c:123:17: note: 	children 0x7515148
Dense.c:123:17: note: node (external) 0x7515148 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:123:17: note: 	{ j_137, wi_136 }
Dense.c:123:17: note: Cost model analysis: 
Dense.c:123:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:123:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:123:17: missed: not vectorized: vectorization is not profitable.
Dense.c:106:17: note: Costing subgraph: 
Dense.c:106:17: note: node 0x75154c8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:106:17: note: op template: _24 = (sizetype) j_134;
Dense.c:106:17: note: 	stmt 0 _24 = (sizetype) j_134;
Dense.c:106:17: note: 	stmt 1 _229 = (sizetype) wi_133;
Dense.c:106:17: note: 	children 0x75153c8
Dense.c:106:17: note: node (external) 0x75153c8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:106:17: note: 	{ j_134, wi_133 }
Dense.c:106:17: note: Cost model analysis: 
Dense.c:106:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:106:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:106:17: missed: not vectorized: vectorization is not profitable.
Dense.c:106:17: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:101:55: note: ***** Analysis failed with vector mode V8QI
Dense.c:101:55: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:101:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:59:27: missed: couldn't vectorize loop
Dense.c:59:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:71:11: optimized: loop vectorized using 32 byte vectors
Dense.c:71:11: optimized: loop vectorized using 16 byte vectors
Dense.c:63:11: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:905:10: missed: not vectorized: no vectype for stmt: _61 = MEM[(__m256_u * {ref-all})_6];
 scalar_type: __m256_u
Dense.c:54:13: note: vectorized 1 loops in function.
Dense.c:77:2: missed: statement clobbers memory: _38 (pretmp_240, _70, 0);
Dense.c:54:13: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:54:13: note: SLPing BB part
Dense.c:54:13: note: Costing subgraph: 
Dense.c:54:13: note: node 0x74128b8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:54:13: note: op template: _3 = (sizetype) j_89;
Dense.c:54:13: note: 	stmt 0 _3 = (sizetype) j_89;
Dense.c:54:13: note: 	stmt 1 _239 = (sizetype) wi_88;
Dense.c:54:13: note: 	children 0x74129b8
Dense.c:54:13: note: node (external) 0x74129b8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:54:13: note: 	{ j_89, wi_88 }
Dense.c:54:13: note: node 0x7412ab8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:54:13: note: op template: _150 = (sizetype) j_68;
Dense.c:54:13: note: 	stmt 0 _150 = (sizetype) j_68;
Dense.c:54:13: note: 	stmt 1 _239 = (sizetype) wi_88;
Dense.c:54:13: note: 	children 0x7412bb8
Dense.c:54:13: note: node (external) 0x7412bb8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:54:13: note: 	{ j_68, wi_88 }
Dense.c:54:13: note: Cost model analysis: 
Dense.c:54:13: note: Cost model analysis for part in loop 1:
  Vector cost: 40
  Scalar cost: 8
Dense.c:54:13: missed: not vectorized: vectorization is not profitable.
Dense.c:54:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:54:13: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:54:13: note: SLPing BB part
Dense.c:54:13: note: Costing subgraph: 
Dense.c:54:13: note: node 0x7412ab8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:54:13: note: op template: _3 = (sizetype) j_89;
Dense.c:54:13: note: 	stmt 0 _3 = (sizetype) j_89;
Dense.c:54:13: note: 	stmt 1 _239 = (sizetype) wi_88;
Dense.c:54:13: note: 	children 0x74128b8
Dense.c:54:13: note: node (external) 0x74128b8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:54:13: note: 	{ j_89, wi_88 }
Dense.c:54:13: note: node 0x7412cb8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:54:13: note: op template: _150 = (sizetype) j_68;
Dense.c:54:13: note: 	stmt 0 _150 = (sizetype) j_68;
Dense.c:54:13: note: 	stmt 1 _239 = (sizetype) wi_88;
Dense.c:54:13: note: 	children 0x7412f38
Dense.c:54:13: note: node (external) 0x7412f38 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:54:13: note: 	{ j_68, wi_88 }
Dense.c:54:13: note: Cost model analysis: 
Dense.c:54:13: note: Cost model analysis for part in loop 1:
  Vector cost: 40
  Scalar cost: 8
Dense.c:54:13: missed: not vectorized: vectorization is not profitable.
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:59:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:59:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:59:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:163:2: missed: statement clobbers memory: free (_1);
Dense.c:164:2: missed: statement clobbers memory: free (_2);
Dense.c:165:2: missed: statement clobbers memory: free (_3);
Dense.c:166:2: missed: statement clobbers memory: free (_4);
Dense.c:167:2: missed: statement clobbers memory: free (dense_7);
Dense.c:168:2: missed: statement clobbers memory: PULSE_DestroyLayer (this_6(D));
Dense.c:169:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:169:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:183:19: missed: couldn't vectorize loop
Dense.c:183:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:173:13: note: vectorized 0 loops in function.
Dense.c:175:47: missed: statement clobbers memory: dense_35 = malloc (40);
Dense.c:177:36: missed: statement clobbers memory: _5 = aligned_alloc (64, _4);
Dense.c:178:38: missed: statement clobbers memory: _6 = aligned_alloc (64, _4);
Dense.c:179:36: missed: statement clobbers memory: _8 = aligned_alloc (64, _7);
Dense.c:180:35: missed: statement clobbers memory: _9 = aligned_alloc (64, _7);
Dense.c:181:36: missed: statement clobbers memory: _10 = aligned_alloc (64, _7);
Dense.c:184:39: missed: statement clobbers memory: _11 = rand ();
Dense.c:184:73: missed: statement clobbers memory: _63 = sqrt (_18);
Dense.c:196:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:197:4: missed: statement clobbers memory: exit (1);
Dense.c:193:4: missed: statement clobbers memory: layer = PULSE_CreateLayer (n_inputs.3_29, n_outputs.2_28, 1, activation_function_51(D), _SIMD_FeedDense, _SIMD_BackDense, _SIMD_FixDense, _DestroyDense, 1); [return slot optimization]
Dense.c:190:12: missed: statement clobbers memory: layer = PULSE_CreateLayer (n_inputs.1_27, n_outputs.0_26, 1, activation_function_51(D), _FeedDense, _BackDense, _FixDense, _DestroyDense, 0); [return slot optimization]
Dense.c:203:9: missed: not vectorized: more than one data ref in stmt: <retval> = layer;
Dense.c:203:9: note: ***** Analysis failed with vector mode V4DI
Dense.c:203:9: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _30 (pretmp_91, _50, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:20: missed: not vectorized: no vectype for stmt: _21 = *_20;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:148:19: missed: couldn't vectorize loop
Dense.c:148:19: missed: not vectorized: number of iterations cannot be computed.
Dense.c:139:19: missed: couldn't vectorize loop
Dense.c:139:19: missed: not vectorized: number of iterations cannot be computed.
Dense.c:132:13: note: vectorized 0 loops in function.
Dense.c:156:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:156:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:101:28: missed: couldn't vectorize loop
Dense.c:101:28: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:104:21: missed: couldn't vectorize loop
Dense.c:104:21: missed: not vectorized: number of iterations cannot be computed.
Dense.c:118:28: missed: couldn't vectorize loop
Dense.c:118:28: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:121:21: missed: couldn't vectorize loop
Dense.c:121:21: missed: not vectorized: number of iterations cannot be computed.
Dense.c:87:19: missed: couldn't vectorize loop
Dense.c:87:19: missed: not vectorized: number of iterations cannot be computed.
Dense.c:81:13: note: vectorized 0 loops in function.
Dense.c:84:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:106:17: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:106:17: note: SLPing BB part
Dense.c:123:17: note: Costing subgraph: 
Dense.c:123:17: note: node 0x6e5f008 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:123:17: note: op template: _49 = (sizetype) j_137;
Dense.c:123:17: note: 	stmt 0 _49 = (sizetype) j_137;
Dense.c:123:17: note: 	stmt 1 _209 = (sizetype) wi_136;
Dense.c:123:17: note: 	children 0x6e5f108
Dense.c:123:17: note: node (external) 0x6e5f108 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:123:17: note: 	{ j_137, wi_136 }
Dense.c:123:17: note: Cost model analysis: 
Dense.c:123:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:123:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:123:17: missed: not vectorized: vectorization is not profitable.
Dense.c:106:17: note: Costing subgraph: 
Dense.c:106:17: note: node 0x6e5f208 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:106:17: note: op template: _24 = (sizetype) j_134;
Dense.c:106:17: note: 	stmt 0 _24 = (sizetype) j_134;
Dense.c:106:17: note: 	stmt 1 _229 = (sizetype) wi_133;
Dense.c:106:17: note: 	children 0x6e5f308
Dense.c:106:17: note: node (external) 0x6e5f308 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:106:17: note: 	{ j_134, wi_133 }
Dense.c:106:17: note: Cost model analysis: 
Dense.c:106:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:106:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:106:17: missed: not vectorized: vectorization is not profitable.
Dense.c:106:17: note: ***** The result for vector mode V32QI would be the same
Dense.c:106:17: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:106:17: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:106:17: note: SLPing BB part
Dense.c:123:17: note: Costing subgraph: 
Dense.c:123:17: note: node 0x6e5f208 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:123:17: note: op template: _49 = (sizetype) j_137;
Dense.c:123:17: note: 	stmt 0 _49 = (sizetype) j_137;
Dense.c:123:17: note: 	stmt 1 _209 = (sizetype) wi_136;
Dense.c:123:17: note: 	children 0x6e5f008
Dense.c:123:17: note: node (external) 0x6e5f008 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:123:17: note: 	{ j_137, wi_136 }
Dense.c:123:17: note: Cost model analysis: 
Dense.c:123:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:123:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:123:17: missed: not vectorized: vectorization is not profitable.
Dense.c:106:17: note: Costing subgraph: 
Dense.c:106:17: note: node 0x6e5f388 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:106:17: note: op template: _24 = (sizetype) j_134;
Dense.c:106:17: note: 	stmt 0 _24 = (sizetype) j_134;
Dense.c:106:17: note: 	stmt 1 _229 = (sizetype) wi_133;
Dense.c:106:17: note: 	children 0x6e5f288
Dense.c:106:17: note: node (external) 0x6e5f288 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:106:17: note: 	{ j_134, wi_133 }
Dense.c:106:17: note: Cost model analysis: 
Dense.c:106:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:106:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:106:17: missed: not vectorized: vectorization is not profitable.
Dense.c:106:17: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:101:55: note: ***** Analysis failed with vector mode V8QI
Dense.c:101:55: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:101:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:59:27: missed: couldn't vectorize loop
Dense.c:59:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:71:11: optimized: loop vectorized using 32 byte vectors
Dense.c:71:11: optimized: loop vectorized using 16 byte vectors
Dense.c:63:38: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:905:10: missed: not vectorized: no vectype for stmt: _61 = MEM[(__m256_u * {ref-all})_6];
 scalar_type: __m256_u
Dense.c:54:13: note: vectorized 1 loops in function.
Dense.c:77:2: missed: statement clobbers memory: _38 (pretmp_198, _69, 0);
Dense.c:54:13: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:54:13: note: SLPing BB part
Dense.c:65:14: note: Costing subgraph: 
Dense.c:65:14: note: node 0x6e8f038 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:65:14: note: op template: _3 = (sizetype) j_89;
Dense.c:65:14: note: 	stmt 0 _3 = (sizetype) j_89;
Dense.c:65:14: note: 	stmt 1 _162 = (sizetype) wi_88;
Dense.c:65:14: note: 	children 0x6e8f138
Dense.c:65:14: note: node (external) 0x6e8f138 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:65:14: note: 	{ j_89, wi_88 }
Dense.c:65:14: note: Cost model analysis: 
Dense.c:65:14: note: Scalar 1 and vector 2 loop part do not match up, skipping scalar part
Dense.c:65:14: note: Cost model analysis for part in loop 2:
  Vector cost: 36
  Scalar cost: 8
Dense.c:65:14: missed: not vectorized: vectorization is not profitable.
Dense.c:54:13: note: Costing subgraph: 
Dense.c:54:13: note: node 0x6e8f238 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:54:13: note: op template: _131 = (sizetype) j_90;
Dense.c:54:13: note: 	stmt 0 _131 = (sizetype) j_90;
Dense.c:54:13: note: 	stmt 1 _132 = (sizetype) wi_88;
Dense.c:54:13: note: 	children 0x6e8f338
Dense.c:54:13: note: node (external) 0x6e8f338 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:54:13: note: 	{ j_90, wi_88 }
Dense.c:54:13: note: Cost model analysis: 
Dense.c:54:13: note: Cost model analysis for part in loop 1:
  Vector cost: 36
  Scalar cost: 12
Dense.c:54:13: missed: not vectorized: vectorization is not profitable.
Dense.c:54:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:54:13: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:54:13: note: SLPing BB part
Dense.c:65:14: note: Costing subgraph: 
Dense.c:65:14: note: node 0x6e8f238 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:65:14: note: op template: _3 = (sizetype) j_89;
Dense.c:65:14: note: 	stmt 0 _3 = (sizetype) j_89;
Dense.c:65:14: note: 	stmt 1 _162 = (sizetype) wi_88;
Dense.c:65:14: note: 	children 0x6e8f038
Dense.c:65:14: note: node (external) 0x6e8f038 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:65:14: note: 	{ j_89, wi_88 }
Dense.c:65:14: note: Cost model analysis: 
Dense.c:65:14: note: Scalar 1 and vector 2 loop part do not match up, skipping scalar part
Dense.c:65:14: note: Cost model analysis for part in loop 2:
  Vector cost: 36
  Scalar cost: 8
Dense.c:65:14: missed: not vectorized: vectorization is not profitable.
Dense.c:54:13: note: Costing subgraph: 
Dense.c:54:13: note: node 0x6e8f3b8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:54:13: note: op template: _131 = (sizetype) j_90;
Dense.c:54:13: note: 	stmt 0 _131 = (sizetype) j_90;
Dense.c:54:13: note: 	stmt 1 _132 = (sizetype) wi_88;
Dense.c:54:13: note: 	children 0x6e8f638
Dense.c:54:13: note: node (external) 0x6e8f638 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:54:13: note: 	{ j_90, wi_88 }
Dense.c:54:13: note: Cost model analysis: 
Dense.c:54:13: note: Cost model analysis for part in loop 1:
  Vector cost: 36
  Scalar cost: 12
Dense.c:54:13: missed: not vectorized: vectorization is not profitable.
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:59:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:59:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:59:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:163:2: missed: statement clobbers memory: free (_1);
Dense.c:164:2: missed: statement clobbers memory: free (_2);
Dense.c:165:2: missed: statement clobbers memory: free (_3);
Dense.c:166:2: missed: statement clobbers memory: free (_4);
Dense.c:167:2: missed: statement clobbers memory: free (dense_7);
Dense.c:168:2: missed: statement clobbers memory: PULSE_DestroyLayer (this_6(D));
Dense.c:169:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:169:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:183:19: missed: couldn't vectorize loop
Dense.c:183:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:173:13: note: vectorized 0 loops in function.
Dense.c:175:47: missed: statement clobbers memory: dense_35 = malloc (40);
Dense.c:177:36: missed: statement clobbers memory: _5 = aligned_alloc (64, _4);
Dense.c:178:38: missed: statement clobbers memory: _6 = aligned_alloc (64, _4);
Dense.c:179:36: missed: statement clobbers memory: _8 = aligned_alloc (64, _7);
Dense.c:180:35: missed: statement clobbers memory: _9 = aligned_alloc (64, _7);
Dense.c:181:36: missed: statement clobbers memory: _10 = aligned_alloc (64, _7);
Dense.c:184:39: missed: statement clobbers memory: _11 = rand ();
Dense.c:184:73: missed: statement clobbers memory: _63 = sqrt (_18);
Dense.c:196:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:197:4: missed: statement clobbers memory: exit (1);
Dense.c:193:4: missed: statement clobbers memory: layer = PULSE_CreateLayer (n_inputs.3_29, n_outputs.2_28, 1, activation_function_51(D), _SIMD_FeedDense, _SIMD_BackDense, _SIMD_FixDense, _DestroyDense, 1); [return slot optimization]
Dense.c:190:12: missed: statement clobbers memory: layer = PULSE_CreateLayer (n_inputs.1_27, n_outputs.0_26, 1, activation_function_51(D), _FeedDense, _BackDense, _FixDense, _DestroyDense, 0); [return slot optimization]
Dense.c:203:9: missed: not vectorized: more than one data ref in stmt: <retval> = layer;
Dense.c:203:9: note: ***** Analysis failed with vector mode V4DI
Dense.c:203:9: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _30 (pretmp_91, _50, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:20: missed: not vectorized: no vectype for stmt: _21 = *_20;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:148:19: missed: couldn't vectorize loop
Dense.c:148:19: missed: not vectorized: number of iterations cannot be computed.
Dense.c:139:19: missed: couldn't vectorize loop
Dense.c:139:19: missed: not vectorized: number of iterations cannot be computed.
Dense.c:132:13: note: vectorized 0 loops in function.
Dense.c:156:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:156:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:101:28: missed: couldn't vectorize loop
Dense.c:101:28: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:104:21: missed: couldn't vectorize loop
Dense.c:104:21: missed: not vectorized: number of iterations cannot be computed.
Dense.c:118:28: missed: couldn't vectorize loop
Dense.c:118:28: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:121:21: missed: couldn't vectorize loop
Dense.c:121:21: missed: not vectorized: number of iterations cannot be computed.
Dense.c:87:19: missed: couldn't vectorize loop
Dense.c:87:19: missed: not vectorized: number of iterations cannot be computed.
Dense.c:81:13: note: vectorized 0 loops in function.
Dense.c:84:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:106:17: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:106:17: note: SLPing BB part
Dense.c:123:17: note: Costing subgraph: 
Dense.c:123:17: note: node 0x885dd78 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:123:17: note: op template: _49 = (sizetype) j_137;
Dense.c:123:17: note: 	stmt 0 _49 = (sizetype) j_137;
Dense.c:123:17: note: 	stmt 1 _209 = (sizetype) wi_136;
Dense.c:123:17: note: 	children 0x885de78
Dense.c:123:17: note: node (external) 0x885de78 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:123:17: note: 	{ j_137, wi_136 }
Dense.c:123:17: note: Cost model analysis: 
Dense.c:123:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:123:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:123:17: missed: not vectorized: vectorization is not profitable.
Dense.c:106:17: note: Costing subgraph: 
Dense.c:106:17: note: node 0x885df78 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:106:17: note: op template: _24 = (sizetype) j_134;
Dense.c:106:17: note: 	stmt 0 _24 = (sizetype) j_134;
Dense.c:106:17: note: 	stmt 1 _229 = (sizetype) wi_133;
Dense.c:106:17: note: 	children 0x885e078
Dense.c:106:17: note: node (external) 0x885e078 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:106:17: note: 	{ j_134, wi_133 }
Dense.c:106:17: note: Cost model analysis: 
Dense.c:106:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:106:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:106:17: missed: not vectorized: vectorization is not profitable.
Dense.c:106:17: note: ***** The result for vector mode V32QI would be the same
Dense.c:106:17: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:106:17: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:106:17: note: SLPing BB part
Dense.c:123:17: note: Costing subgraph: 
Dense.c:123:17: note: node 0x885df78 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:123:17: note: op template: _49 = (sizetype) j_137;
Dense.c:123:17: note: 	stmt 0 _49 = (sizetype) j_137;
Dense.c:123:17: note: 	stmt 1 _209 = (sizetype) wi_136;
Dense.c:123:17: note: 	children 0x885dd78
Dense.c:123:17: note: node (external) 0x885dd78 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:123:17: note: 	{ j_137, wi_136 }
Dense.c:123:17: note: Cost model analysis: 
Dense.c:123:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:123:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:123:17: missed: not vectorized: vectorization is not profitable.
Dense.c:106:17: note: Costing subgraph: 
Dense.c:106:17: note: node 0x885e0f8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:106:17: note: op template: _24 = (sizetype) j_134;
Dense.c:106:17: note: 	stmt 0 _24 = (sizetype) j_134;
Dense.c:106:17: note: 	stmt 1 _229 = (sizetype) wi_133;
Dense.c:106:17: note: 	children 0x885dff8
Dense.c:106:17: note: node (external) 0x885dff8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:106:17: note: 	{ j_134, wi_133 }
Dense.c:106:17: note: Cost model analysis: 
Dense.c:106:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:106:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:106:17: missed: not vectorized: vectorization is not profitable.
Dense.c:106:17: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:101:55: note: ***** Analysis failed with vector mode V8QI
Dense.c:101:55: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:101:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:60:22: missed: couldn't vectorize loop
Dense.c:60:22: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:71:11: optimized: loop vectorized using 32 byte vectors
Dense.c:71:11: optimized: loop vectorized using 16 byte vectors
Dense.c:63:38: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:905:10: missed: not vectorized: no vectype for stmt: _55 = MEM[(__m256_u * {ref-all})_3];
 scalar_type: __m256_u
Dense.c:54:13: note: vectorized 1 loops in function.
Dense.c:77:2: missed: statement clobbers memory: _30 (pretmp_182, _63, 0);
Dense.c:54:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:54:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:163:2: missed: statement clobbers memory: free (_1);
Dense.c:164:2: missed: statement clobbers memory: free (_2);
Dense.c:165:2: missed: statement clobbers memory: free (_3);
Dense.c:166:2: missed: statement clobbers memory: free (_4);
Dense.c:167:2: missed: statement clobbers memory: free (dense_7);
Dense.c:168:2: missed: statement clobbers memory: PULSE_DestroyLayer (this_6(D));
Dense.c:169:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:169:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:183:19: missed: couldn't vectorize loop
Dense.c:183:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:173:13: note: vectorized 0 loops in function.
Dense.c:175:47: missed: statement clobbers memory: dense_35 = malloc (40);
Dense.c:177:36: missed: statement clobbers memory: _5 = aligned_alloc (64, _4);
Dense.c:178:38: missed: statement clobbers memory: _6 = aligned_alloc (64, _4);
Dense.c:179:36: missed: statement clobbers memory: _8 = aligned_alloc (64, _7);
Dense.c:180:35: missed: statement clobbers memory: _9 = aligned_alloc (64, _7);
Dense.c:181:36: missed: statement clobbers memory: _10 = aligned_alloc (64, _7);
Dense.c:184:39: missed: statement clobbers memory: _11 = rand ();
Dense.c:184:73: missed: statement clobbers memory: _63 = sqrt (_18);
Dense.c:196:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:197:4: missed: statement clobbers memory: exit (1);
Dense.c:193:4: missed: statement clobbers memory: layer = PULSE_CreateLayer (n_inputs.3_29, n_outputs.2_28, 1, activation_function_51(D), _SIMD_FeedDense, _SIMD_BackDense, _SIMD_FixDense, _DestroyDense, 1); [return slot optimization]
Dense.c:190:12: missed: statement clobbers memory: layer = PULSE_CreateLayer (n_inputs.1_27, n_outputs.0_26, 1, activation_function_51(D), _FeedDense, _BackDense, _FixDense, _DestroyDense, 0); [return slot optimization]
Dense.c:203:9: missed: not vectorized: more than one data ref in stmt: <retval> = layer;
Dense.c:203:9: note: ***** Analysis failed with vector mode V4DI
Dense.c:203:9: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _30 (pretmp_91, _50, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:20: missed: not vectorized: no vectype for stmt: _21 = *_20;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:149:19: missed: couldn't vectorize loop
Dense.c:149:19: missed: not vectorized: number of iterations cannot be computed.
Dense.c:140:19: missed: couldn't vectorize loop
Dense.c:140:19: missed: not vectorized: number of iterations cannot be computed.
Dense.c:133:13: note: vectorized 0 loops in function.
Dense.c:157:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:157:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:102:28: missed: couldn't vectorize loop
Dense.c:102:28: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:105:21: missed: couldn't vectorize loop
Dense.c:105:21: missed: not vectorized: number of iterations cannot be computed.
Dense.c:119:28: missed: couldn't vectorize loop
Dense.c:119:28: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:122:21: missed: couldn't vectorize loop
Dense.c:122:21: missed: not vectorized: number of iterations cannot be computed.
Dense.c:88:19: missed: couldn't vectorize loop
Dense.c:88:19: missed: not vectorized: number of iterations cannot be computed.
Dense.c:82:13: note: vectorized 0 loops in function.
Dense.c:85:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:107:17: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:107:17: note: SLPing BB part
Dense.c:124:17: note: Costing subgraph: 
Dense.c:124:17: note: node 0x8135ae8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:124:17: note: op template: _49 = (sizetype) j_137;
Dense.c:124:17: note: 	stmt 0 _49 = (sizetype) j_137;
Dense.c:124:17: note: 	stmt 1 _209 = (sizetype) wi_136;
Dense.c:124:17: note: 	children 0x8135be8
Dense.c:124:17: note: node (external) 0x8135be8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:124:17: note: 	{ j_137, wi_136 }
Dense.c:124:17: note: Cost model analysis: 
Dense.c:124:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:124:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:124:17: missed: not vectorized: vectorization is not profitable.
Dense.c:107:17: note: Costing subgraph: 
Dense.c:107:17: note: node 0x8135ce8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:107:17: note: op template: _24 = (sizetype) j_134;
Dense.c:107:17: note: 	stmt 0 _24 = (sizetype) j_134;
Dense.c:107:17: note: 	stmt 1 _229 = (sizetype) wi_133;
Dense.c:107:17: note: 	children 0x8135de8
Dense.c:107:17: note: node (external) 0x8135de8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:107:17: note: 	{ j_134, wi_133 }
Dense.c:107:17: note: Cost model analysis: 
Dense.c:107:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:107:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:107:17: missed: not vectorized: vectorization is not profitable.
Dense.c:107:17: note: ***** The result for vector mode V32QI would be the same
Dense.c:107:17: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:107:17: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:107:17: note: SLPing BB part
Dense.c:124:17: note: Costing subgraph: 
Dense.c:124:17: note: node 0x8135ce8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:124:17: note: op template: _49 = (sizetype) j_137;
Dense.c:124:17: note: 	stmt 0 _49 = (sizetype) j_137;
Dense.c:124:17: note: 	stmt 1 _209 = (sizetype) wi_136;
Dense.c:124:17: note: 	children 0x8135ae8
Dense.c:124:17: note: node (external) 0x8135ae8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:124:17: note: 	{ j_137, wi_136 }
Dense.c:124:17: note: Cost model analysis: 
Dense.c:124:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:124:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:124:17: missed: not vectorized: vectorization is not profitable.
Dense.c:107:17: note: Costing subgraph: 
Dense.c:107:17: note: node 0x8135e68 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:107:17: note: op template: _24 = (sizetype) j_134;
Dense.c:107:17: note: 	stmt 0 _24 = (sizetype) j_134;
Dense.c:107:17: note: 	stmt 1 _229 = (sizetype) wi_133;
Dense.c:107:17: note: 	children 0x8135d68
Dense.c:107:17: note: node (external) 0x8135d68 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:107:17: note: 	{ j_134, wi_133 }
Dense.c:107:17: note: Cost model analysis: 
Dense.c:107:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:107:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:107:17: missed: not vectorized: vectorization is not profitable.
Dense.c:107:17: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:102:55: note: ***** Analysis failed with vector mode V8QI
Dense.c:102:55: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:102:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:61:22: missed: couldn't vectorize loop
Dense.c:61:22: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:72:11: optimized: loop vectorized using 32 byte vectors
Dense.c:72:11: optimized: loop vectorized using 16 byte vectors
Dense.c:64:38: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:905:10: missed: not vectorized: no vectype for stmt: _55 = MEM[(__m256_u * {ref-all})_3];
 scalar_type: __m256_u
Dense.c:54:13: note: vectorized 1 loops in function.
Dense.c:78:2: missed: statement clobbers memory: _30 (pretmp_182, _63, 0);
Dense.c:54:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:54:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:164:2: missed: statement clobbers memory: free (_1);
Dense.c:165:2: missed: statement clobbers memory: free (_2);
Dense.c:166:2: missed: statement clobbers memory: free (_3);
Dense.c:167:2: missed: statement clobbers memory: free (_4);
Dense.c:168:2: missed: statement clobbers memory: free (dense_7);
Dense.c:169:2: missed: statement clobbers memory: PULSE_DestroyLayer (this_6(D));
Dense.c:170:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:170:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:184:19: missed: couldn't vectorize loop
Dense.c:184:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:174:13: note: vectorized 0 loops in function.
Dense.c:176:47: missed: statement clobbers memory: dense_35 = malloc (40);
Dense.c:178:36: missed: statement clobbers memory: _5 = aligned_alloc (64, _4);
Dense.c:179:38: missed: statement clobbers memory: _6 = aligned_alloc (64, _4);
Dense.c:180:36: missed: statement clobbers memory: _8 = aligned_alloc (64, _7);
Dense.c:181:35: missed: statement clobbers memory: _9 = aligned_alloc (64, _7);
Dense.c:182:36: missed: statement clobbers memory: _10 = aligned_alloc (64, _7);
Dense.c:185:39: missed: statement clobbers memory: _11 = rand ();
Dense.c:185:73: missed: statement clobbers memory: _63 = sqrt (_18);
Dense.c:197:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:198:4: missed: statement clobbers memory: exit (1);
Dense.c:194:4: missed: statement clobbers memory: layer = PULSE_CreateLayer (n_inputs.3_29, n_outputs.2_28, 1, activation_function_51(D), _SIMD_FeedDense, _SIMD_BackDense, _SIMD_FixDense, _DestroyDense, 1); [return slot optimization]
Dense.c:191:12: missed: statement clobbers memory: layer = PULSE_CreateLayer (n_inputs.1_27, n_outputs.0_26, 1, activation_function_51(D), _FeedDense, _BackDense, _FixDense, _DestroyDense, 0); [return slot optimization]
Dense.c:204:9: missed: not vectorized: more than one data ref in stmt: <retval> = layer;
Dense.c:204:9: note: ***** Analysis failed with vector mode V4DI
Dense.c:204:9: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _30 (pretmp_91, _50, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:20: missed: not vectorized: no vectype for stmt: _21 = *_20;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:149:19: missed: couldn't vectorize loop
Dense.c:149:19: missed: not vectorized: number of iterations cannot be computed.
Dense.c:140:19: missed: couldn't vectorize loop
Dense.c:140:19: missed: not vectorized: number of iterations cannot be computed.
Dense.c:133:13: note: vectorized 0 loops in function.
Dense.c:157:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:157:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:102:28: missed: couldn't vectorize loop
Dense.c:102:28: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:105:21: missed: couldn't vectorize loop
Dense.c:105:21: missed: not vectorized: number of iterations cannot be computed.
Dense.c:119:28: missed: couldn't vectorize loop
Dense.c:119:28: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:122:21: missed: couldn't vectorize loop
Dense.c:122:21: missed: not vectorized: number of iterations cannot be computed.
Dense.c:88:19: missed: couldn't vectorize loop
Dense.c:88:19: missed: not vectorized: number of iterations cannot be computed.
Dense.c:82:13: note: vectorized 0 loops in function.
Dense.c:85:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:107:17: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:107:17: note: SLPing BB part
Dense.c:124:17: note: Costing subgraph: 
Dense.c:124:17: note: node 0x83f12e8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:124:17: note: op template: _49 = (sizetype) j_137;
Dense.c:124:17: note: 	stmt 0 _49 = (sizetype) j_137;
Dense.c:124:17: note: 	stmt 1 _209 = (sizetype) wi_136;
Dense.c:124:17: note: 	children 0x83f13e8
Dense.c:124:17: note: node (external) 0x83f13e8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:124:17: note: 	{ j_137, wi_136 }
Dense.c:124:17: note: Cost model analysis: 
Dense.c:124:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:124:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:124:17: missed: not vectorized: vectorization is not profitable.
Dense.c:107:17: note: Costing subgraph: 
Dense.c:107:17: note: node 0x83f14e8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:107:17: note: op template: _24 = (sizetype) j_134;
Dense.c:107:17: note: 	stmt 0 _24 = (sizetype) j_134;
Dense.c:107:17: note: 	stmt 1 _229 = (sizetype) wi_133;
Dense.c:107:17: note: 	children 0x83f15e8
Dense.c:107:17: note: node (external) 0x83f15e8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:107:17: note: 	{ j_134, wi_133 }
Dense.c:107:17: note: Cost model analysis: 
Dense.c:107:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:107:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:107:17: missed: not vectorized: vectorization is not profitable.
Dense.c:107:17: note: ***** The result for vector mode V32QI would be the same
Dense.c:107:17: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:107:17: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:107:17: note: SLPing BB part
Dense.c:124:17: note: Costing subgraph: 
Dense.c:124:17: note: node 0x83f14e8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:124:17: note: op template: _49 = (sizetype) j_137;
Dense.c:124:17: note: 	stmt 0 _49 = (sizetype) j_137;
Dense.c:124:17: note: 	stmt 1 _209 = (sizetype) wi_136;
Dense.c:124:17: note: 	children 0x83f12e8
Dense.c:124:17: note: node (external) 0x83f12e8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:124:17: note: 	{ j_137, wi_136 }
Dense.c:124:17: note: Cost model analysis: 
Dense.c:124:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:124:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:124:17: missed: not vectorized: vectorization is not profitable.
Dense.c:107:17: note: Costing subgraph: 
Dense.c:107:17: note: node 0x83f1668 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:107:17: note: op template: _24 = (sizetype) j_134;
Dense.c:107:17: note: 	stmt 0 _24 = (sizetype) j_134;
Dense.c:107:17: note: 	stmt 1 _229 = (sizetype) wi_133;
Dense.c:107:17: note: 	children 0x83f1568
Dense.c:107:17: note: node (external) 0x83f1568 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:107:17: note: 	{ j_134, wi_133 }
Dense.c:107:17: note: Cost model analysis: 
Dense.c:107:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:107:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:107:17: missed: not vectorized: vectorization is not profitable.
Dense.c:107:17: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:102:55: note: ***** Analysis failed with vector mode V8QI
Dense.c:102:55: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:102:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:61:22: missed: couldn't vectorize loop
Dense.c:76:45: missed: not vectorized: no vectype for stmt: _95 = *_96;
 scalar_type: float
Dense.c:61:22: missed: couldn't vectorize loop
Dense.c:61:22: missed: not vectorized: unsupported outerloop form.
Dense.c:64:38: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:905:10: missed: not vectorized: no vectype for stmt: _42 = MEM[(__m256_u * {ref-all})_3];
 scalar_type: __m256_u
Dense.c:54:13: note: vectorized 0 loops in function.
Dense.c:78:2: missed: statement clobbers memory: _21 (pretmp_136, _49, 0);
Dense.c:79:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:79:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:164:2: missed: statement clobbers memory: free (_1);
Dense.c:165:2: missed: statement clobbers memory: free (_2);
Dense.c:166:2: missed: statement clobbers memory: free (_3);
Dense.c:167:2: missed: statement clobbers memory: free (_4);
Dense.c:168:2: missed: statement clobbers memory: free (dense_7);
Dense.c:169:2: missed: statement clobbers memory: PULSE_DestroyLayer (this_6(D));
Dense.c:170:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:170:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:184:19: missed: couldn't vectorize loop
Dense.c:184:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:174:13: note: vectorized 0 loops in function.
Dense.c:176:47: missed: statement clobbers memory: dense_35 = malloc (40);
Dense.c:178:36: missed: statement clobbers memory: _5 = aligned_alloc (64, _4);
Dense.c:179:38: missed: statement clobbers memory: _6 = aligned_alloc (64, _4);
Dense.c:180:36: missed: statement clobbers memory: _8 = aligned_alloc (64, _7);
Dense.c:181:35: missed: statement clobbers memory: _9 = aligned_alloc (64, _7);
Dense.c:182:36: missed: statement clobbers memory: _10 = aligned_alloc (64, _7);
Dense.c:185:39: missed: statement clobbers memory: _11 = rand ();
Dense.c:185:73: missed: statement clobbers memory: _63 = sqrt (_18);
Dense.c:197:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:198:4: missed: statement clobbers memory: exit (1);
Dense.c:194:4: missed: statement clobbers memory: layer = PULSE_CreateLayer (n_inputs.3_29, n_outputs.2_28, 1, activation_function_51(D), _SIMD_FeedDense, _SIMD_BackDense, _SIMD_FixDense, _DestroyDense, 1); [return slot optimization]
Dense.c:191:12: missed: statement clobbers memory: layer = PULSE_CreateLayer (n_inputs.1_27, n_outputs.0_26, 1, activation_function_51(D), _FeedDense, _BackDense, _FixDense, _DestroyDense, 0); [return slot optimization]
Dense.c:204:9: missed: not vectorized: more than one data ref in stmt: <retval> = layer;
Dense.c:204:9: note: ***** Analysis failed with vector mode V4DI
Dense.c:204:9: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _30 (pretmp_91, _50, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:20: missed: not vectorized: no vectype for stmt: _21 = *_20;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:149:19: missed: couldn't vectorize loop
Dense.c:149:19: missed: not vectorized: number of iterations cannot be computed.
Dense.c:140:19: missed: couldn't vectorize loop
Dense.c:140:19: missed: not vectorized: number of iterations cannot be computed.
Dense.c:133:13: note: vectorized 0 loops in function.
Dense.c:157:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:157:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:102:28: missed: couldn't vectorize loop
Dense.c:102:28: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:105:21: missed: couldn't vectorize loop
Dense.c:105:21: missed: not vectorized: number of iterations cannot be computed.
Dense.c:119:28: missed: couldn't vectorize loop
Dense.c:119:28: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:122:21: missed: couldn't vectorize loop
Dense.c:122:21: missed: not vectorized: number of iterations cannot be computed.
Dense.c:88:19: missed: couldn't vectorize loop
Dense.c:88:19: missed: not vectorized: number of iterations cannot be computed.
Dense.c:82:13: note: vectorized 0 loops in function.
Dense.c:85:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:107:17: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:107:17: note: SLPing BB part
Dense.c:124:17: note: Costing subgraph: 
Dense.c:124:17: note: node 0x7bed9e8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:124:17: note: op template: _49 = (sizetype) j_137;
Dense.c:124:17: note: 	stmt 0 _49 = (sizetype) j_137;
Dense.c:124:17: note: 	stmt 1 _209 = (sizetype) wi_136;
Dense.c:124:17: note: 	children 0x7bedae8
Dense.c:124:17: note: node (external) 0x7bedae8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:124:17: note: 	{ j_137, wi_136 }
Dense.c:124:17: note: Cost model analysis: 
Dense.c:124:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:124:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:124:17: missed: not vectorized: vectorization is not profitable.
Dense.c:107:17: note: Costing subgraph: 
Dense.c:107:17: note: node 0x7bedbe8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:107:17: note: op template: _24 = (sizetype) j_134;
Dense.c:107:17: note: 	stmt 0 _24 = (sizetype) j_134;
Dense.c:107:17: note: 	stmt 1 _229 = (sizetype) wi_133;
Dense.c:107:17: note: 	children 0x7bedce8
Dense.c:107:17: note: node (external) 0x7bedce8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:107:17: note: 	{ j_134, wi_133 }
Dense.c:107:17: note: Cost model analysis: 
Dense.c:107:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:107:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:107:17: missed: not vectorized: vectorization is not profitable.
Dense.c:107:17: note: ***** The result for vector mode V32QI would be the same
Dense.c:107:17: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:107:17: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:107:17: note: SLPing BB part
Dense.c:124:17: note: Costing subgraph: 
Dense.c:124:17: note: node 0x7bedbe8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:124:17: note: op template: _49 = (sizetype) j_137;
Dense.c:124:17: note: 	stmt 0 _49 = (sizetype) j_137;
Dense.c:124:17: note: 	stmt 1 _209 = (sizetype) wi_136;
Dense.c:124:17: note: 	children 0x7bed9e8
Dense.c:124:17: note: node (external) 0x7bed9e8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:124:17: note: 	{ j_137, wi_136 }
Dense.c:124:17: note: Cost model analysis: 
Dense.c:124:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:124:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:124:17: missed: not vectorized: vectorization is not profitable.
Dense.c:107:17: note: Costing subgraph: 
Dense.c:107:17: note: node 0x7bedd68 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:107:17: note: op template: _24 = (sizetype) j_134;
Dense.c:107:17: note: 	stmt 0 _24 = (sizetype) j_134;
Dense.c:107:17: note: 	stmt 1 _229 = (sizetype) wi_133;
Dense.c:107:17: note: 	children 0x7bedc68
Dense.c:107:17: note: node (external) 0x7bedc68 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:107:17: note: 	{ j_134, wi_133 }
Dense.c:107:17: note: Cost model analysis: 
Dense.c:107:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:107:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:107:17: missed: not vectorized: vectorization is not profitable.
Dense.c:107:17: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:102:55: note: ***** Analysis failed with vector mode V8QI
Dense.c:102:55: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:102:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:61:22: missed: couldn't vectorize loop
Dense.c:76:45: missed: not vectorized: no vectype for stmt: _102 = *_103;
 scalar_type: float
Dense.c:61:22: missed: couldn't vectorize loop
Dense.c:61:22: missed: not vectorized: unsupported outerloop form.
Dense.c:64:38: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:905:10: missed: not vectorized: no vectype for stmt: _42 = MEM[(__m256_u * {ref-all})_6];
 scalar_type: __m256_u
Dense.c:54:13: note: vectorized 0 loops in function.
Dense.c:78:2: missed: statement clobbers memory: _23 (pretmp_147, _49, 0);
Dense.c:66:14: note: ***** Analysis failed with vector mode V4DI
Dense.c:66:14: note: ***** The result for vector mode V32QI would be the same
Dense.c:66:14: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:66:14: note: ***** Analysis failed with vector mode V16QI
Dense.c:66:14: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:66:14: note: ***** Analysis failed with vector mode V8QI
Dense.c:66:14: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:66:14: note: ***** Analysis failed with vector mode V4QI
Dense.c:164:2: missed: statement clobbers memory: free (_1);
Dense.c:165:2: missed: statement clobbers memory: free (_2);
Dense.c:166:2: missed: statement clobbers memory: free (_3);
Dense.c:167:2: missed: statement clobbers memory: free (_4);
Dense.c:168:2: missed: statement clobbers memory: free (dense_7);
Dense.c:169:2: missed: statement clobbers memory: PULSE_DestroyLayer (this_6(D));
Dense.c:170:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:170:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:184:19: missed: couldn't vectorize loop
Dense.c:184:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:174:13: note: vectorized 0 loops in function.
Dense.c:176:47: missed: statement clobbers memory: dense_35 = malloc (40);
Dense.c:178:36: missed: statement clobbers memory: _5 = aligned_alloc (64, _4);
Dense.c:179:38: missed: statement clobbers memory: _6 = aligned_alloc (64, _4);
Dense.c:180:36: missed: statement clobbers memory: _8 = aligned_alloc (64, _7);
Dense.c:181:35: missed: statement clobbers memory: _9 = aligned_alloc (64, _7);
Dense.c:182:36: missed: statement clobbers memory: _10 = aligned_alloc (64, _7);
Dense.c:185:39: missed: statement clobbers memory: _11 = rand ();
Dense.c:185:73: missed: statement clobbers memory: _63 = sqrt (_18);
Dense.c:197:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:198:4: missed: statement clobbers memory: exit (1);
Dense.c:194:4: missed: statement clobbers memory: layer = PULSE_CreateLayer (n_inputs.3_29, n_outputs.2_28, 1, activation_function_51(D), _SIMD_FeedDense, _SIMD_BackDense, _SIMD_FixDense, _DestroyDense, 1); [return slot optimization]
Dense.c:191:12: missed: statement clobbers memory: layer = PULSE_CreateLayer (n_inputs.1_27, n_outputs.0_26, 1, activation_function_51(D), _FeedDense, _BackDense, _FixDense, _DestroyDense, 0); [return slot optimization]
Dense.c:204:9: missed: not vectorized: more than one data ref in stmt: <retval> = layer;
Dense.c:204:9: note: ***** Analysis failed with vector mode V4DI
Dense.c:204:9: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _30 (pretmp_91, _50, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:20: missed: not vectorized: no vectype for stmt: _21 = *_20;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:150:19: missed: couldn't vectorize loop
Dense.c:150:19: missed: not vectorized: number of iterations cannot be computed.
Dense.c:141:19: missed: couldn't vectorize loop
Dense.c:141:19: missed: not vectorized: number of iterations cannot be computed.
Dense.c:134:13: note: vectorized 0 loops in function.
Dense.c:158:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:158:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:103:28: missed: couldn't vectorize loop
Dense.c:103:28: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:106:21: missed: couldn't vectorize loop
Dense.c:106:21: missed: not vectorized: number of iterations cannot be computed.
Dense.c:120:28: missed: couldn't vectorize loop
Dense.c:120:28: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:123:21: missed: couldn't vectorize loop
Dense.c:123:21: missed: not vectorized: number of iterations cannot be computed.
Dense.c:89:19: missed: couldn't vectorize loop
Dense.c:89:19: missed: not vectorized: number of iterations cannot be computed.
Dense.c:83:13: note: vectorized 0 loops in function.
Dense.c:86:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:108:17: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:108:17: note: SLPing BB part
Dense.c:125:17: note: Costing subgraph: 
Dense.c:125:17: note: node 0x7b0e9e8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:125:17: note: op template: _49 = (sizetype) j_137;
Dense.c:125:17: note: 	stmt 0 _49 = (sizetype) j_137;
Dense.c:125:17: note: 	stmt 1 _209 = (sizetype) wi_136;
Dense.c:125:17: note: 	children 0x7b0eae8
Dense.c:125:17: note: node (external) 0x7b0eae8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:125:17: note: 	{ j_137, wi_136 }
Dense.c:125:17: note: Cost model analysis: 
Dense.c:125:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:125:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:125:17: missed: not vectorized: vectorization is not profitable.
Dense.c:108:17: note: Costing subgraph: 
Dense.c:108:17: note: node 0x7b0ebe8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:108:17: note: op template: _24 = (sizetype) j_134;
Dense.c:108:17: note: 	stmt 0 _24 = (sizetype) j_134;
Dense.c:108:17: note: 	stmt 1 _229 = (sizetype) wi_133;
Dense.c:108:17: note: 	children 0x7b0ece8
Dense.c:108:17: note: node (external) 0x7b0ece8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:108:17: note: 	{ j_134, wi_133 }
Dense.c:108:17: note: Cost model analysis: 
Dense.c:108:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:108:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:108:17: missed: not vectorized: vectorization is not profitable.
Dense.c:108:17: note: ***** The result for vector mode V32QI would be the same
Dense.c:108:17: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:108:17: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:108:17: note: SLPing BB part
Dense.c:125:17: note: Costing subgraph: 
Dense.c:125:17: note: node 0x7b0ebe8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:125:17: note: op template: _49 = (sizetype) j_137;
Dense.c:125:17: note: 	stmt 0 _49 = (sizetype) j_137;
Dense.c:125:17: note: 	stmt 1 _209 = (sizetype) wi_136;
Dense.c:125:17: note: 	children 0x7b0e9e8
Dense.c:125:17: note: node (external) 0x7b0e9e8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:125:17: note: 	{ j_137, wi_136 }
Dense.c:125:17: note: Cost model analysis: 
Dense.c:125:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:125:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:125:17: missed: not vectorized: vectorization is not profitable.
Dense.c:108:17: note: Costing subgraph: 
Dense.c:108:17: note: node 0x7b0ed68 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:108:17: note: op template: _24 = (sizetype) j_134;
Dense.c:108:17: note: 	stmt 0 _24 = (sizetype) j_134;
Dense.c:108:17: note: 	stmt 1 _229 = (sizetype) wi_133;
Dense.c:108:17: note: 	children 0x7b0ec68
Dense.c:108:17: note: node (external) 0x7b0ec68 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:108:17: note: 	{ j_134, wi_133 }
Dense.c:108:17: note: Cost model analysis: 
Dense.c:108:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:108:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:108:17: missed: not vectorized: vectorization is not profitable.
Dense.c:108:17: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:103:55: note: ***** Analysis failed with vector mode V8QI
Dense.c:103:55: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:103:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:61:27: missed: couldn't vectorize loop
Dense.c:77:45: missed: not vectorized: no vectype for stmt: _108 = *_109;
 scalar_type: float
Dense.c:61:27: missed: couldn't vectorize loop
Dense.c:61:27: missed: not vectorized: unsupported outerloop form.
Dense.c:64:38: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:905:10: missed: not vectorized: no vectype for stmt: _46 = MEM[(__m256_u * {ref-all})_6];
 scalar_type: __m256_u
Dense.c:54:13: note: vectorized 0 loops in function.
Dense.c:79:2: missed: statement clobbers memory: _25 (pretmp_156, _53, 0);
Dense.c:66:14: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:66:14: note: SLPing BB part
Dense.c:66:14: note: Costing subgraph: 
Dense.c:66:14: note: node 0x7b3ea18 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:66:14: note: op template: _3 = (sizetype) j_68;
Dense.c:66:14: note: 	stmt 0 _3 = (sizetype) j_68;
Dense.c:66:14: note: 	stmt 1 _140 = (sizetype) wi_72;
Dense.c:66:14: note: 	children 0x7b3eb18
Dense.c:66:14: note: node (external) 0x7b3eb18 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:66:14: note: 	{ j_68, wi_72 }
Dense.c:66:14: note: Cost model analysis: 
Dense.c:66:14: note: Scalar 1 and vector 2 loop part do not match up, skipping scalar part
Dense.c:66:14: note: Cost model analysis for part in loop 2:
  Vector cost: 36
  Scalar cost: 8
Dense.c:66:14: missed: not vectorized: vectorization is not profitable.
Dense.c:66:14: note: ***** The result for vector mode V32QI would be the same
Dense.c:66:14: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:66:14: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:66:14: note: SLPing BB part
Dense.c:66:14: note: Costing subgraph: 
Dense.c:66:14: note: node 0x7b3ea18 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:66:14: note: op template: _3 = (sizetype) j_68;
Dense.c:66:14: note: 	stmt 0 _3 = (sizetype) j_68;
Dense.c:66:14: note: 	stmt 1 _140 = (sizetype) wi_72;
Dense.c:66:14: note: 	children 0x7b3ea98
Dense.c:66:14: note: node (external) 0x7b3ea98 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:66:14: note: 	{ j_68, wi_72 }
Dense.c:66:14: note: Cost model analysis: 
Dense.c:66:14: note: Scalar 1 and vector 2 loop part do not match up, skipping scalar part
Dense.c:66:14: note: Cost model analysis for part in loop 2:
  Vector cost: 36
  Scalar cost: 8
Dense.c:66:14: missed: not vectorized: vectorization is not profitable.
Dense.c:66:14: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:61:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:61:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:61:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:165:2: missed: statement clobbers memory: free (_1);
Dense.c:166:2: missed: statement clobbers memory: free (_2);
Dense.c:167:2: missed: statement clobbers memory: free (_3);
Dense.c:168:2: missed: statement clobbers memory: free (_4);
Dense.c:169:2: missed: statement clobbers memory: free (dense_7);
Dense.c:170:2: missed: statement clobbers memory: PULSE_DestroyLayer (this_6(D));
Dense.c:171:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:171:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:185:19: missed: couldn't vectorize loop
Dense.c:185:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:175:13: note: vectorized 0 loops in function.
Dense.c:177:47: missed: statement clobbers memory: dense_35 = malloc (40);
Dense.c:179:36: missed: statement clobbers memory: _5 = aligned_alloc (64, _4);
Dense.c:180:38: missed: statement clobbers memory: _6 = aligned_alloc (64, _4);
Dense.c:181:36: missed: statement clobbers memory: _8 = aligned_alloc (64, _7);
Dense.c:182:35: missed: statement clobbers memory: _9 = aligned_alloc (64, _7);
Dense.c:183:36: missed: statement clobbers memory: _10 = aligned_alloc (64, _7);
Dense.c:186:39: missed: statement clobbers memory: _11 = rand ();
Dense.c:186:73: missed: statement clobbers memory: _63 = sqrt (_18);
Dense.c:198:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:199:4: missed: statement clobbers memory: exit (1);
Dense.c:195:4: missed: statement clobbers memory: layer = PULSE_CreateLayer (n_inputs.3_29, n_outputs.2_28, 1, activation_function_51(D), _SIMD_FeedDense, _SIMD_BackDense, _SIMD_FixDense, _DestroyDense, 1); [return slot optimization]
Dense.c:192:12: missed: statement clobbers memory: layer = PULSE_CreateLayer (n_inputs.1_27, n_outputs.0_26, 1, activation_function_51(D), _FeedDense, _BackDense, _FixDense, _DestroyDense, 0); [return slot optimization]
Dense.c:205:9: missed: not vectorized: more than one data ref in stmt: <retval> = layer;
Dense.c:205:9: note: ***** Analysis failed with vector mode V4DI
Dense.c:205:9: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _30 (pretmp_91, _50, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:20: missed: not vectorized: no vectype for stmt: _21 = *_20;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:150:19: missed: couldn't vectorize loop
Dense.c:150:19: missed: not vectorized: number of iterations cannot be computed.
Dense.c:141:19: missed: couldn't vectorize loop
Dense.c:141:19: missed: not vectorized: number of iterations cannot be computed.
Dense.c:134:13: note: vectorized 0 loops in function.
Dense.c:158:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:158:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:103:28: missed: couldn't vectorize loop
Dense.c:103:28: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:106:21: missed: couldn't vectorize loop
Dense.c:106:21: missed: not vectorized: number of iterations cannot be computed.
Dense.c:120:28: missed: couldn't vectorize loop
Dense.c:120:28: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:123:21: missed: couldn't vectorize loop
Dense.c:123:21: missed: not vectorized: number of iterations cannot be computed.
Dense.c:89:19: missed: couldn't vectorize loop
Dense.c:89:19: missed: not vectorized: number of iterations cannot be computed.
Dense.c:83:13: note: vectorized 0 loops in function.
Dense.c:86:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:108:17: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:108:17: note: SLPing BB part
Dense.c:125:17: note: Costing subgraph: 
Dense.c:125:17: note: node 0x75577a8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:125:17: note: op template: _49 = (sizetype) j_137;
Dense.c:125:17: note: 	stmt 0 _49 = (sizetype) j_137;
Dense.c:125:17: note: 	stmt 1 _209 = (sizetype) wi_136;
Dense.c:125:17: note: 	children 0x75578a8
Dense.c:125:17: note: node (external) 0x75578a8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:125:17: note: 	{ j_137, wi_136 }
Dense.c:125:17: note: Cost model analysis: 
Dense.c:125:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:125:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:125:17: missed: not vectorized: vectorization is not profitable.
Dense.c:108:17: note: Costing subgraph: 
Dense.c:108:17: note: node 0x75579a8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:108:17: note: op template: _24 = (sizetype) j_134;
Dense.c:108:17: note: 	stmt 0 _24 = (sizetype) j_134;
Dense.c:108:17: note: 	stmt 1 _229 = (sizetype) wi_133;
Dense.c:108:17: note: 	children 0x7557aa8
Dense.c:108:17: note: node (external) 0x7557aa8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:108:17: note: 	{ j_134, wi_133 }
Dense.c:108:17: note: Cost model analysis: 
Dense.c:108:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:108:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:108:17: missed: not vectorized: vectorization is not profitable.
Dense.c:108:17: note: ***** The result for vector mode V32QI would be the same
Dense.c:108:17: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:108:17: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:108:17: note: SLPing BB part
Dense.c:125:17: note: Costing subgraph: 
Dense.c:125:17: note: node 0x75579a8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:125:17: note: op template: _49 = (sizetype) j_137;
Dense.c:125:17: note: 	stmt 0 _49 = (sizetype) j_137;
Dense.c:125:17: note: 	stmt 1 _209 = (sizetype) wi_136;
Dense.c:125:17: note: 	children 0x75577a8
Dense.c:125:17: note: node (external) 0x75577a8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:125:17: note: 	{ j_137, wi_136 }
Dense.c:125:17: note: Cost model analysis: 
Dense.c:125:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:125:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:125:17: missed: not vectorized: vectorization is not profitable.
Dense.c:108:17: note: Costing subgraph: 
Dense.c:108:17: note: node 0x7557b28 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:108:17: note: op template: _24 = (sizetype) j_134;
Dense.c:108:17: note: 	stmt 0 _24 = (sizetype) j_134;
Dense.c:108:17: note: 	stmt 1 _229 = (sizetype) wi_133;
Dense.c:108:17: note: 	children 0x7557a28
Dense.c:108:17: note: node (external) 0x7557a28 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:108:17: note: 	{ j_134, wi_133 }
Dense.c:108:17: note: Cost model analysis: 
Dense.c:108:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:108:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:108:17: missed: not vectorized: vectorization is not profitable.
Dense.c:108:17: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:103:55: note: ***** Analysis failed with vector mode V8QI
Dense.c:103:55: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:103:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:61:23: missed: couldn't vectorize loop
Dense.c:77:45: missed: not vectorized: no vectype for stmt: _108 = *_109;
 scalar_type: float
Dense.c:61:23: missed: couldn't vectorize loop
Dense.c:61:23: missed: not vectorized: unsupported outerloop form.
Dense.c:64:38: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:905:10: missed: not vectorized: no vectype for stmt: _46 = MEM[(__m256_u * {ref-all})_6];
 scalar_type: __m256_u
Dense.c:54:13: note: vectorized 0 loops in function.
Dense.c:79:2: missed: statement clobbers memory: _25 (pretmp_156, _53, 0);
Dense.c:66:14: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:66:14: note: SLPing BB part
Dense.c:66:14: note: Costing subgraph: 
Dense.c:66:14: note: node 0x75877d8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:66:14: note: op template: _3 = (sizetype) j_69;
Dense.c:66:14: note: 	stmt 0 _3 = (sizetype) j_69;
Dense.c:66:14: note: 	stmt 1 _140 = (sizetype) wi_72;
Dense.c:66:14: note: 	children 0x75878d8
Dense.c:66:14: note: node (external) 0x75878d8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:66:14: note: 	{ j_69, wi_72 }
Dense.c:66:14: note: Cost model analysis: 
Dense.c:66:14: note: Scalar 1 and vector 2 loop part do not match up, skipping scalar part
Dense.c:66:14: note: Cost model analysis for part in loop 2:
  Vector cost: 36
  Scalar cost: 8
Dense.c:66:14: missed: not vectorized: vectorization is not profitable.
Dense.c:66:14: note: ***** The result for vector mode V32QI would be the same
Dense.c:66:14: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:66:14: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:66:14: note: SLPing BB part
Dense.c:66:14: note: Costing subgraph: 
Dense.c:66:14: note: node 0x75877d8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:66:14: note: op template: _3 = (sizetype) j_69;
Dense.c:66:14: note: 	stmt 0 _3 = (sizetype) j_69;
Dense.c:66:14: note: 	stmt 1 _140 = (sizetype) wi_72;
Dense.c:66:14: note: 	children 0x7587858
Dense.c:66:14: note: node (external) 0x7587858 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:66:14: note: 	{ j_69, wi_72 }
Dense.c:66:14: note: Cost model analysis: 
Dense.c:66:14: note: Scalar 1 and vector 2 loop part do not match up, skipping scalar part
Dense.c:66:14: note: Cost model analysis for part in loop 2:
  Vector cost: 36
  Scalar cost: 8
Dense.c:66:14: missed: not vectorized: vectorization is not profitable.
Dense.c:66:14: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:61:50: note: ***** Analysis failed with vector mode V8QI
Dense.c:61:50: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:61:50: note: ***** Analysis failed with vector mode V4QI
Dense.c:165:2: missed: statement clobbers memory: free (_1);
Dense.c:166:2: missed: statement clobbers memory: free (_2);
Dense.c:167:2: missed: statement clobbers memory: free (_3);
Dense.c:168:2: missed: statement clobbers memory: free (_4);
Dense.c:169:2: missed: statement clobbers memory: free (dense_7);
Dense.c:170:2: missed: statement clobbers memory: PULSE_DestroyLayer (this_6(D));
Dense.c:171:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:171:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:185:19: missed: couldn't vectorize loop
Dense.c:185:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:175:13: note: vectorized 0 loops in function.
Dense.c:177:47: missed: statement clobbers memory: dense_35 = malloc (40);
Dense.c:179:36: missed: statement clobbers memory: _5 = aligned_alloc (64, _4);
Dense.c:180:38: missed: statement clobbers memory: _6 = aligned_alloc (64, _4);
Dense.c:181:36: missed: statement clobbers memory: _8 = aligned_alloc (64, _7);
Dense.c:182:35: missed: statement clobbers memory: _9 = aligned_alloc (64, _7);
Dense.c:183:36: missed: statement clobbers memory: _10 = aligned_alloc (64, _7);
Dense.c:186:39: missed: statement clobbers memory: _11 = rand ();
Dense.c:186:73: missed: statement clobbers memory: _63 = sqrt (_18);
Dense.c:198:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:199:4: missed: statement clobbers memory: exit (1);
Dense.c:195:4: missed: statement clobbers memory: layer = PULSE_CreateLayer (n_inputs.3_29, n_outputs.2_28, 1, activation_function_51(D), _SIMD_FeedDense, _SIMD_BackDense, _SIMD_FixDense, _DestroyDense, 1); [return slot optimization]
Dense.c:192:12: missed: statement clobbers memory: layer = PULSE_CreateLayer (n_inputs.1_27, n_outputs.0_26, 1, activation_function_51(D), _FeedDense, _BackDense, _FixDense, _DestroyDense, 0); [return slot optimization]
Dense.c:205:9: missed: not vectorized: more than one data ref in stmt: <retval> = layer;
Dense.c:205:9: note: ***** Analysis failed with vector mode V4DI
Dense.c:205:9: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _30 (pretmp_91, _50, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:20: missed: not vectorized: no vectype for stmt: _21 = *_20;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:151:19: missed: couldn't vectorize loop
Dense.c:151:19: missed: not vectorized: number of iterations cannot be computed.
Dense.c:142:19: missed: couldn't vectorize loop
Dense.c:142:19: missed: not vectorized: number of iterations cannot be computed.
Dense.c:135:13: note: vectorized 0 loops in function.
Dense.c:159:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:159:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:104:28: missed: couldn't vectorize loop
Dense.c:104:28: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:107:21: missed: couldn't vectorize loop
Dense.c:107:21: missed: not vectorized: number of iterations cannot be computed.
Dense.c:121:28: missed: couldn't vectorize loop
Dense.c:121:28: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:124:21: missed: couldn't vectorize loop
Dense.c:124:21: missed: not vectorized: number of iterations cannot be computed.
Dense.c:90:19: missed: couldn't vectorize loop
Dense.c:90:19: missed: not vectorized: number of iterations cannot be computed.
Dense.c:84:13: note: vectorized 0 loops in function.
Dense.c:87:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:109:17: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:109:17: note: SLPing BB part
Dense.c:126:17: note: Costing subgraph: 
Dense.c:126:17: note: node 0x8478c18 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:126:17: note: op template: _49 = (sizetype) j_137;
Dense.c:126:17: note: 	stmt 0 _49 = (sizetype) j_137;
Dense.c:126:17: note: 	stmt 1 _209 = (sizetype) wi_136;
Dense.c:126:17: note: 	children 0x8478d18
Dense.c:126:17: note: node (external) 0x8478d18 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:126:17: note: 	{ j_137, wi_136 }
Dense.c:126:17: note: Cost model analysis: 
Dense.c:126:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:126:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:126:17: missed: not vectorized: vectorization is not profitable.
Dense.c:109:17: note: Costing subgraph: 
Dense.c:109:17: note: node 0x8478e18 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:109:17: note: op template: _24 = (sizetype) j_134;
Dense.c:109:17: note: 	stmt 0 _24 = (sizetype) j_134;
Dense.c:109:17: note: 	stmt 1 _229 = (sizetype) wi_133;
Dense.c:109:17: note: 	children 0x8478f18
Dense.c:109:17: note: node (external) 0x8478f18 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:109:17: note: 	{ j_134, wi_133 }
Dense.c:109:17: note: Cost model analysis: 
Dense.c:109:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:109:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:109:17: missed: not vectorized: vectorization is not profitable.
Dense.c:109:17: note: ***** The result for vector mode V32QI would be the same
Dense.c:109:17: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:109:17: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:109:17: note: SLPing BB part
Dense.c:126:17: note: Costing subgraph: 
Dense.c:126:17: note: node 0x8478e18 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:126:17: note: op template: _49 = (sizetype) j_137;
Dense.c:126:17: note: 	stmt 0 _49 = (sizetype) j_137;
Dense.c:126:17: note: 	stmt 1 _209 = (sizetype) wi_136;
Dense.c:126:17: note: 	children 0x8478c18
Dense.c:126:17: note: node (external) 0x8478c18 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:126:17: note: 	{ j_137, wi_136 }
Dense.c:126:17: note: Cost model analysis: 
Dense.c:126:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:126:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:126:17: missed: not vectorized: vectorization is not profitable.
Dense.c:109:17: note: Costing subgraph: 
Dense.c:109:17: note: node 0x8478f98 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:109:17: note: op template: _24 = (sizetype) j_134;
Dense.c:109:17: note: 	stmt 0 _24 = (sizetype) j_134;
Dense.c:109:17: note: 	stmt 1 _229 = (sizetype) wi_133;
Dense.c:109:17: note: 	children 0x8478e98
Dense.c:109:17: note: node (external) 0x8478e98 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:109:17: note: 	{ j_134, wi_133 }
Dense.c:109:17: note: Cost model analysis: 
Dense.c:109:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:109:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:109:17: missed: not vectorized: vectorization is not profitable.
Dense.c:109:17: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:104:55: note: ***** Analysis failed with vector mode V8QI
Dense.c:104:55: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:104:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:61:23: missed: couldn't vectorize loop
Dense.c:61:23: missed: not vectorized: unsupported outerloop form.
Dense.c:65:38: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:905:10: missed: not vectorized: no vectype for stmt: _44 = MEM[(__m256_u * {ref-all})_6];
 scalar_type: __m256_u
Dense.c:54:13: note: vectorized 0 loops in function.
Dense.c:80:2: missed: statement clobbers memory: _25 (pretmp_136, _51, 0);
Dense.c:67:14: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:67:14: note: SLPing BB part
Dense.c:67:14: note: Costing subgraph: 
Dense.c:67:14: note: node 0x84a8c48 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:67:14: note: op template: _3 = (sizetype) j_66;
Dense.c:67:14: note: 	stmt 0 _3 = (sizetype) j_66;
Dense.c:67:14: note: 	stmt 1 _124 = (sizetype) wi_67;
Dense.c:67:14: note: 	children 0x84a8d48
Dense.c:67:14: note: node (external) 0x84a8d48 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:67:14: note: 	{ j_66, wi_67 }
Dense.c:67:14: note: Cost model analysis: 
Dense.c:67:14: note: Scalar 1 and vector 2 loop part do not match up, skipping scalar part
Dense.c:67:14: note: Cost model analysis for part in loop 2:
  Vector cost: 36
  Scalar cost: 8
Dense.c:67:14: missed: not vectorized: vectorization is not profitable.
Dense.c:67:14: note: ***** The result for vector mode V32QI would be the same
Dense.c:67:14: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:67:14: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:67:14: note: SLPing BB part
Dense.c:67:14: note: Costing subgraph: 
Dense.c:67:14: note: node 0x84a8c48 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:67:14: note: op template: _3 = (sizetype) j_66;
Dense.c:67:14: note: 	stmt 0 _3 = (sizetype) j_66;
Dense.c:67:14: note: 	stmt 1 _124 = (sizetype) wi_67;
Dense.c:67:14: note: 	children 0x84a8dc8
Dense.c:67:14: note: node (external) 0x84a8dc8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:67:14: note: 	{ j_66, wi_67 }
Dense.c:67:14: note: Cost model analysis: 
Dense.c:67:14: note: Scalar 1 and vector 2 loop part do not match up, skipping scalar part
Dense.c:67:14: note: Cost model analysis for part in loop 2:
  Vector cost: 36
  Scalar cost: 8
Dense.c:67:14: missed: not vectorized: vectorization is not profitable.
Dense.c:67:14: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:61:50: note: ***** Analysis failed with vector mode V8QI
Dense.c:61:50: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:61:50: note: ***** Analysis failed with vector mode V4QI
Dense.c:166:2: missed: statement clobbers memory: free (_1);
Dense.c:167:2: missed: statement clobbers memory: free (_2);
Dense.c:168:2: missed: statement clobbers memory: free (_3);
Dense.c:169:2: missed: statement clobbers memory: free (_4);
Dense.c:170:2: missed: statement clobbers memory: free (dense_7);
Dense.c:171:2: missed: statement clobbers memory: PULSE_DestroyLayer (this_6(D));
Dense.c:172:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:172:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:186:19: missed: couldn't vectorize loop
Dense.c:186:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:176:13: note: vectorized 0 loops in function.
Dense.c:178:47: missed: statement clobbers memory: dense_35 = malloc (40);
Dense.c:180:36: missed: statement clobbers memory: _5 = aligned_alloc (64, _4);
Dense.c:181:38: missed: statement clobbers memory: _6 = aligned_alloc (64, _4);
Dense.c:182:36: missed: statement clobbers memory: _8 = aligned_alloc (64, _7);
Dense.c:183:35: missed: statement clobbers memory: _9 = aligned_alloc (64, _7);
Dense.c:184:36: missed: statement clobbers memory: _10 = aligned_alloc (64, _7);
Dense.c:187:39: missed: statement clobbers memory: _11 = rand ();
Dense.c:187:73: missed: statement clobbers memory: _63 = sqrt (_18);
Dense.c:199:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:200:4: missed: statement clobbers memory: exit (1);
Dense.c:196:4: missed: statement clobbers memory: layer = PULSE_CreateLayer (n_inputs.3_29, n_outputs.2_28, 1, activation_function_51(D), _SIMD_FeedDense, _SIMD_BackDense, _SIMD_FixDense, _DestroyDense, 1); [return slot optimization]
Dense.c:193:12: missed: statement clobbers memory: layer = PULSE_CreateLayer (n_inputs.1_27, n_outputs.0_26, 1, activation_function_51(D), _FeedDense, _BackDense, _FixDense, _DestroyDense, 0); [return slot optimization]
Dense.c:206:9: missed: not vectorized: more than one data ref in stmt: <retval> = layer;
Dense.c:206:9: note: ***** Analysis failed with vector mode V4DI
Dense.c:206:9: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _30 (pretmp_91, _50, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:20: missed: not vectorized: no vectype for stmt: _21 = *_20;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:152:19: missed: couldn't vectorize loop
Dense.c:152:19: missed: not vectorized: number of iterations cannot be computed.
Dense.c:143:19: missed: couldn't vectorize loop
Dense.c:143:19: missed: not vectorized: number of iterations cannot be computed.
Dense.c:136:13: note: vectorized 0 loops in function.
Dense.c:160:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:160:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:105:28: missed: couldn't vectorize loop
Dense.c:105:28: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:108:21: missed: couldn't vectorize loop
Dense.c:108:21: missed: not vectorized: number of iterations cannot be computed.
Dense.c:122:28: missed: couldn't vectorize loop
Dense.c:122:28: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:125:21: missed: couldn't vectorize loop
Dense.c:125:21: missed: not vectorized: number of iterations cannot be computed.
Dense.c:91:19: missed: couldn't vectorize loop
Dense.c:91:19: missed: not vectorized: number of iterations cannot be computed.
Dense.c:85:13: note: vectorized 0 loops in function.
Dense.c:88:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:110:17: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:110:17: note: SLPing BB part
Dense.c:127:17: note: Costing subgraph: 
Dense.c:127:17: note: node 0x8547448 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:127:17: note: op template: _49 = (sizetype) j_137;
Dense.c:127:17: note: 	stmt 0 _49 = (sizetype) j_137;
Dense.c:127:17: note: 	stmt 1 _209 = (sizetype) wi_136;
Dense.c:127:17: note: 	children 0x8547548
Dense.c:127:17: note: node (external) 0x8547548 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:127:17: note: 	{ j_137, wi_136 }
Dense.c:127:17: note: Cost model analysis: 
Dense.c:127:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:127:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:127:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: Costing subgraph: 
Dense.c:110:17: note: node 0x8547648 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:110:17: note: op template: _24 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 0 _24 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 1 _229 = (sizetype) wi_133;
Dense.c:110:17: note: 	children 0x8547748
Dense.c:110:17: note: node (external) 0x8547748 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:110:17: note: 	{ j_134, wi_133 }
Dense.c:110:17: note: Cost model analysis: 
Dense.c:110:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:110:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:110:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: ***** The result for vector mode V32QI would be the same
Dense.c:110:17: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:110:17: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:110:17: note: SLPing BB part
Dense.c:127:17: note: Costing subgraph: 
Dense.c:127:17: note: node 0x8547648 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:127:17: note: op template: _49 = (sizetype) j_137;
Dense.c:127:17: note: 	stmt 0 _49 = (sizetype) j_137;
Dense.c:127:17: note: 	stmt 1 _209 = (sizetype) wi_136;
Dense.c:127:17: note: 	children 0x8547448
Dense.c:127:17: note: node (external) 0x8547448 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:127:17: note: 	{ j_137, wi_136 }
Dense.c:127:17: note: Cost model analysis: 
Dense.c:127:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:127:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:127:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: Costing subgraph: 
Dense.c:110:17: note: node 0x85477c8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:110:17: note: op template: _24 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 0 _24 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 1 _229 = (sizetype) wi_133;
Dense.c:110:17: note: 	children 0x85476c8
Dense.c:110:17: note: node (external) 0x85476c8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:110:17: note: 	{ j_134, wi_133 }
Dense.c:110:17: note: Cost model analysis: 
Dense.c:110:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:110:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:110:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:105:55: note: ***** Analysis failed with vector mode V8QI
Dense.c:105:55: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:105:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:62:23: missed: couldn't vectorize loop
Dense.c:62:23: missed: not vectorized: unsupported outerloop form.
Dense.c:66:38: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:905:10: missed: not vectorized: no vectype for stmt: _41 = MEM[(__m256_u * {ref-all})_3];
 scalar_type: __m256_u
Dense.c:54:13: note: vectorized 0 loops in function.
Dense.c:81:2: missed: statement clobbers memory: _21 (pretmp_119, _48, 0);
Dense.c:82:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:82:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:167:2: missed: statement clobbers memory: free (_1);
Dense.c:168:2: missed: statement clobbers memory: free (_2);
Dense.c:169:2: missed: statement clobbers memory: free (_3);
Dense.c:170:2: missed: statement clobbers memory: free (_4);
Dense.c:171:2: missed: statement clobbers memory: free (dense_7);
Dense.c:172:2: missed: statement clobbers memory: PULSE_DestroyLayer (this_6(D));
Dense.c:173:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:173:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:187:19: missed: couldn't vectorize loop
Dense.c:187:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:177:13: note: vectorized 0 loops in function.
Dense.c:179:47: missed: statement clobbers memory: dense_35 = malloc (40);
Dense.c:181:36: missed: statement clobbers memory: _5 = aligned_alloc (64, _4);
Dense.c:182:38: missed: statement clobbers memory: _6 = aligned_alloc (64, _4);
Dense.c:183:36: missed: statement clobbers memory: _8 = aligned_alloc (64, _7);
Dense.c:184:35: missed: statement clobbers memory: _9 = aligned_alloc (64, _7);
Dense.c:185:36: missed: statement clobbers memory: _10 = aligned_alloc (64, _7);
Dense.c:188:39: missed: statement clobbers memory: _11 = rand ();
Dense.c:188:73: missed: statement clobbers memory: _63 = sqrt (_18);
Dense.c:200:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:201:4: missed: statement clobbers memory: exit (1);
Dense.c:197:4: missed: statement clobbers memory: layer = PULSE_CreateLayer (n_inputs.3_29, n_outputs.2_28, 1, activation_function_51(D), _SIMD_FeedDense, _SIMD_BackDense, _SIMD_FixDense, _DestroyDense, 1); [return slot optimization]
Dense.c:194:12: missed: statement clobbers memory: layer = PULSE_CreateLayer (n_inputs.1_27, n_outputs.0_26, 1, activation_function_51(D), _FeedDense, _BackDense, _FixDense, _DestroyDense, 0); [return slot optimization]
Dense.c:207:9: missed: not vectorized: more than one data ref in stmt: <retval> = layer;
Dense.c:207:9: note: ***** Analysis failed with vector mode V4DI
Dense.c:207:9: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _30 (pretmp_91, _50, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:20: missed: not vectorized: no vectype for stmt: _21 = *_20;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:152:19: missed: couldn't vectorize loop
Dense.c:152:19: missed: not vectorized: number of iterations cannot be computed.
Dense.c:143:19: missed: couldn't vectorize loop
Dense.c:143:19: missed: not vectorized: number of iterations cannot be computed.
Dense.c:136:13: note: vectorized 0 loops in function.
Dense.c:160:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:160:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:105:28: missed: couldn't vectorize loop
Dense.c:105:28: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:108:21: missed: couldn't vectorize loop
Dense.c:108:21: missed: not vectorized: number of iterations cannot be computed.
Dense.c:122:28: missed: couldn't vectorize loop
Dense.c:122:28: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:125:21: missed: couldn't vectorize loop
Dense.c:125:21: missed: not vectorized: number of iterations cannot be computed.
Dense.c:91:19: missed: couldn't vectorize loop
Dense.c:91:19: missed: not vectorized: number of iterations cannot be computed.
Dense.c:85:13: note: vectorized 0 loops in function.
Dense.c:88:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:110:17: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:110:17: note: SLPing BB part
Dense.c:127:17: note: Costing subgraph: 
Dense.c:127:17: note: node 0x8cfed58 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:127:17: note: op template: _49 = (sizetype) j_137;
Dense.c:127:17: note: 	stmt 0 _49 = (sizetype) j_137;
Dense.c:127:17: note: 	stmt 1 _209 = (sizetype) wi_136;
Dense.c:127:17: note: 	children 0x8cfee58
Dense.c:127:17: note: node (external) 0x8cfee58 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:127:17: note: 	{ j_137, wi_136 }
Dense.c:127:17: note: Cost model analysis: 
Dense.c:127:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:127:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:127:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: Costing subgraph: 
Dense.c:110:17: note: node 0x8cfef58 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:110:17: note: op template: _24 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 0 _24 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 1 _229 = (sizetype) wi_133;
Dense.c:110:17: note: 	children 0x8cff058
Dense.c:110:17: note: node (external) 0x8cff058 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:110:17: note: 	{ j_134, wi_133 }
Dense.c:110:17: note: Cost model analysis: 
Dense.c:110:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:110:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:110:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: ***** The result for vector mode V32QI would be the same
Dense.c:110:17: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:110:17: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:110:17: note: SLPing BB part
Dense.c:127:17: note: Costing subgraph: 
Dense.c:127:17: note: node 0x8cfef58 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:127:17: note: op template: _49 = (sizetype) j_137;
Dense.c:127:17: note: 	stmt 0 _49 = (sizetype) j_137;
Dense.c:127:17: note: 	stmt 1 _209 = (sizetype) wi_136;
Dense.c:127:17: note: 	children 0x8cfed58
Dense.c:127:17: note: node (external) 0x8cfed58 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:127:17: note: 	{ j_137, wi_136 }
Dense.c:127:17: note: Cost model analysis: 
Dense.c:127:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:127:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:127:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: Costing subgraph: 
Dense.c:110:17: note: node 0x8cff0d8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:110:17: note: op template: _24 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 0 _24 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 1 _229 = (sizetype) wi_133;
Dense.c:110:17: note: 	children 0x8cfefd8
Dense.c:110:17: note: node (external) 0x8cfefd8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:110:17: note: 	{ j_134, wi_133 }
Dense.c:110:17: note: Cost model analysis: 
Dense.c:110:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:110:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:110:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:105:55: note: ***** Analysis failed with vector mode V8QI
Dense.c:105:55: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:105:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:62:15: missed: couldn't vectorize loop
Dense.c:62:15: missed: not vectorized: unsupported outerloop form.
Dense.c:66:38: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:905:10: missed: not vectorized: no vectype for stmt: _41 = MEM[(__m256_u * {ref-all})_3];
 scalar_type: __m256_u
Dense.c:54:13: note: vectorized 0 loops in function.
Dense.c:81:2: missed: statement clobbers memory: _21 (pretmp_119, _48, 0);
Dense.c:82:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:82:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:167:2: missed: statement clobbers memory: free (_1);
Dense.c:168:2: missed: statement clobbers memory: free (_2);
Dense.c:169:2: missed: statement clobbers memory: free (_3);
Dense.c:170:2: missed: statement clobbers memory: free (_4);
Dense.c:171:2: missed: statement clobbers memory: free (dense_7);
Dense.c:172:2: missed: statement clobbers memory: PULSE_DestroyLayer (this_6(D));
Dense.c:173:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:173:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:187:19: missed: couldn't vectorize loop
Dense.c:187:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:177:13: note: vectorized 0 loops in function.
Dense.c:179:47: missed: statement clobbers memory: dense_35 = malloc (40);
Dense.c:181:36: missed: statement clobbers memory: _5 = aligned_alloc (64, _4);
Dense.c:182:38: missed: statement clobbers memory: _6 = aligned_alloc (64, _4);
Dense.c:183:36: missed: statement clobbers memory: _8 = aligned_alloc (64, _7);
Dense.c:184:35: missed: statement clobbers memory: _9 = aligned_alloc (64, _7);
Dense.c:185:36: missed: statement clobbers memory: _10 = aligned_alloc (64, _7);
Dense.c:188:39: missed: statement clobbers memory: _11 = rand ();
Dense.c:188:73: missed: statement clobbers memory: _63 = sqrt (_18);
Dense.c:200:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:201:4: missed: statement clobbers memory: exit (1);
Dense.c:197:4: missed: statement clobbers memory: layer = PULSE_CreateLayer (n_inputs.3_29, n_outputs.2_28, 1, activation_function_51(D), _SIMD_FeedDense, _SIMD_BackDense, _SIMD_FixDense, _DestroyDense, 1); [return slot optimization]
Dense.c:194:12: missed: statement clobbers memory: layer = PULSE_CreateLayer (n_inputs.1_27, n_outputs.0_26, 1, activation_function_51(D), _FeedDense, _BackDense, _FixDense, _DestroyDense, 0); [return slot optimization]
Dense.c:207:9: missed: not vectorized: more than one data ref in stmt: <retval> = layer;
Dense.c:207:9: note: ***** Analysis failed with vector mode V4DI
Dense.c:207:9: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _30 (pretmp_91, _50, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:20: missed: not vectorized: no vectype for stmt: _21 = *_20;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:149:19: missed: couldn't vectorize loop
Dense.c:149:19: missed: not vectorized: number of iterations cannot be computed.
Dense.c:140:19: missed: couldn't vectorize loop
Dense.c:140:19: missed: not vectorized: number of iterations cannot be computed.
Dense.c:133:13: note: vectorized 0 loops in function.
Dense.c:157:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:157:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:102:28: missed: couldn't vectorize loop
Dense.c:102:28: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:105:21: missed: couldn't vectorize loop
Dense.c:105:21: missed: not vectorized: number of iterations cannot be computed.
Dense.c:119:28: missed: couldn't vectorize loop
Dense.c:119:28: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:122:21: missed: couldn't vectorize loop
Dense.c:122:21: missed: not vectorized: number of iterations cannot be computed.
Dense.c:88:19: missed: couldn't vectorize loop
Dense.c:88:19: missed: not vectorized: number of iterations cannot be computed.
Dense.c:82:13: note: vectorized 0 loops in function.
Dense.c:85:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:107:17: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:107:17: note: SLPing BB part
Dense.c:124:17: note: Costing subgraph: 
Dense.c:124:17: note: node 0x73a72d8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:124:17: note: op template: _49 = (sizetype) j_137;
Dense.c:124:17: note: 	stmt 0 _49 = (sizetype) j_137;
Dense.c:124:17: note: 	stmt 1 _209 = (sizetype) wi_136;
Dense.c:124:17: note: 	children 0x73a73d8
Dense.c:124:17: note: node (external) 0x73a73d8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:124:17: note: 	{ j_137, wi_136 }
Dense.c:124:17: note: Cost model analysis: 
Dense.c:124:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:124:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:124:17: missed: not vectorized: vectorization is not profitable.
Dense.c:107:17: note: Costing subgraph: 
Dense.c:107:17: note: node 0x73a74d8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:107:17: note: op template: _24 = (sizetype) j_134;
Dense.c:107:17: note: 	stmt 0 _24 = (sizetype) j_134;
Dense.c:107:17: note: 	stmt 1 _229 = (sizetype) wi_133;
Dense.c:107:17: note: 	children 0x73a75d8
Dense.c:107:17: note: node (external) 0x73a75d8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:107:17: note: 	{ j_134, wi_133 }
Dense.c:107:17: note: Cost model analysis: 
Dense.c:107:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:107:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:107:17: missed: not vectorized: vectorization is not profitable.
Dense.c:107:17: note: ***** The result for vector mode V32QI would be the same
Dense.c:107:17: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:107:17: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:107:17: note: SLPing BB part
Dense.c:124:17: note: Costing subgraph: 
Dense.c:124:17: note: node 0x73a74d8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:124:17: note: op template: _49 = (sizetype) j_137;
Dense.c:124:17: note: 	stmt 0 _49 = (sizetype) j_137;
Dense.c:124:17: note: 	stmt 1 _209 = (sizetype) wi_136;
Dense.c:124:17: note: 	children 0x73a72d8
Dense.c:124:17: note: node (external) 0x73a72d8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:124:17: note: 	{ j_137, wi_136 }
Dense.c:124:17: note: Cost model analysis: 
Dense.c:124:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:124:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:124:17: missed: not vectorized: vectorization is not profitable.
Dense.c:107:17: note: Costing subgraph: 
Dense.c:107:17: note: node 0x73a7658 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:107:17: note: op template: _24 = (sizetype) j_134;
Dense.c:107:17: note: 	stmt 0 _24 = (sizetype) j_134;
Dense.c:107:17: note: 	stmt 1 _229 = (sizetype) wi_133;
Dense.c:107:17: note: 	children 0x73a7558
Dense.c:107:17: note: node (external) 0x73a7558 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:107:17: note: 	{ j_134, wi_133 }
Dense.c:107:17: note: Cost model analysis: 
Dense.c:107:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:107:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:107:17: missed: not vectorized: vectorization is not profitable.
Dense.c:107:17: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:102:55: note: ***** Analysis failed with vector mode V8QI
Dense.c:102:55: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:102:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:61:15: missed: couldn't vectorize loop
Dense.c:61:15: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:74:11: optimized: loop vectorized using 32 byte vectors
Dense.c:74:11: optimized: loop vectorized using 16 byte vectors
Dense.c:65:38: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:881:10: missed: not vectorized: no vectype for stmt: _52 = MEM[(__m256 * {ref-all})w_ptr_81];
 scalar_type: __m256
Dense.c:54:13: note: vectorized 1 loops in function.
Dense.c:78:2: missed: statement clobbers memory: _26 (pretmp_158, _63, 0);
Dense.c:54:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:54:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:164:2: missed: statement clobbers memory: free (_1);
Dense.c:165:2: missed: statement clobbers memory: free (_2);
Dense.c:166:2: missed: statement clobbers memory: free (_3);
Dense.c:167:2: missed: statement clobbers memory: free (_4);
Dense.c:168:2: missed: statement clobbers memory: free (dense_7);
Dense.c:169:2: missed: statement clobbers memory: PULSE_DestroyLayer (this_6(D));
Dense.c:170:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:170:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:184:19: missed: couldn't vectorize loop
Dense.c:184:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:174:13: note: vectorized 0 loops in function.
Dense.c:176:47: missed: statement clobbers memory: dense_35 = malloc (40);
Dense.c:178:36: missed: statement clobbers memory: _5 = aligned_alloc (64, _4);
Dense.c:179:38: missed: statement clobbers memory: _6 = aligned_alloc (64, _4);
Dense.c:180:36: missed: statement clobbers memory: _8 = aligned_alloc (64, _7);
Dense.c:181:35: missed: statement clobbers memory: _9 = aligned_alloc (64, _7);
Dense.c:182:36: missed: statement clobbers memory: _10 = aligned_alloc (64, _7);
Dense.c:185:39: missed: statement clobbers memory: _11 = rand ();
Dense.c:185:73: missed: statement clobbers memory: _63 = sqrt (_18);
Dense.c:197:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:198:4: missed: statement clobbers memory: exit (1);
Dense.c:194:4: missed: statement clobbers memory: layer = PULSE_CreateLayer (n_inputs.3_29, n_outputs.2_28, 1, activation_function_51(D), _SIMD_FeedDense, _SIMD_BackDense, _SIMD_FixDense, _DestroyDense, 1); [return slot optimization]
Dense.c:191:12: missed: statement clobbers memory: layer = PULSE_CreateLayer (n_inputs.1_27, n_outputs.0_26, 1, activation_function_51(D), _FeedDense, _BackDense, _FixDense, _DestroyDense, 0); [return slot optimization]
Dense.c:204:9: missed: not vectorized: more than one data ref in stmt: <retval> = layer;
Dense.c:204:9: note: ***** Analysis failed with vector mode V4DI
Dense.c:204:9: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Layer.c:7:34: missed: statement clobbers memory: _3 = aligned_alloc (64, _2);
Layer.c:8:35: missed: statement clobbers memory: _6 = aligned_alloc (64, _5);
Layer.c:9:34: missed: statement clobbers memory: _7 = aligned_alloc (64, _5);
Layer.c:14:19: missed: statement clobbers memory: _8 = PULSE_GetActivationFunctionPtr (activation_function_18(D));
Layer.c:19:9: note: ***** Analysis succeeded with vector mode V8SI
Layer.c:19:9: note: SLPing BB part
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x4d017c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: op template: MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: 	stmt 0 MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: 	stmt 1 MEM <unsigned int> [(struct PULSE_Layer *)&<retval> + 4B] = optimization_type_17(D);
Layer.c:19:9: note: 	children 0x4d01848
Layer.c:19:9: note: node (external) 0x4d01848 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ type_16(D), optimization_type_17(D) }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x4d018c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: op template: <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: 	stmt 0 <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: 	stmt 1 <retval>.n_outputs = n_outputs_12(D);
Layer.c:19:9: note: 	children 0x4d01948
Layer.c:19:9: note: node (external) 0x4d01948 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ n_inputs_11(D), n_outputs_12(D) }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x4d01a48 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: <retval>.inputs = _3;
Layer.c:19:9: note: 	stmt 0 <retval>.inputs = _3;
Layer.c:19:9: note: 	stmt 1 <retval>.outputs = _6;
Layer.c:19:9: note: 	children 0x4d01ac8
Layer.c:19:9: note: node (external) 0x4d01ac8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: 	stmt 0 _3 = aligned_alloc (64, _2);
Layer.c:19:9: note: 	stmt 1 _6 = aligned_alloc (64, _5);
Layer.c:19:9: note: 	children 0x4d01b48 0x4d01bc8
Layer.c:19:9: note: node (constant) 0x4d01b48 (max_nunits=1, refcnt=1)
Layer.c:19:9: note: 	{ 64, 64 }
Layer.c:19:9: note: node 0x4d01bc8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: patt_40 = n_inputs_11(D) w* 4;
Layer.c:19:9: note: 	stmt 0 patt_40 = n_inputs_11(D) w* 4;
Layer.c:19:9: note: 	stmt 1 patt_41 = n_outputs_12(D) w* 4;
Layer.c:19:9: note: 	children 0x4d01c48 0x4d01cc8
Layer.c:19:9: note: node (external) 0x4d01c48 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ n_inputs_11(D), n_outputs_12(D) }
Layer.c:19:9: note: node (constant) 0x4d01cc8 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ 4, 4 }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x4d01dc8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Layer.c:19:9: note: op template: <retval>.feed = feed_20(D);
Layer.c:19:9: note: 	stmt 0 <retval>.feed = feed_20(D);
Layer.c:19:9: note: 	stmt 1 <retval>.back = back_21(D);
Layer.c:19:9: note: 	stmt 2 <retval>.fix = fix_22(D);
Layer.c:19:9: note: 	stmt 3 <retval>.destroy = destroy_23(D);
Layer.c:19:9: note: 	children 0x4d01e48
Layer.c:19:9: note: node (external) 0x4d01e48 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Layer.c:19:9: note: 	{ feed_20(D), back_21(D), fix_22(D), destroy_23(D) }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 64
  Scalar cost: 64
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x4d01f48 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: <retval>.parent = 0B;
Layer.c:19:9: note: 	stmt 0 <retval>.parent = 0B;
Layer.c:19:9: note: 	stmt 1 <retval>.child = 0B;
Layer.c:19:9: note: 	children 0x4d01fc8
Layer.c:19:9: note: node (constant) 0x4d01fc8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: 	{ 0B, 0B }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Layer.c:19:9: note: Basic block will be vectorized using SLP
Layer.c:19:9: optimized: basic block part vectorized using 8 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x4d017c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: op template: MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: 	stmt 0 MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: 	stmt 1 MEM <unsigned int> [(struct PULSE_Layer *)&<retval> + 4B] = optimization_type_17(D);
Layer.c:19:9: note: 	children 0x4d01848
Layer.c:19:9: note: node (external) 0x4d01848 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ type_16(D), optimization_type_17(D) }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: vect_is_simple_use: operand optimization_type_17(D), type of def: external
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: MEM <unsigned int> [(struct PULSE_Layer *)&<retval>]
Layer.c:19:9: note: created &MEM <unsigned int> [(struct PULSE_Layer *)&<retval>]
Layer.c:19:9: note: add new stmt: MEM <vector(2) unsigned int> [(struct PULSE_Layer *)&<retval>] = _42;
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: optimized: basic block part vectorized using 8 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x4d018c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: op template: <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: 	stmt 0 <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: 	stmt 1 <retval>.n_outputs = n_outputs_12(D);
Layer.c:19:9: note: 	children 0x4d01948
Layer.c:19:9: note: node (external) 0x4d01948 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ n_inputs_11(D), n_outputs_12(D) }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: vect_is_simple_use: operand n_outputs_12(D), type of def: external
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: <retval>.n_inputs
Layer.c:19:9: note: created &<retval>.n_inputs
Layer.c:19:9: note: add new stmt: MEM <vector(2) unsigned int> [(unsigned int *)&<retval> + 96B] = _44;
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: optimized: basic block part vectorized using 16 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x4d01a48 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: <retval>.inputs = _3;
Layer.c:19:9: note: 	stmt 0 <retval>.inputs = _3;
Layer.c:19:9: note: 	stmt 1 <retval>.outputs = _6;
Layer.c:19:9: note: 	children 0x4d01ac8
Layer.c:19:9: note: node (external) 0x4d01ac8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: 	stmt 0 _3 = aligned_alloc (64, _2);
Layer.c:19:9: note: 	stmt 1 _6 = aligned_alloc (64, _5);
Layer.c:19:9: note: 	children 0x4d01b48 0x4d01bc8
Layer.c:19:9: note: node (constant) 0x4d01b48 (max_nunits=1, refcnt=1)
Layer.c:19:9: note: 	{ 64, 64 }
Layer.c:19:9: note: node 0x4d01bc8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: patt_40 = n_inputs_11(D) w* 4;
Layer.c:19:9: note: 	stmt 0 patt_40 = n_inputs_11(D) w* 4;
Layer.c:19:9: note: 	stmt 1 patt_41 = n_outputs_12(D) w* 4;
Layer.c:19:9: note: 	children 0x4d01c48 0x4d01cc8
Layer.c:19:9: note: node (external) 0x4d01c48 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ n_inputs_11(D), n_outputs_12(D) }
Layer.c:19:9: note: node (constant) 0x4d01cc8 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ 4, 4 }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: <retval>.inputs = _3;
Layer.c:19:9: note: vect_is_simple_use: operand aligned_alloc (64, _5), type of def: internal
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.inputs
Layer.c:19:9: note: created &<retval>.inputs
Layer.c:19:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval> + 8B] = _48;
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: optimized: basic block part vectorized using 32 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x4d01dc8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Layer.c:19:9: note: op template: <retval>.feed = feed_20(D);
Layer.c:19:9: note: 	stmt 0 <retval>.feed = feed_20(D);
Layer.c:19:9: note: 	stmt 1 <retval>.back = back_21(D);
Layer.c:19:9: note: 	stmt 2 <retval>.fix = fix_22(D);
Layer.c:19:9: note: 	stmt 3 <retval>.destroy = destroy_23(D);
Layer.c:19:9: note: 	children 0x4d01e48
Layer.c:19:9: note: node (external) 0x4d01e48 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Layer.c:19:9: note: 	{ feed_20(D), back_21(D), fix_22(D), destroy_23(D) }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: <retval>.feed = feed_20(D);
Layer.c:19:9: note: vect_is_simple_use: operand back_21(D), type of def: external
Layer.c:19:9: note: vect_is_simple_use: operand fix_22(D), type of def: external
Layer.c:19:9: note: vect_is_simple_use: operand destroy_23(D), type of def: external
Layer.c:19:9: note: conflicting alias set types.
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(4) long unsigned int  vectorizing a pointer ref: <retval>.feed
Layer.c:19:9: note: created &<retval>.feed
Layer.c:19:9: note: add new stmt: MEM <vector(4) long unsigned int> [(void *)&<retval> + 32B] = _54;
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: optimized: basic block part vectorized using 16 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x4d01f48 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: <retval>.parent = 0B;
Layer.c:19:9: note: 	stmt 0 <retval>.parent = 0B;
Layer.c:19:9: note: 	stmt 1 <retval>.child = 0B;
Layer.c:19:9: note: 	children 0x4d01fc8
Layer.c:19:9: note: node (constant) 0x4d01fc8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: 	{ 0B, 0B }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: <retval>.parent = 0B;
Layer.c:19:9: note: vect_is_simple_use: operand 0B, type of def: constant
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.parent
Layer.c:19:9: note: created &<retval>.parent
Layer.c:19:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer * *)&<retval> + 72B] = { 0, 0 };
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: note: ***** The result for vector mode V32QI would be the same
Layer.c:24:2: missed: statement clobbers memory: free (_1);
Layer.c:25:2: missed: statement clobbers memory: free (_2);
Layer.c:26:2: missed: statement clobbers memory: free (_3);
Layer.c:27:1: note: ***** Analysis failed with vector mode V4DI
Layer.c:27:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _30 (pretmp_91, _50, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:20: missed: not vectorized: no vectype for stmt: _21 = *_20;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:149:19: missed: couldn't vectorize loop
Dense.c:149:19: missed: not vectorized: number of iterations cannot be computed.
Dense.c:140:19: missed: couldn't vectorize loop
Dense.c:140:19: missed: not vectorized: number of iterations cannot be computed.
Dense.c:133:13: note: vectorized 0 loops in function.
Dense.c:157:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:157:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:102:28: missed: couldn't vectorize loop
Dense.c:102:28: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:105:21: missed: couldn't vectorize loop
Dense.c:105:21: missed: not vectorized: number of iterations cannot be computed.
Dense.c:119:28: missed: couldn't vectorize loop
Dense.c:119:28: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:122:21: missed: couldn't vectorize loop
Dense.c:122:21: missed: not vectorized: number of iterations cannot be computed.
Dense.c:88:19: missed: couldn't vectorize loop
Dense.c:88:19: missed: not vectorized: number of iterations cannot be computed.
Dense.c:82:13: note: vectorized 0 loops in function.
Dense.c:85:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:107:17: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:107:17: note: SLPing BB part
Dense.c:124:17: note: Costing subgraph: 
Dense.c:124:17: note: node 0x88a43b8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:124:17: note: op template: _49 = (sizetype) j_137;
Dense.c:124:17: note: 	stmt 0 _49 = (sizetype) j_137;
Dense.c:124:17: note: 	stmt 1 _209 = (sizetype) wi_136;
Dense.c:124:17: note: 	children 0x88a44b8
Dense.c:124:17: note: node (external) 0x88a44b8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:124:17: note: 	{ j_137, wi_136 }
Dense.c:124:17: note: Cost model analysis: 
Dense.c:124:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:124:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:124:17: missed: not vectorized: vectorization is not profitable.
Dense.c:107:17: note: Costing subgraph: 
Dense.c:107:17: note: node 0x88a45b8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:107:17: note: op template: _24 = (sizetype) j_134;
Dense.c:107:17: note: 	stmt 0 _24 = (sizetype) j_134;
Dense.c:107:17: note: 	stmt 1 _229 = (sizetype) wi_133;
Dense.c:107:17: note: 	children 0x88a46b8
Dense.c:107:17: note: node (external) 0x88a46b8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:107:17: note: 	{ j_134, wi_133 }
Dense.c:107:17: note: Cost model analysis: 
Dense.c:107:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:107:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:107:17: missed: not vectorized: vectorization is not profitable.
Dense.c:107:17: note: ***** The result for vector mode V32QI would be the same
Dense.c:107:17: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:107:17: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:107:17: note: SLPing BB part
Dense.c:124:17: note: Costing subgraph: 
Dense.c:124:17: note: node 0x88a45b8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:124:17: note: op template: _49 = (sizetype) j_137;
Dense.c:124:17: note: 	stmt 0 _49 = (sizetype) j_137;
Dense.c:124:17: note: 	stmt 1 _209 = (sizetype) wi_136;
Dense.c:124:17: note: 	children 0x88a43b8
Dense.c:124:17: note: node (external) 0x88a43b8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:124:17: note: 	{ j_137, wi_136 }
Dense.c:124:17: note: Cost model analysis: 
Dense.c:124:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:124:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:124:17: missed: not vectorized: vectorization is not profitable.
Dense.c:107:17: note: Costing subgraph: 
Dense.c:107:17: note: node 0x88a4738 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:107:17: note: op template: _24 = (sizetype) j_134;
Dense.c:107:17: note: 	stmt 0 _24 = (sizetype) j_134;
Dense.c:107:17: note: 	stmt 1 _229 = (sizetype) wi_133;
Dense.c:107:17: note: 	children 0x88a4638
Dense.c:107:17: note: node (external) 0x88a4638 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:107:17: note: 	{ j_134, wi_133 }
Dense.c:107:17: note: Cost model analysis: 
Dense.c:107:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:107:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:107:17: missed: not vectorized: vectorization is not profitable.
Dense.c:107:17: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:102:55: note: ***** Analysis failed with vector mode V8QI
Dense.c:102:55: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:102:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:61:15: missed: couldn't vectorize loop
Dense.c:61:15: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:74:11: optimized: loop vectorized using 32 byte vectors
Dense.c:74:11: optimized: loop vectorized using 16 byte vectors
Dense.c:65:11: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:881:10: missed: not vectorized: no vectype for stmt: _52 = MEM[(__m256 * {ref-all})w_ptr_77];
 scalar_type: __m256
Dense.c:54:13: note: vectorized 1 loops in function.
Dense.c:78:2: missed: statement clobbers memory: _25 (pretmp_158, _63, 0);
Dense.c:54:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:54:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:164:2: missed: statement clobbers memory: free (_1);
Dense.c:165:2: missed: statement clobbers memory: free (_2);
Dense.c:166:2: missed: statement clobbers memory: free (_3);
Dense.c:167:2: missed: statement clobbers memory: free (_4);
Dense.c:168:2: missed: statement clobbers memory: free (dense_7);
Dense.c:169:2: missed: statement clobbers memory: PULSE_DestroyLayer (this_6(D));
Dense.c:170:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:170:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:184:19: missed: couldn't vectorize loop
Dense.c:184:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:174:13: note: vectorized 0 loops in function.
Dense.c:176:47: missed: statement clobbers memory: dense_35 = malloc (40);
Dense.c:178:36: missed: statement clobbers memory: _5 = aligned_alloc (64, _4);
Dense.c:179:38: missed: statement clobbers memory: _6 = aligned_alloc (64, _4);
Dense.c:180:36: missed: statement clobbers memory: _8 = aligned_alloc (64, _7);
Dense.c:181:35: missed: statement clobbers memory: _9 = aligned_alloc (64, _7);
Dense.c:182:36: missed: statement clobbers memory: _10 = aligned_alloc (64, _7);
Dense.c:185:39: missed: statement clobbers memory: _11 = rand ();
Dense.c:185:73: missed: statement clobbers memory: _63 = sqrt (_18);
Dense.c:197:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:198:4: missed: statement clobbers memory: exit (1);
Dense.c:194:4: missed: statement clobbers memory: layer = PULSE_CreateLayer (n_inputs.3_29, n_outputs.2_28, 1, activation_function_51(D), _SIMD_FeedDense, _SIMD_BackDense, _SIMD_FixDense, _DestroyDense, 1); [return slot optimization]
Dense.c:191:12: missed: statement clobbers memory: layer = PULSE_CreateLayer (n_inputs.1_27, n_outputs.0_26, 1, activation_function_51(D), _FeedDense, _BackDense, _FixDense, _DestroyDense, 0); [return slot optimization]
Dense.c:204:9: missed: not vectorized: more than one data ref in stmt: <retval> = layer;
Dense.c:204:9: note: ***** Analysis failed with vector mode V4DI
Dense.c:204:9: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Activations.c:17:19: optimized: loop vectorized using 32 byte vectors
Activations.c:17:19: optimized: loop vectorized using 16 byte vectors
Activations.c:17:19: optimized: loop vectorized using 32 byte vectors
Activations.c:17:19: optimized: loop vectorized using 16 byte vectors
Activations.c:14:13: note: vectorized 2 loops in function.
Activations.c:14:13: note: ***** Analysis failed with vector mode V8SF
Activations.c:14:13: note: ***** The result for vector mode V32QI would be the same
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V16QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V16QI
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V8QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V8QI
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V4QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V4QI
Activations.c:28:19: missed: couldn't vectorize loop
Activations.c:28:19: missed: not vectorized: unsupported control flow in loop.
Activations.c:28:19: optimized: loop vectorized using 32 byte vectors
Activations.c:28:19: optimized: loop vectorized using 16 byte vectors
Activations.c:25:13: note: vectorized 1 loops in function.
Activations.c:25:13: note: ***** Analysis failed with vector mode V8SF
Activations.c:25:13: note: ***** The result for vector mode V32QI would be the same
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V16QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V16QI
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V8QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V8QI
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V4QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V4QI
Activations.c:38:1: note: ***** Analysis failed with vector mode VOID
Activations.c:5:19: missed: couldn't vectorize loop
Activations.c:9:21: missed: statement clobbers memory: _36 = expf (_18);
Activations.c:5:19: missed: couldn't vectorize loop
Activations.c:9:21: missed: statement clobbers memory: _12 = expf (_11);
Activations.c:3:13: note: vectorized 0 loops in function.
Activations.c:9:21: missed: statement clobbers memory: _12 = expf (_11);
Activations.c:9:21: missed: statement clobbers memory: _36 = expf (_18);
Activations.c:11:1: note: ***** Analysis failed with vector mode V8SF
Activations.c:11:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SF
Activations.c:44:2: note: ***** Analysis failed with vector mode VOID
Layer.c:7:34: missed: statement clobbers memory: _3 = aligned_alloc (64, _2);
Layer.c:8:35: missed: statement clobbers memory: _6 = aligned_alloc (64, _5);
Layer.c:9:34: missed: statement clobbers memory: _7 = aligned_alloc (64, _5);
Layer.c:14:19: missed: statement clobbers memory: _8 = PULSE_GetActivationFunctionPtr (activation_function_18(D));
Layer.c:19:9: note: ***** Analysis succeeded with vector mode V8SI
Layer.c:19:9: note: SLPing BB part
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x4d277f8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: op template: MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: 	stmt 0 MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: 	stmt 1 MEM <unsigned int> [(struct PULSE_Layer *)&<retval> + 4B] = optimization_type_17(D);
Layer.c:19:9: note: 	children 0x4d27878
Layer.c:19:9: note: node (external) 0x4d27878 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ type_16(D), optimization_type_17(D) }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x4d278f8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: op template: <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: 	stmt 0 <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: 	stmt 1 <retval>.n_outputs = n_outputs_12(D);
Layer.c:19:9: note: 	children 0x4d27978
Layer.c:19:9: note: node (external) 0x4d27978 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ n_inputs_11(D), n_outputs_12(D) }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x4d27a78 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: <retval>.inputs = _3;
Layer.c:19:9: note: 	stmt 0 <retval>.inputs = _3;
Layer.c:19:9: note: 	stmt 1 <retval>.outputs = _6;
Layer.c:19:9: note: 	children 0x4d27af8
Layer.c:19:9: note: node (external) 0x4d27af8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: 	stmt 0 _3 = aligned_alloc (64, _2);
Layer.c:19:9: note: 	stmt 1 _6 = aligned_alloc (64, _5);
Layer.c:19:9: note: 	children 0x4d27b78 0x4d27bf8
Layer.c:19:9: note: node (constant) 0x4d27b78 (max_nunits=1, refcnt=1)
Layer.c:19:9: note: 	{ 64, 64 }
Layer.c:19:9: note: node 0x4d27bf8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: patt_40 = n_inputs_11(D) w* 4;
Layer.c:19:9: note: 	stmt 0 patt_40 = n_inputs_11(D) w* 4;
Layer.c:19:9: note: 	stmt 1 patt_41 = n_outputs_12(D) w* 4;
Layer.c:19:9: note: 	children 0x4d27c78 0x4d27cf8
Layer.c:19:9: note: node (external) 0x4d27c78 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ n_inputs_11(D), n_outputs_12(D) }
Layer.c:19:9: note: node (constant) 0x4d27cf8 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ 4, 4 }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x4d27df8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Layer.c:19:9: note: op template: <retval>.feed = feed_20(D);
Layer.c:19:9: note: 	stmt 0 <retval>.feed = feed_20(D);
Layer.c:19:9: note: 	stmt 1 <retval>.back = back_21(D);
Layer.c:19:9: note: 	stmt 2 <retval>.fix = fix_22(D);
Layer.c:19:9: note: 	stmt 3 <retval>.destroy = destroy_23(D);
Layer.c:19:9: note: 	children 0x4d27e78
Layer.c:19:9: note: node (external) 0x4d27e78 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Layer.c:19:9: note: 	{ feed_20(D), back_21(D), fix_22(D), destroy_23(D) }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 64
  Scalar cost: 64
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x4d27f78 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: <retval>.parent = 0B;
Layer.c:19:9: note: 	stmt 0 <retval>.parent = 0B;
Layer.c:19:9: note: 	stmt 1 <retval>.child = 0B;
Layer.c:19:9: note: 	children 0x4d27ff8
Layer.c:19:9: note: node (constant) 0x4d27ff8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: 	{ 0B, 0B }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Layer.c:19:9: note: Basic block will be vectorized using SLP
Layer.c:19:9: optimized: basic block part vectorized using 8 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x4d277f8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: op template: MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: 	stmt 0 MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: 	stmt 1 MEM <unsigned int> [(struct PULSE_Layer *)&<retval> + 4B] = optimization_type_17(D);
Layer.c:19:9: note: 	children 0x4d27878
Layer.c:19:9: note: node (external) 0x4d27878 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ type_16(D), optimization_type_17(D) }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: vect_is_simple_use: operand optimization_type_17(D), type of def: external
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: MEM <unsigned int> [(struct PULSE_Layer *)&<retval>]
Layer.c:19:9: note: created &MEM <unsigned int> [(struct PULSE_Layer *)&<retval>]
Layer.c:19:9: note: add new stmt: MEM <vector(2) unsigned int> [(struct PULSE_Layer *)&<retval>] = _42;
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: optimized: basic block part vectorized using 8 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x4d278f8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: op template: <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: 	stmt 0 <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: 	stmt 1 <retval>.n_outputs = n_outputs_12(D);
Layer.c:19:9: note: 	children 0x4d27978
Layer.c:19:9: note: node (external) 0x4d27978 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ n_inputs_11(D), n_outputs_12(D) }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: vect_is_simple_use: operand n_outputs_12(D), type of def: external
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: <retval>.n_inputs
Layer.c:19:9: note: created &<retval>.n_inputs
Layer.c:19:9: note: add new stmt: MEM <vector(2) unsigned int> [(unsigned int *)&<retval> + 96B] = _44;
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: optimized: basic block part vectorized using 16 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x4d27a78 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: <retval>.inputs = _3;
Layer.c:19:9: note: 	stmt 0 <retval>.inputs = _3;
Layer.c:19:9: note: 	stmt 1 <retval>.outputs = _6;
Layer.c:19:9: note: 	children 0x4d27af8
Layer.c:19:9: note: node (external) 0x4d27af8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: 	stmt 0 _3 = aligned_alloc (64, _2);
Layer.c:19:9: note: 	stmt 1 _6 = aligned_alloc (64, _5);
Layer.c:19:9: note: 	children 0x4d27b78 0x4d27bf8
Layer.c:19:9: note: node (constant) 0x4d27b78 (max_nunits=1, refcnt=1)
Layer.c:19:9: note: 	{ 64, 64 }
Layer.c:19:9: note: node 0x4d27bf8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: patt_40 = n_inputs_11(D) w* 4;
Layer.c:19:9: note: 	stmt 0 patt_40 = n_inputs_11(D) w* 4;
Layer.c:19:9: note: 	stmt 1 patt_41 = n_outputs_12(D) w* 4;
Layer.c:19:9: note: 	children 0x4d27c78 0x4d27cf8
Layer.c:19:9: note: node (external) 0x4d27c78 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ n_inputs_11(D), n_outputs_12(D) }
Layer.c:19:9: note: node (constant) 0x4d27cf8 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ 4, 4 }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: <retval>.inputs = _3;
Layer.c:19:9: note: vect_is_simple_use: operand aligned_alloc (64, _5), type of def: internal
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.inputs
Layer.c:19:9: note: created &<retval>.inputs
Layer.c:19:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval> + 8B] = _48;
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: optimized: basic block part vectorized using 32 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x4d27df8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Layer.c:19:9: note: op template: <retval>.feed = feed_20(D);
Layer.c:19:9: note: 	stmt 0 <retval>.feed = feed_20(D);
Layer.c:19:9: note: 	stmt 1 <retval>.back = back_21(D);
Layer.c:19:9: note: 	stmt 2 <retval>.fix = fix_22(D);
Layer.c:19:9: note: 	stmt 3 <retval>.destroy = destroy_23(D);
Layer.c:19:9: note: 	children 0x4d27e78
Layer.c:19:9: note: node (external) 0x4d27e78 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Layer.c:19:9: note: 	{ feed_20(D), back_21(D), fix_22(D), destroy_23(D) }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: <retval>.feed = feed_20(D);
Layer.c:19:9: note: vect_is_simple_use: operand back_21(D), type of def: external
Layer.c:19:9: note: vect_is_simple_use: operand fix_22(D), type of def: external
Layer.c:19:9: note: vect_is_simple_use: operand destroy_23(D), type of def: external
Layer.c:19:9: note: conflicting alias set types.
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(4) long unsigned int  vectorizing a pointer ref: <retval>.feed
Layer.c:19:9: note: created &<retval>.feed
Layer.c:19:9: note: add new stmt: MEM <vector(4) long unsigned int> [(void *)&<retval> + 32B] = _54;
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: optimized: basic block part vectorized using 16 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x4d27f78 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: <retval>.parent = 0B;
Layer.c:19:9: note: 	stmt 0 <retval>.parent = 0B;
Layer.c:19:9: note: 	stmt 1 <retval>.child = 0B;
Layer.c:19:9: note: 	children 0x4d27ff8
Layer.c:19:9: note: node (constant) 0x4d27ff8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: 	{ 0B, 0B }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: <retval>.parent = 0B;
Layer.c:19:9: note: vect_is_simple_use: operand 0B, type of def: constant
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.parent
Layer.c:19:9: note: created &<retval>.parent
Layer.c:19:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer * *)&<retval> + 72B] = { 0, 0 };
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: note: ***** The result for vector mode V32QI would be the same
Layer.c:24:2: missed: statement clobbers memory: free (_1);
Layer.c:25:2: missed: statement clobbers memory: free (_2);
Layer.c:26:2: missed: statement clobbers memory: free (_3);
Layer.c:27:1: note: ***** Analysis failed with vector mode V4DI
Layer.c:27:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _30 (pretmp_91, _50, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:20: missed: not vectorized: no vectype for stmt: _21 = *_20;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:149:19: missed: couldn't vectorize loop
Dense.c:149:19: missed: not vectorized: number of iterations cannot be computed.
Dense.c:140:19: missed: couldn't vectorize loop
Dense.c:140:19: missed: not vectorized: number of iterations cannot be computed.
Dense.c:133:13: note: vectorized 0 loops in function.
Dense.c:157:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:157:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:102:28: missed: couldn't vectorize loop
Dense.c:102:28: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:105:21: missed: couldn't vectorize loop
Dense.c:105:21: missed: not vectorized: number of iterations cannot be computed.
Dense.c:119:28: missed: couldn't vectorize loop
Dense.c:119:28: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:122:21: missed: couldn't vectorize loop
Dense.c:122:21: missed: not vectorized: number of iterations cannot be computed.
Dense.c:88:19: missed: couldn't vectorize loop
Dense.c:88:19: missed: not vectorized: number of iterations cannot be computed.
Dense.c:82:13: note: vectorized 0 loops in function.
Dense.c:85:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:107:17: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:107:17: note: SLPing BB part
Dense.c:124:17: note: Costing subgraph: 
Dense.c:124:17: note: node 0x8698168 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:124:17: note: op template: _49 = (sizetype) j_137;
Dense.c:124:17: note: 	stmt 0 _49 = (sizetype) j_137;
Dense.c:124:17: note: 	stmt 1 _209 = (sizetype) wi_136;
Dense.c:124:17: note: 	children 0x8698268
Dense.c:124:17: note: node (external) 0x8698268 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:124:17: note: 	{ j_137, wi_136 }
Dense.c:124:17: note: Cost model analysis: 
Dense.c:124:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:124:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:124:17: missed: not vectorized: vectorization is not profitable.
Dense.c:107:17: note: Costing subgraph: 
Dense.c:107:17: note: node 0x8698368 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:107:17: note: op template: _24 = (sizetype) j_134;
Dense.c:107:17: note: 	stmt 0 _24 = (sizetype) j_134;
Dense.c:107:17: note: 	stmt 1 _229 = (sizetype) wi_133;
Dense.c:107:17: note: 	children 0x8698468
Dense.c:107:17: note: node (external) 0x8698468 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:107:17: note: 	{ j_134, wi_133 }
Dense.c:107:17: note: Cost model analysis: 
Dense.c:107:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:107:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:107:17: missed: not vectorized: vectorization is not profitable.
Dense.c:107:17: note: ***** The result for vector mode V32QI would be the same
Dense.c:107:17: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:107:17: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:107:17: note: SLPing BB part
Dense.c:124:17: note: Costing subgraph: 
Dense.c:124:17: note: node 0x8698368 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:124:17: note: op template: _49 = (sizetype) j_137;
Dense.c:124:17: note: 	stmt 0 _49 = (sizetype) j_137;
Dense.c:124:17: note: 	stmt 1 _209 = (sizetype) wi_136;
Dense.c:124:17: note: 	children 0x8698168
Dense.c:124:17: note: node (external) 0x8698168 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:124:17: note: 	{ j_137, wi_136 }
Dense.c:124:17: note: Cost model analysis: 
Dense.c:124:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:124:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:124:17: missed: not vectorized: vectorization is not profitable.
Dense.c:107:17: note: Costing subgraph: 
Dense.c:107:17: note: node 0x86984e8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:107:17: note: op template: _24 = (sizetype) j_134;
Dense.c:107:17: note: 	stmt 0 _24 = (sizetype) j_134;
Dense.c:107:17: note: 	stmt 1 _229 = (sizetype) wi_133;
Dense.c:107:17: note: 	children 0x86983e8
Dense.c:107:17: note: node (external) 0x86983e8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:107:17: note: 	{ j_134, wi_133 }
Dense.c:107:17: note: Cost model analysis: 
Dense.c:107:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:107:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:107:17: missed: not vectorized: vectorization is not profitable.
Dense.c:107:17: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:102:55: note: ***** Analysis failed with vector mode V8QI
Dense.c:102:55: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:102:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:61:15: missed: couldn't vectorize loop
Dense.c:61:15: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:74:11: optimized: loop vectorized using 32 byte vectors
Dense.c:74:11: optimized: loop vectorized using 16 byte vectors
Dense.c:65:11: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:881:10: missed: not vectorized: no vectype for stmt: _52 = MEM[(__m256 * {ref-all})w_ptr_77];
 scalar_type: __m256
Dense.c:54:13: note: vectorized 1 loops in function.
Dense.c:78:2: missed: statement clobbers memory: _25 (pretmp_158, _63, 0);
Dense.c:54:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:54:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:164:2: missed: statement clobbers memory: free (_1);
Dense.c:165:2: missed: statement clobbers memory: free (_2);
Dense.c:166:2: missed: statement clobbers memory: free (_3);
Dense.c:167:2: missed: statement clobbers memory: free (_4);
Dense.c:168:2: missed: statement clobbers memory: free (dense_7);
Dense.c:169:2: missed: statement clobbers memory: PULSE_DestroyLayer (this_6(D));
Dense.c:170:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:170:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:184:19: missed: couldn't vectorize loop
Dense.c:184:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:174:13: note: vectorized 0 loops in function.
Dense.c:176:47: missed: statement clobbers memory: dense_35 = malloc (40);
Dense.c:178:36: missed: statement clobbers memory: _5 = aligned_alloc (64, _4);
Dense.c:179:38: missed: statement clobbers memory: _6 = aligned_alloc (64, _4);
Dense.c:180:36: missed: statement clobbers memory: _8 = aligned_alloc (64, _7);
Dense.c:181:35: missed: statement clobbers memory: _9 = aligned_alloc (64, _7);
Dense.c:182:36: missed: statement clobbers memory: _10 = aligned_alloc (64, _7);
Dense.c:185:39: missed: statement clobbers memory: _11 = rand ();
Dense.c:185:73: missed: statement clobbers memory: _63 = sqrt (_18);
Dense.c:197:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:198:4: missed: statement clobbers memory: exit (1);
Dense.c:194:4: missed: statement clobbers memory: layer = PULSE_CreateLayer (n_inputs.3_29, n_outputs.2_28, 1, activation_function_51(D), _SIMD_FeedDense, _SIMD_BackDense, _SIMD_FixDense, _DestroyDense, 1); [return slot optimization]
Dense.c:191:12: missed: statement clobbers memory: layer = PULSE_CreateLayer (n_inputs.1_27, n_outputs.0_26, 1, activation_function_51(D), _FeedDense, _BackDense, _FixDense, _DestroyDense, 0); [return slot optimization]
Dense.c:204:9: missed: not vectorized: more than one data ref in stmt: <retval> = layer;
Dense.c:204:9: note: ***** Analysis failed with vector mode V4DI
Dense.c:204:9: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Activations.c:17:19: optimized: loop vectorized using 32 byte vectors
Activations.c:17:19: optimized: loop vectorized using 16 byte vectors
Activations.c:17:19: optimized: loop vectorized using 32 byte vectors
Activations.c:17:19: optimized: loop vectorized using 16 byte vectors
Activations.c:14:13: note: vectorized 2 loops in function.
Activations.c:14:13: note: ***** Analysis failed with vector mode V8SF
Activations.c:14:13: note: ***** The result for vector mode V32QI would be the same
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V16QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V16QI
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V8QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V8QI
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V4QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V4QI
Activations.c:28:19: missed: couldn't vectorize loop
Activations.c:28:19: missed: not vectorized: unsupported control flow in loop.
Activations.c:28:19: optimized: loop vectorized using 32 byte vectors
Activations.c:28:19: optimized: loop vectorized using 16 byte vectors
Activations.c:25:13: note: vectorized 1 loops in function.
Activations.c:25:13: note: ***** Analysis failed with vector mode V8SF
Activations.c:25:13: note: ***** The result for vector mode V32QI would be the same
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V16QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V16QI
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V8QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V8QI
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V4QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V4QI
Activations.c:38:1: note: ***** Analysis failed with vector mode VOID
Activations.c:5:19: missed: couldn't vectorize loop
Activations.c:9:21: missed: statement clobbers memory: _36 = expf (_18);
Activations.c:5:19: missed: couldn't vectorize loop
Activations.c:9:21: missed: statement clobbers memory: _12 = expf (_11);
Activations.c:3:13: note: vectorized 0 loops in function.
Activations.c:9:21: missed: statement clobbers memory: _12 = expf (_11);
Activations.c:9:21: missed: statement clobbers memory: _36 = expf (_18);
Activations.c:11:1: note: ***** Analysis failed with vector mode V8SF
Activations.c:11:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SF
Activations.c:44:2: note: ***** Analysis failed with vector mode VOID
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:4:18: note: vectorized 0 loops in function.
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_4, inputs_15, _3);
PULSE.c:8:2: missed: statement clobbers memory: _5 (layer_17);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_12, pretmp_26, _9);
PULSE.c:13:28: note: ***** Analysis failed with vector mode V8SI
PULSE.c:13:28: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:22:2: missed: statement clobbers memory: _1 (layer_10(D));
PULSE.c:22:2: missed: statement clobbers memory: _14 (_2);
PULSE.c:22:2: missed: statement clobbers memory: _21 (_15);
PULSE.c:22:2: missed: statement clobbers memory: _28 (_22);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_29);
PULSE.c:26:3: missed: statement clobbers memory: memset (_34, 0, _32);
PULSE.c:26:3: missed: statement clobbers memory: memset (_27, 0, _25);
PULSE.c:26:3: missed: statement clobbers memory: memset (_20, 0, _18);
PULSE.c:26:3: missed: statement clobbers memory: memset (_7, 0, _5);
PULSE.c:28:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:28:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:32:6: note: vectorized 1 loops in function.
PULSE.c:34:8: missed: statement clobbers memory: _1 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_2);
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _9 = rand ();
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8SI
PULSE.c:32:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:61:16: missed: couldn't vectorize loop
PULSE.c:61:16: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:64:17: missed: couldn't vectorize loop
PULSE.c:64:17: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:73:19: missed: couldn't vectorize loop
PULSE.c:73:19: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _64 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:53:22: missed: couldn't vectorize loop
PULSE.c:53:22: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:47:6: note: vectorized 1 loops in function.
PULSE.c:48:1: missed: statement clobbers memory: saved_stack.7_35 = __builtin_stack_save ();
PULSE.c:49:46: missed: statement clobbers memory: _1 = PULSE_GetLossFunctionPtr (loss_function_36(D));
PULSE.c:60:10: missed: statement clobbers memory: random.4_42 = __builtin_alloca_with_align (_4, 32);
PULSE.c:34:8: missed: statement clobbers memory: _56 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_57);
PULSE.c:39:29: missed: statement clobbers memory: _64 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _67 = rand ();
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_93, inputs_88, _92);
PULSE.c:8:2: missed: statement clobbers memory: _94 (layer_89);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_100, _99, _98);
PULSE.c:67:11: missed: statement clobbers memory: loss_51 = PULSE_GetLoss_38 (_17, _16, _12, _11);
PULSE.c:22:2: missed: statement clobbers memory: _81 (output_30);
PULSE.c:22:2: missed: statement clobbers memory: _103 (_82);
PULSE.c:22:2: missed: statement clobbers memory: _110 (_104);
PULSE.c:22:2: missed: statement clobbers memory: _117 (_111);
PULSE.c:22:2: missed: statement clobbers memory: _124 (_118);
PULSE.c:22:2: missed: statement clobbers memory: _131 (_125);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_132);
PULSE.c:26:3: missed: statement clobbers memory: memset (_137, 0, _135);
PULSE.c:26:3: missed: statement clobbers memory: memset (_130, 0, _128);
PULSE.c:26:3: missed: statement clobbers memory: memset (_123, 0, _121);
PULSE.c:26:3: missed: statement clobbers memory: memset (_116, 0, _114);
PULSE.c:26:3: missed: statement clobbers memory: memset (_109, 0, _107);
PULSE.c:26:3: missed: statement clobbers memory: memset (_87, 0, _85);
PULSE.c:75:6: missed: statement clobbers memory: _21 (current_183, args);
PULSE.c:80:4: missed: statement clobbers memory: printf ("Epoch: %d Item: %d Loss: %.10f\r", i_181, j_182, _22);
PULSE.c:47:6: missed: statement clobbers memory: __builtin_stack_restore (saved_stack.7_35);
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4DI
PULSE.c:47:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:89:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:89:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:94:8: missed: couldn't vectorize loop
PULSE.c:94:8: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:91:6: note: vectorized 0 loops in function.
PULSE.c:96:3: missed: statement clobbers memory: _1 (current_10);
PULSE.c:99:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:99:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Loss.c:15:19: optimized: loop vectorized using 32 byte vectors
Loss.c:15:19: optimized:  loop versioned for vectorization because of possible aliasing
Loss.c:15:19: optimized: loop vectorized using 16 byte vectors
Loss.c:12:23: note: vectorized 1 loops in function.
Loss.c:12:23: note: ***** Analysis failed with vector mode V8SF
Loss.c:12:23: note: ***** The result for vector mode V32QI would be the same
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V16QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V16QI
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V8QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V8QI
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V4QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V4QI
Loss.c:7:19: missed: couldn't vectorize loop
Loss.c:8:38: missed: not vectorized: unsupported use in stmt.
Loss.c:4:23: note: vectorized 0 loops in function.
Loss.c:9:13: note: ***** Analysis failed with vector mode V8SF
Loss.c:9:13: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SF
Loss.c:23:2: note: ***** Analysis failed with vector mode VOID
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _30 (pretmp_91, _50, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:20: missed: not vectorized: no vectype for stmt: _21 = *_20;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:151:19: missed: couldn't vectorize loop
Dense.c:151:19: missed: not vectorized: number of iterations cannot be computed.
Dense.c:142:19: missed: couldn't vectorize loop
Dense.c:142:19: missed: not vectorized: number of iterations cannot be computed.
Dense.c:135:13: note: vectorized 0 loops in function.
Dense.c:159:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:159:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:104:28: missed: couldn't vectorize loop
Dense.c:104:28: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:107:21: missed: couldn't vectorize loop
Dense.c:107:21: missed: not vectorized: number of iterations cannot be computed.
Dense.c:121:28: missed: couldn't vectorize loop
Dense.c:121:28: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:124:21: missed: couldn't vectorize loop
Dense.c:124:21: missed: not vectorized: number of iterations cannot be computed.
Dense.c:90:19: missed: couldn't vectorize loop
Dense.c:90:19: missed: not vectorized: number of iterations cannot be computed.
Dense.c:84:13: note: vectorized 0 loops in function.
Dense.c:87:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:109:17: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:109:17: note: SLPing BB part
Dense.c:126:17: note: Costing subgraph: 
Dense.c:126:17: note: node 0x8963178 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:126:17: note: op template: _49 = (sizetype) j_137;
Dense.c:126:17: note: 	stmt 0 _49 = (sizetype) j_137;
Dense.c:126:17: note: 	stmt 1 _209 = (sizetype) wi_136;
Dense.c:126:17: note: 	children 0x8963278
Dense.c:126:17: note: node (external) 0x8963278 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:126:17: note: 	{ j_137, wi_136 }
Dense.c:126:17: note: Cost model analysis: 
Dense.c:126:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:126:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:126:17: missed: not vectorized: vectorization is not profitable.
Dense.c:109:17: note: Costing subgraph: 
Dense.c:109:17: note: node 0x8963378 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:109:17: note: op template: _24 = (sizetype) j_134;
Dense.c:109:17: note: 	stmt 0 _24 = (sizetype) j_134;
Dense.c:109:17: note: 	stmt 1 _229 = (sizetype) wi_133;
Dense.c:109:17: note: 	children 0x8963478
Dense.c:109:17: note: node (external) 0x8963478 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:109:17: note: 	{ j_134, wi_133 }
Dense.c:109:17: note: Cost model analysis: 
Dense.c:109:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:109:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:109:17: missed: not vectorized: vectorization is not profitable.
Dense.c:109:17: note: ***** The result for vector mode V32QI would be the same
Dense.c:109:17: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:109:17: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:109:17: note: SLPing BB part
Dense.c:126:17: note: Costing subgraph: 
Dense.c:126:17: note: node 0x8963378 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:126:17: note: op template: _49 = (sizetype) j_137;
Dense.c:126:17: note: 	stmt 0 _49 = (sizetype) j_137;
Dense.c:126:17: note: 	stmt 1 _209 = (sizetype) wi_136;
Dense.c:126:17: note: 	children 0x8963178
Dense.c:126:17: note: node (external) 0x8963178 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:126:17: note: 	{ j_137, wi_136 }
Dense.c:126:17: note: Cost model analysis: 
Dense.c:126:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:126:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:126:17: missed: not vectorized: vectorization is not profitable.
Dense.c:109:17: note: Costing subgraph: 
Dense.c:109:17: note: node 0x89634f8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:109:17: note: op template: _24 = (sizetype) j_134;
Dense.c:109:17: note: 	stmt 0 _24 = (sizetype) j_134;
Dense.c:109:17: note: 	stmt 1 _229 = (sizetype) wi_133;
Dense.c:109:17: note: 	children 0x89633f8
Dense.c:109:17: note: node (external) 0x89633f8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:109:17: note: 	{ j_134, wi_133 }
Dense.c:109:17: note: Cost model analysis: 
Dense.c:109:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:109:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:109:17: missed: not vectorized: vectorization is not profitable.
Dense.c:109:17: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:104:55: note: ***** Analysis failed with vector mode V8QI
Dense.c:104:55: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:104:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:166:2: missed: statement clobbers memory: free (_1);
Dense.c:167:2: missed: statement clobbers memory: free (_2);
Dense.c:168:2: missed: statement clobbers memory: free (_3);
Dense.c:169:2: missed: statement clobbers memory: free (_4);
Dense.c:170:2: missed: statement clobbers memory: free (dense_7);
Dense.c:171:2: missed: statement clobbers memory: PULSE_DestroyLayer (this_6(D));
Dense.c:172:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:172:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Include/PULSE_SIMD.h:101:20: missed: couldn't vectorize loop
Include/PULSE_SIMD.h:101:20: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Include/PULSE_SIMD.h:104:21: missed: couldn't vectorize loop
Include/PULSE_SIMD.h:104:21: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Include/PULSE_SIMD.h:129:23: optimized: loop vectorized using 32 byte vectors
Include/PULSE_SIMD.h:129:23: optimized: loop vectorized using 16 byte vectors
Include/PULSE_SIMD.h:121:23: missed: couldn't vectorize loop
Include/PULSE_SIMD.h:123:18: missed: not vectorized: no vectype for stmt: _47 = localM2[m_180];
 scalar_type: __m256
Include/PULSE_SIMD.h:88:13: note: vectorized 1 loops in function.
Include/PULSE_SIMD.h:109:19: note: ***** Analysis succeeded with vector mode V8SF
Include/PULSE_SIMD.h:109:19: note: SLPing BB part
Include/PULSE_SIMD.h:112:22: note: Costing subgraph: 
Include/PULSE_SIMD.h:112:22: note: node 0x8827d58 (max_nunits=8, refcnt=1) vector(8) float
Include/PULSE_SIMD.h:112:22: note: op template: localDst[0][0] = 0.0;
Include/PULSE_SIMD.h:112:22: note: 	stmt 0 localDst[0][0] = 0.0;
Include/PULSE_SIMD.h:112:22: note: 	stmt 1 localDst[0][1] = 0.0;
Include/PULSE_SIMD.h:112:22: note: 	stmt 2 localDst[0][2] = 0.0;
Include/PULSE_SIMD.h:112:22: note: 	stmt 3 localDst[0][3] = 0.0;
Include/PULSE_SIMD.h:112:22: note: 	stmt 4 localDst[0][4] = 0.0;
Include/PULSE_SIMD.h:112:22: note: 	stmt 5 localDst[0][5] = 0.0;
Include/PULSE_SIMD.h:112:22: note: 	stmt 6 localDst[0][6] = 0.0;
Include/PULSE_SIMD.h:112:22: note: 	stmt 7 localDst[0][7] = 0.0;
Include/PULSE_SIMD.h:112:22: note: 	children 0x8827dd8
Include/PULSE_SIMD.h:112:22: note: node (constant) 0x8827dd8 (max_nunits=1, refcnt=1) vector(8) float
Include/PULSE_SIMD.h:112:22: note: 	{ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 }
Include/PULSE_SIMD.h:112:22: note: Cost model analysis: 
Include/PULSE_SIMD.h:112:22: note: Cost model analysis for part in loop 3:
  Vector cost: 40
  Scalar cost: 128
Include/PULSE_SIMD.h:109:19: note: Costing subgraph: 
Include/PULSE_SIMD.h:109:19: note: node 0x8827e58 (max_nunits=2, refcnt=1) vector(2) sizetype
Include/PULSE_SIMD.h:109:19: note: op template: _136 = (sizetype) k_179;
Include/PULSE_SIMD.h:109:19: note: 	stmt 0 _136 = (sizetype) k_179;
Include/PULSE_SIMD.h:109:19: note: 	stmt 1 _144 = (sizetype) _143;
Include/PULSE_SIMD.h:109:19: note: 	children 0x8827f58
Include/PULSE_SIMD.h:109:19: note: node (external) 0x8827f58 (max_nunits=1, refcnt=1) vector(2) int
Include/PULSE_SIMD.h:109:19: note: 	{ k_179, _143 }
Include/PULSE_SIMD.h:109:19: note: node 0x8827fd8 (max_nunits=2, refcnt=1) vector(2) sizetype
Include/PULSE_SIMD.h:109:19: note: op template: _136 = (sizetype) k_179;
Include/PULSE_SIMD.h:109:19: note: 	stmt 0 _136 = (sizetype) k_179;
Include/PULSE_SIMD.h:109:19: note: 	stmt 1 _173 = (sizetype) _176;
Include/PULSE_SIMD.h:109:19: note: 	children 0x88280d8
Include/PULSE_SIMD.h:109:19: note: node (external) 0x88280d8 (max_nunits=1, refcnt=1) vector(2) int
Include/PULSE_SIMD.h:109:19: note: 	{ k_179, _176 }
Include/PULSE_SIMD.h:109:19: note: node 0x8828158 (max_nunits=2, refcnt=1) vector(2) sizetype
Include/PULSE_SIMD.h:109:19: note: op template: _136 = (sizetype) k_179;
Include/PULSE_SIMD.h:109:19: note: 	stmt 0 _136 = (sizetype) k_179;
Include/PULSE_SIMD.h:109:19: note: 	stmt 1 _406 = (sizetype) _441;
Include/PULSE_SIMD.h:109:19: note: 	children 0x8828258
Include/PULSE_SIMD.h:109:19: note: node (external) 0x8828258 (max_nunits=1, refcnt=1) vector(2) int
Include/PULSE_SIMD.h:109:19: note: 	{ k_179, _441 }
Include/PULSE_SIMD.h:109:19: note: node 0x88282d8 (max_nunits=2, refcnt=1) vector(2) sizetype
Include/PULSE_SIMD.h:109:19: note: op template: _136 = (sizetype) k_179;
Include/PULSE_SIMD.h:109:19: note: 	stmt 0 _136 = (sizetype) k_179;
Include/PULSE_SIMD.h:109:19: note: 	stmt 1 _251 = (sizetype) _204;
Include/PULSE_SIMD.h:109:19: note: 	children 0x88283d8
Include/PULSE_SIMD.h:109:19: note: node (external) 0x88283d8 (max_nunits=1, refcnt=1) vector(2) int
Include/PULSE_SIMD.h:109:19: note: 	{ k_179, _204 }
Include/PULSE_SIMD.h:109:19: note: node 0x8828458 (max_nunits=2, refcnt=1) vector(2) sizetype
Include/PULSE_SIMD.h:109:19: note: op template: _136 = (sizetype) k_179;
Include/PULSE_SIMD.h:109:19: note: 	stmt 0 _136 = (sizetype) k_179;
Include/PULSE_SIMD.h:109:19: note: 	stmt 1 _291 = (sizetype) _290;
Include/PULSE_SIMD.h:109:19: note: 	children 0x8828558
Include/PULSE_SIMD.h:109:19: note: node (external) 0x8828558 (max_nunits=1, refcnt=1) vector(2) int
Include/PULSE_SIMD.h:109:19: note: 	{ k_179, _290 }
Include/PULSE_SIMD.h:109:19: note: node 0x88285d8 (max_nunits=2, refcnt=1) vector(2) sizetype
Include/PULSE_SIMD.h:109:19: note: op template: _136 = (sizetype) k_179;
Include/PULSE_SIMD.h:109:19: note: 	stmt 0 _136 = (sizetype) k_179;
Include/PULSE_SIMD.h:109:19: note: 	stmt 1 _309 = (sizetype) _308;
Include/PULSE_SIMD.h:109:19: note: 	children 0x88286d8
Include/PULSE_SIMD.h:109:19: note: node (external) 0x88286d8 (max_nunits=1, refcnt=1) vector(2) int
Include/PULSE_SIMD.h:109:19: note: 	{ k_179, _308 }
Include/PULSE_SIMD.h:109:19: note: node 0x8828758 (max_nunits=2, refcnt=1) vector(2) sizetype
Include/PULSE_SIMD.h:109:19: note: op template: _136 = (sizetype) k_179;
Include/PULSE_SIMD.h:109:19: note: 	stmt 0 _136 = (sizetype) k_179;
Include/PULSE_SIMD.h:109:19: note: 	stmt 1 _349 = (sizetype) _348;
Include/PULSE_SIMD.h:109:19: note: 	children 0x8828858
Include/PULSE_SIMD.h:109:19: note: node (external) 0x8828858 (max_nunits=1, refcnt=1) vector(2) int
Include/PULSE_SIMD.h:109:19: note: 	{ k_179, _348 }
Include/PULSE_SIMD.h:109:19: note: node 0x88288d8 (max_nunits=2, refcnt=1) vector(2) sizetype
Include/PULSE_SIMD.h:109:19: note: op template: _136 = (sizetype) k_179;
Include/PULSE_SIMD.h:109:19: note: 	stmt 0 _136 = (sizetype) k_179;
Include/PULSE_SIMD.h:109:19: note: 	stmt 1 _412 = (sizetype) _411;
Include/PULSE_SIMD.h:109:19: note: 	children 0x88289d8
Include/PULSE_SIMD.h:109:19: note: node (external) 0x88289d8 (max_nunits=1, refcnt=1) vector(2) int
Include/PULSE_SIMD.h:109:19: note: 	{ k_179, _411 }
Include/PULSE_SIMD.h:109:19: note: Cost model analysis: 
Include/PULSE_SIMD.h:109:19: note: Cost model analysis for part in loop 2:
  Vector cost: 4
  Scalar cost: 32
Include/PULSE_SIMD.h:109:19: note: Cost model analysis for part in loop 3:
  Vector cost: 304
  Scalar cost: 36
Include/PULSE_SIMD.h:109:19: missed: not vectorized: vectorization is not profitable.
Include/PULSE_SIMD.h:109:19: note: Basic block will be vectorized using SLP
Include/PULSE_SIMD.h:112:22: optimized: basic block part vectorized using 32 byte vectors
Include/PULSE_SIMD.h:112:22: note: Vectorizing SLP tree:
Include/PULSE_SIMD.h:112:22: note: node 0x8827d58 (max_nunits=8, refcnt=1) vector(8) float
Include/PULSE_SIMD.h:112:22: note: op template: localDst[0][0] = 0.0;
Include/PULSE_SIMD.h:112:22: note: 	stmt 0 localDst[0][0] = 0.0;
Include/PULSE_SIMD.h:112:22: note: 	stmt 1 localDst[0][1] = 0.0;
Include/PULSE_SIMD.h:112:22: note: 	stmt 2 localDst[0][2] = 0.0;
Include/PULSE_SIMD.h:112:22: note: 	stmt 3 localDst[0][3] = 0.0;
Include/PULSE_SIMD.h:112:22: note: 	stmt 4 localDst[0][4] = 0.0;
Include/PULSE_SIMD.h:112:22: note: 	stmt 5 localDst[0][5] = 0.0;
Include/PULSE_SIMD.h:112:22: note: 	stmt 6 localDst[0][6] = 0.0;
Include/PULSE_SIMD.h:112:22: note: 	stmt 7 localDst[0][7] = 0.0;
Include/PULSE_SIMD.h:112:22: note: 	children 0x8827dd8
Include/PULSE_SIMD.h:112:22: note: node (constant) 0x8827dd8 (max_nunits=1, refcnt=1) vector(8) float
Include/PULSE_SIMD.h:112:22: note: 	{ 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 }
Include/PULSE_SIMD.h:112:22: note: ------>vectorizing SLP node starting from: localDst[0][0] = 0.0;
Include/PULSE_SIMD.h:112:22: note: vect_is_simple_use: operand 0.0, type of def: constant
Include/PULSE_SIMD.h:112:22: note: vect_is_simple_use: operand 0.0, type of def: constant
Include/PULSE_SIMD.h:112:22: note: vect_is_simple_use: operand 0.0, type of def: constant
Include/PULSE_SIMD.h:112:22: note: vect_is_simple_use: operand 0.0, type of def: constant
Include/PULSE_SIMD.h:112:22: note: vect_is_simple_use: operand 0.0, type of def: constant
Include/PULSE_SIMD.h:112:22: note: vect_is_simple_use: operand 0.0, type of def: constant
Include/PULSE_SIMD.h:112:22: note: vect_is_simple_use: operand 0.0, type of def: constant
Include/PULSE_SIMD.h:112:22: note: transform store. ncopies = 1
Include/PULSE_SIMD.h:112:22: note: create vector_type-pointer variable to type: vector(8) float  vectorizing a pointer ref: localDst[0][0]
Include/PULSE_SIMD.h:112:22: note: created &localDst[0][0]
Include/PULSE_SIMD.h:112:22: note: add new stmt: MEM <vector(8) float> [(float *)&localDst] = { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 };
Include/PULSE_SIMD.h:112:22: note: vectorizing stmts using SLP.
Include/PULSE_SIMD.h:109:19: note: ***** The result for vector mode V32QI would be the same
Dense.c:57:2: missed: statement clobbers memory: memcpy (_5, _4, _3);
Dense.c:58:2: missed: statement clobbers memory: __PULSE_SIMD_MxTM.constprop (_12, _11, _10, _9, _7);
Dense.c:80:2: missed: statement clobbers memory: _13 (_10, _8, 0);
Dense.c:81:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:81:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:186:19: missed: couldn't vectorize loop
Dense.c:186:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:176:13: note: vectorized 0 loops in function.
Dense.c:178:47: missed: statement clobbers memory: dense_35 = malloc (40);
Dense.c:180:36: missed: statement clobbers memory: _5 = aligned_alloc (64, _4);
Dense.c:181:38: missed: statement clobbers memory: _6 = aligned_alloc (64, _4);
Dense.c:182:36: missed: statement clobbers memory: _8 = aligned_alloc (64, _7);
Dense.c:183:35: missed: statement clobbers memory: _9 = aligned_alloc (64, _7);
Dense.c:184:36: missed: statement clobbers memory: _10 = aligned_alloc (64, _7);
Dense.c:187:39: missed: statement clobbers memory: _11 = rand ();
Dense.c:187:73: missed: statement clobbers memory: _63 = sqrt (_18);
Dense.c:199:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:200:4: missed: statement clobbers memory: exit (1);
Dense.c:196:4: missed: statement clobbers memory: layer = PULSE_CreateLayer (n_inputs.3_29, n_outputs.2_28, 1, activation_function_51(D), _SIMD_FeedDense, _SIMD_BackDense, _SIMD_FixDense, _DestroyDense, 1); [return slot optimization]
Dense.c:193:12: missed: statement clobbers memory: layer = PULSE_CreateLayer (n_inputs.1_27, n_outputs.0_26, 1, activation_function_51(D), _FeedDense, _BackDense, _FixDense, _DestroyDense, 0); [return slot optimization]
Dense.c:206:9: missed: not vectorized: more than one data ref in stmt: <retval> = layer;
Dense.c:206:9: note: ***** Analysis failed with vector mode V4DI
Dense.c:206:9: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Layer.c:7:34: missed: statement clobbers memory: _3 = aligned_alloc (64, _2);
Layer.c:8:35: missed: statement clobbers memory: _6 = aligned_alloc (64, _5);
Layer.c:9:34: missed: statement clobbers memory: _7 = aligned_alloc (64, _5);
Layer.c:14:19: missed: statement clobbers memory: _8 = PULSE_GetActivationFunctionPtr (activation_function_18(D));
Layer.c:19:9: note: ***** Analysis succeeded with vector mode V8SI
Layer.c:19:9: note: SLPing BB part
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x47cf7c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: op template: MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: 	stmt 0 MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: 	stmt 1 MEM <unsigned int> [(struct PULSE_Layer *)&<retval> + 4B] = optimization_type_17(D);
Layer.c:19:9: note: 	children 0x47cf848
Layer.c:19:9: note: node (external) 0x47cf848 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ type_16(D), optimization_type_17(D) }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x47cf8c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: op template: <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: 	stmt 0 <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: 	stmt 1 <retval>.n_outputs = n_outputs_12(D);
Layer.c:19:9: note: 	children 0x47cf948
Layer.c:19:9: note: node (external) 0x47cf948 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ n_inputs_11(D), n_outputs_12(D) }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x47cfa48 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: <retval>.inputs = _3;
Layer.c:19:9: note: 	stmt 0 <retval>.inputs = _3;
Layer.c:19:9: note: 	stmt 1 <retval>.outputs = _6;
Layer.c:19:9: note: 	children 0x47cfac8
Layer.c:19:9: note: node (external) 0x47cfac8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: 	stmt 0 _3 = aligned_alloc (64, _2);
Layer.c:19:9: note: 	stmt 1 _6 = aligned_alloc (64, _5);
Layer.c:19:9: note: 	children 0x47cfb48 0x47cfbc8
Layer.c:19:9: note: node (constant) 0x47cfb48 (max_nunits=1, refcnt=1)
Layer.c:19:9: note: 	{ 64, 64 }
Layer.c:19:9: note: node 0x47cfbc8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: patt_40 = n_inputs_11(D) w* 4;
Layer.c:19:9: note: 	stmt 0 patt_40 = n_inputs_11(D) w* 4;
Layer.c:19:9: note: 	stmt 1 patt_41 = n_outputs_12(D) w* 4;
Layer.c:19:9: note: 	children 0x47cfc48 0x47cfcc8
Layer.c:19:9: note: node (external) 0x47cfc48 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ n_inputs_11(D), n_outputs_12(D) }
Layer.c:19:9: note: node (constant) 0x47cfcc8 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ 4, 4 }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x47cfdc8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Layer.c:19:9: note: op template: <retval>.feed = feed_20(D);
Layer.c:19:9: note: 	stmt 0 <retval>.feed = feed_20(D);
Layer.c:19:9: note: 	stmt 1 <retval>.back = back_21(D);
Layer.c:19:9: note: 	stmt 2 <retval>.fix = fix_22(D);
Layer.c:19:9: note: 	stmt 3 <retval>.destroy = destroy_23(D);
Layer.c:19:9: note: 	children 0x47cfe48
Layer.c:19:9: note: node (external) 0x47cfe48 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Layer.c:19:9: note: 	{ feed_20(D), back_21(D), fix_22(D), destroy_23(D) }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 64
  Scalar cost: 64
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x47cff48 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: <retval>.parent = 0B;
Layer.c:19:9: note: 	stmt 0 <retval>.parent = 0B;
Layer.c:19:9: note: 	stmt 1 <retval>.child = 0B;
Layer.c:19:9: note: 	children 0x47cffc8
Layer.c:19:9: note: node (constant) 0x47cffc8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: 	{ 0B, 0B }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Layer.c:19:9: note: Basic block will be vectorized using SLP
Layer.c:19:9: optimized: basic block part vectorized using 8 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x47cf7c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: op template: MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: 	stmt 0 MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: 	stmt 1 MEM <unsigned int> [(struct PULSE_Layer *)&<retval> + 4B] = optimization_type_17(D);
Layer.c:19:9: note: 	children 0x47cf848
Layer.c:19:9: note: node (external) 0x47cf848 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ type_16(D), optimization_type_17(D) }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: vect_is_simple_use: operand optimization_type_17(D), type of def: external
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: MEM <unsigned int> [(struct PULSE_Layer *)&<retval>]
Layer.c:19:9: note: created &MEM <unsigned int> [(struct PULSE_Layer *)&<retval>]
Layer.c:19:9: note: add new stmt: MEM <vector(2) unsigned int> [(struct PULSE_Layer *)&<retval>] = _42;
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: optimized: basic block part vectorized using 8 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x47cf8c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: op template: <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: 	stmt 0 <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: 	stmt 1 <retval>.n_outputs = n_outputs_12(D);
Layer.c:19:9: note: 	children 0x47cf948
Layer.c:19:9: note: node (external) 0x47cf948 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ n_inputs_11(D), n_outputs_12(D) }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: vect_is_simple_use: operand n_outputs_12(D), type of def: external
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: <retval>.n_inputs
Layer.c:19:9: note: created &<retval>.n_inputs
Layer.c:19:9: note: add new stmt: MEM <vector(2) unsigned int> [(unsigned int *)&<retval> + 96B] = _44;
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: optimized: basic block part vectorized using 16 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x47cfa48 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: <retval>.inputs = _3;
Layer.c:19:9: note: 	stmt 0 <retval>.inputs = _3;
Layer.c:19:9: note: 	stmt 1 <retval>.outputs = _6;
Layer.c:19:9: note: 	children 0x47cfac8
Layer.c:19:9: note: node (external) 0x47cfac8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: 	stmt 0 _3 = aligned_alloc (64, _2);
Layer.c:19:9: note: 	stmt 1 _6 = aligned_alloc (64, _5);
Layer.c:19:9: note: 	children 0x47cfb48 0x47cfbc8
Layer.c:19:9: note: node (constant) 0x47cfb48 (max_nunits=1, refcnt=1)
Layer.c:19:9: note: 	{ 64, 64 }
Layer.c:19:9: note: node 0x47cfbc8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: patt_40 = n_inputs_11(D) w* 4;
Layer.c:19:9: note: 	stmt 0 patt_40 = n_inputs_11(D) w* 4;
Layer.c:19:9: note: 	stmt 1 patt_41 = n_outputs_12(D) w* 4;
Layer.c:19:9: note: 	children 0x47cfc48 0x47cfcc8
Layer.c:19:9: note: node (external) 0x47cfc48 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ n_inputs_11(D), n_outputs_12(D) }
Layer.c:19:9: note: node (constant) 0x47cfcc8 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ 4, 4 }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: <retval>.inputs = _3;
Layer.c:19:9: note: vect_is_simple_use: operand aligned_alloc (64, _5), type of def: internal
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.inputs
Layer.c:19:9: note: created &<retval>.inputs
Layer.c:19:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval> + 8B] = _48;
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: optimized: basic block part vectorized using 32 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x47cfdc8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Layer.c:19:9: note: op template: <retval>.feed = feed_20(D);
Layer.c:19:9: note: 	stmt 0 <retval>.feed = feed_20(D);
Layer.c:19:9: note: 	stmt 1 <retval>.back = back_21(D);
Layer.c:19:9: note: 	stmt 2 <retval>.fix = fix_22(D);
Layer.c:19:9: note: 	stmt 3 <retval>.destroy = destroy_23(D);
Layer.c:19:9: note: 	children 0x47cfe48
Layer.c:19:9: note: node (external) 0x47cfe48 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Layer.c:19:9: note: 	{ feed_20(D), back_21(D), fix_22(D), destroy_23(D) }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: <retval>.feed = feed_20(D);
Layer.c:19:9: note: vect_is_simple_use: operand back_21(D), type of def: external
Layer.c:19:9: note: vect_is_simple_use: operand fix_22(D), type of def: external
Layer.c:19:9: note: vect_is_simple_use: operand destroy_23(D), type of def: external
Layer.c:19:9: note: conflicting alias set types.
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(4) long unsigned int  vectorizing a pointer ref: <retval>.feed
Layer.c:19:9: note: created &<retval>.feed
Layer.c:19:9: note: add new stmt: MEM <vector(4) long unsigned int> [(void *)&<retval> + 32B] = _54;
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: optimized: basic block part vectorized using 16 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x47cff48 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: <retval>.parent = 0B;
Layer.c:19:9: note: 	stmt 0 <retval>.parent = 0B;
Layer.c:19:9: note: 	stmt 1 <retval>.child = 0B;
Layer.c:19:9: note: 	children 0x47cffc8
Layer.c:19:9: note: node (constant) 0x47cffc8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: 	{ 0B, 0B }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: <retval>.parent = 0B;
Layer.c:19:9: note: vect_is_simple_use: operand 0B, type of def: constant
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.parent
Layer.c:19:9: note: created &<retval>.parent
Layer.c:19:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer * *)&<retval> + 72B] = { 0, 0 };
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: note: ***** The result for vector mode V32QI would be the same
Layer.c:24:2: missed: statement clobbers memory: free (_1);
Layer.c:25:2: missed: statement clobbers memory: free (_2);
Layer.c:26:2: missed: statement clobbers memory: free (_3);
Layer.c:27:1: note: ***** Analysis failed with vector mode V4DI
Layer.c:27:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _30 (pretmp_91, _50, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:20: missed: not vectorized: no vectype for stmt: _21 = *_20;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:151:19: missed: couldn't vectorize loop
Dense.c:151:19: missed: not vectorized: number of iterations cannot be computed.
Dense.c:142:19: missed: couldn't vectorize loop
Dense.c:142:19: missed: not vectorized: number of iterations cannot be computed.
Dense.c:135:13: note: vectorized 0 loops in function.
Dense.c:159:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:159:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:104:28: missed: couldn't vectorize loop
Dense.c:104:28: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:107:21: missed: couldn't vectorize loop
Dense.c:107:21: missed: not vectorized: number of iterations cannot be computed.
Dense.c:121:28: missed: couldn't vectorize loop
Dense.c:121:28: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:124:21: missed: couldn't vectorize loop
Dense.c:124:21: missed: not vectorized: number of iterations cannot be computed.
Dense.c:90:19: missed: couldn't vectorize loop
Dense.c:90:19: missed: not vectorized: number of iterations cannot be computed.
Dense.c:84:13: note: vectorized 0 loops in function.
Dense.c:87:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:109:17: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:109:17: note: SLPing BB part
Dense.c:126:17: note: Costing subgraph: 
Dense.c:126:17: note: node 0x8998da8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:126:17: note: op template: _49 = (sizetype) j_137;
Dense.c:126:17: note: 	stmt 0 _49 = (sizetype) j_137;
Dense.c:126:17: note: 	stmt 1 _209 = (sizetype) wi_136;
Dense.c:126:17: note: 	children 0x8998ea8
Dense.c:126:17: note: node (external) 0x8998ea8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:126:17: note: 	{ j_137, wi_136 }
Dense.c:126:17: note: Cost model analysis: 
Dense.c:126:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:126:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:126:17: missed: not vectorized: vectorization is not profitable.
Dense.c:109:17: note: Costing subgraph: 
Dense.c:109:17: note: node 0x8998fa8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:109:17: note: op template: _24 = (sizetype) j_134;
Dense.c:109:17: note: 	stmt 0 _24 = (sizetype) j_134;
Dense.c:109:17: note: 	stmt 1 _229 = (sizetype) wi_133;
Dense.c:109:17: note: 	children 0x89990a8
Dense.c:109:17: note: node (external) 0x89990a8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:109:17: note: 	{ j_134, wi_133 }
Dense.c:109:17: note: Cost model analysis: 
Dense.c:109:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:109:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:109:17: missed: not vectorized: vectorization is not profitable.
Dense.c:109:17: note: ***** The result for vector mode V32QI would be the same
Dense.c:109:17: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:109:17: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:109:17: note: SLPing BB part
Dense.c:126:17: note: Costing subgraph: 
Dense.c:126:17: note: node 0x8998fa8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:126:17: note: op template: _49 = (sizetype) j_137;
Dense.c:126:17: note: 	stmt 0 _49 = (sizetype) j_137;
Dense.c:126:17: note: 	stmt 1 _209 = (sizetype) wi_136;
Dense.c:126:17: note: 	children 0x8998da8
Dense.c:126:17: note: node (external) 0x8998da8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:126:17: note: 	{ j_137, wi_136 }
Dense.c:126:17: note: Cost model analysis: 
Dense.c:126:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:126:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:126:17: missed: not vectorized: vectorization is not profitable.
Dense.c:109:17: note: Costing subgraph: 
Dense.c:109:17: note: node 0x8999128 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:109:17: note: op template: _24 = (sizetype) j_134;
Dense.c:109:17: note: 	stmt 0 _24 = (sizetype) j_134;
Dense.c:109:17: note: 	stmt 1 _229 = (sizetype) wi_133;
Dense.c:109:17: note: 	children 0x8999028
Dense.c:109:17: note: node (external) 0x8999028 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:109:17: note: 	{ j_134, wi_133 }
Dense.c:109:17: note: Cost model analysis: 
Dense.c:109:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:109:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:109:17: missed: not vectorized: vectorization is not profitable.
Dense.c:109:17: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:104:55: note: ***** Analysis failed with vector mode V8QI
Dense.c:104:55: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:104:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:166:2: missed: statement clobbers memory: free (_1);
Dense.c:167:2: missed: statement clobbers memory: free (_2);
Dense.c:168:2: missed: statement clobbers memory: free (_3);
Dense.c:169:2: missed: statement clobbers memory: free (_4);
Dense.c:170:2: missed: statement clobbers memory: free (dense_7);
Dense.c:171:2: missed: statement clobbers memory: PULSE_DestroyLayer (this_6(D));
Dense.c:172:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:172:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Include/PULSE_SIMD.h:98:20: missed: couldn't vectorize loop
Include/PULSE_SIMD.h:98:20: missed: not vectorized: multiple nested loops.
Include/PULSE_SIMD.h:101:21: missed: couldn't vectorize loop
Include/PULSE_SIMD.h:101:21: missed: not vectorized: multiple nested loops.
Include/PULSE_SIMD.h:103:22: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:905:10: missed: not vectorized: no vectype for stmt: _179 = MEM[(__m256_u * {ref-all})_178];
 scalar_type: __m256_u
Include/PULSE_SIMD.h:105:23: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:905:10: missed: not vectorized: no vectype for stmt: _179 = MEM[(__m256_u * {ref-all})_178];
 scalar_type: __m256_u
Dense.c:54:13: note: vectorized 0 loops in function.
Dense.c:57:2: missed: statement clobbers memory: memcpy (_5, _4, _3);
Dense.c:80:2: missed: statement clobbers memory: _13 (_10, _8, 0);
Include/PULSE_SIMD.h:111:8: note: ***** Analysis succeeded with vector mode V4DI
Include/PULSE_SIMD.h:111:8: note: SLPing BB part
Include/PULSE_SIMD.h:110:13: note: Costing subgraph: 
Include/PULSE_SIMD.h:110:13: note: node 0x8978e88 (max_nunits=2, refcnt=1) vector(2) sizetype
Include/PULSE_SIMD.h:110:13: note: op template: _174 = (sizetype) _173;
Include/PULSE_SIMD.h:110:13: note: 	stmt 0 _174 = (sizetype) _173;
Include/PULSE_SIMD.h:110:13: note: 	stmt 1 _243 = (sizetype) k_22;
Include/PULSE_SIMD.h:110:13: note: 	children 0x8978f88
Include/PULSE_SIMD.h:110:13: note: node (external) 0x8978f88 (max_nunits=1, refcnt=1) vector(2) int
Include/PULSE_SIMD.h:110:13: note: 	{ _173, k_22 }
Include/PULSE_SIMD.h:110:13: note: node 0x8979088 (max_nunits=2, refcnt=1) vector(2) sizetype
Include/PULSE_SIMD.h:110:13: note: op template: _182 = (sizetype) _181;
Include/PULSE_SIMD.h:110:13: note: 	stmt 0 _182 = (sizetype) _181;
Include/PULSE_SIMD.h:110:13: note: 	stmt 1 _243 = (sizetype) k_22;
Include/PULSE_SIMD.h:110:13: note: 	children 0x8979188
Include/PULSE_SIMD.h:110:13: note: node (external) 0x8979188 (max_nunits=1, refcnt=1) vector(2) int
Include/PULSE_SIMD.h:110:13: note: 	{ _181, k_22 }
Include/PULSE_SIMD.h:110:13: note: Cost model analysis: 
Include/PULSE_SIMD.h:110:13: note: Cost model analysis for part in loop 8:
  Vector cost: 8
  Scalar cost: 4
Include/PULSE_SIMD.h:110:13: missed: not vectorized: vectorization is not profitable.
Include/PULSE_SIMD.h:111:8: note: Costing subgraph: 
Include/PULSE_SIMD.h:111:8: note: node 0x8979288 (max_nunits=2, refcnt=1) vector(2) sizetype
Include/PULSE_SIMD.h:111:8: note: op template: _197 = (sizetype) _196;
Include/PULSE_SIMD.h:111:8: note: 	stmt 0 _197 = (sizetype) _196;
Include/PULSE_SIMD.h:111:8: note: 	stmt 1 _199 = (sizetype) _198;
Include/PULSE_SIMD.h:111:8: note: 	children 0x8979388
Include/PULSE_SIMD.h:111:8: note: node (external) 0x8979388 (max_nunits=1, refcnt=1) vector(2) int
Include/PULSE_SIMD.h:111:8: note: 	{ _196, _198 }
Include/PULSE_SIMD.h:111:8: note: Cost model analysis: 
Include/PULSE_SIMD.h:111:8: note: Cost model analysis for part in loop 9:
  Vector cost: 20
  Scalar cost: 4
Include/PULSE_SIMD.h:111:8: missed: not vectorized: vectorization is not profitable.
Include/PULSE_SIMD.h:111:8: note: ***** The result for vector mode V32QI would be the same
Include/PULSE_SIMD.h:111:8: note: ***** Re-trying analysis with vector mode V16QI
Include/PULSE_SIMD.h:111:8: note: ***** Analysis succeeded with vector mode V16QI
Include/PULSE_SIMD.h:111:8: note: SLPing BB part
Include/PULSE_SIMD.h:110:13: note: Costing subgraph: 
Include/PULSE_SIMD.h:110:13: note: node 0x8979088 (max_nunits=2, refcnt=1) vector(2) sizetype
Include/PULSE_SIMD.h:110:13: note: op template: _174 = (sizetype) _173;
Include/PULSE_SIMD.h:110:13: note: 	stmt 0 _174 = (sizetype) _173;
Include/PULSE_SIMD.h:110:13: note: 	stmt 1 _243 = (sizetype) k_22;
Include/PULSE_SIMD.h:110:13: note: 	children 0x8978e88
Include/PULSE_SIMD.h:110:13: note: node (external) 0x8978e88 (max_nunits=1, refcnt=1) vector(2) int
Include/PULSE_SIMD.h:110:13: note: 	{ _173, k_22 }
Include/PULSE_SIMD.h:110:13: note: node 0x8979108 (max_nunits=2, refcnt=1) vector(2) sizetype
Include/PULSE_SIMD.h:110:13: note: op template: _182 = (sizetype) _181;
Include/PULSE_SIMD.h:110:13: note: 	stmt 0 _182 = (sizetype) _181;
Include/PULSE_SIMD.h:110:13: note: 	stmt 1 _243 = (sizetype) k_22;
Include/PULSE_SIMD.h:110:13: note: 	children 0x8978f08
Include/PULSE_SIMD.h:110:13: note: node (external) 0x8978f08 (max_nunits=1, refcnt=1) vector(2) int
Include/PULSE_SIMD.h:110:13: note: 	{ _181, k_22 }
Include/PULSE_SIMD.h:110:13: note: Cost model analysis: 
Include/PULSE_SIMD.h:110:13: note: Cost model analysis for part in loop 8:
  Vector cost: 8
  Scalar cost: 4
Include/PULSE_SIMD.h:110:13: missed: not vectorized: vectorization is not profitable.
Include/PULSE_SIMD.h:111:8: note: Costing subgraph: 
Include/PULSE_SIMD.h:111:8: note: node 0x8979308 (max_nunits=2, refcnt=1) vector(2) sizetype
Include/PULSE_SIMD.h:111:8: note: op template: _197 = (sizetype) _196;
Include/PULSE_SIMD.h:111:8: note: 	stmt 0 _197 = (sizetype) _196;
Include/PULSE_SIMD.h:111:8: note: 	stmt 1 _199 = (sizetype) _198;
Include/PULSE_SIMD.h:111:8: note: 	children 0x8979008
Include/PULSE_SIMD.h:111:8: note: node (external) 0x8979008 (max_nunits=1, refcnt=1) vector(2) int
Include/PULSE_SIMD.h:111:8: note: 	{ _196, _198 }
Include/PULSE_SIMD.h:111:8: note: Cost model analysis: 
Include/PULSE_SIMD.h:111:8: note: Cost model analysis for part in loop 9:
  Vector cost: 20
  Scalar cost: 4
Include/PULSE_SIMD.h:111:8: missed: not vectorized: vectorization is not profitable.
Include/PULSE_SIMD.h:111:8: note: ***** Re-trying analysis with vector mode V8QI
Include/PULSE_SIMD.h:111:8: note: ***** Analysis failed with vector mode V8QI
Include/PULSE_SIMD.h:111:8: note: ***** Re-trying analysis with vector mode V4QI
Include/PULSE_SIMD.h:111:8: note: ***** Analysis failed with vector mode V4QI
Dense.c:186:19: missed: couldn't vectorize loop
Dense.c:186:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:176:13: note: vectorized 0 loops in function.
Dense.c:178:47: missed: statement clobbers memory: dense_35 = malloc (40);
Dense.c:180:36: missed: statement clobbers memory: _5 = aligned_alloc (64, _4);
Dense.c:181:38: missed: statement clobbers memory: _6 = aligned_alloc (64, _4);
Dense.c:182:36: missed: statement clobbers memory: _8 = aligned_alloc (64, _7);
Dense.c:183:35: missed: statement clobbers memory: _9 = aligned_alloc (64, _7);
Dense.c:184:36: missed: statement clobbers memory: _10 = aligned_alloc (64, _7);
Dense.c:187:39: missed: statement clobbers memory: _11 = rand ();
Dense.c:187:73: missed: statement clobbers memory: _63 = sqrt (_18);
Dense.c:199:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:200:4: missed: statement clobbers memory: exit (1);
Dense.c:196:4: missed: statement clobbers memory: layer = PULSE_CreateLayer (n_inputs.3_29, n_outputs.2_28, 1, activation_function_51(D), _SIMD_FeedDense, _SIMD_BackDense, _SIMD_FixDense, _DestroyDense, 1); [return slot optimization]
Dense.c:193:12: missed: statement clobbers memory: layer = PULSE_CreateLayer (n_inputs.1_27, n_outputs.0_26, 1, activation_function_51(D), _FeedDense, _BackDense, _FixDense, _DestroyDense, 0); [return slot optimization]
Dense.c:206:9: missed: not vectorized: more than one data ref in stmt: <retval> = layer;
Dense.c:206:9: note: ***** Analysis failed with vector mode V4DI
Dense.c:206:9: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Activations.c:17:19: optimized: loop vectorized using 32 byte vectors
Activations.c:17:19: optimized: loop vectorized using 16 byte vectors
Activations.c:17:19: optimized: loop vectorized using 32 byte vectors
Activations.c:17:19: optimized: loop vectorized using 16 byte vectors
Activations.c:14:13: note: vectorized 2 loops in function.
Activations.c:14:13: note: ***** Analysis failed with vector mode V8SF
Activations.c:14:13: note: ***** The result for vector mode V32QI would be the same
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V16QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V16QI
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V8QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V8QI
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V4QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V4QI
Activations.c:28:19: missed: couldn't vectorize loop
Activations.c:28:19: missed: not vectorized: unsupported control flow in loop.
Activations.c:28:19: optimized: loop vectorized using 32 byte vectors
Activations.c:28:19: optimized: loop vectorized using 16 byte vectors
Activations.c:25:13: note: vectorized 1 loops in function.
Activations.c:25:13: note: ***** Analysis failed with vector mode V8SF
Activations.c:25:13: note: ***** The result for vector mode V32QI would be the same
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V16QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V16QI
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V8QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V8QI
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V4QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V4QI
Activations.c:38:1: note: ***** Analysis failed with vector mode VOID
Activations.c:5:19: missed: couldn't vectorize loop
Activations.c:9:21: missed: statement clobbers memory: _36 = expf (_18);
Activations.c:5:19: missed: couldn't vectorize loop
Activations.c:9:21: missed: statement clobbers memory: _12 = expf (_11);
Activations.c:3:13: note: vectorized 0 loops in function.
Activations.c:9:21: missed: statement clobbers memory: _12 = expf (_11);
Activations.c:9:21: missed: statement clobbers memory: _36 = expf (_18);
Activations.c:11:1: note: ***** Analysis failed with vector mode V8SF
Activations.c:11:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SF
Activations.c:44:2: note: ***** Analysis failed with vector mode VOID
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:4:18: note: vectorized 0 loops in function.
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_4, inputs_15, _3);
PULSE.c:8:2: missed: statement clobbers memory: _5 (layer_17);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_12, pretmp_26, _9);
PULSE.c:13:28: note: ***** Analysis failed with vector mode V8SI
PULSE.c:13:28: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:22:2: missed: statement clobbers memory: _1 (layer_10(D));
PULSE.c:22:2: missed: statement clobbers memory: _14 (_2);
PULSE.c:22:2: missed: statement clobbers memory: _21 (_15);
PULSE.c:22:2: missed: statement clobbers memory: _28 (_22);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_29);
PULSE.c:26:3: missed: statement clobbers memory: memset (_34, 0, _32);
PULSE.c:26:3: missed: statement clobbers memory: memset (_27, 0, _25);
PULSE.c:26:3: missed: statement clobbers memory: memset (_20, 0, _18);
PULSE.c:26:3: missed: statement clobbers memory: memset (_7, 0, _5);
PULSE.c:28:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:28:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:32:6: note: vectorized 1 loops in function.
PULSE.c:34:8: missed: statement clobbers memory: _1 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_2);
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _9 = rand ();
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8SI
PULSE.c:32:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:61:16: missed: couldn't vectorize loop
PULSE.c:61:16: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:64:17: missed: couldn't vectorize loop
PULSE.c:64:17: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:73:19: missed: couldn't vectorize loop
PULSE.c:73:19: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _64 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:53:22: missed: couldn't vectorize loop
PULSE.c:53:22: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:47:6: note: vectorized 1 loops in function.
PULSE.c:48:1: missed: statement clobbers memory: saved_stack.7_35 = __builtin_stack_save ();
PULSE.c:49:46: missed: statement clobbers memory: _1 = PULSE_GetLossFunctionPtr (loss_function_36(D));
PULSE.c:60:10: missed: statement clobbers memory: random.4_42 = __builtin_alloca_with_align (_4, 32);
PULSE.c:34:8: missed: statement clobbers memory: _56 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_57);
PULSE.c:39:29: missed: statement clobbers memory: _64 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _67 = rand ();
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_93, inputs_88, _92);
PULSE.c:8:2: missed: statement clobbers memory: _94 (layer_89);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_100, _99, _98);
PULSE.c:67:11: missed: statement clobbers memory: loss_51 = PULSE_GetLoss_38 (_17, _16, _12, _11);
PULSE.c:22:2: missed: statement clobbers memory: _81 (output_30);
PULSE.c:22:2: missed: statement clobbers memory: _103 (_82);
PULSE.c:22:2: missed: statement clobbers memory: _110 (_104);
PULSE.c:22:2: missed: statement clobbers memory: _117 (_111);
PULSE.c:22:2: missed: statement clobbers memory: _124 (_118);
PULSE.c:22:2: missed: statement clobbers memory: _131 (_125);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_132);
PULSE.c:26:3: missed: statement clobbers memory: memset (_137, 0, _135);
PULSE.c:26:3: missed: statement clobbers memory: memset (_130, 0, _128);
PULSE.c:26:3: missed: statement clobbers memory: memset (_123, 0, _121);
PULSE.c:26:3: missed: statement clobbers memory: memset (_116, 0, _114);
PULSE.c:26:3: missed: statement clobbers memory: memset (_109, 0, _107);
PULSE.c:26:3: missed: statement clobbers memory: memset (_87, 0, _85);
PULSE.c:75:6: missed: statement clobbers memory: _21 (current_183, args);
PULSE.c:80:4: missed: statement clobbers memory: printf ("Epoch: %d Item: %d Loss: %.10f\r", i_181, j_182, _22);
PULSE.c:47:6: missed: statement clobbers memory: __builtin_stack_restore (saved_stack.7_35);
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4DI
PULSE.c:47:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:89:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:89:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:94:8: missed: couldn't vectorize loop
PULSE.c:94:8: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:91:6: note: vectorized 0 loops in function.
PULSE.c:96:3: missed: statement clobbers memory: _1 (current_10);
PULSE.c:99:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:99:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Loss.c:15:19: optimized: loop vectorized using 32 byte vectors
Loss.c:15:19: optimized:  loop versioned for vectorization because of possible aliasing
Loss.c:15:19: optimized: loop vectorized using 16 byte vectors
Loss.c:12:23: note: vectorized 1 loops in function.
Loss.c:12:23: note: ***** Analysis failed with vector mode V8SF
Loss.c:12:23: note: ***** The result for vector mode V32QI would be the same
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V16QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V16QI
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V8QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V8QI
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V4QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V4QI
Loss.c:7:19: missed: couldn't vectorize loop
Loss.c:8:38: missed: not vectorized: unsupported use in stmt.
Loss.c:4:23: note: vectorized 0 loops in function.
Loss.c:9:13: note: ***** Analysis failed with vector mode V8SF
Loss.c:9:13: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SF
Loss.c:23:2: note: ***** Analysis failed with vector mode VOID
Layer.c:7:34: missed: statement clobbers memory: _3 = aligned_alloc (64, _2);
Layer.c:8:35: missed: statement clobbers memory: _6 = aligned_alloc (64, _5);
Layer.c:9:34: missed: statement clobbers memory: _7 = aligned_alloc (64, _5);
Layer.c:14:19: missed: statement clobbers memory: _8 = PULSE_GetActivationFunctionPtr (activation_function_18(D));
Layer.c:19:9: note: ***** Analysis succeeded with vector mode V8SI
Layer.c:19:9: note: SLPing BB part
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x423f7c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: op template: MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: 	stmt 0 MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: 	stmt 1 MEM <unsigned int> [(struct PULSE_Layer *)&<retval> + 4B] = optimization_type_17(D);
Layer.c:19:9: note: 	children 0x423f848
Layer.c:19:9: note: node (external) 0x423f848 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ type_16(D), optimization_type_17(D) }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x423f8c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: op template: <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: 	stmt 0 <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: 	stmt 1 <retval>.n_outputs = n_outputs_12(D);
Layer.c:19:9: note: 	children 0x423f948
Layer.c:19:9: note: node (external) 0x423f948 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ n_inputs_11(D), n_outputs_12(D) }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x423fa48 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: <retval>.inputs = _3;
Layer.c:19:9: note: 	stmt 0 <retval>.inputs = _3;
Layer.c:19:9: note: 	stmt 1 <retval>.outputs = _6;
Layer.c:19:9: note: 	children 0x423fac8
Layer.c:19:9: note: node (external) 0x423fac8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: 	stmt 0 _3 = aligned_alloc (64, _2);
Layer.c:19:9: note: 	stmt 1 _6 = aligned_alloc (64, _5);
Layer.c:19:9: note: 	children 0x423fb48 0x423fbc8
Layer.c:19:9: note: node (constant) 0x423fb48 (max_nunits=1, refcnt=1)
Layer.c:19:9: note: 	{ 64, 64 }
Layer.c:19:9: note: node 0x423fbc8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: patt_40 = n_inputs_11(D) w* 4;
Layer.c:19:9: note: 	stmt 0 patt_40 = n_inputs_11(D) w* 4;
Layer.c:19:9: note: 	stmt 1 patt_41 = n_outputs_12(D) w* 4;
Layer.c:19:9: note: 	children 0x423fc48 0x423fcc8
Layer.c:19:9: note: node (external) 0x423fc48 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ n_inputs_11(D), n_outputs_12(D) }
Layer.c:19:9: note: node (constant) 0x423fcc8 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ 4, 4 }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x423fdc8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Layer.c:19:9: note: op template: <retval>.feed = feed_20(D);
Layer.c:19:9: note: 	stmt 0 <retval>.feed = feed_20(D);
Layer.c:19:9: note: 	stmt 1 <retval>.back = back_21(D);
Layer.c:19:9: note: 	stmt 2 <retval>.fix = fix_22(D);
Layer.c:19:9: note: 	stmt 3 <retval>.destroy = destroy_23(D);
Layer.c:19:9: note: 	children 0x423fe48
Layer.c:19:9: note: node (external) 0x423fe48 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Layer.c:19:9: note: 	{ feed_20(D), back_21(D), fix_22(D), destroy_23(D) }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 64
  Scalar cost: 64
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x423ff48 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: <retval>.parent = 0B;
Layer.c:19:9: note: 	stmt 0 <retval>.parent = 0B;
Layer.c:19:9: note: 	stmt 1 <retval>.child = 0B;
Layer.c:19:9: note: 	children 0x423ffc8
Layer.c:19:9: note: node (constant) 0x423ffc8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: 	{ 0B, 0B }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Layer.c:19:9: note: Basic block will be vectorized using SLP
Layer.c:19:9: optimized: basic block part vectorized using 8 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x423f7c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: op template: MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: 	stmt 0 MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: 	stmt 1 MEM <unsigned int> [(struct PULSE_Layer *)&<retval> + 4B] = optimization_type_17(D);
Layer.c:19:9: note: 	children 0x423f848
Layer.c:19:9: note: node (external) 0x423f848 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ type_16(D), optimization_type_17(D) }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: vect_is_simple_use: operand optimization_type_17(D), type of def: external
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: MEM <unsigned int> [(struct PULSE_Layer *)&<retval>]
Layer.c:19:9: note: created &MEM <unsigned int> [(struct PULSE_Layer *)&<retval>]
Layer.c:19:9: note: add new stmt: MEM <vector(2) unsigned int> [(struct PULSE_Layer *)&<retval>] = _42;
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: optimized: basic block part vectorized using 8 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x423f8c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: op template: <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: 	stmt 0 <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: 	stmt 1 <retval>.n_outputs = n_outputs_12(D);
Layer.c:19:9: note: 	children 0x423f948
Layer.c:19:9: note: node (external) 0x423f948 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ n_inputs_11(D), n_outputs_12(D) }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: vect_is_simple_use: operand n_outputs_12(D), type of def: external
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: <retval>.n_inputs
Layer.c:19:9: note: created &<retval>.n_inputs
Layer.c:19:9: note: add new stmt: MEM <vector(2) unsigned int> [(unsigned int *)&<retval> + 96B] = _44;
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: optimized: basic block part vectorized using 16 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x423fa48 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: <retval>.inputs = _3;
Layer.c:19:9: note: 	stmt 0 <retval>.inputs = _3;
Layer.c:19:9: note: 	stmt 1 <retval>.outputs = _6;
Layer.c:19:9: note: 	children 0x423fac8
Layer.c:19:9: note: node (external) 0x423fac8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: 	stmt 0 _3 = aligned_alloc (64, _2);
Layer.c:19:9: note: 	stmt 1 _6 = aligned_alloc (64, _5);
Layer.c:19:9: note: 	children 0x423fb48 0x423fbc8
Layer.c:19:9: note: node (constant) 0x423fb48 (max_nunits=1, refcnt=1)
Layer.c:19:9: note: 	{ 64, 64 }
Layer.c:19:9: note: node 0x423fbc8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: patt_40 = n_inputs_11(D) w* 4;
Layer.c:19:9: note: 	stmt 0 patt_40 = n_inputs_11(D) w* 4;
Layer.c:19:9: note: 	stmt 1 patt_41 = n_outputs_12(D) w* 4;
Layer.c:19:9: note: 	children 0x423fc48 0x423fcc8
Layer.c:19:9: note: node (external) 0x423fc48 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ n_inputs_11(D), n_outputs_12(D) }
Layer.c:19:9: note: node (constant) 0x423fcc8 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ 4, 4 }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: <retval>.inputs = _3;
Layer.c:19:9: note: vect_is_simple_use: operand aligned_alloc (64, _5), type of def: internal
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.inputs
Layer.c:19:9: note: created &<retval>.inputs
Layer.c:19:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval> + 8B] = _48;
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: optimized: basic block part vectorized using 32 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x423fdc8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Layer.c:19:9: note: op template: <retval>.feed = feed_20(D);
Layer.c:19:9: note: 	stmt 0 <retval>.feed = feed_20(D);
Layer.c:19:9: note: 	stmt 1 <retval>.back = back_21(D);
Layer.c:19:9: note: 	stmt 2 <retval>.fix = fix_22(D);
Layer.c:19:9: note: 	stmt 3 <retval>.destroy = destroy_23(D);
Layer.c:19:9: note: 	children 0x423fe48
Layer.c:19:9: note: node (external) 0x423fe48 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Layer.c:19:9: note: 	{ feed_20(D), back_21(D), fix_22(D), destroy_23(D) }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: <retval>.feed = feed_20(D);
Layer.c:19:9: note: vect_is_simple_use: operand back_21(D), type of def: external
Layer.c:19:9: note: vect_is_simple_use: operand fix_22(D), type of def: external
Layer.c:19:9: note: vect_is_simple_use: operand destroy_23(D), type of def: external
Layer.c:19:9: note: conflicting alias set types.
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(4) long unsigned int  vectorizing a pointer ref: <retval>.feed
Layer.c:19:9: note: created &<retval>.feed
Layer.c:19:9: note: add new stmt: MEM <vector(4) long unsigned int> [(void *)&<retval> + 32B] = _54;
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: optimized: basic block part vectorized using 16 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x423ff48 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: <retval>.parent = 0B;
Layer.c:19:9: note: 	stmt 0 <retval>.parent = 0B;
Layer.c:19:9: note: 	stmt 1 <retval>.child = 0B;
Layer.c:19:9: note: 	children 0x423ffc8
Layer.c:19:9: note: node (constant) 0x423ffc8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: 	{ 0B, 0B }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: <retval>.parent = 0B;
Layer.c:19:9: note: vect_is_simple_use: operand 0B, type of def: constant
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.parent
Layer.c:19:9: note: created &<retval>.parent
Layer.c:19:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer * *)&<retval> + 72B] = { 0, 0 };
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: note: ***** The result for vector mode V32QI would be the same
Layer.c:24:2: missed: statement clobbers memory: free (_1);
Layer.c:25:2: missed: statement clobbers memory: free (_2);
Layer.c:26:2: missed: statement clobbers memory: free (_3);
Layer.c:27:1: note: ***** Analysis failed with vector mode V4DI
Layer.c:27:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _30 (pretmp_91, _50, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:20: missed: not vectorized: no vectype for stmt: _21 = *_20;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:151:19: missed: couldn't vectorize loop
Dense.c:151:19: missed: not vectorized: number of iterations cannot be computed.
Dense.c:142:19: missed: couldn't vectorize loop
Dense.c:142:19: missed: not vectorized: number of iterations cannot be computed.
Dense.c:135:13: note: vectorized 0 loops in function.
Dense.c:159:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:159:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:104:28: missed: couldn't vectorize loop
Dense.c:104:28: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:107:21: missed: couldn't vectorize loop
Dense.c:107:21: missed: not vectorized: number of iterations cannot be computed.
Dense.c:121:28: missed: couldn't vectorize loop
Dense.c:121:28: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:124:21: missed: couldn't vectorize loop
Dense.c:124:21: missed: not vectorized: number of iterations cannot be computed.
Dense.c:90:19: missed: couldn't vectorize loop
Dense.c:90:19: missed: not vectorized: number of iterations cannot be computed.
Dense.c:84:13: note: vectorized 0 loops in function.
Dense.c:87:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:109:17: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:109:17: note: SLPing BB part
Dense.c:126:17: note: Costing subgraph: 
Dense.c:126:17: note: node 0x7c279d8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:126:17: note: op template: _49 = (sizetype) j_137;
Dense.c:126:17: note: 	stmt 0 _49 = (sizetype) j_137;
Dense.c:126:17: note: 	stmt 1 _209 = (sizetype) wi_136;
Dense.c:126:17: note: 	children 0x7c27ad8
Dense.c:126:17: note: node (external) 0x7c27ad8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:126:17: note: 	{ j_137, wi_136 }
Dense.c:126:17: note: Cost model analysis: 
Dense.c:126:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:126:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:126:17: missed: not vectorized: vectorization is not profitable.
Dense.c:109:17: note: Costing subgraph: 
Dense.c:109:17: note: node 0x7c27bd8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:109:17: note: op template: _24 = (sizetype) j_134;
Dense.c:109:17: note: 	stmt 0 _24 = (sizetype) j_134;
Dense.c:109:17: note: 	stmt 1 _229 = (sizetype) wi_133;
Dense.c:109:17: note: 	children 0x7c27cd8
Dense.c:109:17: note: node (external) 0x7c27cd8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:109:17: note: 	{ j_134, wi_133 }
Dense.c:109:17: note: Cost model analysis: 
Dense.c:109:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:109:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:109:17: missed: not vectorized: vectorization is not profitable.
Dense.c:109:17: note: ***** The result for vector mode V32QI would be the same
Dense.c:109:17: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:109:17: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:109:17: note: SLPing BB part
Dense.c:126:17: note: Costing subgraph: 
Dense.c:126:17: note: node 0x7c27bd8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:126:17: note: op template: _49 = (sizetype) j_137;
Dense.c:126:17: note: 	stmt 0 _49 = (sizetype) j_137;
Dense.c:126:17: note: 	stmt 1 _209 = (sizetype) wi_136;
Dense.c:126:17: note: 	children 0x7c279d8
Dense.c:126:17: note: node (external) 0x7c279d8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:126:17: note: 	{ j_137, wi_136 }
Dense.c:126:17: note: Cost model analysis: 
Dense.c:126:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:126:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:126:17: missed: not vectorized: vectorization is not profitable.
Dense.c:109:17: note: Costing subgraph: 
Dense.c:109:17: note: node 0x7c27d58 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:109:17: note: op template: _24 = (sizetype) j_134;
Dense.c:109:17: note: 	stmt 0 _24 = (sizetype) j_134;
Dense.c:109:17: note: 	stmt 1 _229 = (sizetype) wi_133;
Dense.c:109:17: note: 	children 0x7c27c58
Dense.c:109:17: note: node (external) 0x7c27c58 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:109:17: note: 	{ j_134, wi_133 }
Dense.c:109:17: note: Cost model analysis: 
Dense.c:109:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:109:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:109:17: missed: not vectorized: vectorization is not profitable.
Dense.c:109:17: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:104:55: note: ***** Analysis failed with vector mode V8QI
Dense.c:104:55: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:104:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:166:2: missed: statement clobbers memory: free (_1);
Dense.c:167:2: missed: statement clobbers memory: free (_2);
Dense.c:168:2: missed: statement clobbers memory: free (_3);
Dense.c:169:2: missed: statement clobbers memory: free (_4);
Dense.c:170:2: missed: statement clobbers memory: free (dense_7);
Dense.c:171:2: missed: statement clobbers memory: PULSE_DestroyLayer (this_6(D));
Dense.c:172:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:172:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Include/PULSE_SIMD.h:98:20: missed: couldn't vectorize loop
Include/PULSE_SIMD.h:98:20: missed: not vectorized: multiple nested loops.
Include/PULSE_SIMD.h:101:21: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:905:10: missed: not vectorized: no vectype for stmt: _187 = MEM[(__m256_u * {ref-all})_186];
 scalar_type: __m256_u
Include/PULSE_SIMD.h:105:23: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:905:10: missed: not vectorized: no vectype for stmt: _187 = MEM[(__m256_u * {ref-all})_186];
 scalar_type: __m256_u
Dense.c:54:13: note: vectorized 0 loops in function.
Dense.c:57:2: missed: statement clobbers memory: memcpy (_5, _4, _3);
Dense.c:80:2: missed: statement clobbers memory: _13 (_10, _8, 0);
Include/PULSE_SIMD.h:110:13: note: ***** Analysis succeeded with vector mode V4DI
Include/PULSE_SIMD.h:110:13: note: SLPing BB part
Include/PULSE_SIMD.h:110:13: note: Costing subgraph: 
Include/PULSE_SIMD.h:110:13: note: node 0x7bf7128 (max_nunits=2, refcnt=1) vector(2) sizetype
Include/PULSE_SIMD.h:110:13: note: op template: _183 = (sizetype) k_174;
Include/PULSE_SIMD.h:110:13: note: 	stmt 0 _183 = (sizetype) k_174;
Include/PULSE_SIMD.h:110:13: note: 	stmt 1 _190 = (sizetype) _189;
Include/PULSE_SIMD.h:110:13: note: 	children 0x7bf7228
Include/PULSE_SIMD.h:110:13: note: node (external) 0x7bf7228 (max_nunits=1, refcnt=1) vector(2) int
Include/PULSE_SIMD.h:110:13: note: 	{ k_174, _189 }
Include/PULSE_SIMD.h:110:13: note: Cost model analysis: 
Include/PULSE_SIMD.h:110:13: note: Cost model analysis for part in loop 7:
  Vector cost: 4
  Scalar cost: 4
Include/PULSE_SIMD.h:110:13: note: Cost model analysis for part in loop 8:
  Vector cost: 36
  Scalar cost: 8
Include/PULSE_SIMD.h:110:13: missed: not vectorized: vectorization is not profitable.
Include/PULSE_SIMD.h:110:13: note: ***** The result for vector mode V32QI would be the same
Include/PULSE_SIMD.h:110:13: note: ***** Re-trying analysis with vector mode V16QI
Include/PULSE_SIMD.h:110:13: note: ***** Analysis succeeded with vector mode V16QI
Include/PULSE_SIMD.h:110:13: note: SLPing BB part
Include/PULSE_SIMD.h:110:13: note: Costing subgraph: 
Include/PULSE_SIMD.h:110:13: note: node 0x7bf71a8 (max_nunits=2, refcnt=1) vector(2) sizetype
Include/PULSE_SIMD.h:110:13: note: op template: _183 = (sizetype) k_174;
Include/PULSE_SIMD.h:110:13: note: 	stmt 0 _183 = (sizetype) k_174;
Include/PULSE_SIMD.h:110:13: note: 	stmt 1 _190 = (sizetype) _189;
Include/PULSE_SIMD.h:110:13: note: 	children 0x7bf7028
Include/PULSE_SIMD.h:110:13: note: node (external) 0x7bf7028 (max_nunits=1, refcnt=1) vector(2) int
Include/PULSE_SIMD.h:110:13: note: 	{ k_174, _189 }
Include/PULSE_SIMD.h:110:13: note: Cost model analysis: 
Include/PULSE_SIMD.h:110:13: note: Cost model analysis for part in loop 7:
  Vector cost: 4
  Scalar cost: 4
Include/PULSE_SIMD.h:110:13: note: Cost model analysis for part in loop 8:
  Vector cost: 36
  Scalar cost: 8
Include/PULSE_SIMD.h:110:13: missed: not vectorized: vectorization is not profitable.
Include/PULSE_SIMD.h:110:13: note: ***** Re-trying analysis with vector mode V8QI
Include/PULSE_SIMD.h:111:8: note: ***** Analysis failed with vector mode V8QI
Include/PULSE_SIMD.h:111:8: note: ***** Re-trying analysis with vector mode V4QI
Include/PULSE_SIMD.h:111:8: note: ***** Analysis failed with vector mode V4QI
Dense.c:186:19: missed: couldn't vectorize loop
Dense.c:186:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:176:13: note: vectorized 0 loops in function.
Dense.c:178:47: missed: statement clobbers memory: dense_35 = malloc (40);
Dense.c:180:36: missed: statement clobbers memory: _5 = aligned_alloc (64, _4);
Dense.c:181:38: missed: statement clobbers memory: _6 = aligned_alloc (64, _4);
Dense.c:182:36: missed: statement clobbers memory: _8 = aligned_alloc (64, _7);
Dense.c:183:35: missed: statement clobbers memory: _9 = aligned_alloc (64, _7);
Dense.c:184:36: missed: statement clobbers memory: _10 = aligned_alloc (64, _7);
Dense.c:187:39: missed: statement clobbers memory: _11 = rand ();
Dense.c:187:73: missed: statement clobbers memory: _63 = sqrt (_18);
Dense.c:199:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:200:4: missed: statement clobbers memory: exit (1);
Dense.c:196:4: missed: statement clobbers memory: layer = PULSE_CreateLayer (n_inputs.3_29, n_outputs.2_28, 1, activation_function_51(D), _SIMD_FeedDense, _SIMD_BackDense, _SIMD_FixDense, _DestroyDense, 1); [return slot optimization]
Dense.c:193:12: missed: statement clobbers memory: layer = PULSE_CreateLayer (n_inputs.1_27, n_outputs.0_26, 1, activation_function_51(D), _FeedDense, _BackDense, _FixDense, _DestroyDense, 0); [return slot optimization]
Dense.c:206:9: missed: not vectorized: more than one data ref in stmt: <retval> = layer;
Dense.c:206:9: note: ***** Analysis failed with vector mode V4DI
Dense.c:206:9: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Activations.c:17:19: optimized: loop vectorized using 32 byte vectors
Activations.c:17:19: optimized: loop vectorized using 16 byte vectors
Activations.c:17:19: optimized: loop vectorized using 32 byte vectors
Activations.c:17:19: optimized: loop vectorized using 16 byte vectors
Activations.c:14:13: note: vectorized 2 loops in function.
Activations.c:14:13: note: ***** Analysis failed with vector mode V8SF
Activations.c:14:13: note: ***** The result for vector mode V32QI would be the same
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V16QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V16QI
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V8QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V8QI
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V4QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V4QI
Activations.c:28:19: missed: couldn't vectorize loop
Activations.c:28:19: missed: not vectorized: unsupported control flow in loop.
Activations.c:28:19: optimized: loop vectorized using 32 byte vectors
Activations.c:28:19: optimized: loop vectorized using 16 byte vectors
Activations.c:25:13: note: vectorized 1 loops in function.
Activations.c:25:13: note: ***** Analysis failed with vector mode V8SF
Activations.c:25:13: note: ***** The result for vector mode V32QI would be the same
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V16QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V16QI
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V8QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V8QI
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V4QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V4QI
Activations.c:38:1: note: ***** Analysis failed with vector mode VOID
Activations.c:5:19: missed: couldn't vectorize loop
Activations.c:9:21: missed: statement clobbers memory: _36 = expf (_18);
Activations.c:5:19: missed: couldn't vectorize loop
Activations.c:9:21: missed: statement clobbers memory: _12 = expf (_11);
Activations.c:3:13: note: vectorized 0 loops in function.
Activations.c:9:21: missed: statement clobbers memory: _12 = expf (_11);
Activations.c:9:21: missed: statement clobbers memory: _36 = expf (_18);
Activations.c:11:1: note: ***** Analysis failed with vector mode V8SF
Activations.c:11:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SF
Activations.c:44:2: note: ***** Analysis failed with vector mode VOID
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:4:18: note: vectorized 0 loops in function.
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_4, inputs_15, _3);
PULSE.c:8:2: missed: statement clobbers memory: _5 (layer_17);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_12, pretmp_26, _9);
PULSE.c:13:28: note: ***** Analysis failed with vector mode V8SI
PULSE.c:13:28: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:22:2: missed: statement clobbers memory: _1 (layer_10(D));
PULSE.c:22:2: missed: statement clobbers memory: _14 (_2);
PULSE.c:22:2: missed: statement clobbers memory: _21 (_15);
PULSE.c:22:2: missed: statement clobbers memory: _28 (_22);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_29);
PULSE.c:26:3: missed: statement clobbers memory: memset (_34, 0, _32);
PULSE.c:26:3: missed: statement clobbers memory: memset (_27, 0, _25);
PULSE.c:26:3: missed: statement clobbers memory: memset (_20, 0, _18);
PULSE.c:26:3: missed: statement clobbers memory: memset (_7, 0, _5);
PULSE.c:28:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:28:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:32:6: note: vectorized 1 loops in function.
PULSE.c:34:8: missed: statement clobbers memory: _1 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_2);
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _9 = rand ();
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8SI
PULSE.c:32:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:61:16: missed: couldn't vectorize loop
PULSE.c:61:16: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:64:17: missed: couldn't vectorize loop
PULSE.c:64:17: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:73:19: missed: couldn't vectorize loop
PULSE.c:73:19: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _64 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:53:22: missed: couldn't vectorize loop
PULSE.c:53:22: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:47:6: note: vectorized 1 loops in function.
PULSE.c:48:1: missed: statement clobbers memory: saved_stack.7_35 = __builtin_stack_save ();
PULSE.c:49:46: missed: statement clobbers memory: _1 = PULSE_GetLossFunctionPtr (loss_function_36(D));
PULSE.c:60:10: missed: statement clobbers memory: random.4_42 = __builtin_alloca_with_align (_4, 32);
PULSE.c:34:8: missed: statement clobbers memory: _56 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_57);
PULSE.c:39:29: missed: statement clobbers memory: _64 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _67 = rand ();
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_93, inputs_88, _92);
PULSE.c:8:2: missed: statement clobbers memory: _94 (layer_89);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_100, _99, _98);
PULSE.c:67:11: missed: statement clobbers memory: loss_51 = PULSE_GetLoss_38 (_17, _16, _12, _11);
PULSE.c:22:2: missed: statement clobbers memory: _81 (output_30);
PULSE.c:22:2: missed: statement clobbers memory: _103 (_82);
PULSE.c:22:2: missed: statement clobbers memory: _110 (_104);
PULSE.c:22:2: missed: statement clobbers memory: _117 (_111);
PULSE.c:22:2: missed: statement clobbers memory: _124 (_118);
PULSE.c:22:2: missed: statement clobbers memory: _131 (_125);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_132);
PULSE.c:26:3: missed: statement clobbers memory: memset (_137, 0, _135);
PULSE.c:26:3: missed: statement clobbers memory: memset (_130, 0, _128);
PULSE.c:26:3: missed: statement clobbers memory: memset (_123, 0, _121);
PULSE.c:26:3: missed: statement clobbers memory: memset (_116, 0, _114);
PULSE.c:26:3: missed: statement clobbers memory: memset (_109, 0, _107);
PULSE.c:26:3: missed: statement clobbers memory: memset (_87, 0, _85);
PULSE.c:75:6: missed: statement clobbers memory: _21 (current_183, args);
PULSE.c:80:4: missed: statement clobbers memory: printf ("Epoch: %d Item: %d Loss: %.10f\r", i_181, j_182, _22);
PULSE.c:47:6: missed: statement clobbers memory: __builtin_stack_restore (saved_stack.7_35);
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4DI
PULSE.c:47:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:89:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:89:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:94:8: missed: couldn't vectorize loop
PULSE.c:94:8: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:91:6: note: vectorized 0 loops in function.
PULSE.c:96:3: missed: statement clobbers memory: _1 (current_10);
PULSE.c:99:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:99:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Loss.c:15:19: optimized: loop vectorized using 32 byte vectors
Loss.c:15:19: optimized:  loop versioned for vectorization because of possible aliasing
Loss.c:15:19: optimized: loop vectorized using 16 byte vectors
Loss.c:12:23: note: vectorized 1 loops in function.
Loss.c:12:23: note: ***** Analysis failed with vector mode V8SF
Loss.c:12:23: note: ***** The result for vector mode V32QI would be the same
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V16QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V16QI
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V8QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V8QI
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V4QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V4QI
Loss.c:7:19: missed: couldn't vectorize loop
Loss.c:8:38: missed: not vectorized: unsupported use in stmt.
Loss.c:4:23: note: vectorized 0 loops in function.
Loss.c:9:13: note: ***** Analysis failed with vector mode V8SF
Loss.c:9:13: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SF
Loss.c:23:2: note: ***** Analysis failed with vector mode VOID
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _30 (pretmp_91, _50, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:20: missed: not vectorized: no vectype for stmt: _21 = *_20;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:150:19: missed: couldn't vectorize loop
Dense.c:150:19: missed: not vectorized: number of iterations cannot be computed.
Dense.c:141:19: missed: couldn't vectorize loop
Dense.c:141:19: missed: not vectorized: number of iterations cannot be computed.
Dense.c:134:13: note: vectorized 0 loops in function.
Dense.c:158:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:158:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:103:28: missed: couldn't vectorize loop
Dense.c:103:28: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:106:21: missed: couldn't vectorize loop
Dense.c:106:21: missed: not vectorized: number of iterations cannot be computed.
Dense.c:120:28: missed: couldn't vectorize loop
Dense.c:120:28: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:123:21: missed: couldn't vectorize loop
Dense.c:123:21: missed: not vectorized: number of iterations cannot be computed.
Dense.c:89:19: missed: couldn't vectorize loop
Dense.c:89:19: missed: not vectorized: number of iterations cannot be computed.
Dense.c:83:13: note: vectorized 0 loops in function.
Dense.c:86:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:108:17: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:108:17: note: SLPing BB part
Dense.c:125:17: note: Costing subgraph: 
Dense.c:125:17: note: node 0x7c8c878 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:125:17: note: op template: _49 = (sizetype) j_137;
Dense.c:125:17: note: 	stmt 0 _49 = (sizetype) j_137;
Dense.c:125:17: note: 	stmt 1 _209 = (sizetype) wi_136;
Dense.c:125:17: note: 	children 0x7c8c978
Dense.c:125:17: note: node (external) 0x7c8c978 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:125:17: note: 	{ j_137, wi_136 }
Dense.c:125:17: note: Cost model analysis: 
Dense.c:125:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:125:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:125:17: missed: not vectorized: vectorization is not profitable.
Dense.c:108:17: note: Costing subgraph: 
Dense.c:108:17: note: node 0x7c8ca78 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:108:17: note: op template: _24 = (sizetype) j_134;
Dense.c:108:17: note: 	stmt 0 _24 = (sizetype) j_134;
Dense.c:108:17: note: 	stmt 1 _229 = (sizetype) wi_133;
Dense.c:108:17: note: 	children 0x7c8cb78
Dense.c:108:17: note: node (external) 0x7c8cb78 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:108:17: note: 	{ j_134, wi_133 }
Dense.c:108:17: note: Cost model analysis: 
Dense.c:108:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:108:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:108:17: missed: not vectorized: vectorization is not profitable.
Dense.c:108:17: note: ***** The result for vector mode V32QI would be the same
Dense.c:108:17: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:108:17: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:108:17: note: SLPing BB part
Dense.c:125:17: note: Costing subgraph: 
Dense.c:125:17: note: node 0x7c8ca78 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:125:17: note: op template: _49 = (sizetype) j_137;
Dense.c:125:17: note: 	stmt 0 _49 = (sizetype) j_137;
Dense.c:125:17: note: 	stmt 1 _209 = (sizetype) wi_136;
Dense.c:125:17: note: 	children 0x7c8c878
Dense.c:125:17: note: node (external) 0x7c8c878 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:125:17: note: 	{ j_137, wi_136 }
Dense.c:125:17: note: Cost model analysis: 
Dense.c:125:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:125:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:125:17: missed: not vectorized: vectorization is not profitable.
Dense.c:108:17: note: Costing subgraph: 
Dense.c:108:17: note: node 0x7c8cbf8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:108:17: note: op template: _24 = (sizetype) j_134;
Dense.c:108:17: note: 	stmt 0 _24 = (sizetype) j_134;
Dense.c:108:17: note: 	stmt 1 _229 = (sizetype) wi_133;
Dense.c:108:17: note: 	children 0x7c8caf8
Dense.c:108:17: note: node (external) 0x7c8caf8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:108:17: note: 	{ j_134, wi_133 }
Dense.c:108:17: note: Cost model analysis: 
Dense.c:108:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:108:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:108:17: missed: not vectorized: vectorization is not profitable.
Dense.c:108:17: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:103:55: note: ***** Analysis failed with vector mode V8QI
Dense.c:103:55: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:103:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:62:15: missed: couldn't vectorize loop
Dense.c:62:15: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:75:11: optimized: loop vectorized using 32 byte vectors
Dense.c:75:11: optimized: loop vectorized using 16 byte vectors
Dense.c:66:11: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:881:10: missed: not vectorized: no vectype for stmt: _56 = MEM[(__m256 * {ref-all})w_ptr_81];
 scalar_type: __m256
Dense.c:54:13: note: vectorized 1 loops in function.
Dense.c:57:2: missed: statement clobbers memory: memcpy (_5, _4, _3);
Dense.c:79:2: missed: statement clobbers memory: _28 (pretmp_162, _67, 0);
Dense.c:54:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:54:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:165:2: missed: statement clobbers memory: free (_1);
Dense.c:166:2: missed: statement clobbers memory: free (_2);
Dense.c:167:2: missed: statement clobbers memory: free (_3);
Dense.c:168:2: missed: statement clobbers memory: free (_4);
Dense.c:169:2: missed: statement clobbers memory: free (dense_7);
Dense.c:170:2: missed: statement clobbers memory: PULSE_DestroyLayer (this_6(D));
Dense.c:171:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:171:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:185:19: missed: couldn't vectorize loop
Dense.c:185:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:175:13: note: vectorized 0 loops in function.
Dense.c:177:47: missed: statement clobbers memory: dense_35 = malloc (40);
Dense.c:179:36: missed: statement clobbers memory: _5 = aligned_alloc (64, _4);
Dense.c:180:38: missed: statement clobbers memory: _6 = aligned_alloc (64, _4);
Dense.c:181:36: missed: statement clobbers memory: _8 = aligned_alloc (64, _7);
Dense.c:182:35: missed: statement clobbers memory: _9 = aligned_alloc (64, _7);
Dense.c:183:36: missed: statement clobbers memory: _10 = aligned_alloc (64, _7);
Dense.c:186:39: missed: statement clobbers memory: _11 = rand ();
Dense.c:186:73: missed: statement clobbers memory: _63 = sqrt (_18);
Dense.c:198:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:199:4: missed: statement clobbers memory: exit (1);
Dense.c:195:4: missed: statement clobbers memory: layer = PULSE_CreateLayer (n_inputs.3_29, n_outputs.2_28, 1, activation_function_51(D), _SIMD_FeedDense, _SIMD_BackDense, _SIMD_FixDense, _DestroyDense, 1); [return slot optimization]
Dense.c:192:12: missed: statement clobbers memory: layer = PULSE_CreateLayer (n_inputs.1_27, n_outputs.0_26, 1, activation_function_51(D), _FeedDense, _BackDense, _FixDense, _DestroyDense, 0); [return slot optimization]
Dense.c:205:9: missed: not vectorized: more than one data ref in stmt: <retval> = layer;
Dense.c:205:9: note: ***** Analysis failed with vector mode V4DI
Dense.c:205:9: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _30 (pretmp_91, _50, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:20: missed: not vectorized: no vectype for stmt: _21 = *_20;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:144:3: missed: couldn't vectorize loop
Dense.c:144:3: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:157:19: missed: couldn't vectorize loop
Dense.c:157:19: missed: not vectorized: number of iterations cannot be computed.
Dense.c:151:10: optimized: loop vectorized using 32 byte vectors
Dense.c:151:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:151:10: optimized: loop vectorized using 16 byte vectors
Dense.c:134:13: note: vectorized 1 loops in function.
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_9, _47);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_11, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
Dense.c:134:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:134:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:134:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:134:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:134:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:134:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:134:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:134:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:103:28: missed: couldn't vectorize loop
Dense.c:103:28: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:106:21: missed: couldn't vectorize loop
Dense.c:106:21: missed: not vectorized: number of iterations cannot be computed.
Dense.c:120:28: missed: couldn't vectorize loop
Dense.c:120:28: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:123:21: missed: couldn't vectorize loop
Dense.c:123:21: missed: not vectorized: number of iterations cannot be computed.
Dense.c:89:19: missed: couldn't vectorize loop
Dense.c:89:19: missed: not vectorized: number of iterations cannot be computed.
Dense.c:83:13: note: vectorized 0 loops in function.
Dense.c:86:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:108:17: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:108:17: note: SLPing BB part
Dense.c:125:17: note: Costing subgraph: 
Dense.c:125:17: note: node 0x7191b78 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:125:17: note: op template: _49 = (sizetype) j_137;
Dense.c:125:17: note: 	stmt 0 _49 = (sizetype) j_137;
Dense.c:125:17: note: 	stmt 1 _209 = (sizetype) wi_136;
Dense.c:125:17: note: 	children 0x7191c78
Dense.c:125:17: note: node (external) 0x7191c78 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:125:17: note: 	{ j_137, wi_136 }
Dense.c:125:17: note: Cost model analysis: 
Dense.c:125:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:125:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:125:17: missed: not vectorized: vectorization is not profitable.
Dense.c:108:17: note: Costing subgraph: 
Dense.c:108:17: note: node 0x7191d78 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:108:17: note: op template: _24 = (sizetype) j_134;
Dense.c:108:17: note: 	stmt 0 _24 = (sizetype) j_134;
Dense.c:108:17: note: 	stmt 1 _229 = (sizetype) wi_133;
Dense.c:108:17: note: 	children 0x7191e78
Dense.c:108:17: note: node (external) 0x7191e78 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:108:17: note: 	{ j_134, wi_133 }
Dense.c:108:17: note: Cost model analysis: 
Dense.c:108:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:108:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:108:17: missed: not vectorized: vectorization is not profitable.
Dense.c:108:17: note: ***** The result for vector mode V32QI would be the same
Dense.c:108:17: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:108:17: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:108:17: note: SLPing BB part
Dense.c:125:17: note: Costing subgraph: 
Dense.c:125:17: note: node 0x7191d78 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:125:17: note: op template: _49 = (sizetype) j_137;
Dense.c:125:17: note: 	stmt 0 _49 = (sizetype) j_137;
Dense.c:125:17: note: 	stmt 1 _209 = (sizetype) wi_136;
Dense.c:125:17: note: 	children 0x7191b78
Dense.c:125:17: note: node (external) 0x7191b78 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:125:17: note: 	{ j_137, wi_136 }
Dense.c:125:17: note: Cost model analysis: 
Dense.c:125:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:125:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:125:17: missed: not vectorized: vectorization is not profitable.
Dense.c:108:17: note: Costing subgraph: 
Dense.c:108:17: note: node 0x7191ef8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:108:17: note: op template: _24 = (sizetype) j_134;
Dense.c:108:17: note: 	stmt 0 _24 = (sizetype) j_134;
Dense.c:108:17: note: 	stmt 1 _229 = (sizetype) wi_133;
Dense.c:108:17: note: 	children 0x7191df8
Dense.c:108:17: note: node (external) 0x7191df8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:108:17: note: 	{ j_134, wi_133 }
Dense.c:108:17: note: Cost model analysis: 
Dense.c:108:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:108:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:108:17: missed: not vectorized: vectorization is not profitable.
Dense.c:108:17: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:103:55: note: ***** Analysis failed with vector mode V8QI
Dense.c:103:55: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:103:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:62:15: missed: couldn't vectorize loop
Dense.c:62:15: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:75:11: optimized: loop vectorized using 32 byte vectors
Dense.c:75:11: optimized: loop vectorized using 16 byte vectors
Dense.c:66:11: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:881:10: missed: not vectorized: no vectype for stmt: _56 = MEM[(__m256 * {ref-all})w_ptr_81];
 scalar_type: __m256
Dense.c:54:13: note: vectorized 1 loops in function.
Dense.c:57:2: missed: statement clobbers memory: memcpy (_5, _4, _3);
Dense.c:79:2: missed: statement clobbers memory: _28 (pretmp_162, _67, 0);
Dense.c:54:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:54:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:172:2: missed: statement clobbers memory: free (_1);
Dense.c:173:2: missed: statement clobbers memory: free (_2);
Dense.c:174:2: missed: statement clobbers memory: free (_3);
Dense.c:175:2: missed: statement clobbers memory: free (_4);
Dense.c:176:2: missed: statement clobbers memory: free (dense_7);
Dense.c:177:2: missed: statement clobbers memory: PULSE_DestroyLayer (this_6(D));
Dense.c:178:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:178:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:192:19: missed: couldn't vectorize loop
Dense.c:192:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:182:13: note: vectorized 0 loops in function.
Dense.c:184:47: missed: statement clobbers memory: dense_35 = malloc (40);
Dense.c:186:36: missed: statement clobbers memory: _5 = aligned_alloc (64, _4);
Dense.c:187:38: missed: statement clobbers memory: _6 = aligned_alloc (64, _4);
Dense.c:188:36: missed: statement clobbers memory: _8 = aligned_alloc (64, _7);
Dense.c:189:35: missed: statement clobbers memory: _9 = aligned_alloc (64, _7);
Dense.c:190:36: missed: statement clobbers memory: _10 = aligned_alloc (64, _7);
Dense.c:193:39: missed: statement clobbers memory: _11 = rand ();
Dense.c:193:73: missed: statement clobbers memory: _63 = sqrt (_18);
Dense.c:205:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:206:4: missed: statement clobbers memory: exit (1);
Dense.c:202:4: missed: statement clobbers memory: layer = PULSE_CreateLayer (n_inputs.3_29, n_outputs.2_28, 1, activation_function_51(D), _SIMD_FeedDense, _SIMD_BackDense, _SIMD_FixDense, _DestroyDense, 1); [return slot optimization]
Dense.c:199:12: missed: statement clobbers memory: layer = PULSE_CreateLayer (n_inputs.1_27, n_outputs.0_26, 1, activation_function_51(D), _FeedDense, _BackDense, _FixDense, _DestroyDense, 0); [return slot optimization]
Dense.c:212:9: missed: not vectorized: more than one data ref in stmt: <retval> = layer;
Dense.c:212:9: note: ***** Analysis failed with vector mode V4DI
Dense.c:212:9: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _30 (pretmp_91, _50, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:20: missed: not vectorized: no vectype for stmt: _21 = *_20;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:158:19: missed: couldn't vectorize loop
Dense.c:158:19: missed: not vectorized: number of iterations cannot be computed.
Dense.c:152:10: optimized: loop vectorized using 32 byte vectors
Dense.c:152:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:152:10: optimized: loop vectorized using 16 byte vectors
Dense.c:142:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_12, _53);
Dense.c:134:13: note: vectorized 1 loops in function.
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_12, _53);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_16, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
Dense.c:134:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:134:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:134:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:134:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:134:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:134:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:134:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:134:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:103:28: missed: couldn't vectorize loop
Dense.c:103:28: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:106:21: missed: couldn't vectorize loop
Dense.c:106:21: missed: not vectorized: number of iterations cannot be computed.
Dense.c:120:28: missed: couldn't vectorize loop
Dense.c:120:28: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:123:21: missed: couldn't vectorize loop
Dense.c:123:21: missed: not vectorized: number of iterations cannot be computed.
Dense.c:89:19: missed: couldn't vectorize loop
Dense.c:89:19: missed: not vectorized: number of iterations cannot be computed.
Dense.c:83:13: note: vectorized 0 loops in function.
Dense.c:86:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:108:17: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:108:17: note: SLPing BB part
Dense.c:125:17: note: Costing subgraph: 
Dense.c:125:17: note: node 0x8bf5388 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:125:17: note: op template: _49 = (sizetype) j_137;
Dense.c:125:17: note: 	stmt 0 _49 = (sizetype) j_137;
Dense.c:125:17: note: 	stmt 1 _209 = (sizetype) wi_136;
Dense.c:125:17: note: 	children 0x8bf5488
Dense.c:125:17: note: node (external) 0x8bf5488 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:125:17: note: 	{ j_137, wi_136 }
Dense.c:125:17: note: Cost model analysis: 
Dense.c:125:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:125:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:125:17: missed: not vectorized: vectorization is not profitable.
Dense.c:108:17: note: Costing subgraph: 
Dense.c:108:17: note: node 0x8bf5588 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:108:17: note: op template: _24 = (sizetype) j_134;
Dense.c:108:17: note: 	stmt 0 _24 = (sizetype) j_134;
Dense.c:108:17: note: 	stmt 1 _229 = (sizetype) wi_133;
Dense.c:108:17: note: 	children 0x8bf5688
Dense.c:108:17: note: node (external) 0x8bf5688 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:108:17: note: 	{ j_134, wi_133 }
Dense.c:108:17: note: Cost model analysis: 
Dense.c:108:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:108:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:108:17: missed: not vectorized: vectorization is not profitable.
Dense.c:108:17: note: ***** The result for vector mode V32QI would be the same
Dense.c:108:17: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:108:17: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:108:17: note: SLPing BB part
Dense.c:125:17: note: Costing subgraph: 
Dense.c:125:17: note: node 0x8bf5588 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:125:17: note: op template: _49 = (sizetype) j_137;
Dense.c:125:17: note: 	stmt 0 _49 = (sizetype) j_137;
Dense.c:125:17: note: 	stmt 1 _209 = (sizetype) wi_136;
Dense.c:125:17: note: 	children 0x8bf5388
Dense.c:125:17: note: node (external) 0x8bf5388 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:125:17: note: 	{ j_137, wi_136 }
Dense.c:125:17: note: Cost model analysis: 
Dense.c:125:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:125:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:125:17: missed: not vectorized: vectorization is not profitable.
Dense.c:108:17: note: Costing subgraph: 
Dense.c:108:17: note: node 0x8bf5708 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:108:17: note: op template: _24 = (sizetype) j_134;
Dense.c:108:17: note: 	stmt 0 _24 = (sizetype) j_134;
Dense.c:108:17: note: 	stmt 1 _229 = (sizetype) wi_133;
Dense.c:108:17: note: 	children 0x8bf5608
Dense.c:108:17: note: node (external) 0x8bf5608 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:108:17: note: 	{ j_134, wi_133 }
Dense.c:108:17: note: Cost model analysis: 
Dense.c:108:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:108:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:108:17: missed: not vectorized: vectorization is not profitable.
Dense.c:108:17: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:103:55: note: ***** Analysis failed with vector mode V8QI
Dense.c:103:55: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:103:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:62:15: missed: couldn't vectorize loop
Dense.c:62:15: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:75:11: optimized: loop vectorized using 32 byte vectors
Dense.c:75:11: optimized: loop vectorized using 16 byte vectors
Dense.c:66:11: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:881:10: missed: not vectorized: no vectype for stmt: _56 = MEM[(__m256 * {ref-all})w_ptr_81];
 scalar_type: __m256
Dense.c:54:13: note: vectorized 1 loops in function.
Dense.c:57:2: missed: statement clobbers memory: memcpy (_5, _4, _3);
Dense.c:79:2: missed: statement clobbers memory: _28 (pretmp_162, _67, 0);
Dense.c:54:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:54:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:173:2: missed: statement clobbers memory: free (_1);
Dense.c:174:2: missed: statement clobbers memory: free (_2);
Dense.c:175:2: missed: statement clobbers memory: free (_3);
Dense.c:176:2: missed: statement clobbers memory: free (_4);
Dense.c:177:2: missed: statement clobbers memory: free (dense_7);
Dense.c:178:2: missed: statement clobbers memory: PULSE_DestroyLayer (this_6(D));
Dense.c:179:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:179:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:193:19: missed: couldn't vectorize loop
Dense.c:193:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:183:13: note: vectorized 0 loops in function.
Dense.c:185:47: missed: statement clobbers memory: dense_35 = malloc (40);
Dense.c:187:36: missed: statement clobbers memory: _5 = aligned_alloc (64, _4);
Dense.c:188:38: missed: statement clobbers memory: _6 = aligned_alloc (64, _4);
Dense.c:189:36: missed: statement clobbers memory: _8 = aligned_alloc (64, _7);
Dense.c:190:35: missed: statement clobbers memory: _9 = aligned_alloc (64, _7);
Dense.c:191:36: missed: statement clobbers memory: _10 = aligned_alloc (64, _7);
Dense.c:194:39: missed: statement clobbers memory: _11 = rand ();
Dense.c:194:73: missed: statement clobbers memory: _63 = sqrt (_18);
Dense.c:206:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:207:4: missed: statement clobbers memory: exit (1);
Dense.c:203:4: missed: statement clobbers memory: layer = PULSE_CreateLayer (n_inputs.3_29, n_outputs.2_28, 1, activation_function_51(D), _SIMD_FeedDense, _SIMD_BackDense, _SIMD_FixDense, _DestroyDense, 1); [return slot optimization]
Dense.c:200:12: missed: statement clobbers memory: layer = PULSE_CreateLayer (n_inputs.1_27, n_outputs.0_26, 1, activation_function_51(D), _FeedDense, _BackDense, _FixDense, _DestroyDense, 0); [return slot optimization]
Dense.c:213:9: missed: not vectorized: more than one data ref in stmt: <retval> = layer;
Dense.c:213:9: note: ***** Analysis failed with vector mode V4DI
Dense.c:213:9: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _30 (pretmp_91, _50, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:20: missed: not vectorized: no vectype for stmt: _21 = *_20;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:168:10: optimized: loop vectorized using 32 byte vectors
Dense.c:168:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:168:10: optimized: loop vectorized using 16 byte vectors
Dense.c:159:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_34, _77);
Dense.c:152:10: optimized: loop vectorized using 32 byte vectors
Dense.c:152:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:152:10: optimized: loop vectorized using 16 byte vectors
Dense.c:143:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, _69);
Dense.c:134:13: note: vectorized 2 loops in function.
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, _69);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_19, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_34, _77);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_38, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
Dense.c:134:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:134:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:134:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:134:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:134:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:134:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:134:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:134:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:103:28: missed: couldn't vectorize loop
Dense.c:103:28: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:106:21: missed: couldn't vectorize loop
Dense.c:106:21: missed: not vectorized: number of iterations cannot be computed.
Dense.c:120:28: missed: couldn't vectorize loop
Dense.c:120:28: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:123:21: missed: couldn't vectorize loop
Dense.c:123:21: missed: not vectorized: number of iterations cannot be computed.
Dense.c:89:19: missed: couldn't vectorize loop
Dense.c:89:19: missed: not vectorized: number of iterations cannot be computed.
Dense.c:83:13: note: vectorized 0 loops in function.
Dense.c:86:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:108:17: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:108:17: note: SLPing BB part
Dense.c:125:17: note: Costing subgraph: 
Dense.c:125:17: note: node 0x86658e8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:125:17: note: op template: _49 = (sizetype) j_137;
Dense.c:125:17: note: 	stmt 0 _49 = (sizetype) j_137;
Dense.c:125:17: note: 	stmt 1 _209 = (sizetype) wi_136;
Dense.c:125:17: note: 	children 0x86659e8
Dense.c:125:17: note: node (external) 0x86659e8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:125:17: note: 	{ j_137, wi_136 }
Dense.c:125:17: note: Cost model analysis: 
Dense.c:125:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:125:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:125:17: missed: not vectorized: vectorization is not profitable.
Dense.c:108:17: note: Costing subgraph: 
Dense.c:108:17: note: node 0x8665ae8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:108:17: note: op template: _24 = (sizetype) j_134;
Dense.c:108:17: note: 	stmt 0 _24 = (sizetype) j_134;
Dense.c:108:17: note: 	stmt 1 _229 = (sizetype) wi_133;
Dense.c:108:17: note: 	children 0x8665be8
Dense.c:108:17: note: node (external) 0x8665be8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:108:17: note: 	{ j_134, wi_133 }
Dense.c:108:17: note: Cost model analysis: 
Dense.c:108:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:108:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:108:17: missed: not vectorized: vectorization is not profitable.
Dense.c:108:17: note: ***** The result for vector mode V32QI would be the same
Dense.c:108:17: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:108:17: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:108:17: note: SLPing BB part
Dense.c:125:17: note: Costing subgraph: 
Dense.c:125:17: note: node 0x8665ae8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:125:17: note: op template: _49 = (sizetype) j_137;
Dense.c:125:17: note: 	stmt 0 _49 = (sizetype) j_137;
Dense.c:125:17: note: 	stmt 1 _209 = (sizetype) wi_136;
Dense.c:125:17: note: 	children 0x86658e8
Dense.c:125:17: note: node (external) 0x86658e8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:125:17: note: 	{ j_137, wi_136 }
Dense.c:125:17: note: Cost model analysis: 
Dense.c:125:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:125:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:125:17: missed: not vectorized: vectorization is not profitable.
Dense.c:108:17: note: Costing subgraph: 
Dense.c:108:17: note: node 0x8665c68 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:108:17: note: op template: _24 = (sizetype) j_134;
Dense.c:108:17: note: 	stmt 0 _24 = (sizetype) j_134;
Dense.c:108:17: note: 	stmt 1 _229 = (sizetype) wi_133;
Dense.c:108:17: note: 	children 0x8665b68
Dense.c:108:17: note: node (external) 0x8665b68 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:108:17: note: 	{ j_134, wi_133 }
Dense.c:108:17: note: Cost model analysis: 
Dense.c:108:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:108:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:108:17: missed: not vectorized: vectorization is not profitable.
Dense.c:108:17: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:103:55: note: ***** Analysis failed with vector mode V8QI
Dense.c:103:55: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:103:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:62:15: missed: couldn't vectorize loop
Dense.c:62:15: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:75:11: optimized: loop vectorized using 32 byte vectors
Dense.c:75:11: optimized: loop vectorized using 16 byte vectors
Dense.c:66:11: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:881:10: missed: not vectorized: no vectype for stmt: _56 = MEM[(__m256 * {ref-all})w_ptr_81];
 scalar_type: __m256
Dense.c:54:13: note: vectorized 1 loops in function.
Dense.c:57:2: missed: statement clobbers memory: memcpy (_5, _4, _3);
Dense.c:79:2: missed: statement clobbers memory: _28 (pretmp_162, _67, 0);
Dense.c:54:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:54:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:191:2: missed: statement clobbers memory: free (_1);
Dense.c:192:2: missed: statement clobbers memory: free (_2);
Dense.c:193:2: missed: statement clobbers memory: free (_3);
Dense.c:194:2: missed: statement clobbers memory: free (_4);
Dense.c:195:2: missed: statement clobbers memory: free (dense_7);
Dense.c:196:2: missed: statement clobbers memory: PULSE_DestroyLayer (this_6(D));
Dense.c:197:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:197:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:211:19: missed: couldn't vectorize loop
Dense.c:211:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:201:13: note: vectorized 0 loops in function.
Dense.c:203:47: missed: statement clobbers memory: dense_35 = malloc (40);
Dense.c:205:36: missed: statement clobbers memory: _5 = aligned_alloc (64, _4);
Dense.c:206:38: missed: statement clobbers memory: _6 = aligned_alloc (64, _4);
Dense.c:207:36: missed: statement clobbers memory: _8 = aligned_alloc (64, _7);
Dense.c:208:35: missed: statement clobbers memory: _9 = aligned_alloc (64, _7);
Dense.c:209:36: missed: statement clobbers memory: _10 = aligned_alloc (64, _7);
Dense.c:212:39: missed: statement clobbers memory: _11 = rand ();
Dense.c:212:73: missed: statement clobbers memory: _63 = sqrt (_18);
Dense.c:224:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:225:4: missed: statement clobbers memory: exit (1);
Dense.c:221:4: missed: statement clobbers memory: layer = PULSE_CreateLayer (n_inputs.3_29, n_outputs.2_28, 1, activation_function_51(D), _SIMD_FeedDense, _SIMD_BackDense, _SIMD_FixDense, _DestroyDense, 1); [return slot optimization]
Dense.c:218:12: missed: statement clobbers memory: layer = PULSE_CreateLayer (n_inputs.1_27, n_outputs.0_26, 1, activation_function_51(D), _FeedDense, _BackDense, _FixDense, _DestroyDense, 0); [return slot optimization]
Dense.c:231:9: missed: not vectorized: more than one data ref in stmt: <retval> = layer;
Dense.c:231:9: note: ***** Analysis failed with vector mode V4DI
Dense.c:231:9: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _30 (pretmp_91, _50, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:20: missed: not vectorized: no vectype for stmt: _21 = *_20;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:177:10: optimized: loop vectorized using 32 byte vectors
Dense.c:177:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:177:10: optimized: loop vectorized using 16 byte vectors
Dense.c:168:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_34, _77);
Dense.c:161:10: optimized: loop vectorized using 32 byte vectors
Dense.c:161:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:161:10: optimized: loop vectorized using 16 byte vectors
Dense.c:152:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, _69);
Dense.c:143:13: note: vectorized 2 loops in function.
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, _69);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_19, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_34, _77);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_38, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
Dense.c:143:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:143:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:143:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:143:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:143:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:143:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:143:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:143:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:112:24: missed: couldn't vectorize loop
Dense.c:112:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:115:17: missed: couldn't vectorize loop
Dense.c:115:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:129:24: missed: couldn't vectorize loop
Dense.c:129:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:132:17: missed: couldn't vectorize loop
Dense.c:132:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:103:10: optimized: loop vectorized using 32 byte vectors
Dense.c:103:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:103:10: optimized: loop vectorized using 16 byte vectors
Dense.c:91:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_13, _102);
Dense.c:83:13: note: vectorized 1 loops in function.
Dense.c:86:2: missed: statement clobbers memory: _1 (_3, _2, 1);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_13, _102);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, _103);
Dense.c:117:17: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:117:17: note: SLPing BB part
Dense.c:134:17: note: Costing subgraph: 
Dense.c:134:17: note: node 0x84d15f8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:134:17: note: op template: _66 = (sizetype) j_162;
Dense.c:134:17: note: 	stmt 0 _66 = (sizetype) j_162;
Dense.c:134:17: note: 	stmt 1 _257 = (sizetype) wi_164;
Dense.c:134:17: note: 	children 0x84d16f8
Dense.c:134:17: note: node (external) 0x84d16f8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:134:17: note: 	{ j_162, wi_164 }
Dense.c:134:17: note: Cost model analysis: 
Dense.c:134:17: note: Scalar 3 and vector 4 loop part do not match up, skipping scalar part
Dense.c:134:17: note: Cost model analysis for part in loop 4:
  Vector cost: 36
  Scalar cost: 8
Dense.c:134:17: missed: not vectorized: vectorization is not profitable.
Dense.c:117:17: note: Costing subgraph: 
Dense.c:117:17: note: node 0x84d17f8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:117:17: note: op template: _41 = (sizetype) j_161;
Dense.c:117:17: note: 	stmt 0 _41 = (sizetype) j_161;
Dense.c:117:17: note: 	stmt 1 _277 = (sizetype) wi_163;
Dense.c:117:17: note: 	children 0x84d18f8
Dense.c:117:17: note: node (external) 0x84d18f8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:117:17: note: 	{ j_161, wi_163 }
Dense.c:117:17: note: Cost model analysis: 
Dense.c:117:17: note: Scalar 5 and vector 6 loop part do not match up, skipping scalar part
Dense.c:117:17: note: Cost model analysis for part in loop 6:
  Vector cost: 36
  Scalar cost: 8
Dense.c:117:17: missed: not vectorized: vectorization is not profitable.
Dense.c:117:17: note: ***** The result for vector mode V32QI would be the same
Dense.c:117:17: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:117:17: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:117:17: note: SLPing BB part
Dense.c:134:17: note: Costing subgraph: 
Dense.c:134:17: note: node 0x84d1678 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:134:17: note: op template: _66 = (sizetype) j_162;
Dense.c:134:17: note: 	stmt 0 _66 = (sizetype) j_162;
Dense.c:134:17: note: 	stmt 1 _257 = (sizetype) wi_164;
Dense.c:134:17: note: 	children 0x84d1978
Dense.c:134:17: note: node (external) 0x84d1978 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:134:17: note: 	{ j_162, wi_164 }
Dense.c:134:17: note: Cost model analysis: 
Dense.c:134:17: note: Scalar 3 and vector 4 loop part do not match up, skipping scalar part
Dense.c:134:17: note: Cost model analysis for part in loop 4:
  Vector cost: 36
  Scalar cost: 8
Dense.c:134:17: missed: not vectorized: vectorization is not profitable.
Dense.c:117:17: note: Costing subgraph: 
Dense.c:117:17: note: node 0x84d0f78 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:117:17: note: op template: _41 = (sizetype) j_161;
Dense.c:117:17: note: 	stmt 0 _41 = (sizetype) j_161;
Dense.c:117:17: note: 	stmt 1 _277 = (sizetype) wi_163;
Dense.c:117:17: note: 	children 0x84d19f8
Dense.c:117:17: note: node (external) 0x84d19f8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:117:17: note: 	{ j_161, wi_163 }
Dense.c:117:17: note: Cost model analysis: 
Dense.c:117:17: note: Scalar 5 and vector 6 loop part do not match up, skipping scalar part
Dense.c:117:17: note: Cost model analysis for part in loop 6:
  Vector cost: 36
  Scalar cost: 8
Dense.c:117:17: missed: not vectorized: vectorization is not profitable.
Dense.c:117:17: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:83:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:83:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:112:51: note: ***** Analysis failed with vector mode V4QI
Dense.c:62:15: missed: couldn't vectorize loop
Dense.c:62:15: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:75:11: optimized: loop vectorized using 32 byte vectors
Dense.c:75:11: optimized: loop vectorized using 16 byte vectors
Dense.c:66:11: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:881:10: missed: not vectorized: no vectype for stmt: _56 = MEM[(__m256 * {ref-all})w_ptr_81];
 scalar_type: __m256
Dense.c:54:13: note: vectorized 1 loops in function.
Dense.c:57:2: missed: statement clobbers memory: memcpy (_5, _4, _3);
Dense.c:79:2: missed: statement clobbers memory: _28 (pretmp_162, _67, 0);
Dense.c:54:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:54:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:189:2: missed: statement clobbers memory: free (_1);
Dense.c:190:2: missed: statement clobbers memory: free (_2);
Dense.c:191:2: missed: statement clobbers memory: free (_3);
Dense.c:192:2: missed: statement clobbers memory: free (_4);
Dense.c:193:2: missed: statement clobbers memory: free (dense_7);
Dense.c:194:2: missed: statement clobbers memory: PULSE_DestroyLayer (this_6(D));
Dense.c:195:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:195:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:209:19: missed: couldn't vectorize loop
Dense.c:209:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:199:13: note: vectorized 0 loops in function.
Dense.c:201:47: missed: statement clobbers memory: dense_35 = malloc (40);
Dense.c:203:36: missed: statement clobbers memory: _5 = aligned_alloc (64, _4);
Dense.c:204:38: missed: statement clobbers memory: _6 = aligned_alloc (64, _4);
Dense.c:205:36: missed: statement clobbers memory: _8 = aligned_alloc (64, _7);
Dense.c:206:35: missed: statement clobbers memory: _9 = aligned_alloc (64, _7);
Dense.c:207:36: missed: statement clobbers memory: _10 = aligned_alloc (64, _7);
Dense.c:210:39: missed: statement clobbers memory: _11 = rand ();
Dense.c:210:73: missed: statement clobbers memory: _63 = sqrt (_18);
Dense.c:222:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:223:4: missed: statement clobbers memory: exit (1);
Dense.c:219:4: missed: statement clobbers memory: layer = PULSE_CreateLayer (n_inputs.3_29, n_outputs.2_28, 1, activation_function_51(D), _SIMD_FeedDense, _SIMD_BackDense, _SIMD_FixDense, _DestroyDense, 1); [return slot optimization]
Dense.c:216:12: missed: statement clobbers memory: layer = PULSE_CreateLayer (n_inputs.1_27, n_outputs.0_26, 1, activation_function_51(D), _FeedDense, _BackDense, _FixDense, _DestroyDense, 0); [return slot optimization]
Dense.c:229:9: missed: not vectorized: more than one data ref in stmt: <retval> = layer;
Dense.c:229:9: note: ***** Analysis failed with vector mode V4DI
Dense.c:229:9: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _30 (pretmp_91, _50, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:20: missed: not vectorized: no vectype for stmt: _21 = *_20;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:177:10: optimized: loop vectorized using 32 byte vectors
Dense.c:177:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:177:10: optimized: loop vectorized using 16 byte vectors
Dense.c:168:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_34, _77);
Dense.c:161:10: optimized: loop vectorized using 32 byte vectors
Dense.c:161:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:161:10: optimized: loop vectorized using 16 byte vectors
Dense.c:152:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, _69);
Dense.c:143:13: note: vectorized 2 loops in function.
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, _69);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_19, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_34, _77);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_38, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
Dense.c:143:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:143:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:143:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:143:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:143:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:143:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:143:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:143:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:112:24: missed: couldn't vectorize loop
Dense.c:112:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:115:17: missed: couldn't vectorize loop
Dense.c:115:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:129:24: missed: couldn't vectorize loop
Dense.c:129:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:132:17: missed: couldn't vectorize loop
Dense.c:132:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:103:10: optimized: loop vectorized using 32 byte vectors
Dense.c:103:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:103:10: optimized: loop vectorized using 16 byte vectors
Dense.c:91:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_13, _102);
Dense.c:83:13: note: vectorized 1 loops in function.
Dense.c:86:2: missed: statement clobbers memory: _1 (_3, _2, 1);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_13, _102);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, _103);
Dense.c:117:17: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:117:17: note: SLPing BB part
Dense.c:134:17: note: Costing subgraph: 
Dense.c:134:17: note: node 0x874ae78 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:134:17: note: op template: _66 = (sizetype) j_162;
Dense.c:134:17: note: 	stmt 0 _66 = (sizetype) j_162;
Dense.c:134:17: note: 	stmt 1 _259 = (sizetype) wi_164;
Dense.c:134:17: note: 	children 0x874af78
Dense.c:134:17: note: node (external) 0x874af78 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:134:17: note: 	{ j_162, wi_164 }
Dense.c:134:17: note: Cost model analysis: 
Dense.c:134:17: note: Scalar 3 and vector 4 loop part do not match up, skipping scalar part
Dense.c:134:17: note: Cost model analysis for part in loop 4:
  Vector cost: 36
  Scalar cost: 8
Dense.c:134:17: missed: not vectorized: vectorization is not profitable.
Dense.c:117:17: note: Costing subgraph: 
Dense.c:117:17: note: node 0x874b078 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:117:17: note: op template: _41 = (sizetype) j_161;
Dense.c:117:17: note: 	stmt 0 _41 = (sizetype) j_161;
Dense.c:117:17: note: 	stmt 1 _279 = (sizetype) wi_163;
Dense.c:117:17: note: 	children 0x874b178
Dense.c:117:17: note: node (external) 0x874b178 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:117:17: note: 	{ j_161, wi_163 }
Dense.c:117:17: note: Cost model analysis: 
Dense.c:117:17: note: Scalar 5 and vector 6 loop part do not match up, skipping scalar part
Dense.c:117:17: note: Cost model analysis for part in loop 6:
  Vector cost: 36
  Scalar cost: 8
Dense.c:117:17: missed: not vectorized: vectorization is not profitable.
Dense.c:117:17: note: ***** The result for vector mode V32QI would be the same
Dense.c:117:17: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:117:17: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:117:17: note: SLPing BB part
Dense.c:134:17: note: Costing subgraph: 
Dense.c:134:17: note: node 0x874ac78 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:134:17: note: op template: _66 = (sizetype) j_162;
Dense.c:134:17: note: 	stmt 0 _66 = (sizetype) j_162;
Dense.c:134:17: note: 	stmt 1 _259 = (sizetype) wi_164;
Dense.c:134:17: note: 	children 0x874b0f8
Dense.c:134:17: note: node (external) 0x874b0f8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:134:17: note: 	{ j_162, wi_164 }
Dense.c:134:17: note: Cost model analysis: 
Dense.c:134:17: note: Scalar 3 and vector 4 loop part do not match up, skipping scalar part
Dense.c:134:17: note: Cost model analysis for part in loop 4:
  Vector cost: 36
  Scalar cost: 8
Dense.c:134:17: missed: not vectorized: vectorization is not profitable.
Dense.c:117:17: note: Costing subgraph: 
Dense.c:117:17: note: node 0x874abf8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:117:17: note: op template: _41 = (sizetype) j_161;
Dense.c:117:17: note: 	stmt 0 _41 = (sizetype) j_161;
Dense.c:117:17: note: 	stmt 1 _279 = (sizetype) wi_163;
Dense.c:117:17: note: 	children 0x874b278
Dense.c:117:17: note: node (external) 0x874b278 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:117:17: note: 	{ j_161, wi_163 }
Dense.c:117:17: note: Cost model analysis: 
Dense.c:117:17: note: Scalar 5 and vector 6 loop part do not match up, skipping scalar part
Dense.c:117:17: note: Cost model analysis for part in loop 6:
  Vector cost: 36
  Scalar cost: 8
Dense.c:117:17: missed: not vectorized: vectorization is not profitable.
Dense.c:117:17: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:83:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:83:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:112:51: note: ***** Analysis failed with vector mode V4QI
Dense.c:62:15: missed: couldn't vectorize loop
Dense.c:62:15: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:75:11: optimized: loop vectorized using 32 byte vectors
Dense.c:75:11: optimized: loop vectorized using 16 byte vectors
Dense.c:66:11: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:881:10: missed: not vectorized: no vectype for stmt: _56 = MEM[(__m256 * {ref-all})w_ptr_81];
 scalar_type: __m256
Dense.c:54:13: note: vectorized 1 loops in function.
Dense.c:57:2: missed: statement clobbers memory: memcpy (_5, _4, _3);
Dense.c:79:2: missed: statement clobbers memory: _28 (pretmp_162, _67, 0);
Dense.c:54:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:54:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:189:2: missed: statement clobbers memory: free (_1);
Dense.c:190:2: missed: statement clobbers memory: free (_2);
Dense.c:191:2: missed: statement clobbers memory: free (_3);
Dense.c:192:2: missed: statement clobbers memory: free (_4);
Dense.c:193:2: missed: statement clobbers memory: free (dense_7);
Dense.c:194:2: missed: statement clobbers memory: PULSE_DestroyLayer (this_6(D));
Dense.c:195:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:195:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:209:19: missed: couldn't vectorize loop
Dense.c:209:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:199:13: note: vectorized 0 loops in function.
Dense.c:201:47: missed: statement clobbers memory: dense_35 = malloc (40);
Dense.c:203:36: missed: statement clobbers memory: _5 = aligned_alloc (64, _4);
Dense.c:204:38: missed: statement clobbers memory: _6 = aligned_alloc (64, _4);
Dense.c:205:36: missed: statement clobbers memory: _8 = aligned_alloc (64, _7);
Dense.c:206:35: missed: statement clobbers memory: _9 = aligned_alloc (64, _7);
Dense.c:207:36: missed: statement clobbers memory: _10 = aligned_alloc (64, _7);
Dense.c:210:39: missed: statement clobbers memory: _11 = rand ();
Dense.c:210:73: missed: statement clobbers memory: _63 = sqrt (_18);
Dense.c:222:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:223:4: missed: statement clobbers memory: exit (1);
Dense.c:219:4: missed: statement clobbers memory: layer = PULSE_CreateLayer (n_inputs.3_29, n_outputs.2_28, 1, activation_function_51(D), _SIMD_FeedDense, _SIMD_BackDense, _SIMD_FixDense, _DestroyDense, 1); [return slot optimization]
Dense.c:216:12: missed: statement clobbers memory: layer = PULSE_CreateLayer (n_inputs.1_27, n_outputs.0_26, 1, activation_function_51(D), _FeedDense, _BackDense, _FixDense, _DestroyDense, 0); [return slot optimization]
Dense.c:229:9: missed: not vectorized: more than one data ref in stmt: <retval> = layer;
Dense.c:229:9: note: ***** Analysis failed with vector mode V4DI
Dense.c:229:9: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _30 (pretmp_91, _50, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:20: missed: not vectorized: no vectype for stmt: _21 = *_20;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:189:10: optimized: loop vectorized using 32 byte vectors
Dense.c:189:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:189:10: optimized: loop vectorized using 16 byte vectors
Dense.c:180:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_34, _77);
Dense.c:173:10: optimized: loop vectorized using 32 byte vectors
Dense.c:173:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:173:10: optimized: loop vectorized using 16 byte vectors
Dense.c:164:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, _69);
Dense.c:155:13: note: vectorized 2 loops in function.
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, _69);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_19, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_34, _77);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_38, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
Dense.c:155:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:155:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:155:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:155:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:155:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:155:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:155:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:155:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:124:24: missed: couldn't vectorize loop
Dense.c:124:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:127:17: missed: couldn't vectorize loop
Dense.c:127:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:141:24: missed: couldn't vectorize loop
Dense.c:141:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:144:17: missed: couldn't vectorize loop
Dense.c:144:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:109:15: missed: couldn't vectorize loop
Dense.c:109:15: missed: not vectorized: number of iterations cannot be computed.
Dense.c:83:13: note: vectorized 0 loops in function.
Dense.c:86:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:129:17: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:129:17: note: SLPing BB part
Dense.c:146:17: note: Costing subgraph: 
Dense.c:146:17: note: node 0x75015f8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:146:17: note: op template: _49 = (sizetype) j_135;
Dense.c:146:17: note: 	stmt 0 _49 = (sizetype) j_135;
Dense.c:146:17: note: 	stmt 1 _209 = (sizetype) wi_137;
Dense.c:146:17: note: 	children 0x75016f8
Dense.c:146:17: note: node (external) 0x75016f8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:146:17: note: 	{ j_135, wi_137 }
Dense.c:146:17: note: Cost model analysis: 
Dense.c:146:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:146:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:146:17: missed: not vectorized: vectorization is not profitable.
Dense.c:129:17: note: Costing subgraph: 
Dense.c:129:17: note: node 0x75017f8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:129:17: note: op template: _24 = (sizetype) j_134;
Dense.c:129:17: note: 	stmt 0 _24 = (sizetype) j_134;
Dense.c:129:17: note: 	stmt 1 _229 = (sizetype) wi_136;
Dense.c:129:17: note: 	children 0x75018f8
Dense.c:129:17: note: node (external) 0x75018f8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:129:17: note: 	{ j_134, wi_136 }
Dense.c:129:17: note: Cost model analysis: 
Dense.c:129:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:129:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:129:17: missed: not vectorized: vectorization is not profitable.
Dense.c:129:17: note: ***** The result for vector mode V32QI would be the same
Dense.c:129:17: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:129:17: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:129:17: note: SLPing BB part
Dense.c:146:17: note: Costing subgraph: 
Dense.c:146:17: note: node 0x75017f8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:146:17: note: op template: _49 = (sizetype) j_135;
Dense.c:146:17: note: 	stmt 0 _49 = (sizetype) j_135;
Dense.c:146:17: note: 	stmt 1 _209 = (sizetype) wi_137;
Dense.c:146:17: note: 	children 0x75015f8
Dense.c:146:17: note: node (external) 0x75015f8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:146:17: note: 	{ j_135, wi_137 }
Dense.c:146:17: note: Cost model analysis: 
Dense.c:146:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:146:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:146:17: missed: not vectorized: vectorization is not profitable.
Dense.c:129:17: note: Costing subgraph: 
Dense.c:129:17: note: node 0x7501978 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:129:17: note: op template: _24 = (sizetype) j_134;
Dense.c:129:17: note: 	stmt 0 _24 = (sizetype) j_134;
Dense.c:129:17: note: 	stmt 1 _229 = (sizetype) wi_136;
Dense.c:129:17: note: 	children 0x7501878
Dense.c:129:17: note: node (external) 0x7501878 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:129:17: note: 	{ j_134, wi_136 }
Dense.c:129:17: note: Cost model analysis: 
Dense.c:129:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:129:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:129:17: missed: not vectorized: vectorization is not profitable.
Dense.c:129:17: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:124:51: note: ***** Analysis failed with vector mode V8QI
Dense.c:124:51: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:124:51: note: ***** Analysis failed with vector mode V4QI
Dense.c:62:15: missed: couldn't vectorize loop
Dense.c:62:15: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:75:11: optimized: loop vectorized using 32 byte vectors
Dense.c:75:11: optimized: loop vectorized using 16 byte vectors
Dense.c:66:11: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:881:10: missed: not vectorized: no vectype for stmt: _56 = MEM[(__m256 * {ref-all})w_ptr_81];
 scalar_type: __m256
Dense.c:54:13: note: vectorized 1 loops in function.
Dense.c:57:2: missed: statement clobbers memory: memcpy (_5, _4, _3);
Dense.c:79:2: missed: statement clobbers memory: _28 (pretmp_162, _67, 0);
Dense.c:54:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:54:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:201:2: missed: statement clobbers memory: free (_1);
Dense.c:202:2: missed: statement clobbers memory: free (_2);
Dense.c:203:2: missed: statement clobbers memory: free (_3);
Dense.c:204:2: missed: statement clobbers memory: free (_4);
Dense.c:205:2: missed: statement clobbers memory: free (dense_7);
Dense.c:206:2: missed: statement clobbers memory: PULSE_DestroyLayer (this_6(D));
Dense.c:207:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:207:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:221:19: missed: couldn't vectorize loop
Dense.c:221:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:211:13: note: vectorized 0 loops in function.
Dense.c:213:47: missed: statement clobbers memory: dense_35 = malloc (40);
Dense.c:215:36: missed: statement clobbers memory: _5 = aligned_alloc (64, _4);
Dense.c:216:38: missed: statement clobbers memory: _6 = aligned_alloc (64, _4);
Dense.c:217:36: missed: statement clobbers memory: _8 = aligned_alloc (64, _7);
Dense.c:218:35: missed: statement clobbers memory: _9 = aligned_alloc (64, _7);
Dense.c:219:36: missed: statement clobbers memory: _10 = aligned_alloc (64, _7);
Dense.c:222:39: missed: statement clobbers memory: _11 = rand ();
Dense.c:222:73: missed: statement clobbers memory: _63 = sqrt (_18);
Dense.c:234:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:235:4: missed: statement clobbers memory: exit (1);
Dense.c:231:4: missed: statement clobbers memory: layer = PULSE_CreateLayer (n_inputs.3_29, n_outputs.2_28, 1, activation_function_51(D), _SIMD_FeedDense, _SIMD_BackDense, _SIMD_FixDense, _DestroyDense, 1); [return slot optimization]
Dense.c:228:12: missed: statement clobbers memory: layer = PULSE_CreateLayer (n_inputs.1_27, n_outputs.0_26, 1, activation_function_51(D), _FeedDense, _BackDense, _FixDense, _DestroyDense, 0); [return slot optimization]
Dense.c:241:9: missed: not vectorized: more than one data ref in stmt: <retval> = layer;
Dense.c:241:9: note: ***** Analysis failed with vector mode V4DI
Dense.c:241:9: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _30 (pretmp_91, _50, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:20: missed: not vectorized: no vectype for stmt: _21 = *_20;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:172:19: missed: couldn't vectorize loop
Dense.c:172:19: missed: not vectorized: number of iterations cannot be computed.
Dense.c:163:19: missed: couldn't vectorize loop
Dense.c:163:19: missed: not vectorized: number of iterations cannot be computed.
Dense.c:156:13: note: vectorized 0 loops in function.
Dense.c:180:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:180:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:124:24: missed: couldn't vectorize loop
Dense.c:124:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:127:17: missed: couldn't vectorize loop
Dense.c:127:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:141:24: missed: couldn't vectorize loop
Dense.c:141:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:144:17: missed: couldn't vectorize loop
Dense.c:144:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:109:15: missed: couldn't vectorize loop
Dense.c:109:15: missed: not vectorized: number of iterations cannot be computed.
Dense.c:83:13: note: vectorized 0 loops in function.
Dense.c:86:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:129:17: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:129:17: note: SLPing BB part
Dense.c:146:17: note: Costing subgraph: 
Dense.c:146:17: note: node 0x83ea3a8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:146:17: note: op template: _49 = (sizetype) j_135;
Dense.c:146:17: note: 	stmt 0 _49 = (sizetype) j_135;
Dense.c:146:17: note: 	stmt 1 _209 = (sizetype) wi_137;
Dense.c:146:17: note: 	children 0x83ea4a8
Dense.c:146:17: note: node (external) 0x83ea4a8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:146:17: note: 	{ j_135, wi_137 }
Dense.c:146:17: note: Cost model analysis: 
Dense.c:146:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:146:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:146:17: missed: not vectorized: vectorization is not profitable.
Dense.c:129:17: note: Costing subgraph: 
Dense.c:129:17: note: node 0x83ea5a8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:129:17: note: op template: _24 = (sizetype) j_134;
Dense.c:129:17: note: 	stmt 0 _24 = (sizetype) j_134;
Dense.c:129:17: note: 	stmt 1 _229 = (sizetype) wi_136;
Dense.c:129:17: note: 	children 0x83ea6a8
Dense.c:129:17: note: node (external) 0x83ea6a8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:129:17: note: 	{ j_134, wi_136 }
Dense.c:129:17: note: Cost model analysis: 
Dense.c:129:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:129:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:129:17: missed: not vectorized: vectorization is not profitable.
Dense.c:129:17: note: ***** The result for vector mode V32QI would be the same
Dense.c:129:17: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:129:17: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:129:17: note: SLPing BB part
Dense.c:146:17: note: Costing subgraph: 
Dense.c:146:17: note: node 0x83ea5a8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:146:17: note: op template: _49 = (sizetype) j_135;
Dense.c:146:17: note: 	stmt 0 _49 = (sizetype) j_135;
Dense.c:146:17: note: 	stmt 1 _209 = (sizetype) wi_137;
Dense.c:146:17: note: 	children 0x83ea3a8
Dense.c:146:17: note: node (external) 0x83ea3a8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:146:17: note: 	{ j_135, wi_137 }
Dense.c:146:17: note: Cost model analysis: 
Dense.c:146:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:146:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:146:17: missed: not vectorized: vectorization is not profitable.
Dense.c:129:17: note: Costing subgraph: 
Dense.c:129:17: note: node 0x83ea728 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:129:17: note: op template: _24 = (sizetype) j_134;
Dense.c:129:17: note: 	stmt 0 _24 = (sizetype) j_134;
Dense.c:129:17: note: 	stmt 1 _229 = (sizetype) wi_136;
Dense.c:129:17: note: 	children 0x83ea628
Dense.c:129:17: note: node (external) 0x83ea628 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:129:17: note: 	{ j_134, wi_136 }
Dense.c:129:17: note: Cost model analysis: 
Dense.c:129:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:129:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:129:17: missed: not vectorized: vectorization is not profitable.
Dense.c:129:17: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:124:51: note: ***** Analysis failed with vector mode V8QI
Dense.c:124:51: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:124:51: note: ***** Analysis failed with vector mode V4QI
Dense.c:62:15: missed: couldn't vectorize loop
Dense.c:62:15: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:75:11: optimized: loop vectorized using 32 byte vectors
Dense.c:75:11: optimized: loop vectorized using 16 byte vectors
Dense.c:66:11: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:881:10: missed: not vectorized: no vectype for stmt: _56 = MEM[(__m256 * {ref-all})w_ptr_81];
 scalar_type: __m256
Dense.c:54:13: note: vectorized 1 loops in function.
Dense.c:57:2: missed: statement clobbers memory: memcpy (_5, _4, _3);
Dense.c:79:2: missed: statement clobbers memory: _28 (pretmp_162, _67, 0);
Dense.c:54:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:54:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:229:2: missed: statement clobbers memory: free (_1);
Dense.c:230:2: missed: statement clobbers memory: free (_2);
Dense.c:231:2: missed: statement clobbers memory: free (_3);
Dense.c:232:2: missed: statement clobbers memory: free (_4);
Dense.c:233:2: missed: statement clobbers memory: free (dense_7);
Dense.c:234:2: missed: statement clobbers memory: PULSE_DestroyLayer (this_6(D));
Dense.c:235:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:235:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:249:19: missed: couldn't vectorize loop
Dense.c:249:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:239:13: note: vectorized 0 loops in function.
Dense.c:241:47: missed: statement clobbers memory: dense_35 = malloc (40);
Dense.c:243:36: missed: statement clobbers memory: _5 = aligned_alloc (64, _4);
Dense.c:244:38: missed: statement clobbers memory: _6 = aligned_alloc (64, _4);
Dense.c:245:36: missed: statement clobbers memory: _8 = aligned_alloc (64, _7);
Dense.c:246:35: missed: statement clobbers memory: _9 = aligned_alloc (64, _7);
Dense.c:247:36: missed: statement clobbers memory: _10 = aligned_alloc (64, _7);
Dense.c:250:39: missed: statement clobbers memory: _11 = rand ();
Dense.c:250:73: missed: statement clobbers memory: _63 = sqrt (_18);
Dense.c:262:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:263:4: missed: statement clobbers memory: exit (1);
Dense.c:259:4: missed: statement clobbers memory: layer = PULSE_CreateLayer (n_inputs.3_29, n_outputs.2_28, 1, activation_function_51(D), _SIMD_FeedDense, _SIMD_BackDense, _SIMD_FixDense, _DestroyDense, 1); [return slot optimization]
Dense.c:256:12: missed: statement clobbers memory: layer = PULSE_CreateLayer (n_inputs.1_27, n_outputs.0_26, 1, activation_function_51(D), _FeedDense, _BackDense, _FixDense, _DestroyDense, 0); [return slot optimization]
Dense.c:269:9: missed: not vectorized: more than one data ref in stmt: <retval> = layer;
Dense.c:269:9: note: ***** Analysis failed with vector mode V4DI
Dense.c:269:9: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _30 (pretmp_91, _50, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:20: missed: not vectorized: no vectype for stmt: _21 = *_20;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:217:10: optimized: loop vectorized using 32 byte vectors
Dense.c:217:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:217:10: optimized: loop vectorized using 16 byte vectors
Dense.c:208:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_34, _77);
Dense.c:201:10: optimized: loop vectorized using 32 byte vectors
Dense.c:201:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:201:10: optimized: loop vectorized using 16 byte vectors
Dense.c:192:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, _69);
Dense.c:183:13: note: vectorized 2 loops in function.
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, _69);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_19, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_34, _77);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_38, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
Dense.c:183:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:183:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:183:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:183:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:183:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:183:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:183:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:183:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:124:24: missed: couldn't vectorize loop
Dense.c:124:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:127:17: missed: couldn't vectorize loop
Dense.c:127:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:141:24: missed: couldn't vectorize loop
Dense.c:141:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:144:17: missed: couldn't vectorize loop
Dense.c:144:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:109:15: missed: couldn't vectorize loop
Dense.c:109:15: missed: not vectorized: number of iterations cannot be computed.
Dense.c:83:13: note: vectorized 0 loops in function.
Dense.c:86:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:129:17: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:129:17: note: SLPing BB part
Dense.c:146:17: note: Costing subgraph: 
Dense.c:146:17: note: node 0x88de558 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:146:17: note: op template: _49 = (sizetype) j_135;
Dense.c:146:17: note: 	stmt 0 _49 = (sizetype) j_135;
Dense.c:146:17: note: 	stmt 1 _209 = (sizetype) wi_137;
Dense.c:146:17: note: 	children 0x88de658
Dense.c:146:17: note: node (external) 0x88de658 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:146:17: note: 	{ j_135, wi_137 }
Dense.c:146:17: note: Cost model analysis: 
Dense.c:146:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:146:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:146:17: missed: not vectorized: vectorization is not profitable.
Dense.c:129:17: note: Costing subgraph: 
Dense.c:129:17: note: node 0x88de758 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:129:17: note: op template: _24 = (sizetype) j_134;
Dense.c:129:17: note: 	stmt 0 _24 = (sizetype) j_134;
Dense.c:129:17: note: 	stmt 1 _229 = (sizetype) wi_136;
Dense.c:129:17: note: 	children 0x88de858
Dense.c:129:17: note: node (external) 0x88de858 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:129:17: note: 	{ j_134, wi_136 }
Dense.c:129:17: note: Cost model analysis: 
Dense.c:129:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:129:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:129:17: missed: not vectorized: vectorization is not profitable.
Dense.c:129:17: note: ***** The result for vector mode V32QI would be the same
Dense.c:129:17: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:129:17: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:129:17: note: SLPing BB part
Dense.c:146:17: note: Costing subgraph: 
Dense.c:146:17: note: node 0x88de758 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:146:17: note: op template: _49 = (sizetype) j_135;
Dense.c:146:17: note: 	stmt 0 _49 = (sizetype) j_135;
Dense.c:146:17: note: 	stmt 1 _209 = (sizetype) wi_137;
Dense.c:146:17: note: 	children 0x88de558
Dense.c:146:17: note: node (external) 0x88de558 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:146:17: note: 	{ j_135, wi_137 }
Dense.c:146:17: note: Cost model analysis: 
Dense.c:146:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:146:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:146:17: missed: not vectorized: vectorization is not profitable.
Dense.c:129:17: note: Costing subgraph: 
Dense.c:129:17: note: node 0x88de8d8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:129:17: note: op template: _24 = (sizetype) j_134;
Dense.c:129:17: note: 	stmt 0 _24 = (sizetype) j_134;
Dense.c:129:17: note: 	stmt 1 _229 = (sizetype) wi_136;
Dense.c:129:17: note: 	children 0x88de7d8
Dense.c:129:17: note: node (external) 0x88de7d8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:129:17: note: 	{ j_134, wi_136 }
Dense.c:129:17: note: Cost model analysis: 
Dense.c:129:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:129:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:129:17: missed: not vectorized: vectorization is not profitable.
Dense.c:129:17: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:124:51: note: ***** Analysis failed with vector mode V8QI
Dense.c:124:51: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:124:51: note: ***** Analysis failed with vector mode V4QI
Dense.c:62:15: missed: couldn't vectorize loop
Dense.c:62:15: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:75:11: optimized: loop vectorized using 32 byte vectors
Dense.c:75:11: optimized: loop vectorized using 16 byte vectors
Dense.c:66:11: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:881:10: missed: not vectorized: no vectype for stmt: _56 = MEM[(__m256 * {ref-all})w_ptr_81];
 scalar_type: __m256
Dense.c:54:13: note: vectorized 1 loops in function.
Dense.c:57:2: missed: statement clobbers memory: memcpy (_5, _4, _3);
Dense.c:79:2: missed: statement clobbers memory: _28 (pretmp_162, _67, 0);
Dense.c:54:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:54:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:229:2: missed: statement clobbers memory: free (_1);
Dense.c:230:2: missed: statement clobbers memory: free (_2);
Dense.c:231:2: missed: statement clobbers memory: free (_3);
Dense.c:232:2: missed: statement clobbers memory: free (_4);
Dense.c:233:2: missed: statement clobbers memory: free (dense_7);
Dense.c:234:2: missed: statement clobbers memory: PULSE_DestroyLayer (this_6(D));
Dense.c:235:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:235:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:249:19: missed: couldn't vectorize loop
Dense.c:249:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:239:13: note: vectorized 0 loops in function.
Dense.c:241:47: missed: statement clobbers memory: dense_35 = malloc (40);
Dense.c:243:36: missed: statement clobbers memory: _5 = aligned_alloc (64, _4);
Dense.c:244:38: missed: statement clobbers memory: _6 = aligned_alloc (64, _4);
Dense.c:245:36: missed: statement clobbers memory: _8 = aligned_alloc (64, _7);
Dense.c:246:35: missed: statement clobbers memory: _9 = aligned_alloc (64, _7);
Dense.c:247:36: missed: statement clobbers memory: _10 = aligned_alloc (64, _7);
Dense.c:250:39: missed: statement clobbers memory: _11 = rand ();
Dense.c:250:73: missed: statement clobbers memory: _63 = sqrt (_18);
Dense.c:262:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:263:4: missed: statement clobbers memory: exit (1);
Dense.c:259:4: missed: statement clobbers memory: layer = PULSE_CreateLayer (n_inputs.3_29, n_outputs.2_28, 1, activation_function_51(D), _SIMD_FeedDense, _SIMD_BackDense, _SIMD_FixDense, _DestroyDense, 1); [return slot optimization]
Dense.c:256:12: missed: statement clobbers memory: layer = PULSE_CreateLayer (n_inputs.1_27, n_outputs.0_26, 1, activation_function_51(D), _FeedDense, _BackDense, _FixDense, _DestroyDense, 0); [return slot optimization]
Dense.c:269:9: missed: not vectorized: more than one data ref in stmt: <retval> = layer;
Dense.c:269:9: note: ***** Analysis failed with vector mode V4DI
Dense.c:269:9: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _30 (pretmp_91, _50, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:20: missed: not vectorized: no vectype for stmt: _21 = *_20;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:189:10: optimized: loop vectorized using 32 byte vectors
Dense.c:189:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:189:10: optimized: loop vectorized using 16 byte vectors
Dense.c:180:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_34, _77);
Dense.c:173:10: optimized: loop vectorized using 32 byte vectors
Dense.c:173:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:173:10: optimized: loop vectorized using 16 byte vectors
Dense.c:164:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, _69);
Dense.c:155:13: note: vectorized 2 loops in function.
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, _69);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_19, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_34, _77);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_38, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
Dense.c:155:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:155:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:155:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:155:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:155:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:155:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:155:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:155:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:124:24: missed: couldn't vectorize loop
Dense.c:124:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:127:17: missed: couldn't vectorize loop
Dense.c:127:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:141:24: missed: couldn't vectorize loop
Dense.c:141:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:144:17: missed: couldn't vectorize loop
Dense.c:144:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:103:10: optimized: loop vectorized using 32 byte vectors
Dense.c:103:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:103:10: optimized: loop vectorized using 16 byte vectors
Dense.c:91:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_13, _102);
Dense.c:83:13: note: vectorized 1 loops in function.
Dense.c:86:2: missed: statement clobbers memory: _1 (_3, _2, 1);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_13, _102);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, _103);
Dense.c:129:17: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:129:17: note: SLPing BB part
Dense.c:146:17: note: Costing subgraph: 
Dense.c:146:17: note: node 0x7524318 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:146:17: note: op template: _66 = (sizetype) j_162;
Dense.c:146:17: note: 	stmt 0 _66 = (sizetype) j_162;
Dense.c:146:17: note: 	stmt 1 _259 = (sizetype) wi_164;
Dense.c:146:17: note: 	children 0x7524418
Dense.c:146:17: note: node (external) 0x7524418 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:146:17: note: 	{ j_162, wi_164 }
Dense.c:146:17: note: Cost model analysis: 
Dense.c:146:17: note: Scalar 3 and vector 4 loop part do not match up, skipping scalar part
Dense.c:146:17: note: Cost model analysis for part in loop 4:
  Vector cost: 36
  Scalar cost: 8
Dense.c:146:17: missed: not vectorized: vectorization is not profitable.
Dense.c:129:17: note: Costing subgraph: 
Dense.c:129:17: note: node 0x7524518 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:129:17: note: op template: _41 = (sizetype) j_161;
Dense.c:129:17: note: 	stmt 0 _41 = (sizetype) j_161;
Dense.c:129:17: note: 	stmt 1 _279 = (sizetype) wi_163;
Dense.c:129:17: note: 	children 0x7524618
Dense.c:129:17: note: node (external) 0x7524618 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:129:17: note: 	{ j_161, wi_163 }
Dense.c:129:17: note: Cost model analysis: 
Dense.c:129:17: note: Scalar 5 and vector 6 loop part do not match up, skipping scalar part
Dense.c:129:17: note: Cost model analysis for part in loop 6:
  Vector cost: 36
  Scalar cost: 8
Dense.c:129:17: missed: not vectorized: vectorization is not profitable.
Dense.c:129:17: note: ***** The result for vector mode V32QI would be the same
Dense.c:129:17: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:129:17: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:129:17: note: SLPing BB part
Dense.c:146:17: note: Costing subgraph: 
Dense.c:146:17: note: node 0x7524118 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:146:17: note: op template: _66 = (sizetype) j_162;
Dense.c:146:17: note: 	stmt 0 _66 = (sizetype) j_162;
Dense.c:146:17: note: 	stmt 1 _259 = (sizetype) wi_164;
Dense.c:146:17: note: 	children 0x7524598
Dense.c:146:17: note: node (external) 0x7524598 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:146:17: note: 	{ j_162, wi_164 }
Dense.c:146:17: note: Cost model analysis: 
Dense.c:146:17: note: Scalar 3 and vector 4 loop part do not match up, skipping scalar part
Dense.c:146:17: note: Cost model analysis for part in loop 4:
  Vector cost: 36
  Scalar cost: 8
Dense.c:146:17: missed: not vectorized: vectorization is not profitable.
Dense.c:129:17: note: Costing subgraph: 
Dense.c:129:17: note: node 0x7524098 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:129:17: note: op template: _41 = (sizetype) j_161;
Dense.c:129:17: note: 	stmt 0 _41 = (sizetype) j_161;
Dense.c:129:17: note: 	stmt 1 _279 = (sizetype) wi_163;
Dense.c:129:17: note: 	children 0x7524718
Dense.c:129:17: note: node (external) 0x7524718 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:129:17: note: 	{ j_161, wi_163 }
Dense.c:129:17: note: Cost model analysis: 
Dense.c:129:17: note: Scalar 5 and vector 6 loop part do not match up, skipping scalar part
Dense.c:129:17: note: Cost model analysis for part in loop 6:
  Vector cost: 36
  Scalar cost: 8
Dense.c:129:17: missed: not vectorized: vectorization is not profitable.
Dense.c:129:17: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:83:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:83:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:124:51: note: ***** Analysis failed with vector mode V4QI
Dense.c:62:15: missed: couldn't vectorize loop
Dense.c:62:15: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:75:11: optimized: loop vectorized using 32 byte vectors
Dense.c:75:11: optimized: loop vectorized using 16 byte vectors
Dense.c:66:11: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:881:10: missed: not vectorized: no vectype for stmt: _56 = MEM[(__m256 * {ref-all})w_ptr_81];
 scalar_type: __m256
Dense.c:54:13: note: vectorized 1 loops in function.
Dense.c:57:2: missed: statement clobbers memory: memcpy (_5, _4, _3);
Dense.c:79:2: missed: statement clobbers memory: _28 (pretmp_162, _67, 0);
Dense.c:54:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:54:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:201:2: missed: statement clobbers memory: free (_1);
Dense.c:202:2: missed: statement clobbers memory: free (_2);
Dense.c:203:2: missed: statement clobbers memory: free (_3);
Dense.c:204:2: missed: statement clobbers memory: free (_4);
Dense.c:205:2: missed: statement clobbers memory: free (dense_7);
Dense.c:206:2: missed: statement clobbers memory: PULSE_DestroyLayer (this_6(D));
Dense.c:207:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:207:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:221:19: missed: couldn't vectorize loop
Dense.c:221:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:211:13: note: vectorized 0 loops in function.
Dense.c:213:47: missed: statement clobbers memory: dense_35 = malloc (40);
Dense.c:215:36: missed: statement clobbers memory: _5 = aligned_alloc (64, _4);
Dense.c:216:38: missed: statement clobbers memory: _6 = aligned_alloc (64, _4);
Dense.c:217:36: missed: statement clobbers memory: _8 = aligned_alloc (64, _7);
Dense.c:218:35: missed: statement clobbers memory: _9 = aligned_alloc (64, _7);
Dense.c:219:36: missed: statement clobbers memory: _10 = aligned_alloc (64, _7);
Dense.c:222:39: missed: statement clobbers memory: _11 = rand ();
Dense.c:222:73: missed: statement clobbers memory: _63 = sqrt (_18);
Dense.c:234:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:235:4: missed: statement clobbers memory: exit (1);
Dense.c:231:4: missed: statement clobbers memory: layer = PULSE_CreateLayer (n_inputs.3_29, n_outputs.2_28, 1, activation_function_51(D), _SIMD_FeedDense, _SIMD_BackDense, _SIMD_FixDense, _DestroyDense, 1); [return slot optimization]
Dense.c:228:12: missed: statement clobbers memory: layer = PULSE_CreateLayer (n_inputs.1_27, n_outputs.0_26, 1, activation_function_51(D), _FeedDense, _BackDense, _FixDense, _DestroyDense, 0); [return slot optimization]
Dense.c:241:9: missed: not vectorized: more than one data ref in stmt: <retval> = layer;
Dense.c:241:9: note: ***** Analysis failed with vector mode V4DI
Dense.c:241:9: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _30 (pretmp_91, _50, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:20: missed: not vectorized: no vectype for stmt: _21 = *_20;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:170:10: optimized: loop vectorized using 32 byte vectors
Dense.c:170:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:170:10: optimized: loop vectorized using 16 byte vectors
Dense.c:161:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_34, _77);
Dense.c:154:10: optimized: loop vectorized using 32 byte vectors
Dense.c:154:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:154:10: optimized: loop vectorized using 16 byte vectors
Dense.c:145:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, _69);
Dense.c:136:13: note: vectorized 2 loops in function.
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, _69);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_19, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_34, _77);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_38, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
Dense.c:136:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:136:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:105:24: missed: couldn't vectorize loop
Dense.c:105:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:108:17: missed: couldn't vectorize loop
Dense.c:108:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:122:24: missed: couldn't vectorize loop
Dense.c:122:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:125:17: missed: couldn't vectorize loop
Dense.c:125:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:90:15: missed: couldn't vectorize loop
Dense.c:90:15: missed: not vectorized: number of iterations cannot be computed.
Dense.c:83:13: note: vectorized 0 loops in function.
Dense.c:86:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:110:17: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:110:17: note: SLPing BB part
Dense.c:127:17: note: Costing subgraph: 
Dense.c:127:17: note: node 0x83e9b58 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:127:17: note: op template: _49 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 0 _49 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 1 _209 = (sizetype) wi_137;
Dense.c:127:17: note: 	children 0x83e9c58
Dense.c:127:17: note: node (external) 0x83e9c58 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:127:17: note: 	{ j_135, wi_137 }
Dense.c:127:17: note: Cost model analysis: 
Dense.c:127:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:127:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:127:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: Costing subgraph: 
Dense.c:110:17: note: node 0x83e9d58 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:110:17: note: op template: _24 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 0 _24 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 1 _229 = (sizetype) wi_136;
Dense.c:110:17: note: 	children 0x83e9e58
Dense.c:110:17: note: node (external) 0x83e9e58 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:110:17: note: 	{ j_134, wi_136 }
Dense.c:110:17: note: Cost model analysis: 
Dense.c:110:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:110:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:110:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: ***** The result for vector mode V32QI would be the same
Dense.c:110:17: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:110:17: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:110:17: note: SLPing BB part
Dense.c:127:17: note: Costing subgraph: 
Dense.c:127:17: note: node 0x83e9d58 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:127:17: note: op template: _49 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 0 _49 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 1 _209 = (sizetype) wi_137;
Dense.c:127:17: note: 	children 0x83e9b58
Dense.c:127:17: note: node (external) 0x83e9b58 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:127:17: note: 	{ j_135, wi_137 }
Dense.c:127:17: note: Cost model analysis: 
Dense.c:127:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:127:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:127:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: Costing subgraph: 
Dense.c:110:17: note: node 0x83e9ed8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:110:17: note: op template: _24 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 0 _24 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 1 _229 = (sizetype) wi_136;
Dense.c:110:17: note: 	children 0x83e9dd8
Dense.c:110:17: note: node (external) 0x83e9dd8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:110:17: note: 	{ j_134, wi_136 }
Dense.c:110:17: note: Cost model analysis: 
Dense.c:110:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:110:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:110:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:105:51: note: ***** Analysis failed with vector mode V8QI
Dense.c:105:51: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:105:51: note: ***** Analysis failed with vector mode V4QI
Dense.c:62:15: missed: couldn't vectorize loop
Dense.c:62:15: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:75:11: optimized: loop vectorized using 32 byte vectors
Dense.c:75:11: optimized: loop vectorized using 16 byte vectors
Dense.c:66:11: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:881:10: missed: not vectorized: no vectype for stmt: _56 = MEM[(__m256 * {ref-all})w_ptr_81];
 scalar_type: __m256
Dense.c:54:13: note: vectorized 1 loops in function.
Dense.c:57:2: missed: statement clobbers memory: memcpy (_5, _4, _3);
Dense.c:79:2: missed: statement clobbers memory: _28 (pretmp_162, _67, 0);
Dense.c:54:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:54:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:182:2: missed: statement clobbers memory: free (_1);
Dense.c:183:2: missed: statement clobbers memory: free (_2);
Dense.c:184:2: missed: statement clobbers memory: free (_3);
Dense.c:185:2: missed: statement clobbers memory: free (_4);
Dense.c:186:2: missed: statement clobbers memory: free (dense_7);
Dense.c:187:2: missed: statement clobbers memory: PULSE_DestroyLayer (this_6(D));
Dense.c:188:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:188:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:202:19: missed: couldn't vectorize loop
Dense.c:202:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:192:13: note: vectorized 0 loops in function.
Dense.c:194:47: missed: statement clobbers memory: dense_35 = malloc (40);
Dense.c:196:36: missed: statement clobbers memory: _5 = aligned_alloc (64, _4);
Dense.c:197:38: missed: statement clobbers memory: _6 = aligned_alloc (64, _4);
Dense.c:198:36: missed: statement clobbers memory: _8 = aligned_alloc (64, _7);
Dense.c:199:35: missed: statement clobbers memory: _9 = aligned_alloc (64, _7);
Dense.c:200:36: missed: statement clobbers memory: _10 = aligned_alloc (64, _7);
Dense.c:203:39: missed: statement clobbers memory: _11 = rand ();
Dense.c:203:73: missed: statement clobbers memory: _63 = sqrt (_18);
Dense.c:215:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:216:4: missed: statement clobbers memory: exit (1);
Dense.c:212:4: missed: statement clobbers memory: layer = PULSE_CreateLayer (n_inputs.3_29, n_outputs.2_28, 1, activation_function_51(D), _SIMD_FeedDense, _SIMD_BackDense, _SIMD_FixDense, _DestroyDense, 1); [return slot optimization]
Dense.c:209:12: missed: statement clobbers memory: layer = PULSE_CreateLayer (n_inputs.1_27, n_outputs.0_26, 1, activation_function_51(D), _FeedDense, _BackDense, _FixDense, _DestroyDense, 0); [return slot optimization]
Dense.c:222:9: missed: not vectorized: more than one data ref in stmt: <retval> = layer;
Dense.c:222:9: note: ***** Analysis failed with vector mode V4DI
Dense.c:222:9: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:4:18: note: vectorized 0 loops in function.
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_4, inputs_15, _3);
PULSE.c:8:2: missed: statement clobbers memory: _5 (layer_17);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_12, pretmp_26, _9);
PULSE.c:13:28: note: ***** Analysis failed with vector mode V8SI
PULSE.c:13:28: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:22:2: missed: statement clobbers memory: _1 (layer_10(D));
PULSE.c:22:2: missed: statement clobbers memory: _14 (_2);
PULSE.c:22:2: missed: statement clobbers memory: _21 (_15);
PULSE.c:22:2: missed: statement clobbers memory: _28 (_22);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_29);
PULSE.c:26:3: missed: statement clobbers memory: memset (_34, 0, _32);
PULSE.c:26:3: missed: statement clobbers memory: memset (_27, 0, _25);
PULSE.c:26:3: missed: statement clobbers memory: memset (_20, 0, _18);
PULSE.c:26:3: missed: statement clobbers memory: memset (_7, 0, _5);
PULSE.c:28:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:28:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:32:6: note: vectorized 1 loops in function.
PULSE.c:34:8: missed: statement clobbers memory: _1 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_2);
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _9 = rand ();
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8SI
PULSE.c:32:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:61:16: missed: couldn't vectorize loop
PULSE.c:61:16: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:64:17: missed: couldn't vectorize loop
PULSE.c:64:17: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:73:19: missed: couldn't vectorize loop
PULSE.c:73:19: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _67 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:53:22: missed: couldn't vectorize loop
PULSE.c:53:22: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:47:6: note: vectorized 1 loops in function.
PULSE.c:48:1: missed: statement clobbers memory: saved_stack.7_37 = __builtin_stack_save ();
PULSE.c:49:46: missed: statement clobbers memory: _1 = PULSE_GetLossFunctionPtr (loss_function_38(D));
PULSE.c:60:10: missed: statement clobbers memory: random.4_44 = __builtin_alloca_with_align (_4, 32);
PULSE.c:34:8: missed: statement clobbers memory: _59 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_60);
PULSE.c:39:29: missed: statement clobbers memory: _67 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _70 = rand ();
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_96, inputs_91, _95);
PULSE.c:8:2: missed: statement clobbers memory: _97 (layer_92);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_103, _102, _101);
PULSE.c:67:11: missed: statement clobbers memory: loss_53 = PULSE_GetLoss_40 (_17, _16, _12, _11);
PULSE.c:22:2: missed: statement clobbers memory: _84 (output_32);
PULSE.c:22:2: missed: statement clobbers memory: _106 (_85);
PULSE.c:22:2: missed: statement clobbers memory: _113 (_107);
PULSE.c:22:2: missed: statement clobbers memory: _120 (_114);
PULSE.c:22:2: missed: statement clobbers memory: _127 (_121);
PULSE.c:22:2: missed: statement clobbers memory: _134 (_128);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_135);
PULSE.c:26:3: missed: statement clobbers memory: memset (_140, 0, _138);
PULSE.c:26:3: missed: statement clobbers memory: memset (_133, 0, _131);
PULSE.c:26:3: missed: statement clobbers memory: memset (_126, 0, _124);
PULSE.c:26:3: missed: statement clobbers memory: memset (_119, 0, _117);
PULSE.c:26:3: missed: statement clobbers memory: memset (_112, 0, _110);
PULSE.c:26:3: missed: statement clobbers memory: memset (_90, 0, _88);
PULSE.c:75:6: missed: statement clobbers memory: _21 (current_188, args);
PULSE.c:81:4: missed: statement clobbers memory: printf ("Epoch: %d Item: %d Loss: %.10f Batch Loss: %.10f\r", i_185, j_186, _23, _22);
PULSE.c:47:6: missed: statement clobbers memory: __builtin_stack_restore (saved_stack.7_37);
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4DI
PULSE.c:47:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:91:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:91:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:96:8: missed: couldn't vectorize loop
PULSE.c:96:8: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:93:6: note: vectorized 0 loops in function.
PULSE.c:98:3: missed: statement clobbers memory: _1 (current_10);
PULSE.c:101:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:101:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:4:18: note: vectorized 0 loops in function.
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_4, inputs_15, _3);
PULSE.c:8:2: missed: statement clobbers memory: _5 (layer_17);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_12, pretmp_26, _9);
PULSE.c:13:28: note: ***** Analysis failed with vector mode V8SI
PULSE.c:13:28: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:22:2: missed: statement clobbers memory: _1 (layer_10(D));
PULSE.c:22:2: missed: statement clobbers memory: _14 (_2);
PULSE.c:22:2: missed: statement clobbers memory: _21 (_15);
PULSE.c:22:2: missed: statement clobbers memory: _28 (_22);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_29);
PULSE.c:26:3: missed: statement clobbers memory: memset (_34, 0, _32);
PULSE.c:26:3: missed: statement clobbers memory: memset (_27, 0, _25);
PULSE.c:26:3: missed: statement clobbers memory: memset (_20, 0, _18);
PULSE.c:26:3: missed: statement clobbers memory: memset (_7, 0, _5);
PULSE.c:28:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:28:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:32:6: note: vectorized 1 loops in function.
PULSE.c:34:8: missed: statement clobbers memory: _1 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_2);
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _9 = rand ();
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8SI
PULSE.c:32:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:61:16: missed: couldn't vectorize loop
PULSE.c:61:16: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:64:17: missed: couldn't vectorize loop
PULSE.c:64:17: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:73:19: missed: couldn't vectorize loop
PULSE.c:73:19: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _69 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:53:22: missed: couldn't vectorize loop
PULSE.c:53:22: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:47:6: note: vectorized 1 loops in function.
PULSE.c:48:1: missed: statement clobbers memory: saved_stack.7_39 = __builtin_stack_save ();
PULSE.c:49:46: missed: statement clobbers memory: _1 = PULSE_GetLossFunctionPtr (loss_function_40(D));
PULSE.c:60:10: missed: statement clobbers memory: random.4_46 = __builtin_alloca_with_align (_4, 32);
PULSE.c:34:8: missed: statement clobbers memory: _61 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_62);
PULSE.c:39:29: missed: statement clobbers memory: _69 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _72 = rand ();
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_98, inputs_93, _97);
PULSE.c:8:2: missed: statement clobbers memory: _99 (layer_94);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_105, _104, _103);
PULSE.c:67:11: missed: statement clobbers memory: loss_55 = PULSE_GetLoss_42 (_17, _16, _12, _11);
PULSE.c:22:2: missed: statement clobbers memory: _86 (output_34);
PULSE.c:22:2: missed: statement clobbers memory: _108 (_87);
PULSE.c:22:2: missed: statement clobbers memory: _115 (_109);
PULSE.c:22:2: missed: statement clobbers memory: _122 (_116);
PULSE.c:22:2: missed: statement clobbers memory: _129 (_123);
PULSE.c:22:2: missed: statement clobbers memory: _136 (_130);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_137);
PULSE.c:26:3: missed: statement clobbers memory: memset (_142, 0, _140);
PULSE.c:26:3: missed: statement clobbers memory: memset (_135, 0, _133);
PULSE.c:26:3: missed: statement clobbers memory: memset (_128, 0, _126);
PULSE.c:26:3: missed: statement clobbers memory: memset (_121, 0, _119);
PULSE.c:26:3: missed: statement clobbers memory: memset (_114, 0, _112);
PULSE.c:26:3: missed: statement clobbers memory: memset (_92, 0, _90);
PULSE.c:75:6: missed: statement clobbers memory: _21 (current_191, args);
PULSE.c:81:4: missed: statement clobbers memory: printf ("Epoch: %d Item: %d Loss: %.10f Batch Loss: %.10f\r", i_188, j_189, _25, _24);
PULSE.c:47:6: missed: statement clobbers memory: __builtin_stack_restore (saved_stack.7_39);
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4DI
PULSE.c:47:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:91:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:91:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:96:8: missed: couldn't vectorize loop
PULSE.c:96:8: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:93:6: note: vectorized 0 loops in function.
PULSE.c:98:3: missed: statement clobbers memory: _1 (current_10);
PULSE.c:101:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:101:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:4:18: note: vectorized 0 loops in function.
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_4, inputs_15, _3);
PULSE.c:8:2: missed: statement clobbers memory: _5 (layer_17);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_12, pretmp_26, _9);
PULSE.c:13:28: note: ***** Analysis failed with vector mode V8SI
PULSE.c:13:28: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:22:2: missed: statement clobbers memory: _1 (layer_10(D));
PULSE.c:22:2: missed: statement clobbers memory: _14 (_2);
PULSE.c:22:2: missed: statement clobbers memory: _21 (_15);
PULSE.c:22:2: missed: statement clobbers memory: _28 (_22);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_29);
PULSE.c:26:3: missed: statement clobbers memory: memset (_34, 0, _32);
PULSE.c:26:3: missed: statement clobbers memory: memset (_27, 0, _25);
PULSE.c:26:3: missed: statement clobbers memory: memset (_20, 0, _18);
PULSE.c:26:3: missed: statement clobbers memory: memset (_7, 0, _5);
PULSE.c:28:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:28:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:32:6: note: vectorized 1 loops in function.
PULSE.c:34:8: missed: statement clobbers memory: _1 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_2);
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _9 = rand ();
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8SI
PULSE.c:32:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:61:16: missed: couldn't vectorize loop
PULSE.c:61:16: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:64:17: missed: couldn't vectorize loop
PULSE.c:64:17: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:73:19: missed: couldn't vectorize loop
PULSE.c:73:19: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _69 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:53:22: missed: couldn't vectorize loop
PULSE.c:53:22: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:47:6: note: vectorized 1 loops in function.
PULSE.c:48:1: missed: statement clobbers memory: saved_stack.7_39 = __builtin_stack_save ();
PULSE.c:49:46: missed: statement clobbers memory: _1 = PULSE_GetLossFunctionPtr (loss_function_40(D));
PULSE.c:60:10: missed: statement clobbers memory: random.4_46 = __builtin_alloca_with_align (_4, 32);
PULSE.c:34:8: missed: statement clobbers memory: _61 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_62);
PULSE.c:39:29: missed: statement clobbers memory: _69 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _72 = rand ();
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_98, inputs_93, _97);
PULSE.c:8:2: missed: statement clobbers memory: _99 (layer_94);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_105, _104, _103);
PULSE.c:67:11: missed: statement clobbers memory: loss_55 = PULSE_GetLoss_42 (_17, _16, _12, _11);
PULSE.c:22:2: missed: statement clobbers memory: _86 (output_34);
PULSE.c:22:2: missed: statement clobbers memory: _108 (_87);
PULSE.c:22:2: missed: statement clobbers memory: _115 (_109);
PULSE.c:22:2: missed: statement clobbers memory: _122 (_116);
PULSE.c:22:2: missed: statement clobbers memory: _129 (_123);
PULSE.c:22:2: missed: statement clobbers memory: _136 (_130);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_137);
PULSE.c:26:3: missed: statement clobbers memory: memset (_142, 0, _140);
PULSE.c:26:3: missed: statement clobbers memory: memset (_135, 0, _133);
PULSE.c:26:3: missed: statement clobbers memory: memset (_128, 0, _126);
PULSE.c:26:3: missed: statement clobbers memory: memset (_121, 0, _119);
PULSE.c:26:3: missed: statement clobbers memory: memset (_114, 0, _112);
PULSE.c:26:3: missed: statement clobbers memory: memset (_92, 0, _90);
PULSE.c:75:6: missed: statement clobbers memory: _21 (current_191, args);
PULSE.c:81:4: missed: statement clobbers memory: printf ("Epoch: %d Item: %d Loss: %.10f BatchLoss: %.10f\r", i_188, j_189, _25, _24);
PULSE.c:47:6: missed: statement clobbers memory: __builtin_stack_restore (saved_stack.7_39);
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4DI
PULSE.c:47:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:91:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:91:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:96:8: missed: couldn't vectorize loop
PULSE.c:96:8: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:93:6: note: vectorized 0 loops in function.
PULSE.c:98:3: missed: statement clobbers memory: _1 (current_10);
PULSE.c:101:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:101:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:4:18: note: vectorized 0 loops in function.
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_4, inputs_15, _3);
PULSE.c:8:2: missed: statement clobbers memory: _5 (layer_17);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_12, pretmp_26, _9);
PULSE.c:13:28: note: ***** Analysis failed with vector mode V8SI
PULSE.c:13:28: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:22:2: missed: statement clobbers memory: _1 (layer_10(D));
PULSE.c:22:2: missed: statement clobbers memory: _14 (_2);
PULSE.c:22:2: missed: statement clobbers memory: _21 (_15);
PULSE.c:22:2: missed: statement clobbers memory: _28 (_22);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_29);
PULSE.c:26:3: missed: statement clobbers memory: memset (_34, 0, _32);
PULSE.c:26:3: missed: statement clobbers memory: memset (_27, 0, _25);
PULSE.c:26:3: missed: statement clobbers memory: memset (_20, 0, _18);
PULSE.c:26:3: missed: statement clobbers memory: memset (_7, 0, _5);
PULSE.c:28:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:28:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:32:6: note: vectorized 1 loops in function.
PULSE.c:34:8: missed: statement clobbers memory: _1 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_2);
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _9 = rand ();
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8SI
PULSE.c:32:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:61:16: missed: couldn't vectorize loop
PULSE.c:61:16: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:64:17: missed: couldn't vectorize loop
PULSE.c:64:17: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:73:19: missed: couldn't vectorize loop
PULSE.c:73:19: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _69 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:53:22: missed: couldn't vectorize loop
PULSE.c:53:22: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:47:6: note: vectorized 1 loops in function.
PULSE.c:48:1: missed: statement clobbers memory: saved_stack.7_39 = __builtin_stack_save ();
PULSE.c:49:46: missed: statement clobbers memory: _1 = PULSE_GetLossFunctionPtr (loss_function_40(D));
PULSE.c:60:10: missed: statement clobbers memory: random.4_46 = __builtin_alloca_with_align (_4, 32);
PULSE.c:34:8: missed: statement clobbers memory: _61 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_62);
PULSE.c:39:29: missed: statement clobbers memory: _69 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _72 = rand ();
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_98, inputs_93, _97);
PULSE.c:8:2: missed: statement clobbers memory: _99 (layer_94);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_105, _104, _103);
PULSE.c:67:11: missed: statement clobbers memory: loss_55 = PULSE_GetLoss_42 (_17, _16, _12, _11);
PULSE.c:22:2: missed: statement clobbers memory: _86 (output_34);
PULSE.c:22:2: missed: statement clobbers memory: _108 (_87);
PULSE.c:22:2: missed: statement clobbers memory: _115 (_109);
PULSE.c:22:2: missed: statement clobbers memory: _122 (_116);
PULSE.c:22:2: missed: statement clobbers memory: _129 (_123);
PULSE.c:22:2: missed: statement clobbers memory: _136 (_130);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_137);
PULSE.c:26:3: missed: statement clobbers memory: memset (_142, 0, _140);
PULSE.c:26:3: missed: statement clobbers memory: memset (_135, 0, _133);
PULSE.c:26:3: missed: statement clobbers memory: memset (_128, 0, _126);
PULSE.c:26:3: missed: statement clobbers memory: memset (_121, 0, _119);
PULSE.c:26:3: missed: statement clobbers memory: memset (_114, 0, _112);
PULSE.c:26:3: missed: statement clobbers memory: memset (_92, 0, _90);
PULSE.c:75:6: missed: statement clobbers memory: _21 (current_191, args);
PULSE.c:81:4: missed: statement clobbers memory: printf ("Epoch: %d | Item: %d | Loss: %.10f | Batch Loss: %.10f\r", i_188, j_189, _25, _24);
PULSE.c:47:6: missed: statement clobbers memory: __builtin_stack_restore (saved_stack.7_39);
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4DI
PULSE.c:47:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:91:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:91:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:96:8: missed: couldn't vectorize loop
PULSE.c:96:8: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:93:6: note: vectorized 0 loops in function.
PULSE.c:98:3: missed: statement clobbers memory: _1 (current_10);
PULSE.c:101:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:101:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Layer.c:7:34: missed: statement clobbers memory: _3 = aligned_alloc (64, _2);
Layer.c:8:35: missed: statement clobbers memory: _6 = aligned_alloc (64, _5);
Layer.c:9:34: missed: statement clobbers memory: _7 = aligned_alloc (64, _5);
Layer.c:14:19: missed: statement clobbers memory: _8 = PULSE_GetActivationFunctionPtr (activation_function_18(D));
Layer.c:19:9: note: ***** Analysis succeeded with vector mode V8SI
Layer.c:19:9: note: SLPing BB part
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x50157c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: op template: MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: 	stmt 0 MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: 	stmt 1 MEM <unsigned int> [(struct PULSE_Layer *)&<retval> + 4B] = optimization_type_17(D);
Layer.c:19:9: note: 	children 0x5015848
Layer.c:19:9: note: node (external) 0x5015848 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ type_16(D), optimization_type_17(D) }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x50158c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: op template: <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: 	stmt 0 <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: 	stmt 1 <retval>.n_outputs = n_outputs_12(D);
Layer.c:19:9: note: 	children 0x5015948
Layer.c:19:9: note: node (external) 0x5015948 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ n_inputs_11(D), n_outputs_12(D) }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x5015a48 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: <retval>.inputs = _3;
Layer.c:19:9: note: 	stmt 0 <retval>.inputs = _3;
Layer.c:19:9: note: 	stmt 1 <retval>.outputs = _6;
Layer.c:19:9: note: 	children 0x5015ac8
Layer.c:19:9: note: node (external) 0x5015ac8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: 	stmt 0 _3 = aligned_alloc (64, _2);
Layer.c:19:9: note: 	stmt 1 _6 = aligned_alloc (64, _5);
Layer.c:19:9: note: 	children 0x5015b48 0x5015bc8
Layer.c:19:9: note: node (constant) 0x5015b48 (max_nunits=1, refcnt=1)
Layer.c:19:9: note: 	{ 64, 64 }
Layer.c:19:9: note: node 0x5015bc8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: patt_40 = n_inputs_11(D) w* 4;
Layer.c:19:9: note: 	stmt 0 patt_40 = n_inputs_11(D) w* 4;
Layer.c:19:9: note: 	stmt 1 patt_41 = n_outputs_12(D) w* 4;
Layer.c:19:9: note: 	children 0x5015c48 0x5015cc8
Layer.c:19:9: note: node (external) 0x5015c48 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ n_inputs_11(D), n_outputs_12(D) }
Layer.c:19:9: note: node (constant) 0x5015cc8 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ 4, 4 }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x5015dc8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Layer.c:19:9: note: op template: <retval>.feed = feed_20(D);
Layer.c:19:9: note: 	stmt 0 <retval>.feed = feed_20(D);
Layer.c:19:9: note: 	stmt 1 <retval>.back = back_21(D);
Layer.c:19:9: note: 	stmt 2 <retval>.fix = fix_22(D);
Layer.c:19:9: note: 	stmt 3 <retval>.destroy = destroy_23(D);
Layer.c:19:9: note: 	children 0x5015e48
Layer.c:19:9: note: node (external) 0x5015e48 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Layer.c:19:9: note: 	{ feed_20(D), back_21(D), fix_22(D), destroy_23(D) }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 64
  Scalar cost: 64
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x5015f48 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: <retval>.parent = 0B;
Layer.c:19:9: note: 	stmt 0 <retval>.parent = 0B;
Layer.c:19:9: note: 	stmt 1 <retval>.child = 0B;
Layer.c:19:9: note: 	children 0x5015fc8
Layer.c:19:9: note: node (constant) 0x5015fc8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: 	{ 0B, 0B }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Layer.c:19:9: note: Basic block will be vectorized using SLP
Layer.c:19:9: optimized: basic block part vectorized using 8 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x50157c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: op template: MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: 	stmt 0 MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: 	stmt 1 MEM <unsigned int> [(struct PULSE_Layer *)&<retval> + 4B] = optimization_type_17(D);
Layer.c:19:9: note: 	children 0x5015848
Layer.c:19:9: note: node (external) 0x5015848 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ type_16(D), optimization_type_17(D) }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: vect_is_simple_use: operand optimization_type_17(D), type of def: external
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: MEM <unsigned int> [(struct PULSE_Layer *)&<retval>]
Layer.c:19:9: note: created &MEM <unsigned int> [(struct PULSE_Layer *)&<retval>]
Layer.c:19:9: note: add new stmt: MEM <vector(2) unsigned int> [(struct PULSE_Layer *)&<retval>] = _42;
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: optimized: basic block part vectorized using 8 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x50158c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: op template: <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: 	stmt 0 <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: 	stmt 1 <retval>.n_outputs = n_outputs_12(D);
Layer.c:19:9: note: 	children 0x5015948
Layer.c:19:9: note: node (external) 0x5015948 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ n_inputs_11(D), n_outputs_12(D) }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: vect_is_simple_use: operand n_outputs_12(D), type of def: external
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: <retval>.n_inputs
Layer.c:19:9: note: created &<retval>.n_inputs
Layer.c:19:9: note: add new stmt: MEM <vector(2) unsigned int> [(unsigned int *)&<retval> + 96B] = _44;
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: optimized: basic block part vectorized using 16 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x5015a48 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: <retval>.inputs = _3;
Layer.c:19:9: note: 	stmt 0 <retval>.inputs = _3;
Layer.c:19:9: note: 	stmt 1 <retval>.outputs = _6;
Layer.c:19:9: note: 	children 0x5015ac8
Layer.c:19:9: note: node (external) 0x5015ac8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: 	stmt 0 _3 = aligned_alloc (64, _2);
Layer.c:19:9: note: 	stmt 1 _6 = aligned_alloc (64, _5);
Layer.c:19:9: note: 	children 0x5015b48 0x5015bc8
Layer.c:19:9: note: node (constant) 0x5015b48 (max_nunits=1, refcnt=1)
Layer.c:19:9: note: 	{ 64, 64 }
Layer.c:19:9: note: node 0x5015bc8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: patt_40 = n_inputs_11(D) w* 4;
Layer.c:19:9: note: 	stmt 0 patt_40 = n_inputs_11(D) w* 4;
Layer.c:19:9: note: 	stmt 1 patt_41 = n_outputs_12(D) w* 4;
Layer.c:19:9: note: 	children 0x5015c48 0x5015cc8
Layer.c:19:9: note: node (external) 0x5015c48 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ n_inputs_11(D), n_outputs_12(D) }
Layer.c:19:9: note: node (constant) 0x5015cc8 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ 4, 4 }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: <retval>.inputs = _3;
Layer.c:19:9: note: vect_is_simple_use: operand aligned_alloc (64, _5), type of def: internal
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.inputs
Layer.c:19:9: note: created &<retval>.inputs
Layer.c:19:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval> + 8B] = _48;
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: optimized: basic block part vectorized using 32 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x5015dc8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Layer.c:19:9: note: op template: <retval>.feed = feed_20(D);
Layer.c:19:9: note: 	stmt 0 <retval>.feed = feed_20(D);
Layer.c:19:9: note: 	stmt 1 <retval>.back = back_21(D);
Layer.c:19:9: note: 	stmt 2 <retval>.fix = fix_22(D);
Layer.c:19:9: note: 	stmt 3 <retval>.destroy = destroy_23(D);
Layer.c:19:9: note: 	children 0x5015e48
Layer.c:19:9: note: node (external) 0x5015e48 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Layer.c:19:9: note: 	{ feed_20(D), back_21(D), fix_22(D), destroy_23(D) }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: <retval>.feed = feed_20(D);
Layer.c:19:9: note: vect_is_simple_use: operand back_21(D), type of def: external
Layer.c:19:9: note: vect_is_simple_use: operand fix_22(D), type of def: external
Layer.c:19:9: note: vect_is_simple_use: operand destroy_23(D), type of def: external
Layer.c:19:9: note: conflicting alias set types.
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(4) long unsigned int  vectorizing a pointer ref: <retval>.feed
Layer.c:19:9: note: created &<retval>.feed
Layer.c:19:9: note: add new stmt: MEM <vector(4) long unsigned int> [(void *)&<retval> + 32B] = _54;
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: optimized: basic block part vectorized using 16 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x5015f48 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: <retval>.parent = 0B;
Layer.c:19:9: note: 	stmt 0 <retval>.parent = 0B;
Layer.c:19:9: note: 	stmt 1 <retval>.child = 0B;
Layer.c:19:9: note: 	children 0x5015fc8
Layer.c:19:9: note: node (constant) 0x5015fc8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: 	{ 0B, 0B }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: <retval>.parent = 0B;
Layer.c:19:9: note: vect_is_simple_use: operand 0B, type of def: constant
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.parent
Layer.c:19:9: note: created &<retval>.parent
Layer.c:19:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer * *)&<retval> + 72B] = { 0, 0 };
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: note: ***** The result for vector mode V32QI would be the same
Layer.c:24:2: missed: statement clobbers memory: free (_1);
Layer.c:25:2: missed: statement clobbers memory: free (_2);
Layer.c:26:2: missed: statement clobbers memory: free (_3);
Layer.c:27:1: note: ***** Analysis failed with vector mode V4DI
Layer.c:27:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _30 (pretmp_91, _50, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:20: missed: not vectorized: no vectype for stmt: _21 = *_20;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:170:10: optimized: loop vectorized using 32 byte vectors
Dense.c:170:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:170:10: optimized: loop vectorized using 16 byte vectors
Dense.c:161:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_34, _77);
Dense.c:154:10: optimized: loop vectorized using 32 byte vectors
Dense.c:154:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:154:10: optimized: loop vectorized using 16 byte vectors
Dense.c:145:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, _69);
Dense.c:136:13: note: vectorized 2 loops in function.
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, _69);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_19, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_34, _77);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_38, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
Dense.c:136:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:136:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:105:24: missed: couldn't vectorize loop
Dense.c:105:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:108:17: missed: couldn't vectorize loop
Dense.c:108:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:122:24: missed: couldn't vectorize loop
Dense.c:122:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:125:17: missed: couldn't vectorize loop
Dense.c:125:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:90:15: missed: couldn't vectorize loop
Dense.c:90:15: missed: not vectorized: number of iterations cannot be computed.
Dense.c:83:13: note: vectorized 0 loops in function.
Dense.c:86:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:110:17: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:110:17: note: SLPing BB part
Dense.c:127:17: note: Costing subgraph: 
Dense.c:127:17: note: node 0x807b0f8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:127:17: note: op template: _49 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 0 _49 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 1 _209 = (sizetype) wi_137;
Dense.c:127:17: note: 	children 0x807b1f8
Dense.c:127:17: note: node (external) 0x807b1f8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:127:17: note: 	{ j_135, wi_137 }
Dense.c:127:17: note: Cost model analysis: 
Dense.c:127:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:127:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:127:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: Costing subgraph: 
Dense.c:110:17: note: node 0x807b2f8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:110:17: note: op template: _24 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 0 _24 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 1 _229 = (sizetype) wi_136;
Dense.c:110:17: note: 	children 0x807b3f8
Dense.c:110:17: note: node (external) 0x807b3f8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:110:17: note: 	{ j_134, wi_136 }
Dense.c:110:17: note: Cost model analysis: 
Dense.c:110:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:110:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:110:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: ***** The result for vector mode V32QI would be the same
Dense.c:110:17: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:110:17: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:110:17: note: SLPing BB part
Dense.c:127:17: note: Costing subgraph: 
Dense.c:127:17: note: node 0x807b2f8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:127:17: note: op template: _49 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 0 _49 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 1 _209 = (sizetype) wi_137;
Dense.c:127:17: note: 	children 0x807b0f8
Dense.c:127:17: note: node (external) 0x807b0f8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:127:17: note: 	{ j_135, wi_137 }
Dense.c:127:17: note: Cost model analysis: 
Dense.c:127:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:127:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:127:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: Costing subgraph: 
Dense.c:110:17: note: node 0x807b478 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:110:17: note: op template: _24 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 0 _24 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 1 _229 = (sizetype) wi_136;
Dense.c:110:17: note: 	children 0x807b378
Dense.c:110:17: note: node (external) 0x807b378 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:110:17: note: 	{ j_134, wi_136 }
Dense.c:110:17: note: Cost model analysis: 
Dense.c:110:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:110:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:110:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:105:51: note: ***** Analysis failed with vector mode V8QI
Dense.c:105:51: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:105:51: note: ***** Analysis failed with vector mode V4QI
Dense.c:62:15: missed: couldn't vectorize loop
Dense.c:62:15: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:75:11: optimized: loop vectorized using 32 byte vectors
Dense.c:75:11: optimized: loop vectorized using 16 byte vectors
Dense.c:66:11: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.1.1/include/avxintrin.h:881:10: missed: not vectorized: no vectype for stmt: _56 = MEM[(__m256 * {ref-all})w_ptr_81];
 scalar_type: __m256
Dense.c:54:13: note: vectorized 1 loops in function.
Dense.c:57:2: missed: statement clobbers memory: memcpy (_5, _4, _3);
Dense.c:79:2: missed: statement clobbers memory: _28 (pretmp_162, _67, 0);
Dense.c:54:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:54:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:182:2: missed: statement clobbers memory: free (_1);
Dense.c:183:2: missed: statement clobbers memory: free (_2);
Dense.c:184:2: missed: statement clobbers memory: free (_3);
Dense.c:185:2: missed: statement clobbers memory: free (_4);
Dense.c:186:2: missed: statement clobbers memory: free (dense_7);
Dense.c:187:2: missed: statement clobbers memory: PULSE_DestroyLayer (this_6(D));
Dense.c:188:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:188:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:202:19: missed: couldn't vectorize loop
Dense.c:202:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:192:13: note: vectorized 0 loops in function.
Dense.c:194:47: missed: statement clobbers memory: dense_35 = malloc (40);
Dense.c:196:36: missed: statement clobbers memory: _5 = aligned_alloc (64, _4);
Dense.c:197:38: missed: statement clobbers memory: _6 = aligned_alloc (64, _4);
Dense.c:198:36: missed: statement clobbers memory: _8 = aligned_alloc (64, _7);
Dense.c:199:35: missed: statement clobbers memory: _9 = aligned_alloc (64, _7);
Dense.c:200:36: missed: statement clobbers memory: _10 = aligned_alloc (64, _7);
Dense.c:203:39: missed: statement clobbers memory: _11 = rand ();
Dense.c:203:73: missed: statement clobbers memory: _63 = sqrt (_18);
Dense.c:215:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:216:4: missed: statement clobbers memory: exit (1);
Dense.c:212:4: missed: statement clobbers memory: layer = PULSE_CreateLayer (n_inputs.3_29, n_outputs.2_28, 1, activation_function_51(D), _SIMD_FeedDense, _SIMD_BackDense, _SIMD_FixDense, _DestroyDense, 1); [return slot optimization]
Dense.c:209:12: missed: statement clobbers memory: layer = PULSE_CreateLayer (n_inputs.1_27, n_outputs.0_26, 1, activation_function_51(D), _FeedDense, _BackDense, _FixDense, _DestroyDense, 0); [return slot optimization]
Dense.c:222:9: missed: not vectorized: more than one data ref in stmt: <retval> = layer;
Dense.c:222:9: note: ***** Analysis failed with vector mode V4DI
Dense.c:222:9: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Activations.c:17:19: optimized: loop vectorized using 32 byte vectors
Activations.c:17:19: optimized: loop vectorized using 16 byte vectors
Activations.c:17:19: optimized: loop vectorized using 32 byte vectors
Activations.c:17:19: optimized: loop vectorized using 16 byte vectors
Activations.c:14:13: note: vectorized 2 loops in function.
Activations.c:14:13: note: ***** Analysis failed with vector mode V8SF
Activations.c:14:13: note: ***** The result for vector mode V32QI would be the same
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V16QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V16QI
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V8QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V8QI
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V4QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V4QI
Activations.c:28:19: missed: couldn't vectorize loop
Activations.c:28:19: missed: not vectorized: unsupported control flow in loop.
Activations.c:28:19: optimized: loop vectorized using 32 byte vectors
Activations.c:28:19: optimized: loop vectorized using 16 byte vectors
Activations.c:25:13: note: vectorized 1 loops in function.
Activations.c:25:13: note: ***** Analysis failed with vector mode V8SF
Activations.c:25:13: note: ***** The result for vector mode V32QI would be the same
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V16QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V16QI
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V8QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V8QI
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V4QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V4QI
Activations.c:38:1: note: ***** Analysis failed with vector mode VOID
Activations.c:5:19: missed: couldn't vectorize loop
Activations.c:9:21: missed: statement clobbers memory: _36 = expf (_18);
Activations.c:5:19: missed: couldn't vectorize loop
Activations.c:9:21: missed: statement clobbers memory: _12 = expf (_11);
Activations.c:3:13: note: vectorized 0 loops in function.
Activations.c:9:21: missed: statement clobbers memory: _12 = expf (_11);
Activations.c:9:21: missed: statement clobbers memory: _36 = expf (_18);
Activations.c:11:1: note: ***** Analysis failed with vector mode V8SF
Activations.c:11:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SF
Activations.c:44:2: note: ***** Analysis failed with vector mode VOID
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:4:18: note: vectorized 0 loops in function.
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_4, inputs_15, _3);
PULSE.c:8:2: missed: statement clobbers memory: _5 (layer_17);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_12, pretmp_26, _9);
PULSE.c:13:28: note: ***** Analysis failed with vector mode V8SI
PULSE.c:13:28: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:22:2: missed: statement clobbers memory: _1 (layer_10(D));
PULSE.c:22:2: missed: statement clobbers memory: _14 (_2);
PULSE.c:22:2: missed: statement clobbers memory: _21 (_15);
PULSE.c:22:2: missed: statement clobbers memory: _28 (_22);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_29);
PULSE.c:26:3: missed: statement clobbers memory: memset (_34, 0, _32);
PULSE.c:26:3: missed: statement clobbers memory: memset (_27, 0, _25);
PULSE.c:26:3: missed: statement clobbers memory: memset (_20, 0, _18);
PULSE.c:26:3: missed: statement clobbers memory: memset (_7, 0, _5);
PULSE.c:28:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:28:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:32:6: note: vectorized 1 loops in function.
PULSE.c:34:8: missed: statement clobbers memory: _1 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_2);
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _9 = rand ();
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8SI
PULSE.c:32:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:61:16: missed: couldn't vectorize loop
PULSE.c:61:16: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:64:17: missed: couldn't vectorize loop
PULSE.c:64:17: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:73:19: missed: couldn't vectorize loop
PULSE.c:73:19: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _69 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:53:22: missed: couldn't vectorize loop
PULSE.c:53:22: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:47:6: note: vectorized 1 loops in function.
PULSE.c:48:1: missed: statement clobbers memory: saved_stack.7_39 = __builtin_stack_save ();
PULSE.c:49:46: missed: statement clobbers memory: _1 = PULSE_GetLossFunctionPtr (loss_function_40(D));
PULSE.c:60:10: missed: statement clobbers memory: random.4_46 = __builtin_alloca_with_align (_4, 32);
PULSE.c:34:8: missed: statement clobbers memory: _61 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_62);
PULSE.c:39:29: missed: statement clobbers memory: _69 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _72 = rand ();
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_98, inputs_93, _97);
PULSE.c:8:2: missed: statement clobbers memory: _99 (layer_94);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_105, _104, _103);
PULSE.c:67:11: missed: statement clobbers memory: loss_55 = PULSE_GetLoss_42 (_17, _16, _12, _11);
PULSE.c:22:2: missed: statement clobbers memory: _86 (output_34);
PULSE.c:22:2: missed: statement clobbers memory: _108 (_87);
PULSE.c:22:2: missed: statement clobbers memory: _115 (_109);
PULSE.c:22:2: missed: statement clobbers memory: _122 (_116);
PULSE.c:22:2: missed: statement clobbers memory: _129 (_123);
PULSE.c:22:2: missed: statement clobbers memory: _136 (_130);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_137);
PULSE.c:26:3: missed: statement clobbers memory: memset (_142, 0, _140);
PULSE.c:26:3: missed: statement clobbers memory: memset (_135, 0, _133);
PULSE.c:26:3: missed: statement clobbers memory: memset (_128, 0, _126);
PULSE.c:26:3: missed: statement clobbers memory: memset (_121, 0, _119);
PULSE.c:26:3: missed: statement clobbers memory: memset (_114, 0, _112);
PULSE.c:26:3: missed: statement clobbers memory: memset (_92, 0, _90);
PULSE.c:75:6: missed: statement clobbers memory: _21 (current_191, args);
PULSE.c:81:4: missed: statement clobbers memory: printf ("Epoch: %d | Item: %d | Loss: %.10f | Batch Loss: %.10f\r", i_188, j_189, _25, _24);
PULSE.c:47:6: missed: statement clobbers memory: __builtin_stack_restore (saved_stack.7_39);
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4DI
PULSE.c:47:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:91:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:91:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:96:8: missed: couldn't vectorize loop
PULSE.c:96:8: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:93:6: note: vectorized 0 loops in function.
PULSE.c:98:3: missed: statement clobbers memory: _1 (current_10);
PULSE.c:101:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:101:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Loss.c:15:19: optimized: loop vectorized using 32 byte vectors
Loss.c:15:19: optimized:  loop versioned for vectorization because of possible aliasing
Loss.c:15:19: optimized: loop vectorized using 16 byte vectors
Loss.c:12:23: note: vectorized 1 loops in function.
Loss.c:12:23: note: ***** Analysis failed with vector mode V8SF
Loss.c:12:23: note: ***** The result for vector mode V32QI would be the same
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V16QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V16QI
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V8QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V8QI
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V4QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V4QI
Loss.c:7:19: missed: couldn't vectorize loop
Loss.c:8:38: missed: not vectorized: unsupported use in stmt.
Loss.c:4:23: note: vectorized 0 loops in function.
Loss.c:9:13: note: ***** Analysis failed with vector mode V8SF
Loss.c:9:13: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SF
Loss.c:23:2: note: ***** Analysis failed with vector mode VOID
Layer.c:7:34: missed: statement clobbers memory: _3 = aligned_alloc (64, _2);
Layer.c:8:35: missed: statement clobbers memory: _6 = aligned_alloc (64, _5);
Layer.c:9:34: missed: statement clobbers memory: _7 = aligned_alloc (64, _5);
Layer.c:14:19: missed: statement clobbers memory: _8 = PULSE_GetActivationFunctionPtr (activation_function_18(D));
Layer.c:19:9: note: ***** Analysis succeeded with vector mode V8SI
Layer.c:19:9: note: SLPing BB part
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x3a8a7c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: op template: MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: 	stmt 0 MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: 	stmt 1 MEM <unsigned int> [(struct PULSE_Layer *)&<retval> + 4B] = optimization_type_17(D);
Layer.c:19:9: note: 	children 0x3a8a848
Layer.c:19:9: note: node (external) 0x3a8a848 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ type_16(D), optimization_type_17(D) }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x3a8a8c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: op template: <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: 	stmt 0 <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: 	stmt 1 <retval>.n_outputs = n_outputs_12(D);
Layer.c:19:9: note: 	children 0x3a8a948
Layer.c:19:9: note: node (external) 0x3a8a948 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ n_inputs_11(D), n_outputs_12(D) }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x3a8aa48 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: <retval>.inputs = _3;
Layer.c:19:9: note: 	stmt 0 <retval>.inputs = _3;
Layer.c:19:9: note: 	stmt 1 <retval>.outputs = _6;
Layer.c:19:9: note: 	children 0x3a8aac8
Layer.c:19:9: note: node (external) 0x3a8aac8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: 	stmt 0 _3 = aligned_alloc (64, _2);
Layer.c:19:9: note: 	stmt 1 _6 = aligned_alloc (64, _5);
Layer.c:19:9: note: 	children 0x3a8ab48 0x3a8abc8
Layer.c:19:9: note: node (constant) 0x3a8ab48 (max_nunits=1, refcnt=1)
Layer.c:19:9: note: 	{ 64, 64 }
Layer.c:19:9: note: node 0x3a8abc8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: patt_40 = n_inputs_11(D) w* 4;
Layer.c:19:9: note: 	stmt 0 patt_40 = n_inputs_11(D) w* 4;
Layer.c:19:9: note: 	stmt 1 patt_41 = n_outputs_12(D) w* 4;
Layer.c:19:9: note: 	children 0x3a8ac48 0x3a8acc8
Layer.c:19:9: note: node (external) 0x3a8ac48 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ n_inputs_11(D), n_outputs_12(D) }
Layer.c:19:9: note: node (constant) 0x3a8acc8 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ 4, 4 }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x3a8adc8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Layer.c:19:9: note: op template: <retval>.feed = feed_20(D);
Layer.c:19:9: note: 	stmt 0 <retval>.feed = feed_20(D);
Layer.c:19:9: note: 	stmt 1 <retval>.back = back_21(D);
Layer.c:19:9: note: 	stmt 2 <retval>.fix = fix_22(D);
Layer.c:19:9: note: 	stmt 3 <retval>.destroy = destroy_23(D);
Layer.c:19:9: note: 	children 0x3a8ae48
Layer.c:19:9: note: node (external) 0x3a8ae48 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Layer.c:19:9: note: 	{ feed_20(D), back_21(D), fix_22(D), destroy_23(D) }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 64
  Scalar cost: 64
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x3a8af48 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: <retval>.parent = 0B;
Layer.c:19:9: note: 	stmt 0 <retval>.parent = 0B;
Layer.c:19:9: note: 	stmt 1 <retval>.child = 0B;
Layer.c:19:9: note: 	children 0x3a8afc8
Layer.c:19:9: note: node (constant) 0x3a8afc8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: 	{ 0B, 0B }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Layer.c:19:9: note: Basic block will be vectorized using SLP
Layer.c:19:9: optimized: basic block part vectorized using 8 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x3a8a7c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: op template: MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: 	stmt 0 MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: 	stmt 1 MEM <unsigned int> [(struct PULSE_Layer *)&<retval> + 4B] = optimization_type_17(D);
Layer.c:19:9: note: 	children 0x3a8a848
Layer.c:19:9: note: node (external) 0x3a8a848 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ type_16(D), optimization_type_17(D) }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: vect_is_simple_use: operand optimization_type_17(D), type of def: external
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: MEM <unsigned int> [(struct PULSE_Layer *)&<retval>]
Layer.c:19:9: note: created &MEM <unsigned int> [(struct PULSE_Layer *)&<retval>]
Layer.c:19:9: note: add new stmt: MEM <vector(2) unsigned int> [(struct PULSE_Layer *)&<retval>] = _42;
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: optimized: basic block part vectorized using 8 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x3a8a8c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: op template: <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: 	stmt 0 <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: 	stmt 1 <retval>.n_outputs = n_outputs_12(D);
Layer.c:19:9: note: 	children 0x3a8a948
Layer.c:19:9: note: node (external) 0x3a8a948 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ n_inputs_11(D), n_outputs_12(D) }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: vect_is_simple_use: operand n_outputs_12(D), type of def: external
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: <retval>.n_inputs
Layer.c:19:9: note: created &<retval>.n_inputs
Layer.c:19:9: note: add new stmt: MEM <vector(2) unsigned int> [(unsigned int *)&<retval> + 96B] = _44;
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: optimized: basic block part vectorized using 16 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x3a8aa48 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: <retval>.inputs = _3;
Layer.c:19:9: note: 	stmt 0 <retval>.inputs = _3;
Layer.c:19:9: note: 	stmt 1 <retval>.outputs = _6;
Layer.c:19:9: note: 	children 0x3a8aac8
Layer.c:19:9: note: node (external) 0x3a8aac8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: 	stmt 0 _3 = aligned_alloc (64, _2);
Layer.c:19:9: note: 	stmt 1 _6 = aligned_alloc (64, _5);
Layer.c:19:9: note: 	children 0x3a8ab48 0x3a8abc8
Layer.c:19:9: note: node (constant) 0x3a8ab48 (max_nunits=1, refcnt=1)
Layer.c:19:9: note: 	{ 64, 64 }
Layer.c:19:9: note: node 0x3a8abc8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: patt_40 = n_inputs_11(D) w* 4;
Layer.c:19:9: note: 	stmt 0 patt_40 = n_inputs_11(D) w* 4;
Layer.c:19:9: note: 	stmt 1 patt_41 = n_outputs_12(D) w* 4;
Layer.c:19:9: note: 	children 0x3a8ac48 0x3a8acc8
Layer.c:19:9: note: node (external) 0x3a8ac48 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ n_inputs_11(D), n_outputs_12(D) }
Layer.c:19:9: note: node (constant) 0x3a8acc8 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ 4, 4 }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: <retval>.inputs = _3;
Layer.c:19:9: note: vect_is_simple_use: operand aligned_alloc (64, _5), type of def: internal
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.inputs
Layer.c:19:9: note: created &<retval>.inputs
Layer.c:19:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval> + 8B] = _48;
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: optimized: basic block part vectorized using 32 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x3a8adc8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Layer.c:19:9: note: op template: <retval>.feed = feed_20(D);
Layer.c:19:9: note: 	stmt 0 <retval>.feed = feed_20(D);
Layer.c:19:9: note: 	stmt 1 <retval>.back = back_21(D);
Layer.c:19:9: note: 	stmt 2 <retval>.fix = fix_22(D);
Layer.c:19:9: note: 	stmt 3 <retval>.destroy = destroy_23(D);
Layer.c:19:9: note: 	children 0x3a8ae48
Layer.c:19:9: note: node (external) 0x3a8ae48 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Layer.c:19:9: note: 	{ feed_20(D), back_21(D), fix_22(D), destroy_23(D) }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: <retval>.feed = feed_20(D);
Layer.c:19:9: note: vect_is_simple_use: operand back_21(D), type of def: external
Layer.c:19:9: note: vect_is_simple_use: operand fix_22(D), type of def: external
Layer.c:19:9: note: vect_is_simple_use: operand destroy_23(D), type of def: external
Layer.c:19:9: note: conflicting alias set types.
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(4) long unsigned int  vectorizing a pointer ref: <retval>.feed
Layer.c:19:9: note: created &<retval>.feed
Layer.c:19:9: note: add new stmt: MEM <vector(4) long unsigned int> [(void *)&<retval> + 32B] = _54;
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: optimized: basic block part vectorized using 16 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x3a8af48 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: <retval>.parent = 0B;
Layer.c:19:9: note: 	stmt 0 <retval>.parent = 0B;
Layer.c:19:9: note: 	stmt 1 <retval>.child = 0B;
Layer.c:19:9: note: 	children 0x3a8afc8
Layer.c:19:9: note: node (constant) 0x3a8afc8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: 	{ 0B, 0B }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: <retval>.parent = 0B;
Layer.c:19:9: note: vect_is_simple_use: operand 0B, type of def: constant
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.parent
Layer.c:19:9: note: created &<retval>.parent
Layer.c:19:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer * *)&<retval> + 72B] = { 0, 0 };
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: note: ***** The result for vector mode V32QI would be the same
Layer.c:24:2: missed: statement clobbers memory: free (_1);
Layer.c:25:2: missed: statement clobbers memory: free (_2);
Layer.c:26:2: missed: statement clobbers memory: free (_3);
Layer.c:27:1: note: ***** Analysis failed with vector mode V4DI
Layer.c:27:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _30 (pretmp_91, _50, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:20: missed: not vectorized: no vectype for stmt: _21 = *_20;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:170:10: optimized: loop vectorized using 32 byte vectors
Dense.c:170:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:170:10: optimized: loop vectorized using 16 byte vectors
Dense.c:161:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_34, _77);
Dense.c:154:10: optimized: loop vectorized using 32 byte vectors
Dense.c:154:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:154:10: optimized: loop vectorized using 16 byte vectors
Dense.c:145:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, _69);
Dense.c:136:13: note: vectorized 2 loops in function.
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, _69);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_19, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_34, _77);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_38, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
Dense.c:136:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:136:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:105:24: missed: couldn't vectorize loop
Dense.c:105:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:108:17: missed: couldn't vectorize loop
Dense.c:108:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:122:24: missed: couldn't vectorize loop
Dense.c:122:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:125:17: missed: couldn't vectorize loop
Dense.c:125:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:90:15: missed: couldn't vectorize loop
Dense.c:90:15: missed: not vectorized: number of iterations cannot be computed.
Dense.c:83:13: note: vectorized 0 loops in function.
Dense.c:86:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:110:17: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:110:17: note: SLPing BB part
Dense.c:127:17: note: Costing subgraph: 
Dense.c:127:17: note: node 0x784e1f8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:127:17: note: op template: _49 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 0 _49 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 1 _209 = (sizetype) wi_137;
Dense.c:127:17: note: 	children 0x784e2f8
Dense.c:127:17: note: node (external) 0x784e2f8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:127:17: note: 	{ j_135, wi_137 }
Dense.c:127:17: note: Cost model analysis: 
Dense.c:127:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:127:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:127:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: Costing subgraph: 
Dense.c:110:17: note: node 0x784e3f8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:110:17: note: op template: _24 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 0 _24 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 1 _229 = (sizetype) wi_136;
Dense.c:110:17: note: 	children 0x784e4f8
Dense.c:110:17: note: node (external) 0x784e4f8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:110:17: note: 	{ j_134, wi_136 }
Dense.c:110:17: note: Cost model analysis: 
Dense.c:110:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:110:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:110:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: ***** The result for vector mode V32QI would be the same
Dense.c:110:17: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:110:17: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:110:17: note: SLPing BB part
Dense.c:127:17: note: Costing subgraph: 
Dense.c:127:17: note: node 0x784e3f8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:127:17: note: op template: _49 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 0 _49 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 1 _209 = (sizetype) wi_137;
Dense.c:127:17: note: 	children 0x784e1f8
Dense.c:127:17: note: node (external) 0x784e1f8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:127:17: note: 	{ j_135, wi_137 }
Dense.c:127:17: note: Cost model analysis: 
Dense.c:127:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:127:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:127:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: Costing subgraph: 
Dense.c:110:17: note: node 0x784e578 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:110:17: note: op template: _24 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 0 _24 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 1 _229 = (sizetype) wi_136;
Dense.c:110:17: note: 	children 0x784e478
Dense.c:110:17: note: node (external) 0x784e478 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:110:17: note: 	{ j_134, wi_136 }
Dense.c:110:17: note: Cost model analysis: 
Dense.c:110:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:110:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:110:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:105:51: note: ***** Analysis failed with vector mode V8QI
Dense.c:105:51: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:105:51: note: ***** Analysis failed with vector mode V4QI
Dense.c:62:15: missed: couldn't vectorize loop
Dense.c:62:15: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:75:11: optimized: loop vectorized using 32 byte vectors
Dense.c:75:11: optimized: loop vectorized using 16 byte vectors
Dense.c:66:11: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:881:10: missed: not vectorized: no vectype for stmt: _56 = MEM[(__m256 * {ref-all})w_ptr_81];
 scalar_type: __m256
Dense.c:54:13: note: vectorized 1 loops in function.
Dense.c:57:2: missed: statement clobbers memory: memcpy (_5, _4, _3);
Dense.c:79:2: missed: statement clobbers memory: _28 (pretmp_162, _67, 0);
Dense.c:54:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:54:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:182:2: missed: statement clobbers memory: free (_1);
Dense.c:183:2: missed: statement clobbers memory: free (_2);
Dense.c:184:2: missed: statement clobbers memory: free (_3);
Dense.c:185:2: missed: statement clobbers memory: free (_4);
Dense.c:186:2: missed: statement clobbers memory: free (dense_7);
Dense.c:187:2: missed: statement clobbers memory: PULSE_DestroyLayer (this_6(D));
Dense.c:188:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:188:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:202:19: missed: couldn't vectorize loop
Dense.c:202:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:192:13: note: vectorized 0 loops in function.
Dense.c:194:47: missed: statement clobbers memory: dense_35 = malloc (40);
Dense.c:196:36: missed: statement clobbers memory: _5 = aligned_alloc (64, _4);
Dense.c:197:38: missed: statement clobbers memory: _6 = aligned_alloc (64, _4);
Dense.c:198:36: missed: statement clobbers memory: _8 = aligned_alloc (64, _7);
Dense.c:199:35: missed: statement clobbers memory: _9 = aligned_alloc (64, _7);
Dense.c:200:36: missed: statement clobbers memory: _10 = aligned_alloc (64, _7);
Dense.c:203:39: missed: statement clobbers memory: _11 = rand ();
Dense.c:203:73: missed: statement clobbers memory: _63 = sqrt (_18);
Dense.c:215:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:216:4: missed: statement clobbers memory: exit (1);
Dense.c:212:4: missed: statement clobbers memory: layer = PULSE_CreateLayer (n_inputs.3_29, n_outputs.2_28, 1, activation_function_51(D), _SIMD_FeedDense, _SIMD_BackDense, _SIMD_FixDense, _DestroyDense, 1); [return slot optimization]
Dense.c:209:12: missed: statement clobbers memory: layer = PULSE_CreateLayer (n_inputs.1_27, n_outputs.0_26, 1, activation_function_51(D), _FeedDense, _BackDense, _FixDense, _DestroyDense, 0); [return slot optimization]
Dense.c:222:9: missed: not vectorized: more than one data ref in stmt: <retval> = layer;
Dense.c:222:9: note: ***** Analysis failed with vector mode V4DI
Dense.c:222:9: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:4:18: note: vectorized 0 loops in function.
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_4, inputs_15, _3);
PULSE.c:8:2: missed: statement clobbers memory: _5 (layer_17);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_12, pretmp_26, _9);
PULSE.c:13:28: note: ***** Analysis failed with vector mode V8SI
PULSE.c:13:28: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:22:2: missed: statement clobbers memory: _1 (layer_10(D));
PULSE.c:22:2: missed: statement clobbers memory: _14 (_2);
PULSE.c:22:2: missed: statement clobbers memory: _21 (_15);
PULSE.c:22:2: missed: statement clobbers memory: _28 (_22);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_29);
PULSE.c:26:3: missed: statement clobbers memory: memset (_34, 0, _32);
PULSE.c:26:3: missed: statement clobbers memory: memset (_27, 0, _25);
PULSE.c:26:3: missed: statement clobbers memory: memset (_20, 0, _18);
PULSE.c:26:3: missed: statement clobbers memory: memset (_7, 0, _5);
PULSE.c:28:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:28:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:32:6: note: vectorized 1 loops in function.
PULSE.c:34:8: missed: statement clobbers memory: _1 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_2);
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _9 = rand ();
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8SI
PULSE.c:32:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:61:16: missed: couldn't vectorize loop
PULSE.c:61:16: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:64:17: missed: couldn't vectorize loop
PULSE.c:64:17: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:73:19: missed: couldn't vectorize loop
PULSE.c:73:19: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _69 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:53:22: missed: couldn't vectorize loop
PULSE.c:53:22: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:47:6: note: vectorized 1 loops in function.
PULSE.c:48:1: missed: statement clobbers memory: saved_stack.7_39 = __builtin_stack_save ();
PULSE.c:49:46: missed: statement clobbers memory: _1 = PULSE_GetLossFunctionPtr (loss_function_40(D));
PULSE.c:60:10: missed: statement clobbers memory: random.4_46 = __builtin_alloca_with_align (_4, 32);
PULSE.c:34:8: missed: statement clobbers memory: _61 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_62);
PULSE.c:39:29: missed: statement clobbers memory: _69 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _72 = rand ();
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_98, inputs_93, _97);
PULSE.c:8:2: missed: statement clobbers memory: _99 (layer_94);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_105, _104, _103);
PULSE.c:67:11: missed: statement clobbers memory: loss_55 = PULSE_GetLoss_42 (_17, _16, _12, _11);
PULSE.c:22:2: missed: statement clobbers memory: _86 (output_34);
PULSE.c:22:2: missed: statement clobbers memory: _108 (_87);
PULSE.c:22:2: missed: statement clobbers memory: _115 (_109);
PULSE.c:22:2: missed: statement clobbers memory: _122 (_116);
PULSE.c:22:2: missed: statement clobbers memory: _129 (_123);
PULSE.c:22:2: missed: statement clobbers memory: _136 (_130);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_137);
PULSE.c:26:3: missed: statement clobbers memory: memset (_142, 0, _140);
PULSE.c:26:3: missed: statement clobbers memory: memset (_135, 0, _133);
PULSE.c:26:3: missed: statement clobbers memory: memset (_128, 0, _126);
PULSE.c:26:3: missed: statement clobbers memory: memset (_121, 0, _119);
PULSE.c:26:3: missed: statement clobbers memory: memset (_114, 0, _112);
PULSE.c:26:3: missed: statement clobbers memory: memset (_92, 0, _90);
PULSE.c:75:6: missed: statement clobbers memory: _21 (current_191, args);
PULSE.c:81:4: missed: statement clobbers memory: printf ("Epoch: %d | Item: %d | Loss: %.10f | Batch Loss: %.10f\r", i_188, j_189, _25, _24);
PULSE.c:47:6: missed: statement clobbers memory: __builtin_stack_restore (saved_stack.7_39);
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4DI
PULSE.c:47:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:91:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:91:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:99:20: missed: couldn't vectorize loop
PULSE.c:99:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:99:20: missed: couldn't vectorize loop
PULSE.c:99:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:93:15: note: vectorized 0 loops in function.
PULSE.c:97:2: missed: statement clobbers memory: __builtin_va_start (&layers, 0);
PULSE.c:98:44: missed: statement clobbers memory: layers_list_27 = malloc (_3);
PULSE.c:107:22: missed: statement clobbers memory: *_12 = PULSE_CreateDenseLayer (_9, _7, args$8_51, args$12_63); [return slot optimization]
PULSE.c:107:22: missed: statement clobbers memory: *_76 = PULSE_CreateDenseLayer (_79, _80, args$8_82, args$12_81); [return slot optimization]
PULSE.c:113:2: missed: statement clobbers memory: __builtin_va_end (&layers);
PULSE.c:114:9: note: ***** Analysis failed with vector mode V8SI
PULSE.c:114:9: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:120:8: missed: couldn't vectorize loop
PULSE.c:120:8: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:117:6: note: vectorized 0 loops in function.
PULSE.c:122:3: missed: statement clobbers memory: _1 (current_11);
PULSE.c:125:2: missed: statement clobbers memory: free (layer_4(D));
PULSE.c:126:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:126:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Layer.c:7:34: missed: statement clobbers memory: _3 = aligned_alloc (64, _2);
Layer.c:8:35: missed: statement clobbers memory: _6 = aligned_alloc (64, _5);
Layer.c:9:34: missed: statement clobbers memory: _7 = aligned_alloc (64, _5);
Layer.c:14:19: missed: statement clobbers memory: _8 = PULSE_GetActivationFunctionPtr (activation_function_18(D));
Layer.c:19:9: note: ***** Analysis succeeded with vector mode V8SI
Layer.c:19:9: note: SLPing BB part
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x30647f8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: op template: MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: 	stmt 0 MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: 	stmt 1 MEM <unsigned int> [(struct PULSE_Layer *)&<retval> + 4B] = optimization_type_17(D);
Layer.c:19:9: note: 	children 0x3064878
Layer.c:19:9: note: node (external) 0x3064878 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ type_16(D), optimization_type_17(D) }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x30648f8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: op template: <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: 	stmt 0 <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: 	stmt 1 <retval>.n_outputs = n_outputs_12(D);
Layer.c:19:9: note: 	children 0x3064978
Layer.c:19:9: note: node (external) 0x3064978 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ n_inputs_11(D), n_outputs_12(D) }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x3064a78 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: <retval>.inputs = _3;
Layer.c:19:9: note: 	stmt 0 <retval>.inputs = _3;
Layer.c:19:9: note: 	stmt 1 <retval>.outputs = _6;
Layer.c:19:9: note: 	children 0x3064af8
Layer.c:19:9: note: node (external) 0x3064af8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: 	stmt 0 _3 = aligned_alloc (64, _2);
Layer.c:19:9: note: 	stmt 1 _6 = aligned_alloc (64, _5);
Layer.c:19:9: note: 	children 0x3064b78 0x3064bf8
Layer.c:19:9: note: node (constant) 0x3064b78 (max_nunits=1, refcnt=1)
Layer.c:19:9: note: 	{ 64, 64 }
Layer.c:19:9: note: node 0x3064bf8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: patt_40 = n_inputs_11(D) w* 4;
Layer.c:19:9: note: 	stmt 0 patt_40 = n_inputs_11(D) w* 4;
Layer.c:19:9: note: 	stmt 1 patt_41 = n_outputs_12(D) w* 4;
Layer.c:19:9: note: 	children 0x3064c78 0x3064cf8
Layer.c:19:9: note: node (external) 0x3064c78 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ n_inputs_11(D), n_outputs_12(D) }
Layer.c:19:9: note: node (constant) 0x3064cf8 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ 4, 4 }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x3064df8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Layer.c:19:9: note: op template: <retval>.feed = feed_20(D);
Layer.c:19:9: note: 	stmt 0 <retval>.feed = feed_20(D);
Layer.c:19:9: note: 	stmt 1 <retval>.back = back_21(D);
Layer.c:19:9: note: 	stmt 2 <retval>.fix = fix_22(D);
Layer.c:19:9: note: 	stmt 3 <retval>.destroy = destroy_23(D);
Layer.c:19:9: note: 	children 0x3064e78
Layer.c:19:9: note: node (external) 0x3064e78 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Layer.c:19:9: note: 	{ feed_20(D), back_21(D), fix_22(D), destroy_23(D) }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 64
  Scalar cost: 64
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x3064f78 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: <retval>.parent = 0B;
Layer.c:19:9: note: 	stmt 0 <retval>.parent = 0B;
Layer.c:19:9: note: 	stmt 1 <retval>.child = 0B;
Layer.c:19:9: note: 	children 0x3064ff8
Layer.c:19:9: note: node (constant) 0x3064ff8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: 	{ 0B, 0B }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Layer.c:19:9: note: Basic block will be vectorized using SLP
Layer.c:19:9: optimized: basic block part vectorized using 8 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x30647f8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: op template: MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: 	stmt 0 MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: 	stmt 1 MEM <unsigned int> [(struct PULSE_Layer *)&<retval> + 4B] = optimization_type_17(D);
Layer.c:19:9: note: 	children 0x3064878
Layer.c:19:9: note: node (external) 0x3064878 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ type_16(D), optimization_type_17(D) }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: vect_is_simple_use: operand optimization_type_17(D), type of def: external
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: MEM <unsigned int> [(struct PULSE_Layer *)&<retval>]
Layer.c:19:9: note: created &MEM <unsigned int> [(struct PULSE_Layer *)&<retval>]
Layer.c:19:9: note: add new stmt: MEM <vector(2) unsigned int> [(struct PULSE_Layer *)&<retval>] = _42;
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: optimized: basic block part vectorized using 8 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x30648f8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: op template: <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: 	stmt 0 <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: 	stmt 1 <retval>.n_outputs = n_outputs_12(D);
Layer.c:19:9: note: 	children 0x3064978
Layer.c:19:9: note: node (external) 0x3064978 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ n_inputs_11(D), n_outputs_12(D) }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: vect_is_simple_use: operand n_outputs_12(D), type of def: external
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: <retval>.n_inputs
Layer.c:19:9: note: created &<retval>.n_inputs
Layer.c:19:9: note: add new stmt: MEM <vector(2) unsigned int> [(unsigned int *)&<retval> + 96B] = _44;
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: optimized: basic block part vectorized using 16 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x3064a78 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: <retval>.inputs = _3;
Layer.c:19:9: note: 	stmt 0 <retval>.inputs = _3;
Layer.c:19:9: note: 	stmt 1 <retval>.outputs = _6;
Layer.c:19:9: note: 	children 0x3064af8
Layer.c:19:9: note: node (external) 0x3064af8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: 	stmt 0 _3 = aligned_alloc (64, _2);
Layer.c:19:9: note: 	stmt 1 _6 = aligned_alloc (64, _5);
Layer.c:19:9: note: 	children 0x3064b78 0x3064bf8
Layer.c:19:9: note: node (constant) 0x3064b78 (max_nunits=1, refcnt=1)
Layer.c:19:9: note: 	{ 64, 64 }
Layer.c:19:9: note: node 0x3064bf8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: patt_40 = n_inputs_11(D) w* 4;
Layer.c:19:9: note: 	stmt 0 patt_40 = n_inputs_11(D) w* 4;
Layer.c:19:9: note: 	stmt 1 patt_41 = n_outputs_12(D) w* 4;
Layer.c:19:9: note: 	children 0x3064c78 0x3064cf8
Layer.c:19:9: note: node (external) 0x3064c78 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ n_inputs_11(D), n_outputs_12(D) }
Layer.c:19:9: note: node (constant) 0x3064cf8 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ 4, 4 }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: <retval>.inputs = _3;
Layer.c:19:9: note: vect_is_simple_use: operand aligned_alloc (64, _5), type of def: internal
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.inputs
Layer.c:19:9: note: created &<retval>.inputs
Layer.c:19:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval> + 8B] = _48;
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: optimized: basic block part vectorized using 32 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x3064df8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Layer.c:19:9: note: op template: <retval>.feed = feed_20(D);
Layer.c:19:9: note: 	stmt 0 <retval>.feed = feed_20(D);
Layer.c:19:9: note: 	stmt 1 <retval>.back = back_21(D);
Layer.c:19:9: note: 	stmt 2 <retval>.fix = fix_22(D);
Layer.c:19:9: note: 	stmt 3 <retval>.destroy = destroy_23(D);
Layer.c:19:9: note: 	children 0x3064e78
Layer.c:19:9: note: node (external) 0x3064e78 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Layer.c:19:9: note: 	{ feed_20(D), back_21(D), fix_22(D), destroy_23(D) }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: <retval>.feed = feed_20(D);
Layer.c:19:9: note: vect_is_simple_use: operand back_21(D), type of def: external
Layer.c:19:9: note: vect_is_simple_use: operand fix_22(D), type of def: external
Layer.c:19:9: note: vect_is_simple_use: operand destroy_23(D), type of def: external
Layer.c:19:9: note: conflicting alias set types.
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(4) long unsigned int  vectorizing a pointer ref: <retval>.feed
Layer.c:19:9: note: created &<retval>.feed
Layer.c:19:9: note: add new stmt: MEM <vector(4) long unsigned int> [(void *)&<retval> + 32B] = _54;
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: optimized: basic block part vectorized using 16 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x3064f78 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: <retval>.parent = 0B;
Layer.c:19:9: note: 	stmt 0 <retval>.parent = 0B;
Layer.c:19:9: note: 	stmt 1 <retval>.child = 0B;
Layer.c:19:9: note: 	children 0x3064ff8
Layer.c:19:9: note: node (constant) 0x3064ff8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: 	{ 0B, 0B }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: <retval>.parent = 0B;
Layer.c:19:9: note: vect_is_simple_use: operand 0B, type of def: constant
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.parent
Layer.c:19:9: note: created &<retval>.parent
Layer.c:19:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer * *)&<retval> + 72B] = { 0, 0 };
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: note: ***** The result for vector mode V32QI would be the same
Layer.c:24:2: missed: statement clobbers memory: free (_1);
Layer.c:25:2: missed: statement clobbers memory: free (_2);
Layer.c:26:2: missed: statement clobbers memory: free (_3);
Layer.c:27:1: note: ***** Analysis failed with vector mode V4DI
Layer.c:27:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _30 (pretmp_91, _50, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:20: missed: not vectorized: no vectype for stmt: _21 = *_20;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:170:10: optimized: loop vectorized using 32 byte vectors
Dense.c:170:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:170:10: optimized: loop vectorized using 16 byte vectors
Dense.c:161:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_34, _77);
Dense.c:154:10: optimized: loop vectorized using 32 byte vectors
Dense.c:154:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:154:10: optimized: loop vectorized using 16 byte vectors
Dense.c:145:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, _69);
Dense.c:136:13: note: vectorized 2 loops in function.
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, _69);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_19, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_34, _77);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_38, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
Dense.c:136:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:136:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:105:24: missed: couldn't vectorize loop
Dense.c:105:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:108:17: missed: couldn't vectorize loop
Dense.c:108:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:122:24: missed: couldn't vectorize loop
Dense.c:122:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:125:17: missed: couldn't vectorize loop
Dense.c:125:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:90:15: missed: couldn't vectorize loop
Dense.c:90:15: missed: not vectorized: number of iterations cannot be computed.
Dense.c:83:13: note: vectorized 0 loops in function.
Dense.c:86:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:110:17: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:110:17: note: SLPing BB part
Dense.c:127:17: note: Costing subgraph: 
Dense.c:127:17: note: node 0x84add68 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:127:17: note: op template: _49 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 0 _49 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 1 _209 = (sizetype) wi_137;
Dense.c:127:17: note: 	children 0x84ade68
Dense.c:127:17: note: node (external) 0x84ade68 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:127:17: note: 	{ j_135, wi_137 }
Dense.c:127:17: note: Cost model analysis: 
Dense.c:127:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:127:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:127:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: Costing subgraph: 
Dense.c:110:17: note: node 0x84adf68 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:110:17: note: op template: _24 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 0 _24 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 1 _229 = (sizetype) wi_136;
Dense.c:110:17: note: 	children 0x84ae068
Dense.c:110:17: note: node (external) 0x84ae068 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:110:17: note: 	{ j_134, wi_136 }
Dense.c:110:17: note: Cost model analysis: 
Dense.c:110:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:110:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:110:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: ***** The result for vector mode V32QI would be the same
Dense.c:110:17: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:110:17: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:110:17: note: SLPing BB part
Dense.c:127:17: note: Costing subgraph: 
Dense.c:127:17: note: node 0x84adf68 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:127:17: note: op template: _49 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 0 _49 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 1 _209 = (sizetype) wi_137;
Dense.c:127:17: note: 	children 0x84add68
Dense.c:127:17: note: node (external) 0x84add68 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:127:17: note: 	{ j_135, wi_137 }
Dense.c:127:17: note: Cost model analysis: 
Dense.c:127:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:127:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:127:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: Costing subgraph: 
Dense.c:110:17: note: node 0x84ae0e8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:110:17: note: op template: _24 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 0 _24 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 1 _229 = (sizetype) wi_136;
Dense.c:110:17: note: 	children 0x84adfe8
Dense.c:110:17: note: node (external) 0x84adfe8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:110:17: note: 	{ j_134, wi_136 }
Dense.c:110:17: note: Cost model analysis: 
Dense.c:110:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:110:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:110:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:105:51: note: ***** Analysis failed with vector mode V8QI
Dense.c:105:51: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:105:51: note: ***** Analysis failed with vector mode V4QI
Dense.c:62:15: missed: couldn't vectorize loop
Dense.c:62:15: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:75:11: optimized: loop vectorized using 32 byte vectors
Dense.c:75:11: optimized: loop vectorized using 16 byte vectors
Dense.c:66:11: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:881:10: missed: not vectorized: no vectype for stmt: _56 = MEM[(__m256 * {ref-all})w_ptr_81];
 scalar_type: __m256
Dense.c:54:13: note: vectorized 1 loops in function.
Dense.c:57:2: missed: statement clobbers memory: memcpy (_5, _4, _3);
Dense.c:79:2: missed: statement clobbers memory: _28 (pretmp_162, _67, 0);
Dense.c:54:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:54:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:182:2: missed: statement clobbers memory: free (_1);
Dense.c:183:2: missed: statement clobbers memory: free (_2);
Dense.c:184:2: missed: statement clobbers memory: free (_3);
Dense.c:185:2: missed: statement clobbers memory: free (_4);
Dense.c:186:2: missed: statement clobbers memory: free (dense_7);
Dense.c:187:2: missed: statement clobbers memory: PULSE_DestroyLayer (this_6(D));
Dense.c:188:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:188:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:202:19: missed: couldn't vectorize loop
Dense.c:202:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:192:13: note: vectorized 0 loops in function.
Dense.c:194:47: missed: statement clobbers memory: dense_35 = malloc (40);
Dense.c:196:36: missed: statement clobbers memory: _5 = aligned_alloc (64, _4);
Dense.c:197:38: missed: statement clobbers memory: _6 = aligned_alloc (64, _4);
Dense.c:198:36: missed: statement clobbers memory: _8 = aligned_alloc (64, _7);
Dense.c:199:35: missed: statement clobbers memory: _9 = aligned_alloc (64, _7);
Dense.c:200:36: missed: statement clobbers memory: _10 = aligned_alloc (64, _7);
Dense.c:203:39: missed: statement clobbers memory: _11 = rand ();
Dense.c:203:73: missed: statement clobbers memory: _63 = sqrt (_18);
Dense.c:215:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:216:4: missed: statement clobbers memory: exit (1);
Dense.c:212:4: missed: statement clobbers memory: layer = PULSE_CreateLayer (n_inputs.3_29, n_outputs.2_28, 0, activation_function_51(D), _SIMD_FeedDense, _SIMD_BackDense, _SIMD_FixDense, _DestroyDense, 1); [return slot optimization]
Dense.c:209:12: missed: statement clobbers memory: layer = PULSE_CreateLayer (n_inputs.1_27, n_outputs.0_26, 0, activation_function_51(D), _FeedDense, _BackDense, _FixDense, _DestroyDense, 0); [return slot optimization]
Dense.c:222:9: missed: not vectorized: more than one data ref in stmt: <retval> = layer;
Dense.c:222:9: note: ***** Analysis failed with vector mode V4DI
Dense.c:222:9: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Activations.c:17:19: optimized: loop vectorized using 32 byte vectors
Activations.c:17:19: optimized: loop vectorized using 16 byte vectors
Activations.c:17:19: optimized: loop vectorized using 32 byte vectors
Activations.c:17:19: optimized: loop vectorized using 16 byte vectors
Activations.c:14:13: note: vectorized 2 loops in function.
Activations.c:14:13: note: ***** Analysis failed with vector mode V8SF
Activations.c:14:13: note: ***** The result for vector mode V32QI would be the same
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V16QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V16QI
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V8QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V8QI
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V4QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V4QI
Activations.c:28:19: missed: couldn't vectorize loop
Activations.c:28:19: missed: not vectorized: unsupported control flow in loop.
Activations.c:28:19: optimized: loop vectorized using 32 byte vectors
Activations.c:28:19: optimized: loop vectorized using 16 byte vectors
Activations.c:25:13: note: vectorized 1 loops in function.
Activations.c:25:13: note: ***** Analysis failed with vector mode V8SF
Activations.c:25:13: note: ***** The result for vector mode V32QI would be the same
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V16QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V16QI
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V8QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V8QI
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V4QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V4QI
Activations.c:38:1: note: ***** Analysis failed with vector mode VOID
Activations.c:5:19: missed: couldn't vectorize loop
Activations.c:9:21: missed: statement clobbers memory: _36 = expf (_18);
Activations.c:5:19: missed: couldn't vectorize loop
Activations.c:9:21: missed: statement clobbers memory: _12 = expf (_11);
Activations.c:3:13: note: vectorized 0 loops in function.
Activations.c:9:21: missed: statement clobbers memory: _12 = expf (_11);
Activations.c:9:21: missed: statement clobbers memory: _36 = expf (_18);
Activations.c:11:1: note: ***** Analysis failed with vector mode V8SF
Activations.c:11:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SF
Activations.c:44:2: note: ***** Analysis failed with vector mode VOID
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:4:18: note: vectorized 0 loops in function.
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_4, inputs_15, _3);
PULSE.c:8:2: missed: statement clobbers memory: _5 (layer_17);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_12, pretmp_26, _9);
PULSE.c:13:28: note: ***** Analysis failed with vector mode V8SI
PULSE.c:13:28: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:22:2: missed: statement clobbers memory: _1 (layer_10(D));
PULSE.c:22:2: missed: statement clobbers memory: _14 (_2);
PULSE.c:22:2: missed: statement clobbers memory: _21 (_15);
PULSE.c:22:2: missed: statement clobbers memory: _28 (_22);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_29);
PULSE.c:26:3: missed: statement clobbers memory: memset (_34, 0, _32);
PULSE.c:26:3: missed: statement clobbers memory: memset (_27, 0, _25);
PULSE.c:26:3: missed: statement clobbers memory: memset (_20, 0, _18);
PULSE.c:26:3: missed: statement clobbers memory: memset (_7, 0, _5);
PULSE.c:28:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:28:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:32:6: note: vectorized 1 loops in function.
PULSE.c:34:8: missed: statement clobbers memory: _1 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_2);
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _9 = rand ();
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8SI
PULSE.c:32:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:61:16: missed: couldn't vectorize loop
PULSE.c:61:16: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:64:17: missed: couldn't vectorize loop
PULSE.c:64:17: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:73:19: missed: couldn't vectorize loop
PULSE.c:73:19: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _69 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:53:22: missed: couldn't vectorize loop
PULSE.c:53:22: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:47:6: note: vectorized 1 loops in function.
PULSE.c:48:1: missed: statement clobbers memory: saved_stack.7_39 = __builtin_stack_save ();
PULSE.c:49:46: missed: statement clobbers memory: _1 = PULSE_GetLossFunctionPtr (loss_function_40(D));
PULSE.c:60:10: missed: statement clobbers memory: random.4_46 = __builtin_alloca_with_align (_4, 32);
PULSE.c:34:8: missed: statement clobbers memory: _61 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_62);
PULSE.c:39:29: missed: statement clobbers memory: _69 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _72 = rand ();
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_98, inputs_93, _97);
PULSE.c:8:2: missed: statement clobbers memory: _99 (layer_94);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_105, _104, _103);
PULSE.c:67:11: missed: statement clobbers memory: loss_55 = PULSE_GetLoss_42 (_17, _16, _12, _11);
PULSE.c:22:2: missed: statement clobbers memory: _86 (output_34);
PULSE.c:22:2: missed: statement clobbers memory: _108 (_87);
PULSE.c:22:2: missed: statement clobbers memory: _115 (_109);
PULSE.c:22:2: missed: statement clobbers memory: _122 (_116);
PULSE.c:22:2: missed: statement clobbers memory: _129 (_123);
PULSE.c:22:2: missed: statement clobbers memory: _136 (_130);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_137);
PULSE.c:26:3: missed: statement clobbers memory: memset (_142, 0, _140);
PULSE.c:26:3: missed: statement clobbers memory: memset (_135, 0, _133);
PULSE.c:26:3: missed: statement clobbers memory: memset (_128, 0, _126);
PULSE.c:26:3: missed: statement clobbers memory: memset (_121, 0, _119);
PULSE.c:26:3: missed: statement clobbers memory: memset (_114, 0, _112);
PULSE.c:26:3: missed: statement clobbers memory: memset (_92, 0, _90);
PULSE.c:75:6: missed: statement clobbers memory: _21 (current_191, args);
PULSE.c:81:4: missed: statement clobbers memory: printf ("Epoch: %d | Item: %d | Loss: %.10f | Batch Loss: %.10f\r", i_188, j_189, _25, _24);
PULSE.c:47:6: missed: statement clobbers memory: __builtin_stack_restore (saved_stack.7_39);
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4DI
PULSE.c:47:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:91:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:91:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:99:20: missed: couldn't vectorize loop
PULSE.c:99:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:99:20: missed: couldn't vectorize loop
PULSE.c:99:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:93:15: note: vectorized 0 loops in function.
PULSE.c:97:2: missed: statement clobbers memory: __builtin_va_start (&layers, 0);
PULSE.c:98:44: missed: statement clobbers memory: layers_list_27 = malloc (_3);
PULSE.c:107:22: missed: statement clobbers memory: *_12 = PULSE_CreateDenseLayer (_9, _7, args$8_51, args$12_63); [return slot optimization]
PULSE.c:107:22: missed: statement clobbers memory: *_76 = PULSE_CreateDenseLayer (_79, _80, args$8_82, args$12_81); [return slot optimization]
PULSE.c:113:2: missed: statement clobbers memory: __builtin_va_end (&layers);
PULSE.c:114:9: note: ***** Analysis failed with vector mode V8SI
PULSE.c:114:9: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:120:8: missed: couldn't vectorize loop
PULSE.c:120:8: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:117:6: note: vectorized 0 loops in function.
PULSE.c:122:3: missed: statement clobbers memory: _1 (current_11);
PULSE.c:125:2: missed: statement clobbers memory: free (layer_4(D));
PULSE.c:126:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:126:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Loss.c:15:19: optimized: loop vectorized using 32 byte vectors
Loss.c:15:19: optimized:  loop versioned for vectorization because of possible aliasing
Loss.c:15:19: optimized: loop vectorized using 16 byte vectors
Loss.c:12:23: note: vectorized 1 loops in function.
Loss.c:12:23: note: ***** Analysis failed with vector mode V8SF
Loss.c:12:23: note: ***** The result for vector mode V32QI would be the same
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V16QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V16QI
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V8QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V8QI
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V4QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V4QI
Loss.c:7:19: missed: couldn't vectorize loop
Loss.c:8:38: missed: not vectorized: unsupported use in stmt.
Loss.c:4:23: note: vectorized 0 loops in function.
Loss.c:9:13: note: ***** Analysis failed with vector mode V8SF
Loss.c:9:13: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SF
Loss.c:23:2: note: ***** Analysis failed with vector mode VOID
Layer.c:7:34: missed: statement clobbers memory: _3 = aligned_alloc (64, _2);
Layer.c:8:35: missed: statement clobbers memory: _6 = aligned_alloc (64, _5);
Layer.c:9:34: missed: statement clobbers memory: _7 = aligned_alloc (64, _5);
Layer.c:14:19: missed: statement clobbers memory: _8 = PULSE_GetActivationFunctionPtr (activation_function_18(D));
Layer.c:19:9: note: ***** Analysis succeeded with vector mode V8SI
Layer.c:19:9: note: SLPing BB part
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x42629c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: op template: MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: 	stmt 0 MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: 	stmt 1 MEM <unsigned int> [(struct PULSE_Layer *)&<retval> + 4B] = optimization_type_17(D);
Layer.c:19:9: note: 	children 0x4262a48
Layer.c:19:9: note: node (external) 0x4262a48 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ type_16(D), optimization_type_17(D) }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x4262ac8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: op template: <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: 	stmt 0 <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: 	stmt 1 <retval>.n_outputs = n_outputs_12(D);
Layer.c:19:9: note: 	children 0x4262b48
Layer.c:19:9: note: node (external) 0x4262b48 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ n_inputs_11(D), n_outputs_12(D) }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x4262c48 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: <retval>.inputs = _3;
Layer.c:19:9: note: 	stmt 0 <retval>.inputs = _3;
Layer.c:19:9: note: 	stmt 1 <retval>.outputs = _6;
Layer.c:19:9: note: 	children 0x4262cc8
Layer.c:19:9: note: node (external) 0x4262cc8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: 	stmt 0 _3 = aligned_alloc (64, _2);
Layer.c:19:9: note: 	stmt 1 _6 = aligned_alloc (64, _5);
Layer.c:19:9: note: 	children 0x4262d48 0x4262dc8
Layer.c:19:9: note: node (constant) 0x4262d48 (max_nunits=1, refcnt=1)
Layer.c:19:9: note: 	{ 64, 64 }
Layer.c:19:9: note: node 0x4262dc8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: patt_40 = n_inputs_11(D) w* 4;
Layer.c:19:9: note: 	stmt 0 patt_40 = n_inputs_11(D) w* 4;
Layer.c:19:9: note: 	stmt 1 patt_41 = n_outputs_12(D) w* 4;
Layer.c:19:9: note: 	children 0x4262e48 0x4262ec8
Layer.c:19:9: note: node (external) 0x4262e48 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ n_inputs_11(D), n_outputs_12(D) }
Layer.c:19:9: note: node (constant) 0x4262ec8 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ 4, 4 }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x4262fc8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Layer.c:19:9: note: op template: <retval>.feed = feed_20(D);
Layer.c:19:9: note: 	stmt 0 <retval>.feed = feed_20(D);
Layer.c:19:9: note: 	stmt 1 <retval>.back = back_21(D);
Layer.c:19:9: note: 	stmt 2 <retval>.fix = fix_22(D);
Layer.c:19:9: note: 	stmt 3 <retval>.destroy = destroy_23(D);
Layer.c:19:9: note: 	children 0x4263048
Layer.c:19:9: note: node (external) 0x4263048 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Layer.c:19:9: note: 	{ feed_20(D), back_21(D), fix_22(D), destroy_23(D) }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 64
  Scalar cost: 64
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x4263148 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: <retval>.parent = 0B;
Layer.c:19:9: note: 	stmt 0 <retval>.parent = 0B;
Layer.c:19:9: note: 	stmt 1 <retval>.child = 0B;
Layer.c:19:9: note: 	children 0x42631c8
Layer.c:19:9: note: node (constant) 0x42631c8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: 	{ 0B, 0B }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Layer.c:19:9: note: Basic block will be vectorized using SLP
Layer.c:19:9: optimized: basic block part vectorized using 8 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x42629c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: op template: MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: 	stmt 0 MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: 	stmt 1 MEM <unsigned int> [(struct PULSE_Layer *)&<retval> + 4B] = optimization_type_17(D);
Layer.c:19:9: note: 	children 0x4262a48
Layer.c:19:9: note: node (external) 0x4262a48 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ type_16(D), optimization_type_17(D) }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: vect_is_simple_use: operand optimization_type_17(D), type of def: external
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: MEM <unsigned int> [(struct PULSE_Layer *)&<retval>]
Layer.c:19:9: note: created &MEM <unsigned int> [(struct PULSE_Layer *)&<retval>]
Layer.c:19:9: note: add new stmt: MEM <vector(2) unsigned int> [(struct PULSE_Layer *)&<retval>] = _42;
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: optimized: basic block part vectorized using 8 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x4262ac8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: op template: <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: 	stmt 0 <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: 	stmt 1 <retval>.n_outputs = n_outputs_12(D);
Layer.c:19:9: note: 	children 0x4262b48
Layer.c:19:9: note: node (external) 0x4262b48 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ n_inputs_11(D), n_outputs_12(D) }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: vect_is_simple_use: operand n_outputs_12(D), type of def: external
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: <retval>.n_inputs
Layer.c:19:9: note: created &<retval>.n_inputs
Layer.c:19:9: note: add new stmt: MEM <vector(2) unsigned int> [(unsigned int *)&<retval> + 96B] = _44;
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: optimized: basic block part vectorized using 16 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x4262c48 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: <retval>.inputs = _3;
Layer.c:19:9: note: 	stmt 0 <retval>.inputs = _3;
Layer.c:19:9: note: 	stmt 1 <retval>.outputs = _6;
Layer.c:19:9: note: 	children 0x4262cc8
Layer.c:19:9: note: node (external) 0x4262cc8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: 	stmt 0 _3 = aligned_alloc (64, _2);
Layer.c:19:9: note: 	stmt 1 _6 = aligned_alloc (64, _5);
Layer.c:19:9: note: 	children 0x4262d48 0x4262dc8
Layer.c:19:9: note: node (constant) 0x4262d48 (max_nunits=1, refcnt=1)
Layer.c:19:9: note: 	{ 64, 64 }
Layer.c:19:9: note: node 0x4262dc8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: patt_40 = n_inputs_11(D) w* 4;
Layer.c:19:9: note: 	stmt 0 patt_40 = n_inputs_11(D) w* 4;
Layer.c:19:9: note: 	stmt 1 patt_41 = n_outputs_12(D) w* 4;
Layer.c:19:9: note: 	children 0x4262e48 0x4262ec8
Layer.c:19:9: note: node (external) 0x4262e48 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ n_inputs_11(D), n_outputs_12(D) }
Layer.c:19:9: note: node (constant) 0x4262ec8 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ 4, 4 }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: <retval>.inputs = _3;
Layer.c:19:9: note: vect_is_simple_use: operand aligned_alloc (64, _5), type of def: internal
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.inputs
Layer.c:19:9: note: created &<retval>.inputs
Layer.c:19:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval> + 8B] = _48;
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: optimized: basic block part vectorized using 32 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x4262fc8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Layer.c:19:9: note: op template: <retval>.feed = feed_20(D);
Layer.c:19:9: note: 	stmt 0 <retval>.feed = feed_20(D);
Layer.c:19:9: note: 	stmt 1 <retval>.back = back_21(D);
Layer.c:19:9: note: 	stmt 2 <retval>.fix = fix_22(D);
Layer.c:19:9: note: 	stmt 3 <retval>.destroy = destroy_23(D);
Layer.c:19:9: note: 	children 0x4263048
Layer.c:19:9: note: node (external) 0x4263048 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Layer.c:19:9: note: 	{ feed_20(D), back_21(D), fix_22(D), destroy_23(D) }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: <retval>.feed = feed_20(D);
Layer.c:19:9: note: vect_is_simple_use: operand back_21(D), type of def: external
Layer.c:19:9: note: vect_is_simple_use: operand fix_22(D), type of def: external
Layer.c:19:9: note: vect_is_simple_use: operand destroy_23(D), type of def: external
Layer.c:19:9: note: conflicting alias set types.
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(4) long unsigned int  vectorizing a pointer ref: <retval>.feed
Layer.c:19:9: note: created &<retval>.feed
Layer.c:19:9: note: add new stmt: MEM <vector(4) long unsigned int> [(void *)&<retval> + 32B] = _54;
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: optimized: basic block part vectorized using 16 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x4263148 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: <retval>.parent = 0B;
Layer.c:19:9: note: 	stmt 0 <retval>.parent = 0B;
Layer.c:19:9: note: 	stmt 1 <retval>.child = 0B;
Layer.c:19:9: note: 	children 0x42631c8
Layer.c:19:9: note: node (constant) 0x42631c8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: 	{ 0B, 0B }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: <retval>.parent = 0B;
Layer.c:19:9: note: vect_is_simple_use: operand 0B, type of def: constant
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.parent
Layer.c:19:9: note: created &<retval>.parent
Layer.c:19:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer * *)&<retval> + 72B] = { 0, 0 };
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: note: ***** The result for vector mode V32QI would be the same
Layer.c:24:2: missed: statement clobbers memory: free (_1);
Layer.c:25:2: missed: statement clobbers memory: free (_2);
Layer.c:26:2: missed: statement clobbers memory: free (_3);
Layer.c:27:1: note: ***** Analysis failed with vector mode V4DI
Layer.c:27:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _30 (pretmp_91, _50, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:20: missed: not vectorized: no vectype for stmt: _21 = *_20;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:170:10: optimized: loop vectorized using 32 byte vectors
Dense.c:170:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:170:10: optimized: loop vectorized using 16 byte vectors
Dense.c:161:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_34, _77);
Dense.c:154:10: optimized: loop vectorized using 32 byte vectors
Dense.c:154:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:154:10: optimized: loop vectorized using 16 byte vectors
Dense.c:145:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, _69);
Dense.c:136:13: note: vectorized 2 loops in function.
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, _69);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_19, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_34, _77);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_38, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
Dense.c:136:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:136:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:105:24: missed: couldn't vectorize loop
Dense.c:105:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:108:17: missed: couldn't vectorize loop
Dense.c:108:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:122:24: missed: couldn't vectorize loop
Dense.c:122:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:125:17: missed: couldn't vectorize loop
Dense.c:125:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:90:15: missed: couldn't vectorize loop
Dense.c:90:15: missed: not vectorized: number of iterations cannot be computed.
Dense.c:83:13: note: vectorized 0 loops in function.
Dense.c:86:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:110:17: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:110:17: note: SLPing BB part
Dense.c:127:17: note: Costing subgraph: 
Dense.c:127:17: note: node 0x863d218 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:127:17: note: op template: _49 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 0 _49 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 1 _209 = (sizetype) wi_137;
Dense.c:127:17: note: 	children 0x863d318
Dense.c:127:17: note: node (external) 0x863d318 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:127:17: note: 	{ j_135, wi_137 }
Dense.c:127:17: note: Cost model analysis: 
Dense.c:127:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:127:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:127:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: Costing subgraph: 
Dense.c:110:17: note: node 0x863d418 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:110:17: note: op template: _24 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 0 _24 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 1 _229 = (sizetype) wi_136;
Dense.c:110:17: note: 	children 0x863d518
Dense.c:110:17: note: node (external) 0x863d518 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:110:17: note: 	{ j_134, wi_136 }
Dense.c:110:17: note: Cost model analysis: 
Dense.c:110:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:110:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:110:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: ***** The result for vector mode V32QI would be the same
Dense.c:110:17: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:110:17: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:110:17: note: SLPing BB part
Dense.c:127:17: note: Costing subgraph: 
Dense.c:127:17: note: node 0x863d418 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:127:17: note: op template: _49 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 0 _49 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 1 _209 = (sizetype) wi_137;
Dense.c:127:17: note: 	children 0x863d218
Dense.c:127:17: note: node (external) 0x863d218 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:127:17: note: 	{ j_135, wi_137 }
Dense.c:127:17: note: Cost model analysis: 
Dense.c:127:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:127:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:127:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: Costing subgraph: 
Dense.c:110:17: note: node 0x863d598 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:110:17: note: op template: _24 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 0 _24 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 1 _229 = (sizetype) wi_136;
Dense.c:110:17: note: 	children 0x863d498
Dense.c:110:17: note: node (external) 0x863d498 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:110:17: note: 	{ j_134, wi_136 }
Dense.c:110:17: note: Cost model analysis: 
Dense.c:110:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:110:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:110:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:105:51: note: ***** Analysis failed with vector mode V8QI
Dense.c:105:51: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:105:51: note: ***** Analysis failed with vector mode V4QI
Dense.c:62:15: missed: couldn't vectorize loop
Dense.c:62:15: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:75:11: optimized: loop vectorized using 32 byte vectors
Dense.c:75:11: optimized: loop vectorized using 16 byte vectors
Dense.c:66:11: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:881:10: missed: not vectorized: no vectype for stmt: _56 = MEM[(__m256 * {ref-all})w_ptr_81];
 scalar_type: __m256
Dense.c:54:13: note: vectorized 1 loops in function.
Dense.c:57:2: missed: statement clobbers memory: memcpy (_5, _4, _3);
Dense.c:79:2: missed: statement clobbers memory: _28 (pretmp_162, _67, 0);
Dense.c:54:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:54:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:182:2: missed: statement clobbers memory: free (_1);
Dense.c:183:2: missed: statement clobbers memory: free (_2);
Dense.c:184:2: missed: statement clobbers memory: free (_3);
Dense.c:185:2: missed: statement clobbers memory: free (_4);
Dense.c:186:2: missed: statement clobbers memory: free (dense_7);
Dense.c:187:2: missed: statement clobbers memory: PULSE_DestroyLayer (this_6(D));
Dense.c:188:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:188:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:202:19: missed: couldn't vectorize loop
Dense.c:202:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:192:13: note: vectorized 0 loops in function.
Dense.c:194:47: missed: statement clobbers memory: dense_35 = malloc (40);
Dense.c:196:36: missed: statement clobbers memory: _5 = aligned_alloc (64, _4);
Dense.c:197:38: missed: statement clobbers memory: _6 = aligned_alloc (64, _4);
Dense.c:198:36: missed: statement clobbers memory: _8 = aligned_alloc (64, _7);
Dense.c:199:35: missed: statement clobbers memory: _9 = aligned_alloc (64, _7);
Dense.c:200:36: missed: statement clobbers memory: _10 = aligned_alloc (64, _7);
Dense.c:203:39: missed: statement clobbers memory: _11 = rand ();
Dense.c:203:73: missed: statement clobbers memory: _63 = sqrt (_18);
Dense.c:215:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:216:4: missed: statement clobbers memory: exit (1);
Dense.c:212:4: missed: statement clobbers memory: layer = PULSE_CreateLayer (n_inputs.3_29, n_outputs.2_28, 0, activation_function_51(D), _SIMD_FeedDense, _SIMD_BackDense, _SIMD_FixDense, _DestroyDense, 1); [return slot optimization]
Dense.c:209:12: missed: statement clobbers memory: layer = PULSE_CreateLayer (n_inputs.1_27, n_outputs.0_26, 0, activation_function_51(D), _FeedDense, _BackDense, _FixDense, _DestroyDense, 0); [return slot optimization]
Dense.c:222:9: missed: not vectorized: more than one data ref in stmt: <retval> = layer;
Dense.c:222:9: note: ***** Analysis failed with vector mode V4DI
Dense.c:222:9: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Activations.c:17:19: optimized: loop vectorized using 32 byte vectors
Activations.c:17:19: optimized: loop vectorized using 16 byte vectors
Activations.c:17:19: optimized: loop vectorized using 32 byte vectors
Activations.c:17:19: optimized: loop vectorized using 16 byte vectors
Activations.c:14:13: note: vectorized 2 loops in function.
Activations.c:14:13: note: ***** Analysis failed with vector mode V8SF
Activations.c:14:13: note: ***** The result for vector mode V32QI would be the same
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V16QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V16QI
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V8QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V8QI
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V4QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V4QI
Activations.c:28:19: missed: couldn't vectorize loop
Activations.c:28:19: missed: not vectorized: unsupported control flow in loop.
Activations.c:28:19: optimized: loop vectorized using 32 byte vectors
Activations.c:28:19: optimized: loop vectorized using 16 byte vectors
Activations.c:25:13: note: vectorized 1 loops in function.
Activations.c:25:13: note: ***** Analysis failed with vector mode V8SF
Activations.c:25:13: note: ***** The result for vector mode V32QI would be the same
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V16QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V16QI
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V8QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V8QI
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V4QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V4QI
Activations.c:38:1: note: ***** Analysis failed with vector mode VOID
Activations.c:5:19: missed: couldn't vectorize loop
Activations.c:9:21: missed: statement clobbers memory: _36 = expf (_18);
Activations.c:5:19: missed: couldn't vectorize loop
Activations.c:9:21: missed: statement clobbers memory: _12 = expf (_11);
Activations.c:3:13: note: vectorized 0 loops in function.
Activations.c:9:21: missed: statement clobbers memory: _12 = expf (_11);
Activations.c:9:21: missed: statement clobbers memory: _36 = expf (_18);
Activations.c:11:1: note: ***** Analysis failed with vector mode V8SF
Activations.c:11:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SF
Activations.c:44:2: note: ***** Analysis failed with vector mode VOID
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:4:18: note: vectorized 0 loops in function.
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_4, inputs_15, _3);
PULSE.c:8:2: missed: statement clobbers memory: _5 (layer_17);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_12, pretmp_26, _9);
PULSE.c:13:28: note: ***** Analysis failed with vector mode V8SI
PULSE.c:13:28: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:22:2: missed: statement clobbers memory: _1 (layer_10(D));
PULSE.c:22:2: missed: statement clobbers memory: _14 (_2);
PULSE.c:22:2: missed: statement clobbers memory: _21 (_15);
PULSE.c:22:2: missed: statement clobbers memory: _28 (_22);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_29);
PULSE.c:26:3: missed: statement clobbers memory: memset (_34, 0, _32);
PULSE.c:26:3: missed: statement clobbers memory: memset (_27, 0, _25);
PULSE.c:26:3: missed: statement clobbers memory: memset (_20, 0, _18);
PULSE.c:26:3: missed: statement clobbers memory: memset (_7, 0, _5);
PULSE.c:28:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:28:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:32:6: note: vectorized 1 loops in function.
PULSE.c:34:8: missed: statement clobbers memory: _1 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_2);
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _9 = rand ();
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8SI
PULSE.c:32:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:61:16: missed: couldn't vectorize loop
PULSE.c:61:16: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:64:17: missed: couldn't vectorize loop
PULSE.c:64:17: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:73:19: missed: couldn't vectorize loop
PULSE.c:73:19: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _69 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:53:22: missed: couldn't vectorize loop
PULSE.c:53:22: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:47:6: note: vectorized 1 loops in function.
PULSE.c:48:1: missed: statement clobbers memory: saved_stack.7_39 = __builtin_stack_save ();
PULSE.c:49:46: missed: statement clobbers memory: _1 = PULSE_GetLossFunctionPtr (loss_function_40(D));
PULSE.c:60:10: missed: statement clobbers memory: random.4_46 = __builtin_alloca_with_align (_4, 32);
PULSE.c:34:8: missed: statement clobbers memory: _61 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_62);
PULSE.c:39:29: missed: statement clobbers memory: _69 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _72 = rand ();
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_98, inputs_93, _97);
PULSE.c:8:2: missed: statement clobbers memory: _99 (layer_94);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_105, _104, _103);
PULSE.c:67:11: missed: statement clobbers memory: loss_55 = PULSE_GetLoss_42 (_17, _16, _12, _11);
PULSE.c:22:2: missed: statement clobbers memory: _86 (output_34);
PULSE.c:22:2: missed: statement clobbers memory: _108 (_87);
PULSE.c:22:2: missed: statement clobbers memory: _115 (_109);
PULSE.c:22:2: missed: statement clobbers memory: _122 (_116);
PULSE.c:22:2: missed: statement clobbers memory: _129 (_123);
PULSE.c:22:2: missed: statement clobbers memory: _136 (_130);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_137);
PULSE.c:26:3: missed: statement clobbers memory: memset (_142, 0, _140);
PULSE.c:26:3: missed: statement clobbers memory: memset (_135, 0, _133);
PULSE.c:26:3: missed: statement clobbers memory: memset (_128, 0, _126);
PULSE.c:26:3: missed: statement clobbers memory: memset (_121, 0, _119);
PULSE.c:26:3: missed: statement clobbers memory: memset (_114, 0, _112);
PULSE.c:26:3: missed: statement clobbers memory: memset (_92, 0, _90);
PULSE.c:75:6: missed: statement clobbers memory: _21 (current_191, args);
PULSE.c:81:4: missed: statement clobbers memory: printf ("Epoch: %d | Item: %d | Loss: %.10f | Batch Loss: %.10f\r", i_188, j_189, _25, _24);
PULSE.c:47:6: missed: statement clobbers memory: __builtin_stack_restore (saved_stack.7_39);
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4DI
PULSE.c:47:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:91:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:91:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:99:20: missed: couldn't vectorize loop
PULSE.c:99:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:99:20: missed: couldn't vectorize loop
PULSE.c:99:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:93:15: note: vectorized 0 loops in function.
PULSE.c:97:2: missed: statement clobbers memory: __builtin_va_start (&layers, 0);
PULSE.c:98:44: missed: statement clobbers memory: layers_list_27 = malloc (_3);
PULSE.c:106:22: missed: statement clobbers memory: *_12 = PULSE_CreateDenseLayer (_9, _7, args$8_51, args$12_63); [return slot optimization]
PULSE.c:106:22: missed: statement clobbers memory: *_76 = PULSE_CreateDenseLayer (_79, _80, args$8_82, args$12_81); [return slot optimization]
PULSE.c:112:2: missed: statement clobbers memory: __builtin_va_end (&layers);
PULSE.c:113:9: note: ***** Analysis failed with vector mode V8SI
PULSE.c:113:9: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:119:8: missed: couldn't vectorize loop
PULSE.c:119:8: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:116:6: note: vectorized 0 loops in function.
PULSE.c:121:3: missed: statement clobbers memory: _1 (current_11);
PULSE.c:124:2: missed: statement clobbers memory: free (layer_4(D));
PULSE.c:125:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:125:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Loss.c:15:19: optimized: loop vectorized using 32 byte vectors
Loss.c:15:19: optimized:  loop versioned for vectorization because of possible aliasing
Loss.c:15:19: optimized: loop vectorized using 16 byte vectors
Loss.c:12:23: note: vectorized 1 loops in function.
Loss.c:12:23: note: ***** Analysis failed with vector mode V8SF
Loss.c:12:23: note: ***** The result for vector mode V32QI would be the same
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V16QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V16QI
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V8QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V8QI
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V4QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V4QI
Loss.c:7:19: missed: couldn't vectorize loop
Loss.c:8:38: missed: not vectorized: unsupported use in stmt.
Loss.c:4:23: note: vectorized 0 loops in function.
Loss.c:9:13: note: ***** Analysis failed with vector mode V8SF
Loss.c:9:13: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SF
Loss.c:23:2: note: ***** Analysis failed with vector mode VOID
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _30 (pretmp_91, _50, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:20: missed: not vectorized: no vectype for stmt: _21 = *_20;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:170:10: optimized: loop vectorized using 32 byte vectors
Dense.c:170:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:170:10: optimized: loop vectorized using 16 byte vectors
Dense.c:161:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_34, _77);
Dense.c:154:10: optimized: loop vectorized using 32 byte vectors
Dense.c:154:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:154:10: optimized: loop vectorized using 16 byte vectors
Dense.c:145:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, _69);
Dense.c:136:13: note: vectorized 2 loops in function.
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, _69);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_19, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_34, _77);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_38, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
Dense.c:136:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:136:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:105:24: missed: couldn't vectorize loop
Dense.c:105:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:108:17: missed: couldn't vectorize loop
Dense.c:108:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:122:24: missed: couldn't vectorize loop
Dense.c:122:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:125:17: missed: couldn't vectorize loop
Dense.c:125:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:90:15: missed: couldn't vectorize loop
Dense.c:90:15: missed: not vectorized: number of iterations cannot be computed.
Dense.c:83:13: note: vectorized 0 loops in function.
Dense.c:86:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:110:17: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:110:17: note: SLPing BB part
Dense.c:127:17: note: Costing subgraph: 
Dense.c:127:17: note: node 0x89bcea8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:127:17: note: op template: _49 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 0 _49 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 1 _209 = (sizetype) wi_137;
Dense.c:127:17: note: 	children 0x89bcfa8
Dense.c:127:17: note: node (external) 0x89bcfa8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:127:17: note: 	{ j_135, wi_137 }
Dense.c:127:17: note: Cost model analysis: 
Dense.c:127:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:127:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:127:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: Costing subgraph: 
Dense.c:110:17: note: node 0x89bd0a8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:110:17: note: op template: _24 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 0 _24 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 1 _229 = (sizetype) wi_136;
Dense.c:110:17: note: 	children 0x89bd1a8
Dense.c:110:17: note: node (external) 0x89bd1a8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:110:17: note: 	{ j_134, wi_136 }
Dense.c:110:17: note: Cost model analysis: 
Dense.c:110:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:110:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:110:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: ***** The result for vector mode V32QI would be the same
Dense.c:110:17: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:110:17: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:110:17: note: SLPing BB part
Dense.c:127:17: note: Costing subgraph: 
Dense.c:127:17: note: node 0x89bd0a8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:127:17: note: op template: _49 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 0 _49 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 1 _209 = (sizetype) wi_137;
Dense.c:127:17: note: 	children 0x89bcea8
Dense.c:127:17: note: node (external) 0x89bcea8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:127:17: note: 	{ j_135, wi_137 }
Dense.c:127:17: note: Cost model analysis: 
Dense.c:127:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:127:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:127:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: Costing subgraph: 
Dense.c:110:17: note: node 0x89bd228 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:110:17: note: op template: _24 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 0 _24 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 1 _229 = (sizetype) wi_136;
Dense.c:110:17: note: 	children 0x89bd128
Dense.c:110:17: note: node (external) 0x89bd128 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:110:17: note: 	{ j_134, wi_136 }
Dense.c:110:17: note: Cost model analysis: 
Dense.c:110:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:110:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:110:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:105:51: note: ***** Analysis failed with vector mode V8QI
Dense.c:105:51: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:105:51: note: ***** Analysis failed with vector mode V4QI
Dense.c:62:15: missed: couldn't vectorize loop
Dense.c:62:15: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:75:11: optimized: loop vectorized using 32 byte vectors
Dense.c:75:11: optimized: loop vectorized using 16 byte vectors
Dense.c:66:11: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:881:10: missed: not vectorized: no vectype for stmt: _56 = MEM[(__m256 * {ref-all})w_ptr_81];
 scalar_type: __m256
Dense.c:54:13: note: vectorized 1 loops in function.
Dense.c:57:2: missed: statement clobbers memory: memcpy (_5, _4, _3);
Dense.c:79:2: missed: statement clobbers memory: _28 (pretmp_162, _67, 0);
Dense.c:54:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:54:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:182:2: missed: statement clobbers memory: free (_1);
Dense.c:183:2: missed: statement clobbers memory: free (_2);
Dense.c:184:2: missed: statement clobbers memory: free (_3);
Dense.c:185:2: missed: statement clobbers memory: free (_4);
Dense.c:186:2: missed: statement clobbers memory: free (dense_7);
Dense.c:187:2: missed: statement clobbers memory: PULSE_DestroyLayer (this_6(D));
Dense.c:188:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:188:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:202:19: missed: couldn't vectorize loop
Dense.c:202:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:192:13: note: vectorized 0 loops in function.
Dense.c:194:47: missed: statement clobbers memory: dense_32 = malloc (40);
Dense.c:196:36: missed: statement clobbers memory: _5 = aligned_alloc (64, _4);
Dense.c:197:38: missed: statement clobbers memory: _6 = aligned_alloc (64, _4);
Dense.c:198:36: missed: statement clobbers memory: _8 = aligned_alloc (64, _7);
Dense.c:199:35: missed: statement clobbers memory: _9 = aligned_alloc (64, _7);
Dense.c:200:36: missed: statement clobbers memory: _10 = aligned_alloc (64, _7);
Dense.c:203:39: missed: statement clobbers memory: _11 = rand ();
Dense.c:203:73: missed: statement clobbers memory: _60 = sqrt (_18);
Dense.c:215:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:216:4: missed: statement clobbers memory: exit (1);
Dense.c:212:4: missed: statement clobbers memory: layer = PULSE_CreateLayer (args$n_inputs_43, args$n_outputs_54, 0, args$8_55, _SIMD_FeedDense, _SIMD_BackDense, _SIMD_FixDense, _DestroyDense, 1); [return slot optimization]
Dense.c:209:12: missed: statement clobbers memory: layer = PULSE_CreateLayer (args$n_inputs_43, args$n_outputs_54, 0, args$8_55, _FeedDense, _BackDense, _FixDense, _DestroyDense, 0); [return slot optimization]
Dense.c:222:9: missed: not vectorized: more than one data ref in stmt: <retval> = layer;
Dense.c:203:112: note: ***** Analysis failed with vector mode V8SI
Dense.c:203:112: note: ***** The result for vector mode V32QI would be the same
Dense.c:203:112: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:203:112: note: ***** Analysis failed with vector mode V16QI
Dense.c:203:112: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:203:112: note: ***** Analysis failed with vector mode V8QI
Dense.c:203:112: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:203:112: note: ***** Analysis failed with vector mode V4QI
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:4:18: note: vectorized 0 loops in function.
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_4, inputs_15, _3);
PULSE.c:8:2: missed: statement clobbers memory: _5 (layer_17);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_12, pretmp_26, _9);
PULSE.c:13:28: note: ***** Analysis failed with vector mode V8SI
PULSE.c:13:28: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:22:2: missed: statement clobbers memory: _1 (layer_10(D));
PULSE.c:22:2: missed: statement clobbers memory: _14 (_2);
PULSE.c:22:2: missed: statement clobbers memory: _21 (_15);
PULSE.c:22:2: missed: statement clobbers memory: _28 (_22);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_29);
PULSE.c:26:3: missed: statement clobbers memory: memset (_34, 0, _32);
PULSE.c:26:3: missed: statement clobbers memory: memset (_27, 0, _25);
PULSE.c:26:3: missed: statement clobbers memory: memset (_20, 0, _18);
PULSE.c:26:3: missed: statement clobbers memory: memset (_7, 0, _5);
PULSE.c:28:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:28:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:32:6: note: vectorized 1 loops in function.
PULSE.c:34:8: missed: statement clobbers memory: _1 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_2);
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _9 = rand ();
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8SI
PULSE.c:32:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:61:16: missed: couldn't vectorize loop
PULSE.c:61:16: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:64:17: missed: couldn't vectorize loop
PULSE.c:64:17: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:73:19: missed: couldn't vectorize loop
PULSE.c:73:19: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _69 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:53:22: missed: couldn't vectorize loop
PULSE.c:53:22: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:47:6: note: vectorized 1 loops in function.
PULSE.c:48:1: missed: statement clobbers memory: saved_stack.7_39 = __builtin_stack_save ();
PULSE.c:49:46: missed: statement clobbers memory: _1 = PULSE_GetLossFunctionPtr (loss_function_40(D));
PULSE.c:60:10: missed: statement clobbers memory: random.4_46 = __builtin_alloca_with_align (_4, 32);
PULSE.c:34:8: missed: statement clobbers memory: _61 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_62);
PULSE.c:39:29: missed: statement clobbers memory: _69 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _72 = rand ();
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_98, inputs_93, _97);
PULSE.c:8:2: missed: statement clobbers memory: _99 (layer_94);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_105, _104, _103);
PULSE.c:67:11: missed: statement clobbers memory: loss_55 = PULSE_GetLoss_42 (_17, _16, _12, _11);
PULSE.c:22:2: missed: statement clobbers memory: _86 (output_34);
PULSE.c:22:2: missed: statement clobbers memory: _108 (_87);
PULSE.c:22:2: missed: statement clobbers memory: _115 (_109);
PULSE.c:22:2: missed: statement clobbers memory: _122 (_116);
PULSE.c:22:2: missed: statement clobbers memory: _129 (_123);
PULSE.c:22:2: missed: statement clobbers memory: _136 (_130);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_137);
PULSE.c:26:3: missed: statement clobbers memory: memset (_142, 0, _140);
PULSE.c:26:3: missed: statement clobbers memory: memset (_135, 0, _133);
PULSE.c:26:3: missed: statement clobbers memory: memset (_128, 0, _126);
PULSE.c:26:3: missed: statement clobbers memory: memset (_121, 0, _119);
PULSE.c:26:3: missed: statement clobbers memory: memset (_114, 0, _112);
PULSE.c:26:3: missed: statement clobbers memory: memset (_92, 0, _90);
PULSE.c:75:6: missed: statement clobbers memory: _21 (current_191, args);
PULSE.c:81:4: missed: statement clobbers memory: printf ("Epoch: %d | Item: %d | Loss: %.10f | Batch Loss: %.10f\r", i_188, j_189, _25, _24);
PULSE.c:47:6: missed: statement clobbers memory: __builtin_stack_restore (saved_stack.7_39);
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4DI
PULSE.c:47:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:91:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:91:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:99:20: missed: couldn't vectorize loop
PULSE.c:99:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:99:20: missed: couldn't vectorize loop
PULSE.c:99:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:93:15: note: vectorized 0 loops in function.
PULSE.c:97:2: missed: statement clobbers memory: __builtin_va_start (&layers, 0);
PULSE.c:98:44: missed: statement clobbers memory: layers_list_21 = malloc (_3);
PULSE.c:105:67: missed: not vectorized: more than one data ref in stmt: D.6448 = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.104_34];
PULSE.c:105:22: missed: statement clobbers memory: *_6 = PULSE_CreateDenseLayer (D.6448); [return slot optimization]
PULSE.c:105:67: missed: not vectorized: more than one data ref in stmt: D.6448 = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.104_52];
PULSE.c:105:22: missed: statement clobbers memory: *_37 = PULSE_CreateDenseLayer (D.6448); [return slot optimization]
PULSE.c:111:2: missed: statement clobbers memory: __builtin_va_end (&layers);
PULSE.c:112:9: note: ***** Analysis failed with vector mode V8SI
PULSE.c:112:9: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:118:8: missed: couldn't vectorize loop
PULSE.c:118:8: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:115:6: note: vectorized 0 loops in function.
PULSE.c:120:3: missed: statement clobbers memory: _1 (current_11);
PULSE.c:123:2: missed: statement clobbers memory: free (layer_4(D));
PULSE.c:124:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:124:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Layer.c:7:34: missed: statement clobbers memory: _3 = aligned_alloc (64, _2);
Layer.c:8:35: missed: statement clobbers memory: _6 = aligned_alloc (64, _5);
Layer.c:9:34: missed: statement clobbers memory: _7 = aligned_alloc (64, _5);
Layer.c:14:19: missed: statement clobbers memory: _8 = PULSE_GetActivationFunctionPtr (activation_function_18(D));
Layer.c:19:9: note: ***** Analysis succeeded with vector mode V8SI
Layer.c:19:9: note: SLPing BB part
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x487d3c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: op template: MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: 	stmt 0 MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: 	stmt 1 MEM <unsigned int> [(struct PULSE_Layer *)&<retval> + 4B] = optimization_type_17(D);
Layer.c:19:9: note: 	children 0x487d448
Layer.c:19:9: note: node (external) 0x487d448 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ type_16(D), optimization_type_17(D) }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x487d4c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: op template: <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: 	stmt 0 <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: 	stmt 1 <retval>.n_outputs = n_outputs_12(D);
Layer.c:19:9: note: 	children 0x487d548
Layer.c:19:9: note: node (external) 0x487d548 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ n_inputs_11(D), n_outputs_12(D) }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x487d648 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: <retval>.inputs = _3;
Layer.c:19:9: note: 	stmt 0 <retval>.inputs = _3;
Layer.c:19:9: note: 	stmt 1 <retval>.outputs = _6;
Layer.c:19:9: note: 	children 0x487d6c8
Layer.c:19:9: note: node (external) 0x487d6c8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: 	stmt 0 _3 = aligned_alloc (64, _2);
Layer.c:19:9: note: 	stmt 1 _6 = aligned_alloc (64, _5);
Layer.c:19:9: note: 	children 0x487d748 0x487d7c8
Layer.c:19:9: note: node (constant) 0x487d748 (max_nunits=1, refcnt=1)
Layer.c:19:9: note: 	{ 64, 64 }
Layer.c:19:9: note: node 0x487d7c8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: patt_38 = n_inputs_11(D) w* 4;
Layer.c:19:9: note: 	stmt 0 patt_38 = n_inputs_11(D) w* 4;
Layer.c:19:9: note: 	stmt 1 patt_39 = n_outputs_12(D) w* 4;
Layer.c:19:9: note: 	children 0x487d848 0x487d8c8
Layer.c:19:9: note: node (external) 0x487d848 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ n_inputs_11(D), n_outputs_12(D) }
Layer.c:19:9: note: node (constant) 0x487d8c8 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ 4, 4 }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x487d9c8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Layer.c:19:9: note: op template: <retval>.feed = feed_20(D);
Layer.c:19:9: note: 	stmt 0 <retval>.feed = feed_20(D);
Layer.c:19:9: note: 	stmt 1 <retval>.back = back_21(D);
Layer.c:19:9: note: 	stmt 2 <retval>.fix = fix_22(D);
Layer.c:19:9: note: 	stmt 3 <retval>.destroy = destroy_23(D);
Layer.c:19:9: note: 	children 0x487da48
Layer.c:19:9: note: node (external) 0x487da48 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Layer.c:19:9: note: 	{ feed_20(D), back_21(D), fix_22(D), destroy_23(D) }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 64
  Scalar cost: 64
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x487dac8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: <retval>.parent = 0B;
Layer.c:19:9: note: 	stmt 0 <retval>.parent = 0B;
Layer.c:19:9: note: 	stmt 1 <retval>.child = 0B;
Layer.c:19:9: note: 	children 0x487db48
Layer.c:19:9: note: node (constant) 0x487db48 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: 	{ 0B, 0B }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Layer.c:19:9: note: Basic block will be vectorized using SLP
Layer.c:19:9: optimized: basic block part vectorized using 8 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x487d3c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: op template: MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: 	stmt 0 MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: 	stmt 1 MEM <unsigned int> [(struct PULSE_Layer *)&<retval> + 4B] = optimization_type_17(D);
Layer.c:19:9: note: 	children 0x487d448
Layer.c:19:9: note: node (external) 0x487d448 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ type_16(D), optimization_type_17(D) }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: vect_is_simple_use: operand optimization_type_17(D), type of def: external
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: MEM <unsigned int> [(struct PULSE_Layer *)&<retval>]
Layer.c:19:9: note: created &MEM <unsigned int> [(struct PULSE_Layer *)&<retval>]
Layer.c:19:9: note: add new stmt: MEM <vector(2) unsigned int> [(struct PULSE_Layer *)&<retval>] = _40;
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: optimized: basic block part vectorized using 8 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x487d4c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: op template: <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: 	stmt 0 <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: 	stmt 1 <retval>.n_outputs = n_outputs_12(D);
Layer.c:19:9: note: 	children 0x487d548
Layer.c:19:9: note: node (external) 0x487d548 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ n_inputs_11(D), n_outputs_12(D) }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: vect_is_simple_use: operand n_outputs_12(D), type of def: external
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: <retval>.n_inputs
Layer.c:19:9: note: created &<retval>.n_inputs
Layer.c:19:9: note: add new stmt: MEM <vector(2) unsigned int> [(unsigned int *)&<retval> + 128B] = _42;
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: optimized: basic block part vectorized using 16 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x487d648 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: <retval>.inputs = _3;
Layer.c:19:9: note: 	stmt 0 <retval>.inputs = _3;
Layer.c:19:9: note: 	stmt 1 <retval>.outputs = _6;
Layer.c:19:9: note: 	children 0x487d6c8
Layer.c:19:9: note: node (external) 0x487d6c8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: 	stmt 0 _3 = aligned_alloc (64, _2);
Layer.c:19:9: note: 	stmt 1 _6 = aligned_alloc (64, _5);
Layer.c:19:9: note: 	children 0x487d748 0x487d7c8
Layer.c:19:9: note: node (constant) 0x487d748 (max_nunits=1, refcnt=1)
Layer.c:19:9: note: 	{ 64, 64 }
Layer.c:19:9: note: node 0x487d7c8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: patt_38 = n_inputs_11(D) w* 4;
Layer.c:19:9: note: 	stmt 0 patt_38 = n_inputs_11(D) w* 4;
Layer.c:19:9: note: 	stmt 1 patt_39 = n_outputs_12(D) w* 4;
Layer.c:19:9: note: 	children 0x487d848 0x487d8c8
Layer.c:19:9: note: node (external) 0x487d848 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ n_inputs_11(D), n_outputs_12(D) }
Layer.c:19:9: note: node (constant) 0x487d8c8 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ 4, 4 }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: <retval>.inputs = _3;
Layer.c:19:9: note: vect_is_simple_use: operand aligned_alloc (64, _5), type of def: internal
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.inputs
Layer.c:19:9: note: created &<retval>.inputs
Layer.c:19:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval> + 8B] = _46;
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: optimized: basic block part vectorized using 32 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x487d9c8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Layer.c:19:9: note: op template: <retval>.feed = feed_20(D);
Layer.c:19:9: note: 	stmt 0 <retval>.feed = feed_20(D);
Layer.c:19:9: note: 	stmt 1 <retval>.back = back_21(D);
Layer.c:19:9: note: 	stmt 2 <retval>.fix = fix_22(D);
Layer.c:19:9: note: 	stmt 3 <retval>.destroy = destroy_23(D);
Layer.c:19:9: note: 	children 0x487da48
Layer.c:19:9: note: node (external) 0x487da48 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Layer.c:19:9: note: 	{ feed_20(D), back_21(D), fix_22(D), destroy_23(D) }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: <retval>.feed = feed_20(D);
Layer.c:19:9: note: vect_is_simple_use: operand back_21(D), type of def: external
Layer.c:19:9: note: vect_is_simple_use: operand fix_22(D), type of def: external
Layer.c:19:9: note: vect_is_simple_use: operand destroy_23(D), type of def: external
Layer.c:19:9: note: conflicting alias set types.
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(4) long unsigned int  vectorizing a pointer ref: <retval>.feed
Layer.c:19:9: note: created &<retval>.feed
Layer.c:19:9: note: add new stmt: MEM <vector(4) long unsigned int> [(void *)&<retval> + 32B] = _52;
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: optimized: basic block part vectorized using 16 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x487dac8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: <retval>.parent = 0B;
Layer.c:19:9: note: 	stmt 0 <retval>.parent = 0B;
Layer.c:19:9: note: 	stmt 1 <retval>.child = 0B;
Layer.c:19:9: note: 	children 0x487db48
Layer.c:19:9: note: node (constant) 0x487db48 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: 	{ 0B, 0B }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: <retval>.parent = 0B;
Layer.c:19:9: note: vect_is_simple_use: operand 0B, type of def: constant
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.parent
Layer.c:19:9: note: created &<retval>.parent
Layer.c:19:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer * *)&<retval> + 72B] = { 0, 0 };
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: note: ***** The result for vector mode V32QI would be the same
Layer.c:24:2: missed: statement clobbers memory: free (_1);
Layer.c:25:2: missed: statement clobbers memory: free (_2);
Layer.c:26:2: missed: statement clobbers memory: free (_3);
Layer.c:27:1: note: ***** Analysis failed with vector mode V4DI
Layer.c:27:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _28 (pretmp_91, _51, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:19: missed: not vectorized: no vectype for stmt: _22 = *_21;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:188:1: note: ***** Analysis failed with vector mode VOID
Dense.c:203:19: missed: couldn't vectorize loop
Dense.c:203:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:192:13: note: vectorized 0 loops in function.
Dense.c:197:35: missed: statement clobbers memory: _5 = aligned_alloc (64, _4);
Dense.c:198:37: missed: statement clobbers memory: _6 = aligned_alloc (64, _4);
Dense.c:199:35: missed: statement clobbers memory: _8 = aligned_alloc (64, _7);
Dense.c:200:34: missed: statement clobbers memory: _9 = aligned_alloc (64, _7);
Dense.c:201:35: missed: statement clobbers memory: _10 = aligned_alloc (64, _7);
Dense.c:204:38: missed: statement clobbers memory: _11 = rand ();
Dense.c:204:72: missed: statement clobbers memory: _56 = sqrt (_18);
Dense.c:216:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:217:4: missed: statement clobbers memory: exit (1);
Dense.c:210:12: missed: statement clobbers memory: layer = PULSE_CreateLayer (args$n_inputs_37, args$n_outputs_47, 0, _27, _FeedDense, _BackDense, _FixDense, _DestroyDense, 0); [return slot optimization]
Dense.c:223:9: missed: not vectorized: more than one data ref in stmt: <retval> = layer;
Dense.c:223:9: note: ***** Analysis succeeded with vector mode V8SI
Dense.c:223:9: note: SLPing BB part
Dense.c:223:9: note: Costing subgraph: 
Dense.c:223:9: note: node 0x75ddf98 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:223:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = _5;
Dense.c:223:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = _5;
Dense.c:223:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&layer + 96B] = _8;
Dense.c:223:9: note: 	stmt 2 MEM <float *> [(struct PULSE_Layer *)&layer + 104B] = _9;
Dense.c:223:9: note: 	stmt 3 MEM <float *> [(struct PULSE_Layer *)&layer + 112B] = _10;
Dense.c:223:9: note: 	children 0x75de018
Dense.c:223:9: note: node (external) 0x75de018 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:223:9: note: 	stmt 0 _5 = aligned_alloc (64, _4);
Dense.c:223:9: note: 	stmt 1 _8 = aligned_alloc (64, _7);
Dense.c:223:9: note: 	stmt 2 _9 = aligned_alloc (64, _7);
Dense.c:223:9: note: 	stmt 3 _10 = aligned_alloc (64, _7);
Dense.c:223:9: note: 	children 0x75de098 0x75de198
Dense.c:223:9: note: node (constant) 0x75de098 (max_nunits=1, refcnt=1)
Dense.c:223:9: note: 	{ 64, 64, 64, 64 }
Dense.c:223:9: note: node (external) 0x75de198 (max_nunits=1, refcnt=1)
Dense.c:223:9: note: 	{ _4, _7, _7, _7 }
Dense.c:223:9: note: Cost model analysis: 
Dense.c:223:9: note: Cost model analysis for part in loop 0:
  Vector cost: 64
  Scalar cost: 64
Dense.c:223:9: note: Basic block will be vectorized using SLP
Dense.c:223:9: optimized: basic block part vectorized using 32 byte vectors
Dense.c:223:9: note: Vectorizing SLP tree:
Dense.c:223:9: note: node 0x75ddf98 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:223:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = _5;
Dense.c:223:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = _5;
Dense.c:223:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&layer + 96B] = _8;
Dense.c:223:9: note: 	stmt 2 MEM <float *> [(struct PULSE_Layer *)&layer + 104B] = _9;
Dense.c:223:9: note: 	stmt 3 MEM <float *> [(struct PULSE_Layer *)&layer + 112B] = _10;
Dense.c:223:9: note: 	children 0x75de018
Dense.c:223:9: note: node (external) 0x75de018 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:223:9: note: 	stmt 0 _5 = aligned_alloc (64, _4);
Dense.c:223:9: note: 	stmt 1 _8 = aligned_alloc (64, _7);
Dense.c:223:9: note: 	stmt 2 _9 = aligned_alloc (64, _7);
Dense.c:223:9: note: 	stmt 3 _10 = aligned_alloc (64, _7);
Dense.c:223:9: note: 	children 0x75de098 0x75de198
Dense.c:223:9: note: node (constant) 0x75de098 (max_nunits=1, refcnt=1)
Dense.c:223:9: note: 	{ 64, 64, 64, 64 }
Dense.c:223:9: note: node (external) 0x75de198 (max_nunits=1, refcnt=1)
Dense.c:223:9: note: 	{ _4, _7, _7, _7 }
Dense.c:223:9: note: ------>vectorizing SLP node starting from: MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = _5;
Dense.c:223:9: note: vect_is_simple_use: operand aligned_alloc (64, _7), type of def: internal
Dense.c:223:9: note: vect_is_simple_use: operand aligned_alloc (64, _7), type of def: internal
Dense.c:223:9: note: vect_is_simple_use: operand aligned_alloc (64, _7), type of def: internal
Dense.c:223:9: note: transform store. ncopies = 1
Dense.c:223:9: note: create vector_type-pointer variable to type: vector(4) long unsigned int  vectorizing a pointer ref: MEM <float *> [(struct PULSE_Layer *)&layer + 88B]
Dense.c:223:9: note: created &MEM <float *> [(struct PULSE_Layer *)&layer + 88B]
Dense.c:223:9: note: add new stmt: MEM <vector(4) long unsigned int> [(struct PULSE_Layer *)&layer + 88B] = _66;
Dense.c:223:9: note: vectorizing stmts using SLP.
Dense.c:223:9: note: ***** The result for vector mode V32QI would be the same
Activations.c:17:19: optimized: loop vectorized using 32 byte vectors
Activations.c:17:19: optimized: loop vectorized using 16 byte vectors
Activations.c:17:19: optimized: loop vectorized using 32 byte vectors
Activations.c:17:19: optimized: loop vectorized using 16 byte vectors
Activations.c:14:13: note: vectorized 2 loops in function.
Activations.c:14:13: note: ***** Analysis failed with vector mode V8SF
Activations.c:14:13: note: ***** The result for vector mode V32QI would be the same
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V16QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V16QI
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V8QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V8QI
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V4QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V4QI
Activations.c:28:19: missed: couldn't vectorize loop
Activations.c:28:19: missed: not vectorized: unsupported control flow in loop.
Activations.c:28:19: optimized: loop vectorized using 32 byte vectors
Activations.c:28:19: optimized: loop vectorized using 16 byte vectors
Activations.c:25:13: note: vectorized 1 loops in function.
Activations.c:25:13: note: ***** Analysis failed with vector mode V8SF
Activations.c:25:13: note: ***** The result for vector mode V32QI would be the same
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V16QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V16QI
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V8QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V8QI
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V4QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V4QI
Activations.c:38:1: note: ***** Analysis failed with vector mode VOID
Activations.c:5:19: missed: couldn't vectorize loop
Activations.c:9:21: missed: statement clobbers memory: _36 = expf (_18);
Activations.c:5:19: missed: couldn't vectorize loop
Activations.c:9:21: missed: statement clobbers memory: _12 = expf (_11);
Activations.c:3:13: note: vectorized 0 loops in function.
Activations.c:9:21: missed: statement clobbers memory: _12 = expf (_11);
Activations.c:9:21: missed: statement clobbers memory: _36 = expf (_18);
Activations.c:11:1: note: ***** Analysis failed with vector mode V8SF
Activations.c:11:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SF
Activations.c:44:2: note: ***** Analysis failed with vector mode VOID
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:4:18: note: vectorized 0 loops in function.
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_4, inputs_15, _3);
PULSE.c:8:2: missed: statement clobbers memory: _5 (layer_17);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_12, pretmp_26, _9);
PULSE.c:13:28: note: ***** Analysis failed with vector mode V8SI
PULSE.c:13:28: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:22:2: missed: statement clobbers memory: _1 (layer_10(D));
PULSE.c:22:2: missed: statement clobbers memory: _14 (_2);
PULSE.c:22:2: missed: statement clobbers memory: _21 (_15);
PULSE.c:22:2: missed: statement clobbers memory: _28 (_22);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_29);
PULSE.c:26:3: missed: statement clobbers memory: memset (_34, 0, _32);
PULSE.c:26:3: missed: statement clobbers memory: memset (_27, 0, _25);
PULSE.c:26:3: missed: statement clobbers memory: memset (_20, 0, _18);
PULSE.c:26:3: missed: statement clobbers memory: memset (_7, 0, _5);
PULSE.c:28:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:28:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:32:6: note: vectorized 1 loops in function.
PULSE.c:34:8: missed: statement clobbers memory: _1 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_2);
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _9 = rand ();
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8SI
PULSE.c:32:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:61:16: missed: couldn't vectorize loop
PULSE.c:61:16: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:64:17: missed: couldn't vectorize loop
PULSE.c:64:17: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:73:19: missed: couldn't vectorize loop
PULSE.c:73:19: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _69 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:53:22: missed: couldn't vectorize loop
PULSE.c:53:22: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:47:6: note: vectorized 1 loops in function.
PULSE.c:48:1: missed: statement clobbers memory: saved_stack.7_39 = __builtin_stack_save ();
PULSE.c:49:46: missed: statement clobbers memory: _1 = PULSE_GetLossFunctionPtr (loss_function_40(D));
PULSE.c:60:10: missed: statement clobbers memory: random.4_46 = __builtin_alloca_with_align (_4, 32);
PULSE.c:34:8: missed: statement clobbers memory: _61 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_62);
PULSE.c:39:29: missed: statement clobbers memory: _69 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _72 = rand ();
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_98, inputs_93, _97);
PULSE.c:8:2: missed: statement clobbers memory: _99 (layer_94);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_105, _104, _103);
PULSE.c:67:11: missed: statement clobbers memory: loss_55 = PULSE_GetLoss_42 (_17, _16, _12, _11);
PULSE.c:22:2: missed: statement clobbers memory: _86 (output_34);
PULSE.c:22:2: missed: statement clobbers memory: _108 (_87);
PULSE.c:22:2: missed: statement clobbers memory: _115 (_109);
PULSE.c:22:2: missed: statement clobbers memory: _122 (_116);
PULSE.c:22:2: missed: statement clobbers memory: _129 (_123);
PULSE.c:22:2: missed: statement clobbers memory: _136 (_130);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_137);
PULSE.c:26:3: missed: statement clobbers memory: memset (_142, 0, _140);
PULSE.c:26:3: missed: statement clobbers memory: memset (_135, 0, _133);
PULSE.c:26:3: missed: statement clobbers memory: memset (_128, 0, _126);
PULSE.c:26:3: missed: statement clobbers memory: memset (_121, 0, _119);
PULSE.c:26:3: missed: statement clobbers memory: memset (_114, 0, _112);
PULSE.c:26:3: missed: statement clobbers memory: memset (_92, 0, _90);
PULSE.c:75:6: missed: statement clobbers memory: _21 (current_191, args);
PULSE.c:81:4: missed: statement clobbers memory: printf ("Epoch: %d | Item: %d | Loss: %.10f | Batch Loss: %.10f\r", i_188, j_189, _25, _24);
PULSE.c:47:6: missed: statement clobbers memory: __builtin_stack_restore (saved_stack.7_39);
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4DI
PULSE.c:47:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:91:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:91:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:99:20: missed: couldn't vectorize loop
PULSE.c:99:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:99:20: missed: couldn't vectorize loop
PULSE.c:99:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:93:15: note: vectorized 0 loops in function.
PULSE.c:97:2: missed: statement clobbers memory: __builtin_va_start (&layers, 0);
PULSE.c:98:44: missed: statement clobbers memory: layers_list_21 = malloc (_3);
PULSE.c:105:67: missed: not vectorized: more than one data ref in stmt: D.6451 = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.104_34];
PULSE.c:105:22: missed: statement clobbers memory: *_6 = PULSE_CreateDenseLayer (D.6451); [return slot optimization]
PULSE.c:105:67: missed: not vectorized: more than one data ref in stmt: D.6451 = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.104_52];
PULSE.c:105:22: missed: statement clobbers memory: *_37 = PULSE_CreateDenseLayer (D.6451); [return slot optimization]
PULSE.c:111:2: missed: statement clobbers memory: __builtin_va_end (&layers);
PULSE.c:112:9: note: ***** Analysis failed with vector mode V8SI
PULSE.c:112:9: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:118:8: missed: couldn't vectorize loop
PULSE.c:118:8: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:115:6: note: vectorized 0 loops in function.
PULSE.c:120:3: missed: statement clobbers memory: _1 (current_11);
PULSE.c:123:2: missed: statement clobbers memory: free (layer_4(D));
PULSE.c:124:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:124:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Loss.c:15:19: optimized: loop vectorized using 32 byte vectors
Loss.c:15:19: optimized:  loop versioned for vectorization because of possible aliasing
Loss.c:15:19: optimized: loop vectorized using 16 byte vectors
Loss.c:12:23: note: vectorized 1 loops in function.
Loss.c:12:23: note: ***** Analysis failed with vector mode V8SF
Loss.c:12:23: note: ***** The result for vector mode V32QI would be the same
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V16QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V16QI
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V8QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V8QI
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V4QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V4QI
Loss.c:7:19: missed: couldn't vectorize loop
Loss.c:8:38: missed: not vectorized: unsupported use in stmt.
Loss.c:4:23: note: vectorized 0 loops in function.
Loss.c:9:13: note: ***** Analysis failed with vector mode V8SF
Loss.c:9:13: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SF
Loss.c:23:2: note: ***** Analysis failed with vector mode VOID
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _28 (pretmp_91, _51, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:19: missed: not vectorized: no vectype for stmt: _22 = *_21;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:188:1: note: ***** Analysis failed with vector mode VOID
Dense.c:202:19: missed: couldn't vectorize loop
Dense.c:202:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:192:13: note: vectorized 0 loops in function.
Dense.c:196:35: missed: statement clobbers memory: _5 = aligned_alloc (64, _4);
Dense.c:197:37: missed: statement clobbers memory: _6 = aligned_alloc (64, _4);
Dense.c:198:35: missed: statement clobbers memory: _8 = aligned_alloc (64, _7);
Dense.c:199:34: missed: statement clobbers memory: _9 = aligned_alloc (64, _7);
Dense.c:200:35: missed: statement clobbers memory: _10 = aligned_alloc (64, _7);
Dense.c:203:38: missed: statement clobbers memory: _11 = rand ();
Dense.c:203:72: missed: statement clobbers memory: _56 = sqrt (_18);
Dense.c:215:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:216:4: missed: statement clobbers memory: exit (1);
Dense.c:209:12: missed: statement clobbers memory: layer = PULSE_CreateLayer (args$n_inputs_37, args$n_outputs_47, 0, _27, _FeedDense, _BackDense, _FixDense, _DestroyDense, 0); [return slot optimization]
Dense.c:222:9: missed: not vectorized: more than one data ref in stmt: <retval> = layer;
Dense.c:222:9: note: ***** Analysis succeeded with vector mode V8SI
Dense.c:222:9: note: SLPing BB part
Dense.c:222:9: note: Costing subgraph: 
Dense.c:222:9: note: node 0x7c80768 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:222:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = _5;
Dense.c:222:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = _5;
Dense.c:222:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&layer + 96B] = _8;
Dense.c:222:9: note: 	stmt 2 MEM <float *> [(struct PULSE_Layer *)&layer + 104B] = _9;
Dense.c:222:9: note: 	stmt 3 MEM <float *> [(struct PULSE_Layer *)&layer + 112B] = _10;
Dense.c:222:9: note: 	children 0x7c807e8
Dense.c:222:9: note: node (external) 0x7c807e8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:222:9: note: 	stmt 0 _5 = aligned_alloc (64, _4);
Dense.c:222:9: note: 	stmt 1 _8 = aligned_alloc (64, _7);
Dense.c:222:9: note: 	stmt 2 _9 = aligned_alloc (64, _7);
Dense.c:222:9: note: 	stmt 3 _10 = aligned_alloc (64, _7);
Dense.c:222:9: note: 	children 0x7c80868 0x7c80968
Dense.c:222:9: note: node (constant) 0x7c80868 (max_nunits=1, refcnt=1)
Dense.c:222:9: note: 	{ 64, 64, 64, 64 }
Dense.c:222:9: note: node (external) 0x7c80968 (max_nunits=1, refcnt=1)
Dense.c:222:9: note: 	{ _4, _7, _7, _7 }
Dense.c:222:9: note: Cost model analysis: 
Dense.c:222:9: note: Cost model analysis for part in loop 0:
  Vector cost: 64
  Scalar cost: 64
Dense.c:222:9: note: Basic block will be vectorized using SLP
Dense.c:222:9: optimized: basic block part vectorized using 32 byte vectors
Dense.c:222:9: note: Vectorizing SLP tree:
Dense.c:222:9: note: node 0x7c80768 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:222:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = _5;
Dense.c:222:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = _5;
Dense.c:222:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&layer + 96B] = _8;
Dense.c:222:9: note: 	stmt 2 MEM <float *> [(struct PULSE_Layer *)&layer + 104B] = _9;
Dense.c:222:9: note: 	stmt 3 MEM <float *> [(struct PULSE_Layer *)&layer + 112B] = _10;
Dense.c:222:9: note: 	children 0x7c807e8
Dense.c:222:9: note: node (external) 0x7c807e8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:222:9: note: 	stmt 0 _5 = aligned_alloc (64, _4);
Dense.c:222:9: note: 	stmt 1 _8 = aligned_alloc (64, _7);
Dense.c:222:9: note: 	stmt 2 _9 = aligned_alloc (64, _7);
Dense.c:222:9: note: 	stmt 3 _10 = aligned_alloc (64, _7);
Dense.c:222:9: note: 	children 0x7c80868 0x7c80968
Dense.c:222:9: note: node (constant) 0x7c80868 (max_nunits=1, refcnt=1)
Dense.c:222:9: note: 	{ 64, 64, 64, 64 }
Dense.c:222:9: note: node (external) 0x7c80968 (max_nunits=1, refcnt=1)
Dense.c:222:9: note: 	{ _4, _7, _7, _7 }
Dense.c:222:9: note: ------>vectorizing SLP node starting from: MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = _5;
Dense.c:222:9: note: vect_is_simple_use: operand aligned_alloc (64, _7), type of def: internal
Dense.c:222:9: note: vect_is_simple_use: operand aligned_alloc (64, _7), type of def: internal
Dense.c:222:9: note: vect_is_simple_use: operand aligned_alloc (64, _7), type of def: internal
Dense.c:222:9: note: transform store. ncopies = 1
Dense.c:222:9: note: create vector_type-pointer variable to type: vector(4) long unsigned int  vectorizing a pointer ref: MEM <float *> [(struct PULSE_Layer *)&layer + 88B]
Dense.c:222:9: note: created &MEM <float *> [(struct PULSE_Layer *)&layer + 88B]
Dense.c:222:9: note: add new stmt: MEM <vector(4) long unsigned int> [(struct PULSE_Layer *)&layer + 88B] = _66;
Dense.c:222:9: note: vectorizing stmts using SLP.
Dense.c:222:9: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _28 (pretmp_91, _51, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:19: missed: not vectorized: no vectype for stmt: _22 = *_21;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:188:1: note: ***** Analysis failed with vector mode VOID
Dense.c:170:10: optimized: loop vectorized using 32 byte vectors
Dense.c:170:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:170:10: optimized: loop vectorized using 16 byte vectors
Dense.c:161:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
Dense.c:154:10: optimized: loop vectorized using 32 byte vectors
Dense.c:154:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:154:10: optimized: loop vectorized using 16 byte vectors
Dense.c:145:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
Dense.c:136:13: note: vectorized 2 loops in function.
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_28, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
Dense.c:136:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:136:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:105:24: missed: couldn't vectorize loop
Dense.c:105:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:108:17: missed: couldn't vectorize loop
Dense.c:108:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:122:24: missed: couldn't vectorize loop
Dense.c:122:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:125:17: missed: couldn't vectorize loop
Dense.c:125:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:90:15: missed: couldn't vectorize loop
Dense.c:90:15: missed: not vectorized: number of iterations cannot be computed.
Dense.c:83:13: note: vectorized 0 loops in function.
Dense.c:86:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:110:17: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:110:17: note: SLPing BB part
Dense.c:127:17: note: Costing subgraph: 
Dense.c:127:17: note: node 0x7f40178 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:127:17: note: op template: _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 0 _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 1 _198 = (sizetype) wi_137;
Dense.c:127:17: note: 	children 0x7f40278
Dense.c:127:17: note: node (external) 0x7f40278 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:127:17: note: 	{ j_135, wi_137 }
Dense.c:127:17: note: Cost model analysis: 
Dense.c:127:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:127:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:127:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: Costing subgraph: 
Dense.c:110:17: note: node 0x7f40378 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:110:17: note: op template: _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 0 _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 1 _217 = (sizetype) wi_136;
Dense.c:110:17: note: 	children 0x7f40478
Dense.c:110:17: note: node (external) 0x7f40478 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:110:17: note: 	{ j_134, wi_136 }
Dense.c:110:17: note: Cost model analysis: 
Dense.c:110:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:110:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:110:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: ***** The result for vector mode V32QI would be the same
Dense.c:110:17: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:110:17: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:110:17: note: SLPing BB part
Dense.c:127:17: note: Costing subgraph: 
Dense.c:127:17: note: node 0x7f40378 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:127:17: note: op template: _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 0 _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 1 _198 = (sizetype) wi_137;
Dense.c:127:17: note: 	children 0x7f40178
Dense.c:127:17: note: node (external) 0x7f40178 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:127:17: note: 	{ j_135, wi_137 }
Dense.c:127:17: note: Cost model analysis: 
Dense.c:127:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:127:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:127:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: Costing subgraph: 
Dense.c:110:17: note: node 0x7f402f8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:110:17: note: op template: _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 0 _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 1 _217 = (sizetype) wi_136;
Dense.c:110:17: note: 	children 0x7f403f8
Dense.c:110:17: note: node (external) 0x7f403f8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:110:17: note: 	{ j_134, wi_136 }
Dense.c:110:17: note: Cost model analysis: 
Dense.c:110:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:110:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:110:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:105:51: note: ***** Analysis failed with vector mode V8QI
Dense.c:105:51: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:105:51: note: ***** Analysis failed with vector mode V4QI
Dense.c:62:15: missed: couldn't vectorize loop
Dense.c:62:15: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:75:11: optimized: loop vectorized using 32 byte vectors
Dense.c:75:11: optimized: loop vectorized using 16 byte vectors
Dense.c:66:11: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:881:10: missed: not vectorized: no vectype for stmt: _53 = MEM[(__m256 * {ref-all})w_ptr_82];
 scalar_type: __m256
Dense.c:54:13: note: vectorized 1 loops in function.
Dense.c:57:2: missed: statement clobbers memory: memcpy (_4, dense$baiases_63, _3);
Dense.c:79:2: missed: statement clobbers memory: _27 (pretmp_163, _68, 0);
Dense.c:54:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:54:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:202:19: missed: couldn't vectorize loop
Dense.c:202:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:192:13: note: vectorized 0 loops in function.
Dense.c:196:35: missed: statement clobbers memory: _5 = aligned_alloc (64, _4);
Dense.c:197:37: missed: statement clobbers memory: _6 = aligned_alloc (64, _4);
Dense.c:198:35: missed: statement clobbers memory: _8 = aligned_alloc (64, _7);
Dense.c:199:34: missed: statement clobbers memory: _9 = aligned_alloc (64, _7);
Dense.c:200:35: missed: statement clobbers memory: _10 = aligned_alloc (64, _7);
Dense.c:203:38: missed: statement clobbers memory: _11 = rand ();
Dense.c:203:72: missed: statement clobbers memory: _57 = sqrt (_18);
Dense.c:215:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:216:4: missed: statement clobbers memory: exit (1);
Dense.c:212:4: missed: statement clobbers memory: layer = PULSE_CreateLayer (args$n_inputs_36, args$n_outputs_47, 0, args$8_48, _SIMD_FeedDense, _SIMD_BackDense, _SIMD_FixDense, _DestroyDense, 1); [return slot optimization]
Dense.c:209:12: missed: statement clobbers memory: layer = PULSE_CreateLayer (args$n_inputs_36, args$n_outputs_47, 0, args$8_48, _FeedDense, _BackDense, _FixDense, _DestroyDense, 0); [return slot optimization]
Dense.c:222:9: missed: not vectorized: more than one data ref in stmt: <retval> = layer;
Dense.c:222:9: note: ***** Analysis succeeded with vector mode V8SI
Dense.c:222:9: note: SLPing BB part
Dense.c:222:9: note: Costing subgraph: 
Dense.c:222:9: note: node 0x7d83438 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:222:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = _5;
Dense.c:222:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = _5;
Dense.c:222:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&layer + 96B] = _8;
Dense.c:222:9: note: 	stmt 2 MEM <float *> [(struct PULSE_Layer *)&layer + 104B] = _9;
Dense.c:222:9: note: 	stmt 3 MEM <float *> [(struct PULSE_Layer *)&layer + 112B] = _10;
Dense.c:222:9: note: 	children 0x7d834b8
Dense.c:222:9: note: node (external) 0x7d834b8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:222:9: note: 	stmt 0 _5 = aligned_alloc (64, _4);
Dense.c:222:9: note: 	stmt 1 _8 = aligned_alloc (64, _7);
Dense.c:222:9: note: 	stmt 2 _9 = aligned_alloc (64, _7);
Dense.c:222:9: note: 	stmt 3 _10 = aligned_alloc (64, _7);
Dense.c:222:9: note: 	children 0x7d83538 0x7d83638
Dense.c:222:9: note: node (constant) 0x7d83538 (max_nunits=1, refcnt=1)
Dense.c:222:9: note: 	{ 64, 64, 64, 64 }
Dense.c:222:9: note: node (external) 0x7d83638 (max_nunits=1, refcnt=1)
Dense.c:222:9: note: 	{ _4, _7, _7, _7 }
Dense.c:222:9: note: Cost model analysis: 
Dense.c:222:9: note: Cost model analysis for part in loop 0:
  Vector cost: 64
  Scalar cost: 64
Dense.c:222:9: note: Basic block will be vectorized using SLP
Dense.c:222:9: optimized: basic block part vectorized using 32 byte vectors
Dense.c:222:9: note: Vectorizing SLP tree:
Dense.c:222:9: note: node 0x7d83438 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:222:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = _5;
Dense.c:222:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = _5;
Dense.c:222:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&layer + 96B] = _8;
Dense.c:222:9: note: 	stmt 2 MEM <float *> [(struct PULSE_Layer *)&layer + 104B] = _9;
Dense.c:222:9: note: 	stmt 3 MEM <float *> [(struct PULSE_Layer *)&layer + 112B] = _10;
Dense.c:222:9: note: 	children 0x7d834b8
Dense.c:222:9: note: node (external) 0x7d834b8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:222:9: note: 	stmt 0 _5 = aligned_alloc (64, _4);
Dense.c:222:9: note: 	stmt 1 _8 = aligned_alloc (64, _7);
Dense.c:222:9: note: 	stmt 2 _9 = aligned_alloc (64, _7);
Dense.c:222:9: note: 	stmt 3 _10 = aligned_alloc (64, _7);
Dense.c:222:9: note: 	children 0x7d83538 0x7d83638
Dense.c:222:9: note: node (constant) 0x7d83538 (max_nunits=1, refcnt=1)
Dense.c:222:9: note: 	{ 64, 64, 64, 64 }
Dense.c:222:9: note: node (external) 0x7d83638 (max_nunits=1, refcnt=1)
Dense.c:222:9: note: 	{ _4, _7, _7, _7 }
Dense.c:222:9: note: ------>vectorizing SLP node starting from: MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = _5;
Dense.c:222:9: note: vect_is_simple_use: operand aligned_alloc (64, _7), type of def: internal
Dense.c:222:9: note: vect_is_simple_use: operand aligned_alloc (64, _7), type of def: internal
Dense.c:222:9: note: vect_is_simple_use: operand aligned_alloc (64, _7), type of def: internal
Dense.c:222:9: note: transform store. ncopies = 1
Dense.c:222:9: note: create vector_type-pointer variable to type: vector(4) long unsigned int  vectorizing a pointer ref: MEM <float *> [(struct PULSE_Layer *)&layer + 88B]
Dense.c:222:9: note: created &MEM <float *> [(struct PULSE_Layer *)&layer + 88B]
Dense.c:222:9: note: add new stmt: MEM <vector(4) long unsigned int> [(struct PULSE_Layer *)&layer + 88B] = _55;
Dense.c:222:9: note: vectorizing stmts using SLP.
Dense.c:222:9: note: ***** The result for vector mode V32QI would be the same
Layer.c:7:34: missed: statement clobbers memory: _3 = aligned_alloc (64, _2);
Layer.c:8:35: missed: statement clobbers memory: _6 = aligned_alloc (64, _5);
Layer.c:9:34: missed: statement clobbers memory: _7 = aligned_alloc (64, _5);
Layer.c:14:19: missed: statement clobbers memory: _8 = PULSE_GetActivationFunctionPtr (activation_function_18(D));
Layer.c:19:9: note: ***** Analysis succeeded with vector mode V8SI
Layer.c:19:9: note: SLPing BB part
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x39423c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: op template: MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: 	stmt 0 MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: 	stmt 1 MEM <unsigned int> [(struct PULSE_Layer *)&<retval> + 4B] = optimization_type_17(D);
Layer.c:19:9: note: 	children 0x3942448
Layer.c:19:9: note: node (external) 0x3942448 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ type_16(D), optimization_type_17(D) }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x39424c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: op template: <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: 	stmt 0 <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: 	stmt 1 <retval>.n_outputs = n_outputs_12(D);
Layer.c:19:9: note: 	children 0x3942548
Layer.c:19:9: note: node (external) 0x3942548 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ n_inputs_11(D), n_outputs_12(D) }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x3942648 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: <retval>.inputs = _3;
Layer.c:19:9: note: 	stmt 0 <retval>.inputs = _3;
Layer.c:19:9: note: 	stmt 1 <retval>.outputs = _6;
Layer.c:19:9: note: 	children 0x39426c8
Layer.c:19:9: note: node (external) 0x39426c8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: 	stmt 0 _3 = aligned_alloc (64, _2);
Layer.c:19:9: note: 	stmt 1 _6 = aligned_alloc (64, _5);
Layer.c:19:9: note: 	children 0x3942748 0x39427c8
Layer.c:19:9: note: node (constant) 0x3942748 (max_nunits=1, refcnt=1)
Layer.c:19:9: note: 	{ 64, 64 }
Layer.c:19:9: note: node 0x39427c8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: patt_38 = n_inputs_11(D) w* 4;
Layer.c:19:9: note: 	stmt 0 patt_38 = n_inputs_11(D) w* 4;
Layer.c:19:9: note: 	stmt 1 patt_39 = n_outputs_12(D) w* 4;
Layer.c:19:9: note: 	children 0x3942848 0x39428c8
Layer.c:19:9: note: node (external) 0x3942848 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ n_inputs_11(D), n_outputs_12(D) }
Layer.c:19:9: note: node (constant) 0x39428c8 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ 4, 4 }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x39429c8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Layer.c:19:9: note: op template: <retval>.feed = feed_20(D);
Layer.c:19:9: note: 	stmt 0 <retval>.feed = feed_20(D);
Layer.c:19:9: note: 	stmt 1 <retval>.back = back_21(D);
Layer.c:19:9: note: 	stmt 2 <retval>.fix = fix_22(D);
Layer.c:19:9: note: 	stmt 3 <retval>.destroy = destroy_23(D);
Layer.c:19:9: note: 	children 0x3942a48
Layer.c:19:9: note: node (external) 0x3942a48 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Layer.c:19:9: note: 	{ feed_20(D), back_21(D), fix_22(D), destroy_23(D) }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 64
  Scalar cost: 64
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x3942ac8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: <retval>.parent = 0B;
Layer.c:19:9: note: 	stmt 0 <retval>.parent = 0B;
Layer.c:19:9: note: 	stmt 1 <retval>.child = 0B;
Layer.c:19:9: note: 	children 0x3942b48
Layer.c:19:9: note: node (constant) 0x3942b48 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: 	{ 0B, 0B }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Layer.c:19:9: note: Basic block will be vectorized using SLP
Layer.c:19:9: optimized: basic block part vectorized using 8 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x39423c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: op template: MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: 	stmt 0 MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: 	stmt 1 MEM <unsigned int> [(struct PULSE_Layer *)&<retval> + 4B] = optimization_type_17(D);
Layer.c:19:9: note: 	children 0x3942448
Layer.c:19:9: note: node (external) 0x3942448 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ type_16(D), optimization_type_17(D) }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: vect_is_simple_use: operand optimization_type_17(D), type of def: external
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: MEM <unsigned int> [(struct PULSE_Layer *)&<retval>]
Layer.c:19:9: note: created &MEM <unsigned int> [(struct PULSE_Layer *)&<retval>]
Layer.c:19:9: note: add new stmt: MEM <vector(2) unsigned int> [(struct PULSE_Layer *)&<retval>] = _40;
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: optimized: basic block part vectorized using 8 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x39424c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: op template: <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: 	stmt 0 <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: 	stmt 1 <retval>.n_outputs = n_outputs_12(D);
Layer.c:19:9: note: 	children 0x3942548
Layer.c:19:9: note: node (external) 0x3942548 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ n_inputs_11(D), n_outputs_12(D) }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: vect_is_simple_use: operand n_outputs_12(D), type of def: external
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: <retval>.n_inputs
Layer.c:19:9: note: created &<retval>.n_inputs
Layer.c:19:9: note: add new stmt: MEM <vector(2) unsigned int> [(unsigned int *)&<retval> + 128B] = _42;
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: optimized: basic block part vectorized using 16 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x3942648 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: <retval>.inputs = _3;
Layer.c:19:9: note: 	stmt 0 <retval>.inputs = _3;
Layer.c:19:9: note: 	stmt 1 <retval>.outputs = _6;
Layer.c:19:9: note: 	children 0x39426c8
Layer.c:19:9: note: node (external) 0x39426c8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: 	stmt 0 _3 = aligned_alloc (64, _2);
Layer.c:19:9: note: 	stmt 1 _6 = aligned_alloc (64, _5);
Layer.c:19:9: note: 	children 0x3942748 0x39427c8
Layer.c:19:9: note: node (constant) 0x3942748 (max_nunits=1, refcnt=1)
Layer.c:19:9: note: 	{ 64, 64 }
Layer.c:19:9: note: node 0x39427c8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: patt_38 = n_inputs_11(D) w* 4;
Layer.c:19:9: note: 	stmt 0 patt_38 = n_inputs_11(D) w* 4;
Layer.c:19:9: note: 	stmt 1 patt_39 = n_outputs_12(D) w* 4;
Layer.c:19:9: note: 	children 0x3942848 0x39428c8
Layer.c:19:9: note: node (external) 0x3942848 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ n_inputs_11(D), n_outputs_12(D) }
Layer.c:19:9: note: node (constant) 0x39428c8 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ 4, 4 }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: <retval>.inputs = _3;
Layer.c:19:9: note: vect_is_simple_use: operand aligned_alloc (64, _5), type of def: internal
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.inputs
Layer.c:19:9: note: created &<retval>.inputs
Layer.c:19:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval> + 8B] = _46;
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: optimized: basic block part vectorized using 32 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x39429c8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Layer.c:19:9: note: op template: <retval>.feed = feed_20(D);
Layer.c:19:9: note: 	stmt 0 <retval>.feed = feed_20(D);
Layer.c:19:9: note: 	stmt 1 <retval>.back = back_21(D);
Layer.c:19:9: note: 	stmt 2 <retval>.fix = fix_22(D);
Layer.c:19:9: note: 	stmt 3 <retval>.destroy = destroy_23(D);
Layer.c:19:9: note: 	children 0x3942a48
Layer.c:19:9: note: node (external) 0x3942a48 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Layer.c:19:9: note: 	{ feed_20(D), back_21(D), fix_22(D), destroy_23(D) }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: <retval>.feed = feed_20(D);
Layer.c:19:9: note: vect_is_simple_use: operand back_21(D), type of def: external
Layer.c:19:9: note: vect_is_simple_use: operand fix_22(D), type of def: external
Layer.c:19:9: note: vect_is_simple_use: operand destroy_23(D), type of def: external
Layer.c:19:9: note: conflicting alias set types.
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(4) long unsigned int  vectorizing a pointer ref: <retval>.feed
Layer.c:19:9: note: created &<retval>.feed
Layer.c:19:9: note: add new stmt: MEM <vector(4) long unsigned int> [(void *)&<retval> + 32B] = _52;
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: optimized: basic block part vectorized using 16 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x3942ac8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: <retval>.parent = 0B;
Layer.c:19:9: note: 	stmt 0 <retval>.parent = 0B;
Layer.c:19:9: note: 	stmt 1 <retval>.child = 0B;
Layer.c:19:9: note: 	children 0x3942b48
Layer.c:19:9: note: node (constant) 0x3942b48 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: 	{ 0B, 0B }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: <retval>.parent = 0B;
Layer.c:19:9: note: vect_is_simple_use: operand 0B, type of def: constant
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.parent
Layer.c:19:9: note: created &<retval>.parent
Layer.c:19:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer * *)&<retval> + 72B] = { 0, 0 };
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: note: ***** The result for vector mode V32QI would be the same
Layer.c:24:2: missed: statement clobbers memory: free (_1);
Layer.c:25:2: missed: statement clobbers memory: free (_2);
Layer.c:26:2: missed: statement clobbers memory: free (_3);
Layer.c:27:1: note: ***** Analysis failed with vector mode V4DI
Layer.c:27:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _28 (pretmp_91, _51, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:19: missed: not vectorized: no vectype for stmt: _22 = *_21;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:188:1: note: ***** Analysis failed with vector mode VOID
Dense.c:170:10: optimized: loop vectorized using 32 byte vectors
Dense.c:170:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:170:10: optimized: loop vectorized using 16 byte vectors
Dense.c:161:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
Dense.c:154:10: optimized: loop vectorized using 32 byte vectors
Dense.c:154:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:154:10: optimized: loop vectorized using 16 byte vectors
Dense.c:145:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
Dense.c:136:13: note: vectorized 2 loops in function.
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_28, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
Dense.c:136:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:136:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:105:24: missed: couldn't vectorize loop
Dense.c:105:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:108:17: missed: couldn't vectorize loop
Dense.c:108:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:122:24: missed: couldn't vectorize loop
Dense.c:122:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:125:17: missed: couldn't vectorize loop
Dense.c:125:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:90:15: missed: couldn't vectorize loop
Dense.c:90:15: missed: not vectorized: number of iterations cannot be computed.
Dense.c:83:13: note: vectorized 0 loops in function.
Dense.c:86:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:110:17: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:110:17: note: SLPing BB part
Dense.c:127:17: note: Costing subgraph: 
Dense.c:127:17: note: node 0x87fe178 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:127:17: note: op template: _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 0 _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 1 _198 = (sizetype) wi_137;
Dense.c:127:17: note: 	children 0x87fe278
Dense.c:127:17: note: node (external) 0x87fe278 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:127:17: note: 	{ j_135, wi_137 }
Dense.c:127:17: note: Cost model analysis: 
Dense.c:127:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:127:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:127:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: Costing subgraph: 
Dense.c:110:17: note: node 0x87fe378 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:110:17: note: op template: _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 0 _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 1 _217 = (sizetype) wi_136;
Dense.c:110:17: note: 	children 0x87fe478
Dense.c:110:17: note: node (external) 0x87fe478 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:110:17: note: 	{ j_134, wi_136 }
Dense.c:110:17: note: Cost model analysis: 
Dense.c:110:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:110:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:110:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: ***** The result for vector mode V32QI would be the same
Dense.c:110:17: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:110:17: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:110:17: note: SLPing BB part
Dense.c:127:17: note: Costing subgraph: 
Dense.c:127:17: note: node 0x87fe378 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:127:17: note: op template: _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 0 _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 1 _198 = (sizetype) wi_137;
Dense.c:127:17: note: 	children 0x87fe178
Dense.c:127:17: note: node (external) 0x87fe178 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:127:17: note: 	{ j_135, wi_137 }
Dense.c:127:17: note: Cost model analysis: 
Dense.c:127:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:127:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:127:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: Costing subgraph: 
Dense.c:110:17: note: node 0x87fe2f8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:110:17: note: op template: _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 0 _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 1 _217 = (sizetype) wi_136;
Dense.c:110:17: note: 	children 0x87fe3f8
Dense.c:110:17: note: node (external) 0x87fe3f8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:110:17: note: 	{ j_134, wi_136 }
Dense.c:110:17: note: Cost model analysis: 
Dense.c:110:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:110:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:110:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:105:51: note: ***** Analysis failed with vector mode V8QI
Dense.c:105:51: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:105:51: note: ***** Analysis failed with vector mode V4QI
Dense.c:62:15: missed: couldn't vectorize loop
Dense.c:62:15: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:75:11: optimized: loop vectorized using 32 byte vectors
Dense.c:75:11: optimized: loop vectorized using 16 byte vectors
Dense.c:66:11: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:881:10: missed: not vectorized: no vectype for stmt: _53 = MEM[(__m256 * {ref-all})w_ptr_82];
 scalar_type: __m256
Dense.c:54:13: note: vectorized 1 loops in function.
Dense.c:57:2: missed: statement clobbers memory: memcpy (_4, dense$baiases_63, _3);
Dense.c:79:2: missed: statement clobbers memory: _27 (pretmp_163, _68, 0);
Dense.c:54:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:54:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:202:19: missed: couldn't vectorize loop
Dense.c:202:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:192:13: note: vectorized 0 loops in function.
Dense.c:196:35: missed: statement clobbers memory: _5 = aligned_alloc (64, _4);
Dense.c:197:37: missed: statement clobbers memory: _6 = aligned_alloc (64, _4);
Dense.c:198:35: missed: statement clobbers memory: _8 = aligned_alloc (64, _7);
Dense.c:199:34: missed: statement clobbers memory: _9 = aligned_alloc (64, _7);
Dense.c:200:35: missed: statement clobbers memory: _10 = aligned_alloc (64, _7);
Dense.c:203:38: missed: statement clobbers memory: _11 = rand ();
Dense.c:203:72: missed: statement clobbers memory: _57 = sqrt (_18);
Dense.c:215:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:216:4: missed: statement clobbers memory: exit (1);
Dense.c:212:4: missed: statement clobbers memory: layer = PULSE_CreateLayer (args$n_inputs_36, args$n_outputs_47, 0, args$8_48, _SIMD_FeedDense, _SIMD_BackDense, _SIMD_FixDense, _DestroyDense, 1); [return slot optimization]
Dense.c:209:12: missed: statement clobbers memory: layer = PULSE_CreateLayer (args$n_inputs_36, args$n_outputs_47, 0, args$8_48, _FeedDense, _BackDense, _FixDense, _DestroyDense, 0); [return slot optimization]
Dense.c:222:9: missed: not vectorized: more than one data ref in stmt: <retval> = layer;
Dense.c:222:9: note: ***** Analysis succeeded with vector mode V8SI
Dense.c:222:9: note: SLPing BB part
Dense.c:222:9: note: Costing subgraph: 
Dense.c:222:9: note: node 0x86978a8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:222:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = _5;
Dense.c:222:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = _5;
Dense.c:222:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&layer + 96B] = _8;
Dense.c:222:9: note: 	stmt 2 MEM <float *> [(struct PULSE_Layer *)&layer + 104B] = _9;
Dense.c:222:9: note: 	stmt 3 MEM <float *> [(struct PULSE_Layer *)&layer + 112B] = _10;
Dense.c:222:9: note: 	children 0x8697928
Dense.c:222:9: note: node (external) 0x8697928 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:222:9: note: 	stmt 0 _5 = aligned_alloc (64, _4);
Dense.c:222:9: note: 	stmt 1 _8 = aligned_alloc (64, _7);
Dense.c:222:9: note: 	stmt 2 _9 = aligned_alloc (64, _7);
Dense.c:222:9: note: 	stmt 3 _10 = aligned_alloc (64, _7);
Dense.c:222:9: note: 	children 0x86979a8 0x8697aa8
Dense.c:222:9: note: node (constant) 0x86979a8 (max_nunits=1, refcnt=1)
Dense.c:222:9: note: 	{ 64, 64, 64, 64 }
Dense.c:222:9: note: node (external) 0x8697aa8 (max_nunits=1, refcnt=1)
Dense.c:222:9: note: 	{ _4, _7, _7, _7 }
Dense.c:222:9: note: Cost model analysis: 
Dense.c:222:9: note: Cost model analysis for part in loop 0:
  Vector cost: 64
  Scalar cost: 64
Dense.c:222:9: note: Basic block will be vectorized using SLP
Dense.c:222:9: optimized: basic block part vectorized using 32 byte vectors
Dense.c:222:9: note: Vectorizing SLP tree:
Dense.c:222:9: note: node 0x86978a8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:222:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = _5;
Dense.c:222:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = _5;
Dense.c:222:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&layer + 96B] = _8;
Dense.c:222:9: note: 	stmt 2 MEM <float *> [(struct PULSE_Layer *)&layer + 104B] = _9;
Dense.c:222:9: note: 	stmt 3 MEM <float *> [(struct PULSE_Layer *)&layer + 112B] = _10;
Dense.c:222:9: note: 	children 0x8697928
Dense.c:222:9: note: node (external) 0x8697928 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:222:9: note: 	stmt 0 _5 = aligned_alloc (64, _4);
Dense.c:222:9: note: 	stmt 1 _8 = aligned_alloc (64, _7);
Dense.c:222:9: note: 	stmt 2 _9 = aligned_alloc (64, _7);
Dense.c:222:9: note: 	stmt 3 _10 = aligned_alloc (64, _7);
Dense.c:222:9: note: 	children 0x86979a8 0x8697aa8
Dense.c:222:9: note: node (constant) 0x86979a8 (max_nunits=1, refcnt=1)
Dense.c:222:9: note: 	{ 64, 64, 64, 64 }
Dense.c:222:9: note: node (external) 0x8697aa8 (max_nunits=1, refcnt=1)
Dense.c:222:9: note: 	{ _4, _7, _7, _7 }
Dense.c:222:9: note: ------>vectorizing SLP node starting from: MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = _5;
Dense.c:222:9: note: vect_is_simple_use: operand aligned_alloc (64, _7), type of def: internal
Dense.c:222:9: note: vect_is_simple_use: operand aligned_alloc (64, _7), type of def: internal
Dense.c:222:9: note: vect_is_simple_use: operand aligned_alloc (64, _7), type of def: internal
Dense.c:222:9: note: transform store. ncopies = 1
Dense.c:222:9: note: create vector_type-pointer variable to type: vector(4) long unsigned int  vectorizing a pointer ref: MEM <float *> [(struct PULSE_Layer *)&layer + 88B]
Dense.c:222:9: note: created &MEM <float *> [(struct PULSE_Layer *)&layer + 88B]
Dense.c:222:9: note: add new stmt: MEM <vector(4) long unsigned int> [(struct PULSE_Layer *)&layer + 88B] = _55;
Dense.c:222:9: note: vectorizing stmts using SLP.
Dense.c:222:9: note: ***** The result for vector mode V32QI would be the same
Activations.c:17:19: optimized: loop vectorized using 32 byte vectors
Activations.c:17:19: optimized: loop vectorized using 16 byte vectors
Activations.c:17:19: optimized: loop vectorized using 32 byte vectors
Activations.c:17:19: optimized: loop vectorized using 16 byte vectors
Activations.c:14:13: note: vectorized 2 loops in function.
Activations.c:14:13: note: ***** Analysis failed with vector mode V8SF
Activations.c:14:13: note: ***** The result for vector mode V32QI would be the same
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V16QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V16QI
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V8QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V8QI
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V4QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V4QI
Activations.c:28:19: missed: couldn't vectorize loop
Activations.c:28:19: missed: not vectorized: unsupported control flow in loop.
Activations.c:28:19: optimized: loop vectorized using 32 byte vectors
Activations.c:28:19: optimized: loop vectorized using 16 byte vectors
Activations.c:25:13: note: vectorized 1 loops in function.
Activations.c:25:13: note: ***** Analysis failed with vector mode V8SF
Activations.c:25:13: note: ***** The result for vector mode V32QI would be the same
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V16QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V16QI
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V8QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V8QI
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V4QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V4QI
Activations.c:38:1: note: ***** Analysis failed with vector mode VOID
Activations.c:5:19: missed: couldn't vectorize loop
Activations.c:9:21: missed: statement clobbers memory: _36 = expf (_18);
Activations.c:5:19: missed: couldn't vectorize loop
Activations.c:9:21: missed: statement clobbers memory: _12 = expf (_11);
Activations.c:3:13: note: vectorized 0 loops in function.
Activations.c:9:21: missed: statement clobbers memory: _12 = expf (_11);
Activations.c:9:21: missed: statement clobbers memory: _36 = expf (_18);
Activations.c:11:1: note: ***** Analysis failed with vector mode V8SF
Activations.c:11:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SF
Activations.c:44:2: note: ***** Analysis failed with vector mode VOID
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:4:18: note: vectorized 0 loops in function.
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_4, inputs_15, _3);
PULSE.c:8:2: missed: statement clobbers memory: _5 (layer_17);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_12, pretmp_26, _9);
PULSE.c:13:28: note: ***** Analysis failed with vector mode V8SI
PULSE.c:13:28: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:22:2: missed: statement clobbers memory: _1 (layer_10(D));
PULSE.c:22:2: missed: statement clobbers memory: _14 (_2);
PULSE.c:22:2: missed: statement clobbers memory: _21 (_15);
PULSE.c:22:2: missed: statement clobbers memory: _28 (_22);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_29);
PULSE.c:26:3: missed: statement clobbers memory: memset (_34, 0, _32);
PULSE.c:26:3: missed: statement clobbers memory: memset (_27, 0, _25);
PULSE.c:26:3: missed: statement clobbers memory: memset (_20, 0, _18);
PULSE.c:26:3: missed: statement clobbers memory: memset (_7, 0, _5);
PULSE.c:28:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:28:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:32:6: note: vectorized 1 loops in function.
PULSE.c:34:8: missed: statement clobbers memory: _1 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_2);
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _9 = rand ();
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8SI
PULSE.c:32:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:61:16: missed: couldn't vectorize loop
PULSE.c:61:16: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:64:17: missed: couldn't vectorize loop
PULSE.c:64:17: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:73:19: missed: couldn't vectorize loop
PULSE.c:73:19: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _69 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:53:22: missed: couldn't vectorize loop
PULSE.c:53:22: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:47:6: note: vectorized 1 loops in function.
PULSE.c:48:1: missed: statement clobbers memory: saved_stack.7_39 = __builtin_stack_save ();
PULSE.c:49:46: missed: statement clobbers memory: _1 = PULSE_GetLossFunctionPtr (loss_function_40(D));
PULSE.c:60:10: missed: statement clobbers memory: random.4_46 = __builtin_alloca_with_align (_4, 32);
PULSE.c:34:8: missed: statement clobbers memory: _61 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_62);
PULSE.c:39:29: missed: statement clobbers memory: _69 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _72 = rand ();
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_98, inputs_93, _97);
PULSE.c:8:2: missed: statement clobbers memory: _99 (layer_94);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_105, _104, _103);
PULSE.c:67:11: missed: statement clobbers memory: loss_55 = PULSE_GetLoss_42 (_17, _16, _12, _11);
PULSE.c:22:2: missed: statement clobbers memory: _86 (output_34);
PULSE.c:22:2: missed: statement clobbers memory: _108 (_87);
PULSE.c:22:2: missed: statement clobbers memory: _115 (_109);
PULSE.c:22:2: missed: statement clobbers memory: _122 (_116);
PULSE.c:22:2: missed: statement clobbers memory: _129 (_123);
PULSE.c:22:2: missed: statement clobbers memory: _136 (_130);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_137);
PULSE.c:26:3: missed: statement clobbers memory: memset (_142, 0, _140);
PULSE.c:26:3: missed: statement clobbers memory: memset (_135, 0, _133);
PULSE.c:26:3: missed: statement clobbers memory: memset (_128, 0, _126);
PULSE.c:26:3: missed: statement clobbers memory: memset (_121, 0, _119);
PULSE.c:26:3: missed: statement clobbers memory: memset (_114, 0, _112);
PULSE.c:26:3: missed: statement clobbers memory: memset (_92, 0, _90);
PULSE.c:75:6: missed: statement clobbers memory: _21 (current_191, args);
PULSE.c:81:4: missed: statement clobbers memory: printf ("Epoch: %d | Item: %d | Loss: %.10f | Batch Loss: %.10f\r", i_188, j_189, _25, _24);
PULSE.c:47:6: missed: statement clobbers memory: __builtin_stack_restore (saved_stack.7_39);
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4DI
PULSE.c:47:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:91:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:91:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:98:20: missed: couldn't vectorize loop
PULSE.c:98:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:98:20: missed: couldn't vectorize loop
PULSE.c:98:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:93:6: note: vectorized 0 loops in function.
PULSE.c:97:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:104:62: missed: not vectorized: more than one data ref in stmt: D.6452 = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.104_29];
PULSE.c:104:17: missed: statement clobbers memory: *_3 = PULSE_CreateDenseLayer (D.6452);
PULSE.c:104:62: missed: not vectorized: more than one data ref in stmt: D.6452 = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.104_47];
PULSE.c:104:17: missed: statement clobbers memory: *_32 = PULSE_CreateDenseLayer (D.6452);
PULSE.c:110:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:111:1: note: ***** Analysis failed with vector mode V8SI
PULSE.c:111:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:116:8: missed: couldn't vectorize loop
PULSE.c:116:8: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:113:6: note: vectorized 0 loops in function.
PULSE.c:118:3: missed: statement clobbers memory: _1 (current_11);
PULSE.c:121:2: missed: statement clobbers memory: free (layer_4(D));
PULSE.c:122:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:122:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Loss.c:15:19: optimized: loop vectorized using 32 byte vectors
Loss.c:15:19: optimized:  loop versioned for vectorization because of possible aliasing
Loss.c:15:19: optimized: loop vectorized using 16 byte vectors
Loss.c:12:23: note: vectorized 1 loops in function.
Loss.c:12:23: note: ***** Analysis failed with vector mode V8SF
Loss.c:12:23: note: ***** The result for vector mode V32QI would be the same
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V16QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V16QI
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V8QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V8QI
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V4QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V4QI
Loss.c:7:19: missed: couldn't vectorize loop
Loss.c:8:38: missed: not vectorized: unsupported use in stmt.
Loss.c:4:23: note: vectorized 0 loops in function.
Loss.c:9:13: note: ***** Analysis failed with vector mode V8SF
Loss.c:9:13: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SF
Loss.c:23:2: note: ***** Analysis failed with vector mode VOID
Layer.c:7:34: missed: statement clobbers memory: _3 = aligned_alloc (64, _2);
Layer.c:8:35: missed: statement clobbers memory: _6 = aligned_alloc (64, _5);
Layer.c:9:34: missed: statement clobbers memory: _7 = aligned_alloc (64, _5);
Layer.c:14:19: missed: statement clobbers memory: _8 = PULSE_GetActivationFunctionPtr (activation_function_18(D));
Layer.c:19:9: note: ***** Analysis succeeded with vector mode V8SI
Layer.c:19:9: note: SLPing BB part
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x3c228b8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: op template: MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: 	stmt 0 MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: 	stmt 1 MEM <unsigned int> [(struct PULSE_Layer *)&<retval> + 4B] = optimization_type_17(D);
Layer.c:19:9: note: 	children 0x3c22938
Layer.c:19:9: note: node (external) 0x3c22938 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ type_16(D), optimization_type_17(D) }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x3c229b8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: op template: <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: 	stmt 0 <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: 	stmt 1 <retval>.n_outputs = n_outputs_12(D);
Layer.c:19:9: note: 	children 0x3c22a38
Layer.c:19:9: note: node (external) 0x3c22a38 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ n_inputs_11(D), n_outputs_12(D) }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x3c22b38 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: <retval>.inputs = _3;
Layer.c:19:9: note: 	stmt 0 <retval>.inputs = _3;
Layer.c:19:9: note: 	stmt 1 <retval>.outputs = _6;
Layer.c:19:9: note: 	children 0x3c22bb8
Layer.c:19:9: note: node (external) 0x3c22bb8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: 	stmt 0 _3 = aligned_alloc (64, _2);
Layer.c:19:9: note: 	stmt 1 _6 = aligned_alloc (64, _5);
Layer.c:19:9: note: 	children 0x3c22c38 0x3c22cb8
Layer.c:19:9: note: node (constant) 0x3c22c38 (max_nunits=1, refcnt=1)
Layer.c:19:9: note: 	{ 64, 64 }
Layer.c:19:9: note: node 0x3c22cb8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: patt_38 = n_inputs_11(D) w* 4;
Layer.c:19:9: note: 	stmt 0 patt_38 = n_inputs_11(D) w* 4;
Layer.c:19:9: note: 	stmt 1 patt_39 = n_outputs_12(D) w* 4;
Layer.c:19:9: note: 	children 0x3c22d38 0x3c22db8
Layer.c:19:9: note: node (external) 0x3c22d38 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ n_inputs_11(D), n_outputs_12(D) }
Layer.c:19:9: note: node (constant) 0x3c22db8 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ 4, 4 }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x3c22eb8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Layer.c:19:9: note: op template: <retval>.feed = feed_20(D);
Layer.c:19:9: note: 	stmt 0 <retval>.feed = feed_20(D);
Layer.c:19:9: note: 	stmt 1 <retval>.back = back_21(D);
Layer.c:19:9: note: 	stmt 2 <retval>.fix = fix_22(D);
Layer.c:19:9: note: 	stmt 3 <retval>.destroy = destroy_23(D);
Layer.c:19:9: note: 	children 0x3c22f38
Layer.c:19:9: note: node (external) 0x3c22f38 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Layer.c:19:9: note: 	{ feed_20(D), back_21(D), fix_22(D), destroy_23(D) }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 64
  Scalar cost: 64
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x3c22fb8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: <retval>.parent = 0B;
Layer.c:19:9: note: 	stmt 0 <retval>.parent = 0B;
Layer.c:19:9: note: 	stmt 1 <retval>.child = 0B;
Layer.c:19:9: note: 	children 0x3c23038
Layer.c:19:9: note: node (constant) 0x3c23038 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: 	{ 0B, 0B }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Layer.c:19:9: note: Basic block will be vectorized using SLP
Layer.c:19:9: optimized: basic block part vectorized using 8 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x3c228b8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: op template: MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: 	stmt 0 MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: 	stmt 1 MEM <unsigned int> [(struct PULSE_Layer *)&<retval> + 4B] = optimization_type_17(D);
Layer.c:19:9: note: 	children 0x3c22938
Layer.c:19:9: note: node (external) 0x3c22938 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ type_16(D), optimization_type_17(D) }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: vect_is_simple_use: operand optimization_type_17(D), type of def: external
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: MEM <unsigned int> [(struct PULSE_Layer *)&<retval>]
Layer.c:19:9: note: created &MEM <unsigned int> [(struct PULSE_Layer *)&<retval>]
Layer.c:19:9: note: add new stmt: MEM <vector(2) unsigned int> [(struct PULSE_Layer *)&<retval>] = _40;
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: optimized: basic block part vectorized using 8 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x3c229b8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: op template: <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: 	stmt 0 <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: 	stmt 1 <retval>.n_outputs = n_outputs_12(D);
Layer.c:19:9: note: 	children 0x3c22a38
Layer.c:19:9: note: node (external) 0x3c22a38 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ n_inputs_11(D), n_outputs_12(D) }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: vect_is_simple_use: operand n_outputs_12(D), type of def: external
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: <retval>.n_inputs
Layer.c:19:9: note: created &<retval>.n_inputs
Layer.c:19:9: note: add new stmt: MEM <vector(2) unsigned int> [(unsigned int *)&<retval> + 128B] = _42;
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: optimized: basic block part vectorized using 16 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x3c22b38 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: <retval>.inputs = _3;
Layer.c:19:9: note: 	stmt 0 <retval>.inputs = _3;
Layer.c:19:9: note: 	stmt 1 <retval>.outputs = _6;
Layer.c:19:9: note: 	children 0x3c22bb8
Layer.c:19:9: note: node (external) 0x3c22bb8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: 	stmt 0 _3 = aligned_alloc (64, _2);
Layer.c:19:9: note: 	stmt 1 _6 = aligned_alloc (64, _5);
Layer.c:19:9: note: 	children 0x3c22c38 0x3c22cb8
Layer.c:19:9: note: node (constant) 0x3c22c38 (max_nunits=1, refcnt=1)
Layer.c:19:9: note: 	{ 64, 64 }
Layer.c:19:9: note: node 0x3c22cb8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: patt_38 = n_inputs_11(D) w* 4;
Layer.c:19:9: note: 	stmt 0 patt_38 = n_inputs_11(D) w* 4;
Layer.c:19:9: note: 	stmt 1 patt_39 = n_outputs_12(D) w* 4;
Layer.c:19:9: note: 	children 0x3c22d38 0x3c22db8
Layer.c:19:9: note: node (external) 0x3c22d38 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ n_inputs_11(D), n_outputs_12(D) }
Layer.c:19:9: note: node (constant) 0x3c22db8 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ 4, 4 }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: <retval>.inputs = _3;
Layer.c:19:9: note: vect_is_simple_use: operand aligned_alloc (64, _5), type of def: internal
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.inputs
Layer.c:19:9: note: created &<retval>.inputs
Layer.c:19:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval> + 8B] = _46;
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: optimized: basic block part vectorized using 32 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x3c22eb8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Layer.c:19:9: note: op template: <retval>.feed = feed_20(D);
Layer.c:19:9: note: 	stmt 0 <retval>.feed = feed_20(D);
Layer.c:19:9: note: 	stmt 1 <retval>.back = back_21(D);
Layer.c:19:9: note: 	stmt 2 <retval>.fix = fix_22(D);
Layer.c:19:9: note: 	stmt 3 <retval>.destroy = destroy_23(D);
Layer.c:19:9: note: 	children 0x3c22f38
Layer.c:19:9: note: node (external) 0x3c22f38 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Layer.c:19:9: note: 	{ feed_20(D), back_21(D), fix_22(D), destroy_23(D) }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: <retval>.feed = feed_20(D);
Layer.c:19:9: note: vect_is_simple_use: operand back_21(D), type of def: external
Layer.c:19:9: note: vect_is_simple_use: operand fix_22(D), type of def: external
Layer.c:19:9: note: vect_is_simple_use: operand destroy_23(D), type of def: external
Layer.c:19:9: note: conflicting alias set types.
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(4) long unsigned int  vectorizing a pointer ref: <retval>.feed
Layer.c:19:9: note: created &<retval>.feed
Layer.c:19:9: note: add new stmt: MEM <vector(4) long unsigned int> [(void *)&<retval> + 32B] = _52;
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: optimized: basic block part vectorized using 16 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x3c22fb8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: <retval>.parent = 0B;
Layer.c:19:9: note: 	stmt 0 <retval>.parent = 0B;
Layer.c:19:9: note: 	stmt 1 <retval>.child = 0B;
Layer.c:19:9: note: 	children 0x3c23038
Layer.c:19:9: note: node (constant) 0x3c23038 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: 	{ 0B, 0B }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: <retval>.parent = 0B;
Layer.c:19:9: note: vect_is_simple_use: operand 0B, type of def: constant
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.parent
Layer.c:19:9: note: created &<retval>.parent
Layer.c:19:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer * *)&<retval> + 72B] = { 0, 0 };
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: note: ***** The result for vector mode V32QI would be the same
Layer.c:24:2: missed: statement clobbers memory: free (_1);
Layer.c:25:2: missed: statement clobbers memory: free (_2);
Layer.c:26:2: missed: statement clobbers memory: free (_3);
Layer.c:27:1: note: ***** Analysis failed with vector mode V4DI
Layer.c:27:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _28 (pretmp_91, _51, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:19: missed: not vectorized: no vectype for stmt: _22 = *_21;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:188:1: note: ***** Analysis failed with vector mode VOID
Dense.c:170:10: optimized: loop vectorized using 32 byte vectors
Dense.c:170:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:170:10: optimized: loop vectorized using 16 byte vectors
Dense.c:161:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
Dense.c:154:10: optimized: loop vectorized using 32 byte vectors
Dense.c:154:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:154:10: optimized: loop vectorized using 16 byte vectors
Dense.c:145:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
Dense.c:136:13: note: vectorized 2 loops in function.
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_28, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
Dense.c:136:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:136:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:105:24: missed: couldn't vectorize loop
Dense.c:105:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:108:17: missed: couldn't vectorize loop
Dense.c:108:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:122:24: missed: couldn't vectorize loop
Dense.c:122:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:125:17: missed: couldn't vectorize loop
Dense.c:125:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:90:15: missed: couldn't vectorize loop
Dense.c:90:15: missed: not vectorized: number of iterations cannot be computed.
Dense.c:83:13: note: vectorized 0 loops in function.
Dense.c:86:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:110:17: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:110:17: note: SLPing BB part
Dense.c:127:17: note: Costing subgraph: 
Dense.c:127:17: note: node 0x79f7178 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:127:17: note: op template: _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 0 _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 1 _198 = (sizetype) wi_137;
Dense.c:127:17: note: 	children 0x79f7278
Dense.c:127:17: note: node (external) 0x79f7278 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:127:17: note: 	{ j_135, wi_137 }
Dense.c:127:17: note: Cost model analysis: 
Dense.c:127:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:127:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:127:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: Costing subgraph: 
Dense.c:110:17: note: node 0x79f7378 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:110:17: note: op template: _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 0 _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 1 _217 = (sizetype) wi_136;
Dense.c:110:17: note: 	children 0x79f7478
Dense.c:110:17: note: node (external) 0x79f7478 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:110:17: note: 	{ j_134, wi_136 }
Dense.c:110:17: note: Cost model analysis: 
Dense.c:110:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:110:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:110:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: ***** The result for vector mode V32QI would be the same
Dense.c:110:17: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:110:17: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:110:17: note: SLPing BB part
Dense.c:127:17: note: Costing subgraph: 
Dense.c:127:17: note: node 0x79f7378 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:127:17: note: op template: _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 0 _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 1 _198 = (sizetype) wi_137;
Dense.c:127:17: note: 	children 0x79f7178
Dense.c:127:17: note: node (external) 0x79f7178 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:127:17: note: 	{ j_135, wi_137 }
Dense.c:127:17: note: Cost model analysis: 
Dense.c:127:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:127:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:127:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: Costing subgraph: 
Dense.c:110:17: note: node 0x79f72f8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:110:17: note: op template: _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 0 _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 1 _217 = (sizetype) wi_136;
Dense.c:110:17: note: 	children 0x79f73f8
Dense.c:110:17: note: node (external) 0x79f73f8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:110:17: note: 	{ j_134, wi_136 }
Dense.c:110:17: note: Cost model analysis: 
Dense.c:110:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:110:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:110:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:105:51: note: ***** Analysis failed with vector mode V8QI
Dense.c:105:51: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:105:51: note: ***** Analysis failed with vector mode V4QI
Dense.c:62:15: missed: couldn't vectorize loop
Dense.c:62:15: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:75:11: optimized: loop vectorized using 32 byte vectors
Dense.c:75:11: optimized: loop vectorized using 16 byte vectors
Dense.c:66:11: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:881:10: missed: not vectorized: no vectype for stmt: _53 = MEM[(__m256 * {ref-all})w_ptr_82];
 scalar_type: __m256
Dense.c:54:13: note: vectorized 1 loops in function.
Dense.c:57:2: missed: statement clobbers memory: memcpy (_4, dense$baiases_63, _3);
Dense.c:79:2: missed: statement clobbers memory: _27 (pretmp_163, _68, 0);
Dense.c:54:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:54:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:202:19: missed: couldn't vectorize loop
Dense.c:202:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:192:13: note: vectorized 0 loops in function.
Dense.c:196:35: missed: statement clobbers memory: _5 = aligned_alloc (64, _4);
Dense.c:197:37: missed: statement clobbers memory: _6 = aligned_alloc (64, _4);
Dense.c:198:35: missed: statement clobbers memory: _8 = aligned_alloc (64, _7);
Dense.c:199:34: missed: statement clobbers memory: _9 = aligned_alloc (64, _7);
Dense.c:200:35: missed: statement clobbers memory: _10 = aligned_alloc (64, _7);
Dense.c:203:38: missed: statement clobbers memory: _11 = rand ();
Dense.c:203:72: missed: statement clobbers memory: _57 = sqrt (_18);
Dense.c:215:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:216:4: missed: statement clobbers memory: exit (1);
Dense.c:212:4: missed: statement clobbers memory: layer = PULSE_CreateLayer (args$n_inputs_36, args$n_outputs_47, 0, args$8_48, _SIMD_FeedDense, _SIMD_BackDense, _SIMD_FixDense, _DestroyDense, 1); [return slot optimization]
Dense.c:209:12: missed: statement clobbers memory: layer = PULSE_CreateLayer (args$n_inputs_36, args$n_outputs_47, 0, args$8_48, _FeedDense, _BackDense, _FixDense, _DestroyDense, 0); [return slot optimization]
Dense.c:222:9: missed: not vectorized: more than one data ref in stmt: <retval> = layer;
Dense.c:222:9: note: ***** Analysis succeeded with vector mode V8SI
Dense.c:222:9: note: SLPing BB part
Dense.c:222:9: note: Costing subgraph: 
Dense.c:222:9: note: node 0x783a438 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:222:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = _5;
Dense.c:222:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = _5;
Dense.c:222:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&layer + 96B] = _8;
Dense.c:222:9: note: 	stmt 2 MEM <float *> [(struct PULSE_Layer *)&layer + 104B] = _9;
Dense.c:222:9: note: 	stmt 3 MEM <float *> [(struct PULSE_Layer *)&layer + 112B] = _10;
Dense.c:222:9: note: 	children 0x783a4b8
Dense.c:222:9: note: node (external) 0x783a4b8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:222:9: note: 	stmt 0 _5 = aligned_alloc (64, _4);
Dense.c:222:9: note: 	stmt 1 _8 = aligned_alloc (64, _7);
Dense.c:222:9: note: 	stmt 2 _9 = aligned_alloc (64, _7);
Dense.c:222:9: note: 	stmt 3 _10 = aligned_alloc (64, _7);
Dense.c:222:9: note: 	children 0x783a538 0x783a638
Dense.c:222:9: note: node (constant) 0x783a538 (max_nunits=1, refcnt=1)
Dense.c:222:9: note: 	{ 64, 64, 64, 64 }
Dense.c:222:9: note: node (external) 0x783a638 (max_nunits=1, refcnt=1)
Dense.c:222:9: note: 	{ _4, _7, _7, _7 }
Dense.c:222:9: note: Cost model analysis: 
Dense.c:222:9: note: Cost model analysis for part in loop 0:
  Vector cost: 64
  Scalar cost: 64
Dense.c:222:9: note: Basic block will be vectorized using SLP
Dense.c:222:9: optimized: basic block part vectorized using 32 byte vectors
Dense.c:222:9: note: Vectorizing SLP tree:
Dense.c:222:9: note: node 0x783a438 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:222:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = _5;
Dense.c:222:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = _5;
Dense.c:222:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&layer + 96B] = _8;
Dense.c:222:9: note: 	stmt 2 MEM <float *> [(struct PULSE_Layer *)&layer + 104B] = _9;
Dense.c:222:9: note: 	stmt 3 MEM <float *> [(struct PULSE_Layer *)&layer + 112B] = _10;
Dense.c:222:9: note: 	children 0x783a4b8
Dense.c:222:9: note: node (external) 0x783a4b8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:222:9: note: 	stmt 0 _5 = aligned_alloc (64, _4);
Dense.c:222:9: note: 	stmt 1 _8 = aligned_alloc (64, _7);
Dense.c:222:9: note: 	stmt 2 _9 = aligned_alloc (64, _7);
Dense.c:222:9: note: 	stmt 3 _10 = aligned_alloc (64, _7);
Dense.c:222:9: note: 	children 0x783a538 0x783a638
Dense.c:222:9: note: node (constant) 0x783a538 (max_nunits=1, refcnt=1)
Dense.c:222:9: note: 	{ 64, 64, 64, 64 }
Dense.c:222:9: note: node (external) 0x783a638 (max_nunits=1, refcnt=1)
Dense.c:222:9: note: 	{ _4, _7, _7, _7 }
Dense.c:222:9: note: ------>vectorizing SLP node starting from: MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = _5;
Dense.c:222:9: note: vect_is_simple_use: operand aligned_alloc (64, _7), type of def: internal
Dense.c:222:9: note: vect_is_simple_use: operand aligned_alloc (64, _7), type of def: internal
Dense.c:222:9: note: vect_is_simple_use: operand aligned_alloc (64, _7), type of def: internal
Dense.c:222:9: note: transform store. ncopies = 1
Dense.c:222:9: note: create vector_type-pointer variable to type: vector(4) long unsigned int  vectorizing a pointer ref: MEM <float *> [(struct PULSE_Layer *)&layer + 88B]
Dense.c:222:9: note: created &MEM <float *> [(struct PULSE_Layer *)&layer + 88B]
Dense.c:222:9: note: add new stmt: MEM <vector(4) long unsigned int> [(struct PULSE_Layer *)&layer + 88B] = _55;
Dense.c:222:9: note: vectorizing stmts using SLP.
Dense.c:222:9: note: ***** The result for vector mode V32QI would be the same
Activations.c:17:19: optimized: loop vectorized using 32 byte vectors
Activations.c:17:19: optimized: loop vectorized using 16 byte vectors
Activations.c:17:19: optimized: loop vectorized using 32 byte vectors
Activations.c:17:19: optimized: loop vectorized using 16 byte vectors
Activations.c:14:13: note: vectorized 2 loops in function.
Activations.c:14:13: note: ***** Analysis failed with vector mode V8SF
Activations.c:14:13: note: ***** The result for vector mode V32QI would be the same
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V16QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V16QI
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V8QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V8QI
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V4QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V4QI
Activations.c:28:19: missed: couldn't vectorize loop
Activations.c:28:19: missed: not vectorized: unsupported control flow in loop.
Activations.c:28:19: optimized: loop vectorized using 32 byte vectors
Activations.c:28:19: optimized: loop vectorized using 16 byte vectors
Activations.c:25:13: note: vectorized 1 loops in function.
Activations.c:25:13: note: ***** Analysis failed with vector mode V8SF
Activations.c:25:13: note: ***** The result for vector mode V32QI would be the same
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V16QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V16QI
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V8QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V8QI
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V4QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V4QI
Activations.c:38:1: note: ***** Analysis failed with vector mode VOID
Activations.c:5:19: missed: couldn't vectorize loop
Activations.c:9:21: missed: statement clobbers memory: _36 = expf (_18);
Activations.c:5:19: missed: couldn't vectorize loop
Activations.c:9:21: missed: statement clobbers memory: _12 = expf (_11);
Activations.c:3:13: note: vectorized 0 loops in function.
Activations.c:9:21: missed: statement clobbers memory: _12 = expf (_11);
Activations.c:9:21: missed: statement clobbers memory: _36 = expf (_18);
Activations.c:11:1: note: ***** Analysis failed with vector mode V8SF
Activations.c:11:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SF
Activations.c:44:2: note: ***** Analysis failed with vector mode VOID
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:4:18: note: vectorized 0 loops in function.
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_4, inputs_15, _3);
PULSE.c:8:2: missed: statement clobbers memory: _5 (layer_17);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_12, pretmp_26, _9);
PULSE.c:13:28: note: ***** Analysis failed with vector mode V8SI
PULSE.c:13:28: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:22:2: missed: statement clobbers memory: _1 (layer_10(D));
PULSE.c:22:2: missed: statement clobbers memory: _14 (_2);
PULSE.c:22:2: missed: statement clobbers memory: _21 (_15);
PULSE.c:22:2: missed: statement clobbers memory: _28 (_22);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_29);
PULSE.c:26:3: missed: statement clobbers memory: memset (_34, 0, _32);
PULSE.c:26:3: missed: statement clobbers memory: memset (_27, 0, _25);
PULSE.c:26:3: missed: statement clobbers memory: memset (_20, 0, _18);
PULSE.c:26:3: missed: statement clobbers memory: memset (_7, 0, _5);
PULSE.c:28:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:28:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:32:6: note: vectorized 1 loops in function.
PULSE.c:34:8: missed: statement clobbers memory: _1 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_2);
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _9 = rand ();
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8SI
PULSE.c:32:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:61:16: missed: couldn't vectorize loop
PULSE.c:61:16: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:64:17: missed: couldn't vectorize loop
PULSE.c:64:17: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:73:19: missed: couldn't vectorize loop
PULSE.c:73:19: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _69 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:53:22: missed: couldn't vectorize loop
PULSE.c:53:22: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:47:6: note: vectorized 1 loops in function.
PULSE.c:48:1: missed: statement clobbers memory: saved_stack.7_39 = __builtin_stack_save ();
PULSE.c:49:46: missed: statement clobbers memory: _1 = PULSE_GetLossFunctionPtr (loss_function_40(D));
PULSE.c:60:10: missed: statement clobbers memory: random.4_46 = __builtin_alloca_with_align (_4, 32);
PULSE.c:34:8: missed: statement clobbers memory: _61 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_62);
PULSE.c:39:29: missed: statement clobbers memory: _69 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _72 = rand ();
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_98, inputs_93, _97);
PULSE.c:8:2: missed: statement clobbers memory: _99 (layer_94);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_105, _104, _103);
PULSE.c:67:11: missed: statement clobbers memory: loss_55 = PULSE_GetLoss_42 (_17, _16, _12, _11);
PULSE.c:22:2: missed: statement clobbers memory: _86 (output_34);
PULSE.c:22:2: missed: statement clobbers memory: _108 (_87);
PULSE.c:22:2: missed: statement clobbers memory: _115 (_109);
PULSE.c:22:2: missed: statement clobbers memory: _122 (_116);
PULSE.c:22:2: missed: statement clobbers memory: _129 (_123);
PULSE.c:22:2: missed: statement clobbers memory: _136 (_130);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_137);
PULSE.c:26:3: missed: statement clobbers memory: memset (_142, 0, _140);
PULSE.c:26:3: missed: statement clobbers memory: memset (_135, 0, _133);
PULSE.c:26:3: missed: statement clobbers memory: memset (_128, 0, _126);
PULSE.c:26:3: missed: statement clobbers memory: memset (_121, 0, _119);
PULSE.c:26:3: missed: statement clobbers memory: memset (_114, 0, _112);
PULSE.c:26:3: missed: statement clobbers memory: memset (_92, 0, _90);
PULSE.c:75:6: missed: statement clobbers memory: _21 (current_191, args);
PULSE.c:81:4: missed: statement clobbers memory: printf ("Epoch: %d | Item: %d | Loss: %.10f | Batch Loss: %.10f\r", i_188, j_189, _25, _24);
PULSE.c:47:6: missed: statement clobbers memory: __builtin_stack_restore (saved_stack.7_39);
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4DI
PULSE.c:47:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:91:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:91:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:98:20: missed: couldn't vectorize loop
PULSE.c:98:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:98:20: missed: couldn't vectorize loop
PULSE.c:98:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:93:6: note: vectorized 0 loops in function.
PULSE.c:97:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:104:62: missed: not vectorized: more than one data ref in stmt: D.6452 = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.104_29];
PULSE.c:104:17: missed: statement clobbers memory: *_3 = PULSE_CreateDenseLayer (D.6452);
PULSE.c:104:62: missed: not vectorized: more than one data ref in stmt: D.6452 = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.104_47];
PULSE.c:104:17: missed: statement clobbers memory: *_32 = PULSE_CreateDenseLayer (D.6452);
PULSE.c:110:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:111:1: note: ***** Analysis failed with vector mode V8SI
PULSE.c:111:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:116:8: missed: couldn't vectorize loop
PULSE.c:116:8: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:113:6: note: vectorized 0 loops in function.
PULSE.c:118:3: missed: statement clobbers memory: _1 (current_11);
PULSE.c:121:2: missed: statement clobbers memory: free (layer_4(D));
PULSE.c:122:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:122:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Loss.c:15:19: optimized: loop vectorized using 32 byte vectors
Loss.c:15:19: optimized:  loop versioned for vectorization because of possible aliasing
Loss.c:15:19: optimized: loop vectorized using 16 byte vectors
Loss.c:12:23: note: vectorized 1 loops in function.
Loss.c:12:23: note: ***** Analysis failed with vector mode V8SF
Loss.c:12:23: note: ***** The result for vector mode V32QI would be the same
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V16QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V16QI
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V8QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V8QI
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V4QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V4QI
Loss.c:7:19: missed: couldn't vectorize loop
Loss.c:8:38: missed: not vectorized: unsupported use in stmt.
Loss.c:4:23: note: vectorized 0 loops in function.
Loss.c:9:13: note: ***** Analysis failed with vector mode V8SF
Loss.c:9:13: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SF
Loss.c:23:2: note: ***** Analysis failed with vector mode VOID
Layer.c:7:34: missed: statement clobbers memory: _3 = aligned_alloc (64, _2);
Layer.c:8:35: missed: statement clobbers memory: _6 = aligned_alloc (64, _5);
Layer.c:9:34: missed: statement clobbers memory: _7 = aligned_alloc (64, _5);
Layer.c:14:19: missed: statement clobbers memory: _8 = PULSE_GetActivationFunctionPtr (activation_function_18(D));
Layer.c:19:9: note: ***** Analysis succeeded with vector mode V8SI
Layer.c:19:9: note: SLPing BB part
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x3f043c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: op template: MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: 	stmt 0 MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: 	stmt 1 MEM <unsigned int> [(struct PULSE_Layer *)&<retval> + 4B] = optimization_type_17(D);
Layer.c:19:9: note: 	children 0x3f04448
Layer.c:19:9: note: node (external) 0x3f04448 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ type_16(D), optimization_type_17(D) }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x3f044c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: op template: <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: 	stmt 0 <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: 	stmt 1 <retval>.n_outputs = n_outputs_12(D);
Layer.c:19:9: note: 	children 0x3f04548
Layer.c:19:9: note: node (external) 0x3f04548 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ n_inputs_11(D), n_outputs_12(D) }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x3f04648 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: <retval>.inputs = _3;
Layer.c:19:9: note: 	stmt 0 <retval>.inputs = _3;
Layer.c:19:9: note: 	stmt 1 <retval>.outputs = _6;
Layer.c:19:9: note: 	children 0x3f046c8
Layer.c:19:9: note: node (external) 0x3f046c8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: 	stmt 0 _3 = aligned_alloc (64, _2);
Layer.c:19:9: note: 	stmt 1 _6 = aligned_alloc (64, _5);
Layer.c:19:9: note: 	children 0x3f04748 0x3f047c8
Layer.c:19:9: note: node (constant) 0x3f04748 (max_nunits=1, refcnt=1)
Layer.c:19:9: note: 	{ 64, 64 }
Layer.c:19:9: note: node 0x3f047c8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: patt_38 = n_inputs_11(D) w* 4;
Layer.c:19:9: note: 	stmt 0 patt_38 = n_inputs_11(D) w* 4;
Layer.c:19:9: note: 	stmt 1 patt_39 = n_outputs_12(D) w* 4;
Layer.c:19:9: note: 	children 0x3f04848 0x3f048c8
Layer.c:19:9: note: node (external) 0x3f04848 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ n_inputs_11(D), n_outputs_12(D) }
Layer.c:19:9: note: node (constant) 0x3f048c8 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ 4, 4 }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x3f049c8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Layer.c:19:9: note: op template: <retval>.feed = feed_20(D);
Layer.c:19:9: note: 	stmt 0 <retval>.feed = feed_20(D);
Layer.c:19:9: note: 	stmt 1 <retval>.back = back_21(D);
Layer.c:19:9: note: 	stmt 2 <retval>.fix = fix_22(D);
Layer.c:19:9: note: 	stmt 3 <retval>.destroy = destroy_23(D);
Layer.c:19:9: note: 	children 0x3f04a48
Layer.c:19:9: note: node (external) 0x3f04a48 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Layer.c:19:9: note: 	{ feed_20(D), back_21(D), fix_22(D), destroy_23(D) }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 64
  Scalar cost: 64
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x3f04ac8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: <retval>.parent = 0B;
Layer.c:19:9: note: 	stmt 0 <retval>.parent = 0B;
Layer.c:19:9: note: 	stmt 1 <retval>.child = 0B;
Layer.c:19:9: note: 	children 0x3f04b48
Layer.c:19:9: note: node (constant) 0x3f04b48 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: 	{ 0B, 0B }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Layer.c:19:9: note: Basic block will be vectorized using SLP
Layer.c:19:9: optimized: basic block part vectorized using 8 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x3f043c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: op template: MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: 	stmt 0 MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: 	stmt 1 MEM <unsigned int> [(struct PULSE_Layer *)&<retval> + 4B] = optimization_type_17(D);
Layer.c:19:9: note: 	children 0x3f04448
Layer.c:19:9: note: node (external) 0x3f04448 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ type_16(D), optimization_type_17(D) }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: vect_is_simple_use: operand optimization_type_17(D), type of def: external
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: MEM <unsigned int> [(struct PULSE_Layer *)&<retval>]
Layer.c:19:9: note: created &MEM <unsigned int> [(struct PULSE_Layer *)&<retval>]
Layer.c:19:9: note: add new stmt: MEM <vector(2) unsigned int> [(struct PULSE_Layer *)&<retval>] = _40;
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: optimized: basic block part vectorized using 8 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x3f044c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: op template: <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: 	stmt 0 <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: 	stmt 1 <retval>.n_outputs = n_outputs_12(D);
Layer.c:19:9: note: 	children 0x3f04548
Layer.c:19:9: note: node (external) 0x3f04548 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ n_inputs_11(D), n_outputs_12(D) }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: vect_is_simple_use: operand n_outputs_12(D), type of def: external
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: <retval>.n_inputs
Layer.c:19:9: note: created &<retval>.n_inputs
Layer.c:19:9: note: add new stmt: MEM <vector(2) unsigned int> [(unsigned int *)&<retval> + 128B] = _42;
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: optimized: basic block part vectorized using 16 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x3f04648 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: <retval>.inputs = _3;
Layer.c:19:9: note: 	stmt 0 <retval>.inputs = _3;
Layer.c:19:9: note: 	stmt 1 <retval>.outputs = _6;
Layer.c:19:9: note: 	children 0x3f046c8
Layer.c:19:9: note: node (external) 0x3f046c8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: 	stmt 0 _3 = aligned_alloc (64, _2);
Layer.c:19:9: note: 	stmt 1 _6 = aligned_alloc (64, _5);
Layer.c:19:9: note: 	children 0x3f04748 0x3f047c8
Layer.c:19:9: note: node (constant) 0x3f04748 (max_nunits=1, refcnt=1)
Layer.c:19:9: note: 	{ 64, 64 }
Layer.c:19:9: note: node 0x3f047c8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: patt_38 = n_inputs_11(D) w* 4;
Layer.c:19:9: note: 	stmt 0 patt_38 = n_inputs_11(D) w* 4;
Layer.c:19:9: note: 	stmt 1 patt_39 = n_outputs_12(D) w* 4;
Layer.c:19:9: note: 	children 0x3f04848 0x3f048c8
Layer.c:19:9: note: node (external) 0x3f04848 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ n_inputs_11(D), n_outputs_12(D) }
Layer.c:19:9: note: node (constant) 0x3f048c8 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ 4, 4 }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: <retval>.inputs = _3;
Layer.c:19:9: note: vect_is_simple_use: operand aligned_alloc (64, _5), type of def: internal
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.inputs
Layer.c:19:9: note: created &<retval>.inputs
Layer.c:19:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval> + 8B] = _46;
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: optimized: basic block part vectorized using 32 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x3f049c8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Layer.c:19:9: note: op template: <retval>.feed = feed_20(D);
Layer.c:19:9: note: 	stmt 0 <retval>.feed = feed_20(D);
Layer.c:19:9: note: 	stmt 1 <retval>.back = back_21(D);
Layer.c:19:9: note: 	stmt 2 <retval>.fix = fix_22(D);
Layer.c:19:9: note: 	stmt 3 <retval>.destroy = destroy_23(D);
Layer.c:19:9: note: 	children 0x3f04a48
Layer.c:19:9: note: node (external) 0x3f04a48 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Layer.c:19:9: note: 	{ feed_20(D), back_21(D), fix_22(D), destroy_23(D) }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: <retval>.feed = feed_20(D);
Layer.c:19:9: note: vect_is_simple_use: operand back_21(D), type of def: external
Layer.c:19:9: note: vect_is_simple_use: operand fix_22(D), type of def: external
Layer.c:19:9: note: vect_is_simple_use: operand destroy_23(D), type of def: external
Layer.c:19:9: note: conflicting alias set types.
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(4) long unsigned int  vectorizing a pointer ref: <retval>.feed
Layer.c:19:9: note: created &<retval>.feed
Layer.c:19:9: note: add new stmt: MEM <vector(4) long unsigned int> [(void *)&<retval> + 32B] = _52;
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: optimized: basic block part vectorized using 16 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x3f04ac8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: <retval>.parent = 0B;
Layer.c:19:9: note: 	stmt 0 <retval>.parent = 0B;
Layer.c:19:9: note: 	stmt 1 <retval>.child = 0B;
Layer.c:19:9: note: 	children 0x3f04b48
Layer.c:19:9: note: node (constant) 0x3f04b48 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: 	{ 0B, 0B }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: <retval>.parent = 0B;
Layer.c:19:9: note: vect_is_simple_use: operand 0B, type of def: constant
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.parent
Layer.c:19:9: note: created &<retval>.parent
Layer.c:19:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer * *)&<retval> + 72B] = { 0, 0 };
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: note: ***** The result for vector mode V32QI would be the same
Layer.c:24:2: missed: statement clobbers memory: free (_1);
Layer.c:25:2: missed: statement clobbers memory: free (_2);
Layer.c:26:2: missed: statement clobbers memory: free (_3);
Layer.c:27:1: note: ***** Analysis failed with vector mode V4DI
Layer.c:27:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _28 (pretmp_91, _51, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:19: missed: not vectorized: no vectype for stmt: _22 = *_21;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:188:1: note: ***** Analysis failed with vector mode VOID
Dense.c:170:10: optimized: loop vectorized using 32 byte vectors
Dense.c:170:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:170:10: optimized: loop vectorized using 16 byte vectors
Dense.c:161:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
Dense.c:154:10: optimized: loop vectorized using 32 byte vectors
Dense.c:154:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:154:10: optimized: loop vectorized using 16 byte vectors
Dense.c:145:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
Dense.c:136:13: note: vectorized 2 loops in function.
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_28, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
Dense.c:136:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:136:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:105:24: missed: couldn't vectorize loop
Dense.c:105:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:108:17: missed: couldn't vectorize loop
Dense.c:108:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:122:24: missed: couldn't vectorize loop
Dense.c:122:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:125:17: missed: couldn't vectorize loop
Dense.c:125:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:90:15: missed: couldn't vectorize loop
Dense.c:90:15: missed: not vectorized: number of iterations cannot be computed.
Dense.c:83:13: note: vectorized 0 loops in function.
Dense.c:86:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:110:17: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:110:17: note: SLPing BB part
Dense.c:127:17: note: Costing subgraph: 
Dense.c:127:17: note: node 0x758b4d8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:127:17: note: op template: _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 0 _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 1 _198 = (sizetype) wi_137;
Dense.c:127:17: note: 	children 0x758b5d8
Dense.c:127:17: note: node (external) 0x758b5d8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:127:17: note: 	{ j_135, wi_137 }
Dense.c:127:17: note: Cost model analysis: 
Dense.c:127:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:127:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:127:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: Costing subgraph: 
Dense.c:110:17: note: node 0x758b6d8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:110:17: note: op template: _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 0 _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 1 _217 = (sizetype) wi_136;
Dense.c:110:17: note: 	children 0x758b7d8
Dense.c:110:17: note: node (external) 0x758b7d8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:110:17: note: 	{ j_134, wi_136 }
Dense.c:110:17: note: Cost model analysis: 
Dense.c:110:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:110:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:110:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: ***** The result for vector mode V32QI would be the same
Dense.c:110:17: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:110:17: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:110:17: note: SLPing BB part
Dense.c:127:17: note: Costing subgraph: 
Dense.c:127:17: note: node 0x758b6d8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:127:17: note: op template: _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 0 _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 1 _198 = (sizetype) wi_137;
Dense.c:127:17: note: 	children 0x758b4d8
Dense.c:127:17: note: node (external) 0x758b4d8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:127:17: note: 	{ j_135, wi_137 }
Dense.c:127:17: note: Cost model analysis: 
Dense.c:127:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:127:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:127:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: Costing subgraph: 
Dense.c:110:17: note: node 0x758b658 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:110:17: note: op template: _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 0 _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 1 _217 = (sizetype) wi_136;
Dense.c:110:17: note: 	children 0x758b758
Dense.c:110:17: note: node (external) 0x758b758 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:110:17: note: 	{ j_134, wi_136 }
Dense.c:110:17: note: Cost model analysis: 
Dense.c:110:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:110:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:110:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:105:51: note: ***** Analysis failed with vector mode V8QI
Dense.c:105:51: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:105:51: note: ***** Analysis failed with vector mode V4QI
Dense.c:62:15: missed: couldn't vectorize loop
Dense.c:62:15: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:75:11: optimized: loop vectorized using 32 byte vectors
Dense.c:75:11: optimized: loop vectorized using 16 byte vectors
Dense.c:66:11: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:881:10: missed: not vectorized: no vectype for stmt: _53 = MEM[(__m256 * {ref-all})w_ptr_82];
 scalar_type: __m256
Dense.c:54:13: note: vectorized 1 loops in function.
Dense.c:57:2: missed: statement clobbers memory: memcpy (_4, dense$baiases_63, _3);
Dense.c:79:2: missed: statement clobbers memory: _27 (pretmp_163, _68, 0);
Dense.c:54:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:54:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:202:19: missed: couldn't vectorize loop
Dense.c:202:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:192:13: note: vectorized 0 loops in function.
Dense.c:196:35: missed: statement clobbers memory: _5 = aligned_alloc (64, _4);
Dense.c:197:37: missed: statement clobbers memory: _6 = aligned_alloc (64, _4);
Dense.c:198:35: missed: statement clobbers memory: _8 = aligned_alloc (64, _7);
Dense.c:199:34: missed: statement clobbers memory: _9 = aligned_alloc (64, _7);
Dense.c:200:35: missed: statement clobbers memory: _10 = aligned_alloc (64, _7);
Dense.c:203:38: missed: statement clobbers memory: _11 = rand ();
Dense.c:203:72: missed: statement clobbers memory: _57 = sqrt (_18);
Dense.c:215:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:216:4: missed: statement clobbers memory: exit (1);
Dense.c:212:4: missed: statement clobbers memory: layer = PULSE_CreateLayer (args$n_inputs_36, args$n_outputs_47, 0, args$8_48, _SIMD_FeedDense, _SIMD_BackDense, _SIMD_FixDense, _DestroyDense, 1); [return slot optimization]
Dense.c:209:12: missed: statement clobbers memory: layer = PULSE_CreateLayer (args$n_inputs_36, args$n_outputs_47, 0, args$8_48, _FeedDense, _BackDense, _FixDense, _DestroyDense, 0); [return slot optimization]
Dense.c:222:9: missed: not vectorized: more than one data ref in stmt: <retval> = layer;
Dense.c:222:9: note: ***** Analysis succeeded with vector mode V8SI
Dense.c:222:9: note: SLPing BB part
Dense.c:222:9: note: Costing subgraph: 
Dense.c:222:9: note: node 0x7423bd8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:222:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = _5;
Dense.c:222:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = _5;
Dense.c:222:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&layer + 96B] = _8;
Dense.c:222:9: note: 	stmt 2 MEM <float *> [(struct PULSE_Layer *)&layer + 104B] = _9;
Dense.c:222:9: note: 	stmt 3 MEM <float *> [(struct PULSE_Layer *)&layer + 112B] = _10;
Dense.c:222:9: note: 	children 0x7423c58
Dense.c:222:9: note: node (external) 0x7423c58 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:222:9: note: 	stmt 0 _5 = aligned_alloc (64, _4);
Dense.c:222:9: note: 	stmt 1 _8 = aligned_alloc (64, _7);
Dense.c:222:9: note: 	stmt 2 _9 = aligned_alloc (64, _7);
Dense.c:222:9: note: 	stmt 3 _10 = aligned_alloc (64, _7);
Dense.c:222:9: note: 	children 0x7423cd8 0x7423dd8
Dense.c:222:9: note: node (constant) 0x7423cd8 (max_nunits=1, refcnt=1)
Dense.c:222:9: note: 	{ 64, 64, 64, 64 }
Dense.c:222:9: note: node (external) 0x7423dd8 (max_nunits=1, refcnt=1)
Dense.c:222:9: note: 	{ _4, _7, _7, _7 }
Dense.c:222:9: note: Cost model analysis: 
Dense.c:222:9: note: Cost model analysis for part in loop 0:
  Vector cost: 64
  Scalar cost: 64
Dense.c:222:9: note: Basic block will be vectorized using SLP
Dense.c:222:9: optimized: basic block part vectorized using 32 byte vectors
Dense.c:222:9: note: Vectorizing SLP tree:
Dense.c:222:9: note: node 0x7423bd8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:222:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = _5;
Dense.c:222:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = _5;
Dense.c:222:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&layer + 96B] = _8;
Dense.c:222:9: note: 	stmt 2 MEM <float *> [(struct PULSE_Layer *)&layer + 104B] = _9;
Dense.c:222:9: note: 	stmt 3 MEM <float *> [(struct PULSE_Layer *)&layer + 112B] = _10;
Dense.c:222:9: note: 	children 0x7423c58
Dense.c:222:9: note: node (external) 0x7423c58 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:222:9: note: 	stmt 0 _5 = aligned_alloc (64, _4);
Dense.c:222:9: note: 	stmt 1 _8 = aligned_alloc (64, _7);
Dense.c:222:9: note: 	stmt 2 _9 = aligned_alloc (64, _7);
Dense.c:222:9: note: 	stmt 3 _10 = aligned_alloc (64, _7);
Dense.c:222:9: note: 	children 0x7423cd8 0x7423dd8
Dense.c:222:9: note: node (constant) 0x7423cd8 (max_nunits=1, refcnt=1)
Dense.c:222:9: note: 	{ 64, 64, 64, 64 }
Dense.c:222:9: note: node (external) 0x7423dd8 (max_nunits=1, refcnt=1)
Dense.c:222:9: note: 	{ _4, _7, _7, _7 }
Dense.c:222:9: note: ------>vectorizing SLP node starting from: MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = _5;
Dense.c:222:9: note: vect_is_simple_use: operand aligned_alloc (64, _7), type of def: internal
Dense.c:222:9: note: vect_is_simple_use: operand aligned_alloc (64, _7), type of def: internal
Dense.c:222:9: note: vect_is_simple_use: operand aligned_alloc (64, _7), type of def: internal
Dense.c:222:9: note: transform store. ncopies = 1
Dense.c:222:9: note: create vector_type-pointer variable to type: vector(4) long unsigned int  vectorizing a pointer ref: MEM <float *> [(struct PULSE_Layer *)&layer + 88B]
Dense.c:222:9: note: created &MEM <float *> [(struct PULSE_Layer *)&layer + 88B]
Dense.c:222:9: note: add new stmt: MEM <vector(4) long unsigned int> [(struct PULSE_Layer *)&layer + 88B] = _55;
Dense.c:222:9: note: vectorizing stmts using SLP.
Dense.c:222:9: note: ***** The result for vector mode V32QI would be the same
Activations.c:17:19: optimized: loop vectorized using 32 byte vectors
Activations.c:17:19: optimized: loop vectorized using 16 byte vectors
Activations.c:17:19: optimized: loop vectorized using 32 byte vectors
Activations.c:17:19: optimized: loop vectorized using 16 byte vectors
Activations.c:14:13: note: vectorized 2 loops in function.
Activations.c:14:13: note: ***** Analysis failed with vector mode V8SF
Activations.c:14:13: note: ***** The result for vector mode V32QI would be the same
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V16QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V16QI
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V8QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V8QI
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V4QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V4QI
Activations.c:28:19: missed: couldn't vectorize loop
Activations.c:28:19: missed: not vectorized: unsupported control flow in loop.
Activations.c:28:19: optimized: loop vectorized using 32 byte vectors
Activations.c:28:19: optimized: loop vectorized using 16 byte vectors
Activations.c:25:13: note: vectorized 1 loops in function.
Activations.c:25:13: note: ***** Analysis failed with vector mode V8SF
Activations.c:25:13: note: ***** The result for vector mode V32QI would be the same
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V16QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V16QI
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V8QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V8QI
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V4QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V4QI
Activations.c:38:1: note: ***** Analysis failed with vector mode VOID
Activations.c:5:19: missed: couldn't vectorize loop
Activations.c:9:21: missed: statement clobbers memory: _36 = expf (_18);
Activations.c:5:19: missed: couldn't vectorize loop
Activations.c:9:21: missed: statement clobbers memory: _12 = expf (_11);
Activations.c:3:13: note: vectorized 0 loops in function.
Activations.c:9:21: missed: statement clobbers memory: _12 = expf (_11);
Activations.c:9:21: missed: statement clobbers memory: _36 = expf (_18);
Activations.c:11:1: note: ***** Analysis failed with vector mode V8SF
Activations.c:11:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SF
Activations.c:44:2: note: ***** Analysis failed with vector mode VOID
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:4:18: note: vectorized 0 loops in function.
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_4, inputs_15, _3);
PULSE.c:8:2: missed: statement clobbers memory: _5 (layer_17);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_12, pretmp_26, _9);
PULSE.c:13:28: note: ***** Analysis failed with vector mode V8SI
PULSE.c:13:28: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:22:2: missed: statement clobbers memory: _1 (layer_10(D));
PULSE.c:22:2: missed: statement clobbers memory: _14 (_2);
PULSE.c:22:2: missed: statement clobbers memory: _21 (_15);
PULSE.c:22:2: missed: statement clobbers memory: _28 (_22);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_29);
PULSE.c:26:3: missed: statement clobbers memory: memset (_34, 0, _32);
PULSE.c:26:3: missed: statement clobbers memory: memset (_27, 0, _25);
PULSE.c:26:3: missed: statement clobbers memory: memset (_20, 0, _18);
PULSE.c:26:3: missed: statement clobbers memory: memset (_7, 0, _5);
PULSE.c:28:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:28:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:32:6: note: vectorized 1 loops in function.
PULSE.c:34:8: missed: statement clobbers memory: _1 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_2);
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _9 = rand ();
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8SI
PULSE.c:32:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:61:16: missed: couldn't vectorize loop
PULSE.c:61:16: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:64:17: missed: couldn't vectorize loop
PULSE.c:64:17: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:73:19: missed: couldn't vectorize loop
PULSE.c:73:19: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _69 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:53:22: missed: couldn't vectorize loop
PULSE.c:53:22: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:47:6: note: vectorized 1 loops in function.
PULSE.c:48:1: missed: statement clobbers memory: saved_stack.7_39 = __builtin_stack_save ();
PULSE.c:49:46: missed: statement clobbers memory: _1 = PULSE_GetLossFunctionPtr (loss_function_40(D));
PULSE.c:60:10: missed: statement clobbers memory: random.4_46 = __builtin_alloca_with_align (_4, 32);
PULSE.c:34:8: missed: statement clobbers memory: _61 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_62);
PULSE.c:39:29: missed: statement clobbers memory: _69 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _72 = rand ();
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_98, inputs_93, _97);
PULSE.c:8:2: missed: statement clobbers memory: _99 (layer_94);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_105, _104, _103);
PULSE.c:67:11: missed: statement clobbers memory: loss_55 = PULSE_GetLoss_42 (_17, _16, _12, _11);
PULSE.c:22:2: missed: statement clobbers memory: _86 (output_34);
PULSE.c:22:2: missed: statement clobbers memory: _108 (_87);
PULSE.c:22:2: missed: statement clobbers memory: _115 (_109);
PULSE.c:22:2: missed: statement clobbers memory: _122 (_116);
PULSE.c:22:2: missed: statement clobbers memory: _129 (_123);
PULSE.c:22:2: missed: statement clobbers memory: _136 (_130);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_137);
PULSE.c:26:3: missed: statement clobbers memory: memset (_142, 0, _140);
PULSE.c:26:3: missed: statement clobbers memory: memset (_135, 0, _133);
PULSE.c:26:3: missed: statement clobbers memory: memset (_128, 0, _126);
PULSE.c:26:3: missed: statement clobbers memory: memset (_121, 0, _119);
PULSE.c:26:3: missed: statement clobbers memory: memset (_114, 0, _112);
PULSE.c:26:3: missed: statement clobbers memory: memset (_92, 0, _90);
PULSE.c:75:6: missed: statement clobbers memory: _21 (current_191, args);
PULSE.c:81:4: missed: statement clobbers memory: printf ("Epoch: %d | Item: %d | Loss: %.10f | Batch Loss: %.10f\r", i_188, j_189, _25, _24);
PULSE.c:47:6: missed: statement clobbers memory: __builtin_stack_restore (saved_stack.7_39);
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4DI
PULSE.c:47:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:91:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:91:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:98:20: missed: couldn't vectorize loop
PULSE.c:98:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:98:20: missed: couldn't vectorize loop
PULSE.c:98:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:93:6: note: vectorized 0 loops in function.
PULSE.c:97:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:104:62: missed: not vectorized: more than one data ref in stmt: D.6452 = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.104_29];
PULSE.c:104:17: missed: statement clobbers memory: *_3 = PULSE_CreateDenseLayer (D.6452);
PULSE.c:104:62: missed: not vectorized: more than one data ref in stmt: D.6452 = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.104_47];
PULSE.c:104:17: missed: statement clobbers memory: *_32 = PULSE_CreateDenseLayer (D.6452);
PULSE.c:110:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:111:1: note: ***** Analysis failed with vector mode V8SI
PULSE.c:111:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:116:8: missed: couldn't vectorize loop
PULSE.c:116:8: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:113:6: note: vectorized 0 loops in function.
PULSE.c:118:3: missed: statement clobbers memory: _1 (current_11);
PULSE.c:121:2: missed: statement clobbers memory: free (layer_4(D));
PULSE.c:122:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:122:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Loss.c:15:19: optimized: loop vectorized using 32 byte vectors
Loss.c:15:19: optimized:  loop versioned for vectorization because of possible aliasing
Loss.c:15:19: optimized: loop vectorized using 16 byte vectors
Loss.c:12:23: note: vectorized 1 loops in function.
Loss.c:12:23: note: ***** Analysis failed with vector mode V8SF
Loss.c:12:23: note: ***** The result for vector mode V32QI would be the same
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V16QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V16QI
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V8QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V8QI
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V4QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V4QI
Loss.c:7:19: missed: couldn't vectorize loop
Loss.c:8:38: missed: not vectorized: unsupported use in stmt.
Loss.c:4:23: note: vectorized 0 loops in function.
Loss.c:9:13: note: ***** Analysis failed with vector mode V8SF
Loss.c:9:13: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SF
Loss.c:23:2: note: ***** Analysis failed with vector mode VOID
Layer.c:7:34: missed: statement clobbers memory: _3 = aligned_alloc (64, _2);
Layer.c:8:35: missed: statement clobbers memory: _6 = aligned_alloc (64, _5);
Layer.c:9:34: missed: statement clobbers memory: _7 = aligned_alloc (64, _5);
Layer.c:14:19: missed: statement clobbers memory: _8 = PULSE_GetActivationFunctionPtr (activation_function_18(D));
Layer.c:19:9: note: ***** Analysis succeeded with vector mode V8SI
Layer.c:19:9: note: SLPing BB part
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x3b943c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: op template: MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: 	stmt 0 MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: 	stmt 1 MEM <unsigned int> [(struct PULSE_Layer *)&<retval> + 4B] = optimization_type_17(D);
Layer.c:19:9: note: 	children 0x3b94448
Layer.c:19:9: note: node (external) 0x3b94448 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ type_16(D), optimization_type_17(D) }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x3b944c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: op template: <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: 	stmt 0 <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: 	stmt 1 <retval>.n_outputs = n_outputs_12(D);
Layer.c:19:9: note: 	children 0x3b94548
Layer.c:19:9: note: node (external) 0x3b94548 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ n_inputs_11(D), n_outputs_12(D) }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x3b94648 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: <retval>.inputs = _3;
Layer.c:19:9: note: 	stmt 0 <retval>.inputs = _3;
Layer.c:19:9: note: 	stmt 1 <retval>.outputs = _6;
Layer.c:19:9: note: 	children 0x3b946c8
Layer.c:19:9: note: node (external) 0x3b946c8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: 	stmt 0 _3 = aligned_alloc (64, _2);
Layer.c:19:9: note: 	stmt 1 _6 = aligned_alloc (64, _5);
Layer.c:19:9: note: 	children 0x3b94748 0x3b947c8
Layer.c:19:9: note: node (constant) 0x3b94748 (max_nunits=1, refcnt=1)
Layer.c:19:9: note: 	{ 64, 64 }
Layer.c:19:9: note: node 0x3b947c8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: patt_38 = n_inputs_11(D) w* 4;
Layer.c:19:9: note: 	stmt 0 patt_38 = n_inputs_11(D) w* 4;
Layer.c:19:9: note: 	stmt 1 patt_39 = n_outputs_12(D) w* 4;
Layer.c:19:9: note: 	children 0x3b94848 0x3b948c8
Layer.c:19:9: note: node (external) 0x3b94848 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ n_inputs_11(D), n_outputs_12(D) }
Layer.c:19:9: note: node (constant) 0x3b948c8 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ 4, 4 }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x3b949c8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Layer.c:19:9: note: op template: <retval>.feed = feed_20(D);
Layer.c:19:9: note: 	stmt 0 <retval>.feed = feed_20(D);
Layer.c:19:9: note: 	stmt 1 <retval>.back = back_21(D);
Layer.c:19:9: note: 	stmt 2 <retval>.fix = fix_22(D);
Layer.c:19:9: note: 	stmt 3 <retval>.destroy = destroy_23(D);
Layer.c:19:9: note: 	children 0x3b94a48
Layer.c:19:9: note: node (external) 0x3b94a48 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Layer.c:19:9: note: 	{ feed_20(D), back_21(D), fix_22(D), destroy_23(D) }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 64
  Scalar cost: 64
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x3b94ac8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: <retval>.parent = 0B;
Layer.c:19:9: note: 	stmt 0 <retval>.parent = 0B;
Layer.c:19:9: note: 	stmt 1 <retval>.child = 0B;
Layer.c:19:9: note: 	children 0x3b94b48
Layer.c:19:9: note: node (constant) 0x3b94b48 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: 	{ 0B, 0B }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Layer.c:19:9: note: Basic block will be vectorized using SLP
Layer.c:19:9: optimized: basic block part vectorized using 8 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x3b943c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: op template: MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: 	stmt 0 MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: 	stmt 1 MEM <unsigned int> [(struct PULSE_Layer *)&<retval> + 4B] = optimization_type_17(D);
Layer.c:19:9: note: 	children 0x3b94448
Layer.c:19:9: note: node (external) 0x3b94448 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ type_16(D), optimization_type_17(D) }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: vect_is_simple_use: operand optimization_type_17(D), type of def: external
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: MEM <unsigned int> [(struct PULSE_Layer *)&<retval>]
Layer.c:19:9: note: created &MEM <unsigned int> [(struct PULSE_Layer *)&<retval>]
Layer.c:19:9: note: add new stmt: MEM <vector(2) unsigned int> [(struct PULSE_Layer *)&<retval>] = _40;
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: optimized: basic block part vectorized using 8 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x3b944c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: op template: <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: 	stmt 0 <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: 	stmt 1 <retval>.n_outputs = n_outputs_12(D);
Layer.c:19:9: note: 	children 0x3b94548
Layer.c:19:9: note: node (external) 0x3b94548 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ n_inputs_11(D), n_outputs_12(D) }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: vect_is_simple_use: operand n_outputs_12(D), type of def: external
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: <retval>.n_inputs
Layer.c:19:9: note: created &<retval>.n_inputs
Layer.c:19:9: note: add new stmt: MEM <vector(2) unsigned int> [(unsigned int *)&<retval> + 128B] = _42;
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: optimized: basic block part vectorized using 16 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x3b94648 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: <retval>.inputs = _3;
Layer.c:19:9: note: 	stmt 0 <retval>.inputs = _3;
Layer.c:19:9: note: 	stmt 1 <retval>.outputs = _6;
Layer.c:19:9: note: 	children 0x3b946c8
Layer.c:19:9: note: node (external) 0x3b946c8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: 	stmt 0 _3 = aligned_alloc (64, _2);
Layer.c:19:9: note: 	stmt 1 _6 = aligned_alloc (64, _5);
Layer.c:19:9: note: 	children 0x3b94748 0x3b947c8
Layer.c:19:9: note: node (constant) 0x3b94748 (max_nunits=1, refcnt=1)
Layer.c:19:9: note: 	{ 64, 64 }
Layer.c:19:9: note: node 0x3b947c8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: patt_38 = n_inputs_11(D) w* 4;
Layer.c:19:9: note: 	stmt 0 patt_38 = n_inputs_11(D) w* 4;
Layer.c:19:9: note: 	stmt 1 patt_39 = n_outputs_12(D) w* 4;
Layer.c:19:9: note: 	children 0x3b94848 0x3b948c8
Layer.c:19:9: note: node (external) 0x3b94848 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ n_inputs_11(D), n_outputs_12(D) }
Layer.c:19:9: note: node (constant) 0x3b948c8 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ 4, 4 }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: <retval>.inputs = _3;
Layer.c:19:9: note: vect_is_simple_use: operand aligned_alloc (64, _5), type of def: internal
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.inputs
Layer.c:19:9: note: created &<retval>.inputs
Layer.c:19:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval> + 8B] = _46;
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: optimized: basic block part vectorized using 32 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x3b949c8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Layer.c:19:9: note: op template: <retval>.feed = feed_20(D);
Layer.c:19:9: note: 	stmt 0 <retval>.feed = feed_20(D);
Layer.c:19:9: note: 	stmt 1 <retval>.back = back_21(D);
Layer.c:19:9: note: 	stmt 2 <retval>.fix = fix_22(D);
Layer.c:19:9: note: 	stmt 3 <retval>.destroy = destroy_23(D);
Layer.c:19:9: note: 	children 0x3b94a48
Layer.c:19:9: note: node (external) 0x3b94a48 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Layer.c:19:9: note: 	{ feed_20(D), back_21(D), fix_22(D), destroy_23(D) }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: <retval>.feed = feed_20(D);
Layer.c:19:9: note: vect_is_simple_use: operand back_21(D), type of def: external
Layer.c:19:9: note: vect_is_simple_use: operand fix_22(D), type of def: external
Layer.c:19:9: note: vect_is_simple_use: operand destroy_23(D), type of def: external
Layer.c:19:9: note: conflicting alias set types.
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(4) long unsigned int  vectorizing a pointer ref: <retval>.feed
Layer.c:19:9: note: created &<retval>.feed
Layer.c:19:9: note: add new stmt: MEM <vector(4) long unsigned int> [(void *)&<retval> + 32B] = _52;
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: optimized: basic block part vectorized using 16 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x3b94ac8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: <retval>.parent = 0B;
Layer.c:19:9: note: 	stmt 0 <retval>.parent = 0B;
Layer.c:19:9: note: 	stmt 1 <retval>.child = 0B;
Layer.c:19:9: note: 	children 0x3b94b48
Layer.c:19:9: note: node (constant) 0x3b94b48 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: 	{ 0B, 0B }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: <retval>.parent = 0B;
Layer.c:19:9: note: vect_is_simple_use: operand 0B, type of def: constant
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.parent
Layer.c:19:9: note: created &<retval>.parent
Layer.c:19:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer * *)&<retval> + 72B] = { 0, 0 };
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: note: ***** The result for vector mode V32QI would be the same
Layer.c:24:2: missed: statement clobbers memory: free (_1);
Layer.c:25:2: missed: statement clobbers memory: free (_2);
Layer.c:26:2: missed: statement clobbers memory: free (_3);
Layer.c:27:1: note: ***** Analysis failed with vector mode V4DI
Layer.c:27:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _28 (pretmp_91, _51, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:19: missed: not vectorized: no vectype for stmt: _22 = *_21;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:170:10: optimized: loop vectorized using 32 byte vectors
Dense.c:170:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:170:10: optimized: loop vectorized using 16 byte vectors
Dense.c:161:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
Dense.c:154:10: optimized: loop vectorized using 32 byte vectors
Dense.c:154:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:154:10: optimized: loop vectorized using 16 byte vectors
Dense.c:145:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
Dense.c:136:13: note: vectorized 2 loops in function.
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_28, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
Dense.c:136:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:136:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:105:24: missed: couldn't vectorize loop
Dense.c:105:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:108:17: missed: couldn't vectorize loop
Dense.c:108:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:122:24: missed: couldn't vectorize loop
Dense.c:122:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:125:17: missed: couldn't vectorize loop
Dense.c:125:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:90:15: missed: couldn't vectorize loop
Dense.c:90:15: missed: not vectorized: number of iterations cannot be computed.
Dense.c:83:13: note: vectorized 0 loops in function.
Dense.c:86:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:110:17: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:110:17: note: SLPing BB part
Dense.c:127:17: note: Costing subgraph: 
Dense.c:127:17: note: node 0x86cc3e8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:127:17: note: op template: _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 0 _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 1 _198 = (sizetype) wi_137;
Dense.c:127:17: note: 	children 0x86cc4e8
Dense.c:127:17: note: node (external) 0x86cc4e8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:127:17: note: 	{ j_135, wi_137 }
Dense.c:127:17: note: Cost model analysis: 
Dense.c:127:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:127:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:127:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: Costing subgraph: 
Dense.c:110:17: note: node 0x86cc5e8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:110:17: note: op template: _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 0 _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 1 _217 = (sizetype) wi_136;
Dense.c:110:17: note: 	children 0x86cc6e8
Dense.c:110:17: note: node (external) 0x86cc6e8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:110:17: note: 	{ j_134, wi_136 }
Dense.c:110:17: note: Cost model analysis: 
Dense.c:110:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:110:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:110:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: ***** The result for vector mode V32QI would be the same
Dense.c:110:17: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:110:17: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:110:17: note: SLPing BB part
Dense.c:127:17: note: Costing subgraph: 
Dense.c:127:17: note: node 0x86cc5e8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:127:17: note: op template: _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 0 _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 1 _198 = (sizetype) wi_137;
Dense.c:127:17: note: 	children 0x86cc3e8
Dense.c:127:17: note: node (external) 0x86cc3e8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:127:17: note: 	{ j_135, wi_137 }
Dense.c:127:17: note: Cost model analysis: 
Dense.c:127:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:127:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:127:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: Costing subgraph: 
Dense.c:110:17: note: node 0x86cc568 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:110:17: note: op template: _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 0 _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 1 _217 = (sizetype) wi_136;
Dense.c:110:17: note: 	children 0x86cc668
Dense.c:110:17: note: node (external) 0x86cc668 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:110:17: note: 	{ j_134, wi_136 }
Dense.c:110:17: note: Cost model analysis: 
Dense.c:110:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:110:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:110:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:105:51: note: ***** Analysis failed with vector mode V8QI
Dense.c:105:51: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:105:51: note: ***** Analysis failed with vector mode V4QI
Dense.c:62:15: missed: couldn't vectorize loop
Dense.c:62:15: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:75:11: optimized: loop vectorized using 32 byte vectors
Dense.c:75:11: optimized: loop vectorized using 16 byte vectors
Dense.c:66:11: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:881:10: missed: not vectorized: no vectype for stmt: _53 = MEM[(__m256 * {ref-all})w_ptr_82];
 scalar_type: __m256
Dense.c:54:13: note: vectorized 1 loops in function.
Dense.c:57:2: missed: statement clobbers memory: memcpy (_4, dense$baiases_63, _3);
Dense.c:79:2: missed: statement clobbers memory: _27 (pretmp_163, _68, 0);
Dense.c:54:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:54:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:182:2: missed: statement clobbers memory: free (dense$weights_7);
Dense.c:183:2: missed: statement clobbers memory: free (dense$baiases_8);
Dense.c:184:2: missed: statement clobbers memory: free (dense$gradients_10);
Dense.c:185:2: missed: statement clobbers memory: free (dense$deltas_9);
Dense.c:186:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:186:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:200:19: missed: couldn't vectorize loop
Dense.c:200:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:190:13: note: vectorized 0 loops in function.
Dense.c:194:35: missed: statement clobbers memory: _5 = aligned_alloc (64, _4);
Dense.c:195:37: missed: statement clobbers memory: _6 = aligned_alloc (64, _4);
Dense.c:196:35: missed: statement clobbers memory: _8 = aligned_alloc (64, _7);
Dense.c:197:34: missed: statement clobbers memory: _9 = aligned_alloc (64, _7);
Dense.c:198:35: missed: statement clobbers memory: _10 = aligned_alloc (64, _7);
Dense.c:201:38: missed: statement clobbers memory: _11 = rand ();
Dense.c:201:72: missed: statement clobbers memory: _57 = sqrt (_18);
Dense.c:213:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:214:4: missed: statement clobbers memory: exit (1);
Dense.c:210:4: missed: statement clobbers memory: layer = PULSE_CreateLayer (args$n_inputs_36, args$n_outputs_47, 0, args$8_48, _SIMD_FeedDense, _SIMD_BackDense, _SIMD_FixDense, _DestroyDense, 1); [return slot optimization]
Dense.c:207:12: missed: statement clobbers memory: layer = PULSE_CreateLayer (args$n_inputs_36, args$n_outputs_47, 0, args$8_48, _FeedDense, _BackDense, _FixDense, _DestroyDense, 0); [return slot optimization]
Dense.c:220:9: missed: not vectorized: more than one data ref in stmt: <retval> = layer;
Dense.c:220:9: note: ***** Analysis succeeded with vector mode V8SI
Dense.c:220:9: note: SLPing BB part
Dense.c:220:9: note: Costing subgraph: 
Dense.c:220:9: note: node 0x85832c8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:220:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = _5;
Dense.c:220:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = _5;
Dense.c:220:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&layer + 96B] = _8;
Dense.c:220:9: note: 	stmt 2 MEM <float *> [(struct PULSE_Layer *)&layer + 104B] = _9;
Dense.c:220:9: note: 	stmt 3 MEM <float *> [(struct PULSE_Layer *)&layer + 112B] = _10;
Dense.c:220:9: note: 	children 0x8583348
Dense.c:220:9: note: node (external) 0x8583348 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:220:9: note: 	stmt 0 _5 = aligned_alloc (64, _4);
Dense.c:220:9: note: 	stmt 1 _8 = aligned_alloc (64, _7);
Dense.c:220:9: note: 	stmt 2 _9 = aligned_alloc (64, _7);
Dense.c:220:9: note: 	stmt 3 _10 = aligned_alloc (64, _7);
Dense.c:220:9: note: 	children 0x85833c8 0x85834c8
Dense.c:220:9: note: node (constant) 0x85833c8 (max_nunits=1, refcnt=1)
Dense.c:220:9: note: 	{ 64, 64, 64, 64 }
Dense.c:220:9: note: node (external) 0x85834c8 (max_nunits=1, refcnt=1)
Dense.c:220:9: note: 	{ _4, _7, _7, _7 }
Dense.c:220:9: note: Cost model analysis: 
Dense.c:220:9: note: Cost model analysis for part in loop 0:
  Vector cost: 64
  Scalar cost: 64
Dense.c:220:9: note: Basic block will be vectorized using SLP
Dense.c:220:9: optimized: basic block part vectorized using 32 byte vectors
Dense.c:220:9: note: Vectorizing SLP tree:
Dense.c:220:9: note: node 0x85832c8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:220:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = _5;
Dense.c:220:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = _5;
Dense.c:220:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&layer + 96B] = _8;
Dense.c:220:9: note: 	stmt 2 MEM <float *> [(struct PULSE_Layer *)&layer + 104B] = _9;
Dense.c:220:9: note: 	stmt 3 MEM <float *> [(struct PULSE_Layer *)&layer + 112B] = _10;
Dense.c:220:9: note: 	children 0x8583348
Dense.c:220:9: note: node (external) 0x8583348 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:220:9: note: 	stmt 0 _5 = aligned_alloc (64, _4);
Dense.c:220:9: note: 	stmt 1 _8 = aligned_alloc (64, _7);
Dense.c:220:9: note: 	stmt 2 _9 = aligned_alloc (64, _7);
Dense.c:220:9: note: 	stmt 3 _10 = aligned_alloc (64, _7);
Dense.c:220:9: note: 	children 0x85833c8 0x85834c8
Dense.c:220:9: note: node (constant) 0x85833c8 (max_nunits=1, refcnt=1)
Dense.c:220:9: note: 	{ 64, 64, 64, 64 }
Dense.c:220:9: note: node (external) 0x85834c8 (max_nunits=1, refcnt=1)
Dense.c:220:9: note: 	{ _4, _7, _7, _7 }
Dense.c:220:9: note: ------>vectorizing SLP node starting from: MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = _5;
Dense.c:220:9: note: vect_is_simple_use: operand aligned_alloc (64, _7), type of def: internal
Dense.c:220:9: note: vect_is_simple_use: operand aligned_alloc (64, _7), type of def: internal
Dense.c:220:9: note: vect_is_simple_use: operand aligned_alloc (64, _7), type of def: internal
Dense.c:220:9: note: transform store. ncopies = 1
Dense.c:220:9: note: create vector_type-pointer variable to type: vector(4) long unsigned int  vectorizing a pointer ref: MEM <float *> [(struct PULSE_Layer *)&layer + 88B]
Dense.c:220:9: note: created &MEM <float *> [(struct PULSE_Layer *)&layer + 88B]
Dense.c:220:9: note: add new stmt: MEM <vector(4) long unsigned int> [(struct PULSE_Layer *)&layer + 88B] = _55;
Dense.c:220:9: note: vectorizing stmts using SLP.
Dense.c:220:9: note: ***** The result for vector mode V32QI would be the same
Activations.c:17:19: optimized: loop vectorized using 32 byte vectors
Activations.c:17:19: optimized: loop vectorized using 16 byte vectors
Activations.c:17:19: optimized: loop vectorized using 32 byte vectors
Activations.c:17:19: optimized: loop vectorized using 16 byte vectors
Activations.c:14:13: note: vectorized 2 loops in function.
Activations.c:14:13: note: ***** Analysis failed with vector mode V8SF
Activations.c:14:13: note: ***** The result for vector mode V32QI would be the same
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V16QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V16QI
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V8QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V8QI
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V4QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V4QI
Activations.c:28:19: missed: couldn't vectorize loop
Activations.c:28:19: missed: not vectorized: unsupported control flow in loop.
Activations.c:28:19: optimized: loop vectorized using 32 byte vectors
Activations.c:28:19: optimized: loop vectorized using 16 byte vectors
Activations.c:25:13: note: vectorized 1 loops in function.
Activations.c:25:13: note: ***** Analysis failed with vector mode V8SF
Activations.c:25:13: note: ***** The result for vector mode V32QI would be the same
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V16QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V16QI
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V8QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V8QI
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V4QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V4QI
Activations.c:38:1: note: ***** Analysis failed with vector mode VOID
Activations.c:5:19: missed: couldn't vectorize loop
Activations.c:9:21: missed: statement clobbers memory: _36 = expf (_18);
Activations.c:5:19: missed: couldn't vectorize loop
Activations.c:9:21: missed: statement clobbers memory: _12 = expf (_11);
Activations.c:3:13: note: vectorized 0 loops in function.
Activations.c:9:21: missed: statement clobbers memory: _12 = expf (_11);
Activations.c:9:21: missed: statement clobbers memory: _36 = expf (_18);
Activations.c:11:1: note: ***** Analysis failed with vector mode V8SF
Activations.c:11:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SF
Activations.c:44:2: note: ***** Analysis failed with vector mode VOID
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:4:18: note: vectorized 0 loops in function.
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_4, inputs_15, _3);
PULSE.c:8:2: missed: statement clobbers memory: _5 (layer_17);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_12, pretmp_26, _9);
PULSE.c:13:28: note: ***** Analysis failed with vector mode V8SI
PULSE.c:13:28: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:22:2: missed: statement clobbers memory: _1 (layer_10(D));
PULSE.c:22:2: missed: statement clobbers memory: _14 (_2);
PULSE.c:22:2: missed: statement clobbers memory: _21 (_15);
PULSE.c:22:2: missed: statement clobbers memory: _28 (_22);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_29);
PULSE.c:26:3: missed: statement clobbers memory: memset (_34, 0, _32);
PULSE.c:26:3: missed: statement clobbers memory: memset (_27, 0, _25);
PULSE.c:26:3: missed: statement clobbers memory: memset (_20, 0, _18);
PULSE.c:26:3: missed: statement clobbers memory: memset (_7, 0, _5);
PULSE.c:28:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:28:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:32:6: note: vectorized 1 loops in function.
PULSE.c:34:8: missed: statement clobbers memory: _1 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_2);
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _9 = rand ();
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8SI
PULSE.c:32:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:61:16: missed: couldn't vectorize loop
PULSE.c:61:16: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:64:17: missed: couldn't vectorize loop
PULSE.c:64:17: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:73:19: missed: couldn't vectorize loop
PULSE.c:73:19: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _69 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:53:22: missed: couldn't vectorize loop
PULSE.c:53:22: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:47:6: note: vectorized 1 loops in function.
PULSE.c:48:1: missed: statement clobbers memory: saved_stack.7_39 = __builtin_stack_save ();
PULSE.c:49:46: missed: statement clobbers memory: _1 = PULSE_GetLossFunctionPtr (loss_function_40(D));
PULSE.c:60:10: missed: statement clobbers memory: random.4_46 = __builtin_alloca_with_align (_4, 32);
PULSE.c:34:8: missed: statement clobbers memory: _61 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_62);
PULSE.c:39:29: missed: statement clobbers memory: _69 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _72 = rand ();
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_98, inputs_93, _97);
PULSE.c:8:2: missed: statement clobbers memory: _99 (layer_94);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_105, _104, _103);
PULSE.c:67:11: missed: statement clobbers memory: loss_55 = PULSE_GetLoss_42 (_17, _16, _12, _11);
PULSE.c:22:2: missed: statement clobbers memory: _86 (output_34);
PULSE.c:22:2: missed: statement clobbers memory: _108 (_87);
PULSE.c:22:2: missed: statement clobbers memory: _115 (_109);
PULSE.c:22:2: missed: statement clobbers memory: _122 (_116);
PULSE.c:22:2: missed: statement clobbers memory: _129 (_123);
PULSE.c:22:2: missed: statement clobbers memory: _136 (_130);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_137);
PULSE.c:26:3: missed: statement clobbers memory: memset (_142, 0, _140);
PULSE.c:26:3: missed: statement clobbers memory: memset (_135, 0, _133);
PULSE.c:26:3: missed: statement clobbers memory: memset (_128, 0, _126);
PULSE.c:26:3: missed: statement clobbers memory: memset (_121, 0, _119);
PULSE.c:26:3: missed: statement clobbers memory: memset (_114, 0, _112);
PULSE.c:26:3: missed: statement clobbers memory: memset (_92, 0, _90);
PULSE.c:75:6: missed: statement clobbers memory: _21 (current_191, args);
PULSE.c:81:4: missed: statement clobbers memory: printf ("Epoch: %d | Item: %d | Loss: %.10f | Batch Loss: %.10f\r", i_188, j_189, _25, _24);
PULSE.c:47:6: missed: statement clobbers memory: __builtin_stack_restore (saved_stack.7_39);
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4DI
PULSE.c:47:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:91:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:91:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:98:20: missed: couldn't vectorize loop
PULSE.c:98:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:98:20: missed: couldn't vectorize loop
PULSE.c:98:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:93:6: note: vectorized 0 loops in function.
PULSE.c:97:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:104:62: missed: not vectorized: more than one data ref in stmt: D.6452 = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.104_29];
PULSE.c:104:17: missed: statement clobbers memory: *_3 = PULSE_CreateDenseLayer (D.6452);
PULSE.c:104:62: missed: not vectorized: more than one data ref in stmt: D.6452 = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.104_47];
PULSE.c:104:17: missed: statement clobbers memory: *_32 = PULSE_CreateDenseLayer (D.6452);
PULSE.c:110:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:111:1: note: ***** Analysis failed with vector mode V8SI
PULSE.c:111:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:116:8: missed: couldn't vectorize loop
PULSE.c:116:8: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:113:6: note: vectorized 0 loops in function.
PULSE.c:118:3: missed: statement clobbers memory: _1 (current_11);
PULSE.c:121:2: missed: statement clobbers memory: free (layer_4(D));
PULSE.c:122:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:122:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Loss.c:15:19: optimized: loop vectorized using 32 byte vectors
Loss.c:15:19: optimized:  loop versioned for vectorization because of possible aliasing
Loss.c:15:19: optimized: loop vectorized using 16 byte vectors
Loss.c:12:23: note: vectorized 1 loops in function.
Loss.c:12:23: note: ***** Analysis failed with vector mode V8SF
Loss.c:12:23: note: ***** The result for vector mode V32QI would be the same
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V16QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V16QI
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V8QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V8QI
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V4QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V4QI
Loss.c:7:19: missed: couldn't vectorize loop
Loss.c:8:38: missed: not vectorized: unsupported use in stmt.
Loss.c:4:23: note: vectorized 0 loops in function.
Loss.c:9:13: note: ***** Analysis failed with vector mode V8SF
Loss.c:9:13: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SF
Loss.c:23:2: note: ***** Analysis failed with vector mode VOID
Layer.c:7:34: missed: statement clobbers memory: _3 = aligned_alloc (64, _2);
Layer.c:8:35: missed: statement clobbers memory: _6 = aligned_alloc (64, _5);
Layer.c:9:34: missed: statement clobbers memory: _7 = aligned_alloc (64, _5);
Layer.c:14:19: missed: statement clobbers memory: _8 = PULSE_GetActivationFunctionPtr (activation_function_18(D));
Layer.c:19:9: note: ***** Analysis succeeded with vector mode V8SI
Layer.c:19:9: note: SLPing BB part
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x442c3c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: op template: MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: 	stmt 0 MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: 	stmt 1 MEM <unsigned int> [(struct PULSE_Layer *)&<retval> + 4B] = optimization_type_17(D);
Layer.c:19:9: note: 	children 0x442c448
Layer.c:19:9: note: node (external) 0x442c448 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ type_16(D), optimization_type_17(D) }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x442c4c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: op template: <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: 	stmt 0 <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: 	stmt 1 <retval>.n_outputs = n_outputs_12(D);
Layer.c:19:9: note: 	children 0x442c548
Layer.c:19:9: note: node (external) 0x442c548 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ n_inputs_11(D), n_outputs_12(D) }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x442c648 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: <retval>.inputs = _3;
Layer.c:19:9: note: 	stmt 0 <retval>.inputs = _3;
Layer.c:19:9: note: 	stmt 1 <retval>.outputs = _6;
Layer.c:19:9: note: 	children 0x442c6c8
Layer.c:19:9: note: node (external) 0x442c6c8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: 	stmt 0 _3 = aligned_alloc (64, _2);
Layer.c:19:9: note: 	stmt 1 _6 = aligned_alloc (64, _5);
Layer.c:19:9: note: 	children 0x442c748 0x442c7c8
Layer.c:19:9: note: node (constant) 0x442c748 (max_nunits=1, refcnt=1)
Layer.c:19:9: note: 	{ 64, 64 }
Layer.c:19:9: note: node 0x442c7c8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: patt_38 = n_inputs_11(D) w* 4;
Layer.c:19:9: note: 	stmt 0 patt_38 = n_inputs_11(D) w* 4;
Layer.c:19:9: note: 	stmt 1 patt_39 = n_outputs_12(D) w* 4;
Layer.c:19:9: note: 	children 0x442c848 0x442c8c8
Layer.c:19:9: note: node (external) 0x442c848 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ n_inputs_11(D), n_outputs_12(D) }
Layer.c:19:9: note: node (constant) 0x442c8c8 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ 4, 4 }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x442c9c8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Layer.c:19:9: note: op template: <retval>.feed = feed_20(D);
Layer.c:19:9: note: 	stmt 0 <retval>.feed = feed_20(D);
Layer.c:19:9: note: 	stmt 1 <retval>.back = back_21(D);
Layer.c:19:9: note: 	stmt 2 <retval>.fix = fix_22(D);
Layer.c:19:9: note: 	stmt 3 <retval>.destroy = destroy_23(D);
Layer.c:19:9: note: 	children 0x442ca48
Layer.c:19:9: note: node (external) 0x442ca48 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Layer.c:19:9: note: 	{ feed_20(D), back_21(D), fix_22(D), destroy_23(D) }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 64
  Scalar cost: 64
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x442cac8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: <retval>.parent = 0B;
Layer.c:19:9: note: 	stmt 0 <retval>.parent = 0B;
Layer.c:19:9: note: 	stmt 1 <retval>.child = 0B;
Layer.c:19:9: note: 	children 0x442cb48
Layer.c:19:9: note: node (constant) 0x442cb48 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: 	{ 0B, 0B }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Layer.c:19:9: note: Basic block will be vectorized using SLP
Layer.c:19:9: optimized: basic block part vectorized using 8 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x442c3c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: op template: MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: 	stmt 0 MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: 	stmt 1 MEM <unsigned int> [(struct PULSE_Layer *)&<retval> + 4B] = optimization_type_17(D);
Layer.c:19:9: note: 	children 0x442c448
Layer.c:19:9: note: node (external) 0x442c448 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ type_16(D), optimization_type_17(D) }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: vect_is_simple_use: operand optimization_type_17(D), type of def: external
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: MEM <unsigned int> [(struct PULSE_Layer *)&<retval>]
Layer.c:19:9: note: created &MEM <unsigned int> [(struct PULSE_Layer *)&<retval>]
Layer.c:19:9: note: add new stmt: MEM <vector(2) unsigned int> [(struct PULSE_Layer *)&<retval>] = _40;
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: optimized: basic block part vectorized using 8 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x442c4c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: op template: <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: 	stmt 0 <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: 	stmt 1 <retval>.n_outputs = n_outputs_12(D);
Layer.c:19:9: note: 	children 0x442c548
Layer.c:19:9: note: node (external) 0x442c548 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ n_inputs_11(D), n_outputs_12(D) }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: vect_is_simple_use: operand n_outputs_12(D), type of def: external
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: <retval>.n_inputs
Layer.c:19:9: note: created &<retval>.n_inputs
Layer.c:19:9: note: add new stmt: MEM <vector(2) unsigned int> [(unsigned int *)&<retval> + 128B] = _42;
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: optimized: basic block part vectorized using 16 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x442c648 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: <retval>.inputs = _3;
Layer.c:19:9: note: 	stmt 0 <retval>.inputs = _3;
Layer.c:19:9: note: 	stmt 1 <retval>.outputs = _6;
Layer.c:19:9: note: 	children 0x442c6c8
Layer.c:19:9: note: node (external) 0x442c6c8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: 	stmt 0 _3 = aligned_alloc (64, _2);
Layer.c:19:9: note: 	stmt 1 _6 = aligned_alloc (64, _5);
Layer.c:19:9: note: 	children 0x442c748 0x442c7c8
Layer.c:19:9: note: node (constant) 0x442c748 (max_nunits=1, refcnt=1)
Layer.c:19:9: note: 	{ 64, 64 }
Layer.c:19:9: note: node 0x442c7c8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: patt_38 = n_inputs_11(D) w* 4;
Layer.c:19:9: note: 	stmt 0 patt_38 = n_inputs_11(D) w* 4;
Layer.c:19:9: note: 	stmt 1 patt_39 = n_outputs_12(D) w* 4;
Layer.c:19:9: note: 	children 0x442c848 0x442c8c8
Layer.c:19:9: note: node (external) 0x442c848 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ n_inputs_11(D), n_outputs_12(D) }
Layer.c:19:9: note: node (constant) 0x442c8c8 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ 4, 4 }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: <retval>.inputs = _3;
Layer.c:19:9: note: vect_is_simple_use: operand aligned_alloc (64, _5), type of def: internal
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.inputs
Layer.c:19:9: note: created &<retval>.inputs
Layer.c:19:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval> + 8B] = _46;
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: optimized: basic block part vectorized using 32 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x442c9c8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Layer.c:19:9: note: op template: <retval>.feed = feed_20(D);
Layer.c:19:9: note: 	stmt 0 <retval>.feed = feed_20(D);
Layer.c:19:9: note: 	stmt 1 <retval>.back = back_21(D);
Layer.c:19:9: note: 	stmt 2 <retval>.fix = fix_22(D);
Layer.c:19:9: note: 	stmt 3 <retval>.destroy = destroy_23(D);
Layer.c:19:9: note: 	children 0x442ca48
Layer.c:19:9: note: node (external) 0x442ca48 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Layer.c:19:9: note: 	{ feed_20(D), back_21(D), fix_22(D), destroy_23(D) }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: <retval>.feed = feed_20(D);
Layer.c:19:9: note: vect_is_simple_use: operand back_21(D), type of def: external
Layer.c:19:9: note: vect_is_simple_use: operand fix_22(D), type of def: external
Layer.c:19:9: note: vect_is_simple_use: operand destroy_23(D), type of def: external
Layer.c:19:9: note: conflicting alias set types.
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(4) long unsigned int  vectorizing a pointer ref: <retval>.feed
Layer.c:19:9: note: created &<retval>.feed
Layer.c:19:9: note: add new stmt: MEM <vector(4) long unsigned int> [(void *)&<retval> + 32B] = _52;
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: optimized: basic block part vectorized using 16 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x442cac8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: <retval>.parent = 0B;
Layer.c:19:9: note: 	stmt 0 <retval>.parent = 0B;
Layer.c:19:9: note: 	stmt 1 <retval>.child = 0B;
Layer.c:19:9: note: 	children 0x442cb48
Layer.c:19:9: note: node (constant) 0x442cb48 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: 	{ 0B, 0B }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: <retval>.parent = 0B;
Layer.c:19:9: note: vect_is_simple_use: operand 0B, type of def: constant
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.parent
Layer.c:19:9: note: created &<retval>.parent
Layer.c:19:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer * *)&<retval> + 72B] = { 0, 0 };
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: note: ***** The result for vector mode V32QI would be the same
Layer.c:24:2: missed: statement clobbers memory: free (_1);
Layer.c:25:2: missed: statement clobbers memory: free (_2);
Layer.c:26:2: missed: statement clobbers memory: free (_3);
Layer.c:27:1: note: ***** Analysis failed with vector mode V4DI
Layer.c:27:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _28 (pretmp_91, _51, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:19: missed: not vectorized: no vectype for stmt: _22 = *_21;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:170:10: optimized: loop vectorized using 32 byte vectors
Dense.c:170:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:170:10: optimized: loop vectorized using 16 byte vectors
Dense.c:161:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
Dense.c:154:10: optimized: loop vectorized using 32 byte vectors
Dense.c:154:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:154:10: optimized: loop vectorized using 16 byte vectors
Dense.c:145:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
Dense.c:136:13: note: vectorized 2 loops in function.
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_28, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
Dense.c:136:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:136:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:105:24: missed: couldn't vectorize loop
Dense.c:105:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:108:17: missed: couldn't vectorize loop
Dense.c:108:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:122:24: missed: couldn't vectorize loop
Dense.c:122:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:125:17: missed: couldn't vectorize loop
Dense.c:125:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:90:15: missed: couldn't vectorize loop
Dense.c:90:15: missed: not vectorized: number of iterations cannot be computed.
Dense.c:83:13: note: vectorized 0 loops in function.
Dense.c:86:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:110:17: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:110:17: note: SLPing BB part
Dense.c:127:17: note: Costing subgraph: 
Dense.c:127:17: note: node 0x8390428 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:127:17: note: op template: _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 0 _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 1 _198 = (sizetype) wi_137;
Dense.c:127:17: note: 	children 0x8390528
Dense.c:127:17: note: node (external) 0x8390528 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:127:17: note: 	{ j_135, wi_137 }
Dense.c:127:17: note: Cost model analysis: 
Dense.c:127:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:127:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:127:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: Costing subgraph: 
Dense.c:110:17: note: node 0x8390628 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:110:17: note: op template: _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 0 _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 1 _217 = (sizetype) wi_136;
Dense.c:110:17: note: 	children 0x8390728
Dense.c:110:17: note: node (external) 0x8390728 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:110:17: note: 	{ j_134, wi_136 }
Dense.c:110:17: note: Cost model analysis: 
Dense.c:110:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:110:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:110:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: ***** The result for vector mode V32QI would be the same
Dense.c:110:17: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:110:17: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:110:17: note: SLPing BB part
Dense.c:127:17: note: Costing subgraph: 
Dense.c:127:17: note: node 0x8390628 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:127:17: note: op template: _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 0 _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 1 _198 = (sizetype) wi_137;
Dense.c:127:17: note: 	children 0x8390428
Dense.c:127:17: note: node (external) 0x8390428 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:127:17: note: 	{ j_135, wi_137 }
Dense.c:127:17: note: Cost model analysis: 
Dense.c:127:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:127:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:127:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: Costing subgraph: 
Dense.c:110:17: note: node 0x83905a8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:110:17: note: op template: _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 0 _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 1 _217 = (sizetype) wi_136;
Dense.c:110:17: note: 	children 0x83906a8
Dense.c:110:17: note: node (external) 0x83906a8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:110:17: note: 	{ j_134, wi_136 }
Dense.c:110:17: note: Cost model analysis: 
Dense.c:110:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:110:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:110:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:105:51: note: ***** Analysis failed with vector mode V8QI
Dense.c:105:51: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:105:51: note: ***** Analysis failed with vector mode V4QI
Dense.c:62:15: missed: couldn't vectorize loop
Dense.c:62:15: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:75:11: optimized: loop vectorized using 32 byte vectors
Dense.c:75:11: optimized: loop vectorized using 16 byte vectors
Dense.c:66:11: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:881:10: missed: not vectorized: no vectype for stmt: _53 = MEM[(__m256 * {ref-all})w_ptr_82];
 scalar_type: __m256
Dense.c:54:13: note: vectorized 1 loops in function.
Dense.c:57:2: missed: statement clobbers memory: memcpy (_4, dense$baiases_63, _3);
Dense.c:79:2: missed: statement clobbers memory: _27 (pretmp_163, _68, 0);
Dense.c:54:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:54:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:182:2: missed: statement clobbers memory: free (dense$weights_7);
Dense.c:183:2: missed: statement clobbers memory: free (dense$baiases_8);
Dense.c:184:2: missed: statement clobbers memory: free (dense$gradients_10);
Dense.c:185:2: missed: statement clobbers memory: free (dense$deltas_9);
Dense.c:186:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:186:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:200:19: missed: couldn't vectorize loop
Dense.c:200:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:190:13: note: vectorized 0 loops in function.
Dense.c:194:35: missed: statement clobbers memory: _5 = aligned_alloc (64, _4);
Dense.c:195:37: missed: statement clobbers memory: _6 = aligned_alloc (64, _4);
Dense.c:196:35: missed: statement clobbers memory: _8 = aligned_alloc (64, _7);
Dense.c:197:34: missed: statement clobbers memory: _9 = aligned_alloc (64, _7);
Dense.c:198:35: missed: statement clobbers memory: _10 = aligned_alloc (64, _7);
Dense.c:201:38: missed: statement clobbers memory: _11 = rand ();
Dense.c:201:72: missed: statement clobbers memory: _57 = sqrt (_18);
Dense.c:213:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:214:4: missed: statement clobbers memory: exit (1);
Dense.c:210:4: missed: statement clobbers memory: layer = PULSE_CreateLayer (args$n_inputs_36, args$n_outputs_47, 0, args$8_48, _SIMD_FeedDense, _SIMD_BackDense, _SIMD_FixDense, _DestroyDense, 1); [return slot optimization]
Dense.c:207:12: missed: statement clobbers memory: layer = PULSE_CreateLayer (args$n_inputs_36, args$n_outputs_47, 0, args$8_48, _FeedDense, _BackDense, _FixDense, _DestroyDense, 0); [return slot optimization]
Dense.c:220:9: missed: not vectorized: more than one data ref in stmt: <retval> = layer;
Dense.c:220:9: note: ***** Analysis succeeded with vector mode V8SI
Dense.c:220:9: note: SLPing BB part
Dense.c:220:9: note: Costing subgraph: 
Dense.c:220:9: note: node 0x8248458 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:220:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = _5;
Dense.c:220:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = _5;
Dense.c:220:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&layer + 96B] = _8;
Dense.c:220:9: note: 	stmt 2 MEM <float *> [(struct PULSE_Layer *)&layer + 104B] = _9;
Dense.c:220:9: note: 	stmt 3 MEM <float *> [(struct PULSE_Layer *)&layer + 112B] = _10;
Dense.c:220:9: note: 	children 0x82484d8
Dense.c:220:9: note: node (external) 0x82484d8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:220:9: note: 	stmt 0 _5 = aligned_alloc (64, _4);
Dense.c:220:9: note: 	stmt 1 _8 = aligned_alloc (64, _7);
Dense.c:220:9: note: 	stmt 2 _9 = aligned_alloc (64, _7);
Dense.c:220:9: note: 	stmt 3 _10 = aligned_alloc (64, _7);
Dense.c:220:9: note: 	children 0x8248558 0x8248658
Dense.c:220:9: note: node (constant) 0x8248558 (max_nunits=1, refcnt=1)
Dense.c:220:9: note: 	{ 64, 64, 64, 64 }
Dense.c:220:9: note: node (external) 0x8248658 (max_nunits=1, refcnt=1)
Dense.c:220:9: note: 	{ _4, _7, _7, _7 }
Dense.c:220:9: note: Cost model analysis: 
Dense.c:220:9: note: Cost model analysis for part in loop 0:
  Vector cost: 64
  Scalar cost: 64
Dense.c:220:9: note: Basic block will be vectorized using SLP
Dense.c:220:9: optimized: basic block part vectorized using 32 byte vectors
Dense.c:220:9: note: Vectorizing SLP tree:
Dense.c:220:9: note: node 0x8248458 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:220:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = _5;
Dense.c:220:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = _5;
Dense.c:220:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&layer + 96B] = _8;
Dense.c:220:9: note: 	stmt 2 MEM <float *> [(struct PULSE_Layer *)&layer + 104B] = _9;
Dense.c:220:9: note: 	stmt 3 MEM <float *> [(struct PULSE_Layer *)&layer + 112B] = _10;
Dense.c:220:9: note: 	children 0x82484d8
Dense.c:220:9: note: node (external) 0x82484d8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:220:9: note: 	stmt 0 _5 = aligned_alloc (64, _4);
Dense.c:220:9: note: 	stmt 1 _8 = aligned_alloc (64, _7);
Dense.c:220:9: note: 	stmt 2 _9 = aligned_alloc (64, _7);
Dense.c:220:9: note: 	stmt 3 _10 = aligned_alloc (64, _7);
Dense.c:220:9: note: 	children 0x8248558 0x8248658
Dense.c:220:9: note: node (constant) 0x8248558 (max_nunits=1, refcnt=1)
Dense.c:220:9: note: 	{ 64, 64, 64, 64 }
Dense.c:220:9: note: node (external) 0x8248658 (max_nunits=1, refcnt=1)
Dense.c:220:9: note: 	{ _4, _7, _7, _7 }
Dense.c:220:9: note: ------>vectorizing SLP node starting from: MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = _5;
Dense.c:220:9: note: vect_is_simple_use: operand aligned_alloc (64, _7), type of def: internal
Dense.c:220:9: note: vect_is_simple_use: operand aligned_alloc (64, _7), type of def: internal
Dense.c:220:9: note: vect_is_simple_use: operand aligned_alloc (64, _7), type of def: internal
Dense.c:220:9: note: transform store. ncopies = 1
Dense.c:220:9: note: create vector_type-pointer variable to type: vector(4) long unsigned int  vectorizing a pointer ref: MEM <float *> [(struct PULSE_Layer *)&layer + 88B]
Dense.c:220:9: note: created &MEM <float *> [(struct PULSE_Layer *)&layer + 88B]
Dense.c:220:9: note: add new stmt: MEM <vector(4) long unsigned int> [(struct PULSE_Layer *)&layer + 88B] = _55;
Dense.c:220:9: note: vectorizing stmts using SLP.
Dense.c:220:9: note: ***** The result for vector mode V32QI would be the same
Activations.c:17:19: optimized: loop vectorized using 32 byte vectors
Activations.c:17:19: optimized: loop vectorized using 16 byte vectors
Activations.c:17:19: optimized: loop vectorized using 32 byte vectors
Activations.c:17:19: optimized: loop vectorized using 16 byte vectors
Activations.c:14:13: note: vectorized 2 loops in function.
Activations.c:14:13: note: ***** Analysis failed with vector mode V8SF
Activations.c:14:13: note: ***** The result for vector mode V32QI would be the same
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V16QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V16QI
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V8QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V8QI
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V4QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V4QI
Activations.c:28:19: missed: couldn't vectorize loop
Activations.c:28:19: missed: not vectorized: unsupported control flow in loop.
Activations.c:28:19: optimized: loop vectorized using 32 byte vectors
Activations.c:28:19: optimized: loop vectorized using 16 byte vectors
Activations.c:25:13: note: vectorized 1 loops in function.
Activations.c:25:13: note: ***** Analysis failed with vector mode V8SF
Activations.c:25:13: note: ***** The result for vector mode V32QI would be the same
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V16QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V16QI
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V8QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V8QI
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V4QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V4QI
Activations.c:38:1: note: ***** Analysis failed with vector mode VOID
Activations.c:5:19: missed: couldn't vectorize loop
Activations.c:9:21: missed: statement clobbers memory: _36 = expf (_18);
Activations.c:5:19: missed: couldn't vectorize loop
Activations.c:9:21: missed: statement clobbers memory: _12 = expf (_11);
Activations.c:3:13: note: vectorized 0 loops in function.
Activations.c:9:21: missed: statement clobbers memory: _12 = expf (_11);
Activations.c:9:21: missed: statement clobbers memory: _36 = expf (_18);
Activations.c:11:1: note: ***** Analysis failed with vector mode V8SF
Activations.c:11:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SF
Activations.c:44:2: note: ***** Analysis failed with vector mode VOID
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:4:18: note: vectorized 0 loops in function.
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_4, inputs_15, _3);
PULSE.c:8:2: missed: statement clobbers memory: _5 (layer_17);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_12, pretmp_26, _9);
PULSE.c:13:28: note: ***** Analysis failed with vector mode V8SI
PULSE.c:13:28: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:22:2: missed: statement clobbers memory: _1 (layer_10(D));
PULSE.c:22:2: missed: statement clobbers memory: _14 (_2);
PULSE.c:22:2: missed: statement clobbers memory: _21 (_15);
PULSE.c:22:2: missed: statement clobbers memory: _28 (_22);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_29);
PULSE.c:26:3: missed: statement clobbers memory: memset (_34, 0, _32);
PULSE.c:26:3: missed: statement clobbers memory: memset (_27, 0, _25);
PULSE.c:26:3: missed: statement clobbers memory: memset (_20, 0, _18);
PULSE.c:26:3: missed: statement clobbers memory: memset (_7, 0, _5);
PULSE.c:28:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:28:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:32:6: note: vectorized 1 loops in function.
PULSE.c:34:8: missed: statement clobbers memory: _1 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_2);
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _9 = rand ();
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8SI
PULSE.c:32:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:61:16: missed: couldn't vectorize loop
PULSE.c:61:16: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:64:17: missed: couldn't vectorize loop
PULSE.c:64:17: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:73:19: missed: couldn't vectorize loop
PULSE.c:73:19: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _69 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:53:22: missed: couldn't vectorize loop
PULSE.c:53:22: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:47:6: note: vectorized 1 loops in function.
PULSE.c:48:1: missed: statement clobbers memory: saved_stack.7_39 = __builtin_stack_save ();
PULSE.c:49:46: missed: statement clobbers memory: _1 = PULSE_GetLossFunctionPtr (loss_function_40(D));
PULSE.c:60:10: missed: statement clobbers memory: random.4_46 = __builtin_alloca_with_align (_4, 32);
PULSE.c:34:8: missed: statement clobbers memory: _61 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_62);
PULSE.c:39:29: missed: statement clobbers memory: _69 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _72 = rand ();
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_98, inputs_93, _97);
PULSE.c:8:2: missed: statement clobbers memory: _99 (layer_94);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_105, _104, _103);
PULSE.c:67:11: missed: statement clobbers memory: loss_55 = PULSE_GetLoss_42 (_17, _16, _12, _11);
PULSE.c:22:2: missed: statement clobbers memory: _86 (output_34);
PULSE.c:22:2: missed: statement clobbers memory: _108 (_87);
PULSE.c:22:2: missed: statement clobbers memory: _115 (_109);
PULSE.c:22:2: missed: statement clobbers memory: _122 (_116);
PULSE.c:22:2: missed: statement clobbers memory: _129 (_123);
PULSE.c:22:2: missed: statement clobbers memory: _136 (_130);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_137);
PULSE.c:26:3: missed: statement clobbers memory: memset (_142, 0, _140);
PULSE.c:26:3: missed: statement clobbers memory: memset (_135, 0, _133);
PULSE.c:26:3: missed: statement clobbers memory: memset (_128, 0, _126);
PULSE.c:26:3: missed: statement clobbers memory: memset (_121, 0, _119);
PULSE.c:26:3: missed: statement clobbers memory: memset (_114, 0, _112);
PULSE.c:26:3: missed: statement clobbers memory: memset (_92, 0, _90);
PULSE.c:75:6: missed: statement clobbers memory: _21 (current_191, args);
PULSE.c:81:4: missed: statement clobbers memory: printf ("Epoch: %d | Item: %d | Loss: %.10f | Batch Loss: %.10f\r", i_188, j_189, _25, _24);
PULSE.c:47:6: missed: statement clobbers memory: __builtin_stack_restore (saved_stack.7_39);
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4DI
PULSE.c:47:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:91:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:91:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:98:20: missed: couldn't vectorize loop
PULSE.c:98:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:98:20: missed: couldn't vectorize loop
PULSE.c:98:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:93:6: note: vectorized 0 loops in function.
PULSE.c:97:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:104:62: missed: not vectorized: more than one data ref in stmt: D.6452 = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.104_29];
PULSE.c:104:17: missed: statement clobbers memory: *_3 = PULSE_CreateDenseLayer (D.6452);
PULSE.c:104:62: missed: not vectorized: more than one data ref in stmt: D.6452 = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.104_47];
PULSE.c:104:17: missed: statement clobbers memory: *_32 = PULSE_CreateDenseLayer (D.6452);
PULSE.c:110:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:111:1: note: ***** Analysis failed with vector mode V8SI
PULSE.c:111:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:116:8: missed: couldn't vectorize loop
PULSE.c:116:8: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:113:6: note: vectorized 0 loops in function.
PULSE.c:118:3: missed: statement clobbers memory: _1 (current_10);
PULSE.c:121:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:121:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Loss.c:15:19: optimized: loop vectorized using 32 byte vectors
Loss.c:15:19: optimized:  loop versioned for vectorization because of possible aliasing
Loss.c:15:19: optimized: loop vectorized using 16 byte vectors
Loss.c:12:23: note: vectorized 1 loops in function.
Loss.c:12:23: note: ***** Analysis failed with vector mode V8SF
Loss.c:12:23: note: ***** The result for vector mode V32QI would be the same
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V16QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V16QI
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V8QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V8QI
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V4QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V4QI
Loss.c:7:19: missed: couldn't vectorize loop
Loss.c:8:38: missed: not vectorized: unsupported use in stmt.
Loss.c:4:23: note: vectorized 0 loops in function.
Loss.c:9:13: note: ***** Analysis failed with vector mode V8SF
Loss.c:9:13: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SF
Loss.c:23:2: note: ***** Analysis failed with vector mode VOID
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _28 (pretmp_91, _51, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:19: missed: not vectorized: no vectype for stmt: _22 = *_21;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:170:10: optimized: loop vectorized using 32 byte vectors
Dense.c:170:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:170:10: optimized: loop vectorized using 16 byte vectors
Dense.c:161:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
Dense.c:154:10: optimized: loop vectorized using 32 byte vectors
Dense.c:154:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:154:10: optimized: loop vectorized using 16 byte vectors
Dense.c:145:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
Dense.c:136:13: note: vectorized 2 loops in function.
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_28, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
Dense.c:136:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:136:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:105:24: missed: couldn't vectorize loop
Dense.c:105:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:108:17: missed: couldn't vectorize loop
Dense.c:108:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:122:24: missed: couldn't vectorize loop
Dense.c:122:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:125:17: missed: couldn't vectorize loop
Dense.c:125:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:90:15: missed: couldn't vectorize loop
Dense.c:90:15: missed: not vectorized: number of iterations cannot be computed.
Dense.c:83:13: note: vectorized 0 loops in function.
Dense.c:86:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:110:17: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:110:17: note: SLPing BB part
Dense.c:127:17: note: Costing subgraph: 
Dense.c:127:17: note: node 0x8c9e3e8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:127:17: note: op template: _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 0 _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 1 _198 = (sizetype) wi_137;
Dense.c:127:17: note: 	children 0x8c9e4e8
Dense.c:127:17: note: node (external) 0x8c9e4e8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:127:17: note: 	{ j_135, wi_137 }
Dense.c:127:17: note: Cost model analysis: 
Dense.c:127:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:127:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:127:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: Costing subgraph: 
Dense.c:110:17: note: node 0x8c9e5e8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:110:17: note: op template: _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 0 _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 1 _217 = (sizetype) wi_136;
Dense.c:110:17: note: 	children 0x8c9e6e8
Dense.c:110:17: note: node (external) 0x8c9e6e8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:110:17: note: 	{ j_134, wi_136 }
Dense.c:110:17: note: Cost model analysis: 
Dense.c:110:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:110:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:110:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: ***** The result for vector mode V32QI would be the same
Dense.c:110:17: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:110:17: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:110:17: note: SLPing BB part
Dense.c:127:17: note: Costing subgraph: 
Dense.c:127:17: note: node 0x8c9e5e8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:127:17: note: op template: _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 0 _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 1 _198 = (sizetype) wi_137;
Dense.c:127:17: note: 	children 0x8c9e3e8
Dense.c:127:17: note: node (external) 0x8c9e3e8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:127:17: note: 	{ j_135, wi_137 }
Dense.c:127:17: note: Cost model analysis: 
Dense.c:127:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:127:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:127:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: Costing subgraph: 
Dense.c:110:17: note: node 0x8c9e568 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:110:17: note: op template: _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 0 _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 1 _217 = (sizetype) wi_136;
Dense.c:110:17: note: 	children 0x8c9e668
Dense.c:110:17: note: node (external) 0x8c9e668 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:110:17: note: 	{ j_134, wi_136 }
Dense.c:110:17: note: Cost model analysis: 
Dense.c:110:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:110:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:110:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:105:51: note: ***** Analysis failed with vector mode V8QI
Dense.c:105:51: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:105:51: note: ***** Analysis failed with vector mode V4QI
Dense.c:62:15: missed: couldn't vectorize loop
Dense.c:62:15: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:75:11: optimized: loop vectorized using 32 byte vectors
Dense.c:75:11: optimized: loop vectorized using 16 byte vectors
Dense.c:66:11: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:881:10: missed: not vectorized: no vectype for stmt: _53 = MEM[(__m256 * {ref-all})w_ptr_82];
 scalar_type: __m256
Dense.c:54:13: note: vectorized 1 loops in function.
Dense.c:57:2: missed: statement clobbers memory: memcpy (_4, dense$baiases_63, _3);
Dense.c:79:2: missed: statement clobbers memory: _27 (pretmp_163, _68, 0);
Dense.c:54:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:54:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:182:2: missed: statement clobbers memory: free (dense$weights_7);
Dense.c:183:2: missed: statement clobbers memory: free (dense$baiases_8);
Dense.c:184:2: missed: statement clobbers memory: free (dense$gradients_10);
Dense.c:185:2: missed: statement clobbers memory: free (dense$deltas_9);
Dense.c:186:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:186:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:200:19: missed: couldn't vectorize loop
Dense.c:200:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:190:13: note: vectorized 0 loops in function.
Dense.c:194:35: missed: statement clobbers memory: _5 = aligned_alloc (64, _4);
Dense.c:195:37: missed: statement clobbers memory: _6 = aligned_alloc (64, _4);
Dense.c:196:35: missed: statement clobbers memory: _8 = aligned_alloc (64, _7);
Dense.c:197:34: missed: statement clobbers memory: _9 = aligned_alloc (64, _7);
Dense.c:198:35: missed: statement clobbers memory: _10 = aligned_alloc (64, _7);
Dense.c:201:38: missed: statement clobbers memory: _11 = rand ();
Dense.c:201:72: missed: statement clobbers memory: _57 = sqrt (_18);
Dense.c:213:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:214:4: missed: statement clobbers memory: exit (1);
Dense.c:210:4: missed: statement clobbers memory: layer = PULSE_CreateLayer (args$n_inputs_36, args$n_outputs_47, 0, args$8_48, _SIMD_FeedDense, _SIMD_BackDense, _SIMD_FixDense, _DestroyDense, 1); [return slot optimization]
Dense.c:207:12: missed: statement clobbers memory: layer = PULSE_CreateLayer (args$n_inputs_36, args$n_outputs_47, 0, args$8_48, _FeedDense, _BackDense, _FixDense, _DestroyDense, 0); [return slot optimization]
Dense.c:219:9: missed: not vectorized: more than one data ref in stmt: <retval> = layer;
Dense.c:219:9: note: ***** Analysis succeeded with vector mode V8SI
Dense.c:219:9: note: SLPing BB part
Dense.c:219:9: note: Costing subgraph: 
Dense.c:219:9: note: node 0x8b552c8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:219:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = _5;
Dense.c:219:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = _5;
Dense.c:219:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&layer + 96B] = _8;
Dense.c:219:9: note: 	stmt 2 MEM <float *> [(struct PULSE_Layer *)&layer + 104B] = _9;
Dense.c:219:9: note: 	stmt 3 MEM <float *> [(struct PULSE_Layer *)&layer + 112B] = _10;
Dense.c:219:9: note: 	children 0x8b55348
Dense.c:219:9: note: node (external) 0x8b55348 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:219:9: note: 	stmt 0 _5 = aligned_alloc (64, _4);
Dense.c:219:9: note: 	stmt 1 _8 = aligned_alloc (64, _7);
Dense.c:219:9: note: 	stmt 2 _9 = aligned_alloc (64, _7);
Dense.c:219:9: note: 	stmt 3 _10 = aligned_alloc (64, _7);
Dense.c:219:9: note: 	children 0x8b553c8 0x8b554c8
Dense.c:219:9: note: node (constant) 0x8b553c8 (max_nunits=1, refcnt=1)
Dense.c:219:9: note: 	{ 64, 64, 64, 64 }
Dense.c:219:9: note: node (external) 0x8b554c8 (max_nunits=1, refcnt=1)
Dense.c:219:9: note: 	{ _4, _7, _7, _7 }
Dense.c:219:9: note: Cost model analysis: 
Dense.c:219:9: note: Cost model analysis for part in loop 0:
  Vector cost: 64
  Scalar cost: 64
Dense.c:219:9: note: Basic block will be vectorized using SLP
Dense.c:219:9: optimized: basic block part vectorized using 32 byte vectors
Dense.c:219:9: note: Vectorizing SLP tree:
Dense.c:219:9: note: node 0x8b552c8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:219:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = _5;
Dense.c:219:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = _5;
Dense.c:219:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&layer + 96B] = _8;
Dense.c:219:9: note: 	stmt 2 MEM <float *> [(struct PULSE_Layer *)&layer + 104B] = _9;
Dense.c:219:9: note: 	stmt 3 MEM <float *> [(struct PULSE_Layer *)&layer + 112B] = _10;
Dense.c:219:9: note: 	children 0x8b55348
Dense.c:219:9: note: node (external) 0x8b55348 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:219:9: note: 	stmt 0 _5 = aligned_alloc (64, _4);
Dense.c:219:9: note: 	stmt 1 _8 = aligned_alloc (64, _7);
Dense.c:219:9: note: 	stmt 2 _9 = aligned_alloc (64, _7);
Dense.c:219:9: note: 	stmt 3 _10 = aligned_alloc (64, _7);
Dense.c:219:9: note: 	children 0x8b553c8 0x8b554c8
Dense.c:219:9: note: node (constant) 0x8b553c8 (max_nunits=1, refcnt=1)
Dense.c:219:9: note: 	{ 64, 64, 64, 64 }
Dense.c:219:9: note: node (external) 0x8b554c8 (max_nunits=1, refcnt=1)
Dense.c:219:9: note: 	{ _4, _7, _7, _7 }
Dense.c:219:9: note: ------>vectorizing SLP node starting from: MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = _5;
Dense.c:219:9: note: vect_is_simple_use: operand aligned_alloc (64, _7), type of def: internal
Dense.c:219:9: note: vect_is_simple_use: operand aligned_alloc (64, _7), type of def: internal
Dense.c:219:9: note: vect_is_simple_use: operand aligned_alloc (64, _7), type of def: internal
Dense.c:219:9: note: transform store. ncopies = 1
Dense.c:219:9: note: create vector_type-pointer variable to type: vector(4) long unsigned int  vectorizing a pointer ref: MEM <float *> [(struct PULSE_Layer *)&layer + 88B]
Dense.c:219:9: note: created &MEM <float *> [(struct PULSE_Layer *)&layer + 88B]
Dense.c:219:9: note: add new stmt: MEM <vector(4) long unsigned int> [(struct PULSE_Layer *)&layer + 88B] = _55;
Dense.c:219:9: note: vectorizing stmts using SLP.
Dense.c:219:9: note: ***** The result for vector mode V32QI would be the same
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:4:18: note: vectorized 0 loops in function.
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_4, inputs_15, _3);
PULSE.c:8:2: missed: statement clobbers memory: _5 (layer_17);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_12, pretmp_26, _9);
PULSE.c:13:28: note: ***** Analysis failed with vector mode V8SI
PULSE.c:13:28: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:22:2: missed: statement clobbers memory: _1 (layer_10(D));
PULSE.c:22:2: missed: statement clobbers memory: _14 (_2);
PULSE.c:22:2: missed: statement clobbers memory: _21 (_15);
PULSE.c:22:2: missed: statement clobbers memory: _28 (_22);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_29);
PULSE.c:26:3: missed: statement clobbers memory: memset (_34, 0, _32);
PULSE.c:26:3: missed: statement clobbers memory: memset (_27, 0, _25);
PULSE.c:26:3: missed: statement clobbers memory: memset (_20, 0, _18);
PULSE.c:26:3: missed: statement clobbers memory: memset (_7, 0, _5);
PULSE.c:28:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:28:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:32:6: note: vectorized 1 loops in function.
PULSE.c:34:8: missed: statement clobbers memory: _1 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_2);
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _9 = rand ();
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8SI
PULSE.c:32:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:61:16: missed: couldn't vectorize loop
PULSE.c:61:16: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:64:17: missed: couldn't vectorize loop
PULSE.c:64:17: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:73:19: missed: couldn't vectorize loop
PULSE.c:73:19: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _69 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:53:22: missed: couldn't vectorize loop
PULSE.c:53:22: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:47:6: note: vectorized 1 loops in function.
PULSE.c:48:1: missed: statement clobbers memory: saved_stack.7_39 = __builtin_stack_save ();
PULSE.c:49:46: missed: statement clobbers memory: _1 = PULSE_GetLossFunctionPtr (loss_function_40(D));
PULSE.c:60:10: missed: statement clobbers memory: random.4_46 = __builtin_alloca_with_align (_4, 32);
PULSE.c:34:8: missed: statement clobbers memory: _61 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_62);
PULSE.c:39:29: missed: statement clobbers memory: _69 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _72 = rand ();
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_98, inputs_93, _97);
PULSE.c:8:2: missed: statement clobbers memory: _99 (layer_94);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_105, _104, _103);
PULSE.c:67:11: missed: statement clobbers memory: loss_55 = PULSE_GetLoss_42 (_17, _16, _12, _11);
PULSE.c:22:2: missed: statement clobbers memory: _86 (output_34);
PULSE.c:22:2: missed: statement clobbers memory: _108 (_87);
PULSE.c:22:2: missed: statement clobbers memory: _115 (_109);
PULSE.c:22:2: missed: statement clobbers memory: _122 (_116);
PULSE.c:22:2: missed: statement clobbers memory: _129 (_123);
PULSE.c:22:2: missed: statement clobbers memory: _136 (_130);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_137);
PULSE.c:26:3: missed: statement clobbers memory: memset (_142, 0, _140);
PULSE.c:26:3: missed: statement clobbers memory: memset (_135, 0, _133);
PULSE.c:26:3: missed: statement clobbers memory: memset (_128, 0, _126);
PULSE.c:26:3: missed: statement clobbers memory: memset (_121, 0, _119);
PULSE.c:26:3: missed: statement clobbers memory: memset (_114, 0, _112);
PULSE.c:26:3: missed: statement clobbers memory: memset (_92, 0, _90);
PULSE.c:75:6: missed: statement clobbers memory: _21 (current_191, args);
PULSE.c:81:4: missed: statement clobbers memory: printf ("Epoch: %d | Item: %d | Loss: %.10f | Batch Loss: %.10f\r", i_188, j_189, _25, _24);
PULSE.c:47:6: missed: statement clobbers memory: __builtin_stack_restore (saved_stack.7_39);
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4DI
PULSE.c:47:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:91:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:91:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:119:20: missed: couldn't vectorize loop
PULSE.c:119:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:119:20: missed: couldn't vectorize loop
PULSE.c:119:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:102:20: missed: couldn't vectorize loop
PULSE.c:102:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:102:20: missed: couldn't vectorize loop
PULSE.c:102:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:93:13: note: vectorized 0 loops in function.
PULSE.c:96:39: missed: statement clobbers memory: layers_33 = malloc (_3);
PULSE.c:99:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:114:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:115:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:116:56: missed: statement clobbers memory: MODEL_PARAMETHERS_122 = malloc (0);
PULSE.c:114:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:115:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:116:56: missed: statement clobbers memory: MODEL_PARAMETHERS_38 = malloc (_11);
PULSE.c:125:62: missed: not vectorized: more than one data ref in stmt: D.6472 = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.106_66];
PULSE.c:125:17: missed: statement clobbers memory: *_14 = PULSE_CreateDenseLayer (D.6472); [return slot optimization]
PULSE.c:125:62: missed: not vectorized: more than one data ref in stmt: D.6472 = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.106_181];
PULSE.c:125:17: missed: statement clobbers memory: *_175 = PULSE_CreateDenseLayer (D.6472); [return slot optimization]
PULSE.c:131:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:132:9: note: ***** Analysis succeeded with vector mode V4DI
PULSE.c:132:9: note: SLPing BB part
PULSE.c:132:9: note: Costing subgraph: 
PULSE.c:132:9: note: node 0x4c0faf8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:132:9: note: op template: <retval>.layers = layers_33;
PULSE.c:132:9: note: 	stmt 0 <retval>.layers = layers_33;
PULSE.c:132:9: note: 	stmt 1 <retval>.paramethers = MODEL_PARAMETHERS_123;
PULSE.c:132:9: note: 	children 0x4c0fbf8
PULSE.c:132:9: note: node (external) 0x4c0fbf8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:132:9: note: 	{ layers_33, MODEL_PARAMETHERS_123 }
PULSE.c:132:9: note: Cost model analysis: 
PULSE.c:132:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:132:9: note: Costing subgraph: 
PULSE.c:132:9: note: node 0x4c0fc78 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:132:9: note: op template: <retval>.io = 0B;
PULSE.c:132:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:132:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:132:9: note: 	children 0x4c0fcf8
PULSE.c:132:9: note: node (constant) 0x4c0fcf8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:132:9: note: 	{ 0B, 0B }
PULSE.c:132:9: note: Cost model analysis: 
PULSE.c:132:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:132:9: note: Basic block will be vectorized using SLP
PULSE.c:132:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:132:9: note: Vectorizing SLP tree:
PULSE.c:132:9: note: node 0x4c0faf8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:132:9: note: op template: <retval>.layers = layers_33;
PULSE.c:132:9: note: 	stmt 0 <retval>.layers = layers_33;
PULSE.c:132:9: note: 	stmt 1 <retval>.paramethers = MODEL_PARAMETHERS_123;
PULSE.c:132:9: note: 	children 0x4c0fbf8
PULSE.c:132:9: note: node (external) 0x4c0fbf8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:132:9: note: 	{ layers_33, MODEL_PARAMETHERS_123 }
PULSE.c:132:9: note: ------>vectorizing SLP node starting from: <retval>.layers = layers_33;
PULSE.c:132:9: note: vect_is_simple_use: operand MODEL_PARAMETHERS_123 = PHI <MODEL_PARAMETHERS_38(37), MODEL_PARAMETHERS_122(13)>, type of def: internal
PULSE.c:132:9: note: conflicting alias set types.
PULSE.c:132:9: note: transform store. ncopies = 1
PULSE.c:132:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.layers
PULSE.c:132:9: note: created &<retval>
PULSE.c:132:9: note: add new stmt: MEM <vector(2) long unsigned int> [(void *)&<retval>] = _196;
PULSE.c:132:9: note: vectorizing stmts using SLP.
PULSE.c:132:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:132:9: note: Vectorizing SLP tree:
PULSE.c:132:9: note: node 0x4c0fc78 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:132:9: note: op template: <retval>.io = 0B;
PULSE.c:132:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:132:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:132:9: note: 	children 0x4c0fcf8
PULSE.c:132:9: note: node (constant) 0x4c0fcf8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:132:9: note: 	{ 0B, 0B }
PULSE.c:132:9: note: ------>vectorizing SLP node starting from: <retval>.io = 0B;
PULSE.c:132:9: note: vect_is_simple_use: operand 0B, type of def: constant
PULSE.c:132:9: note: transform store. ncopies = 1
PULSE.c:132:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.io
PULSE.c:132:9: note: created &<retval>.io
PULSE.c:132:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval> + 16B] = { 0, 0 };
PULSE.c:132:9: note: vectorizing stmts using SLP.
PULSE.c:132:9: note: ***** The result for vector mode V32QI would be the same
PULSE.c:138:8: missed: couldn't vectorize loop
PULSE.c:138:8: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:135:6: note: vectorized 0 loops in function.
PULSE.c:140:3: missed: statement clobbers memory: _1 (current_10);
PULSE.c:143:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:143:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:4:18: note: vectorized 0 loops in function.
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_4, inputs_15, _3);
PULSE.c:8:2: missed: statement clobbers memory: _5 (layer_17);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_12, pretmp_26, _9);
PULSE.c:13:28: note: ***** Analysis failed with vector mode V8SI
PULSE.c:13:28: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:22:2: missed: statement clobbers memory: _1 (layer_10(D));
PULSE.c:22:2: missed: statement clobbers memory: _14 (_2);
PULSE.c:22:2: missed: statement clobbers memory: _21 (_15);
PULSE.c:22:2: missed: statement clobbers memory: _28 (_22);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_29);
PULSE.c:26:3: missed: statement clobbers memory: memset (_34, 0, _32);
PULSE.c:26:3: missed: statement clobbers memory: memset (_27, 0, _25);
PULSE.c:26:3: missed: statement clobbers memory: memset (_20, 0, _18);
PULSE.c:26:3: missed: statement clobbers memory: memset (_7, 0, _5);
PULSE.c:28:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:28:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:32:6: note: vectorized 1 loops in function.
PULSE.c:34:8: missed: statement clobbers memory: _1 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_2);
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _9 = rand ();
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8SI
PULSE.c:32:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:61:16: missed: couldn't vectorize loop
PULSE.c:61:16: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:64:17: missed: couldn't vectorize loop
PULSE.c:64:17: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:73:19: missed: couldn't vectorize loop
PULSE.c:73:19: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _69 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:53:22: missed: couldn't vectorize loop
PULSE.c:53:22: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:47:6: note: vectorized 1 loops in function.
PULSE.c:48:1: missed: statement clobbers memory: saved_stack.7_39 = __builtin_stack_save ();
PULSE.c:49:46: missed: statement clobbers memory: _1 = PULSE_GetLossFunctionPtr (loss_function_40(D));
PULSE.c:60:10: missed: statement clobbers memory: random.4_46 = __builtin_alloca_with_align (_4, 32);
PULSE.c:34:8: missed: statement clobbers memory: _61 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_62);
PULSE.c:39:29: missed: statement clobbers memory: _69 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _72 = rand ();
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_98, inputs_93, _97);
PULSE.c:8:2: missed: statement clobbers memory: _99 (layer_94);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_105, _104, _103);
PULSE.c:67:11: missed: statement clobbers memory: loss_55 = PULSE_GetLoss_42 (_17, _16, _12, _11);
PULSE.c:22:2: missed: statement clobbers memory: _86 (output_34);
PULSE.c:22:2: missed: statement clobbers memory: _108 (_87);
PULSE.c:22:2: missed: statement clobbers memory: _115 (_109);
PULSE.c:22:2: missed: statement clobbers memory: _122 (_116);
PULSE.c:22:2: missed: statement clobbers memory: _129 (_123);
PULSE.c:22:2: missed: statement clobbers memory: _136 (_130);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_137);
PULSE.c:26:3: missed: statement clobbers memory: memset (_142, 0, _140);
PULSE.c:26:3: missed: statement clobbers memory: memset (_135, 0, _133);
PULSE.c:26:3: missed: statement clobbers memory: memset (_128, 0, _126);
PULSE.c:26:3: missed: statement clobbers memory: memset (_121, 0, _119);
PULSE.c:26:3: missed: statement clobbers memory: memset (_114, 0, _112);
PULSE.c:26:3: missed: statement clobbers memory: memset (_92, 0, _90);
PULSE.c:75:6: missed: statement clobbers memory: _21 (current_191, args);
PULSE.c:81:4: missed: statement clobbers memory: printf ("Epoch: %d | Item: %d | Loss: %.10f | Batch Loss: %.10f\r", i_188, j_189, _25, _24);
PULSE.c:47:6: missed: statement clobbers memory: __builtin_stack_restore (saved_stack.7_39);
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4DI
PULSE.c:47:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:91:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:91:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:119:20: missed: couldn't vectorize loop
PULSE.c:119:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:119:20: missed: couldn't vectorize loop
PULSE.c:119:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:102:20: missed: couldn't vectorize loop
PULSE.c:102:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:102:20: missed: couldn't vectorize loop
PULSE.c:102:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:93:13: note: vectorized 0 loops in function.
PULSE.c:96:39: missed: statement clobbers memory: layers_33 = malloc (_3);
PULSE.c:99:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:114:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:115:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:116:56: missed: statement clobbers memory: MODEL_PARAMETHERS_122 = malloc (0);
PULSE.c:114:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:115:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:116:56: missed: statement clobbers memory: MODEL_PARAMETHERS_38 = malloc (_11);
PULSE.c:125:62: missed: not vectorized: more than one data ref in stmt: D.6468 = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.106_66];
PULSE.c:125:17: missed: statement clobbers memory: *_14 = PULSE_CreateDenseLayer (D.6468); [return slot optimization]
PULSE.c:125:62: missed: not vectorized: more than one data ref in stmt: D.6468 = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.106_181];
PULSE.c:125:17: missed: statement clobbers memory: *_175 = PULSE_CreateDenseLayer (D.6468); [return slot optimization]
PULSE.c:131:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:132:9: note: ***** Analysis succeeded with vector mode V4DI
PULSE.c:132:9: note: SLPing BB part
PULSE.c:132:9: note: Costing subgraph: 
PULSE.c:132:9: note: node 0x4fd8b08 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:132:9: note: op template: <retval>.layers = layers_33;
PULSE.c:132:9: note: 	stmt 0 <retval>.layers = layers_33;
PULSE.c:132:9: note: 	stmt 1 <retval>.paramethers = MODEL_PARAMETHERS_123;
PULSE.c:132:9: note: 	children 0x4fd8c08
PULSE.c:132:9: note: node (external) 0x4fd8c08 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:132:9: note: 	{ layers_33, MODEL_PARAMETHERS_123 }
PULSE.c:132:9: note: Cost model analysis: 
PULSE.c:132:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:132:9: note: Costing subgraph: 
PULSE.c:132:9: note: node 0x4fd8c88 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:132:9: note: op template: <retval>.io = 0B;
PULSE.c:132:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:132:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:132:9: note: 	children 0x4fd8d08
PULSE.c:132:9: note: node (constant) 0x4fd8d08 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:132:9: note: 	{ 0B, 0B }
PULSE.c:132:9: note: Cost model analysis: 
PULSE.c:132:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:132:9: note: Basic block will be vectorized using SLP
PULSE.c:132:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:132:9: note: Vectorizing SLP tree:
PULSE.c:132:9: note: node 0x4fd8b08 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:132:9: note: op template: <retval>.layers = layers_33;
PULSE.c:132:9: note: 	stmt 0 <retval>.layers = layers_33;
PULSE.c:132:9: note: 	stmt 1 <retval>.paramethers = MODEL_PARAMETHERS_123;
PULSE.c:132:9: note: 	children 0x4fd8c08
PULSE.c:132:9: note: node (external) 0x4fd8c08 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:132:9: note: 	{ layers_33, MODEL_PARAMETHERS_123 }
PULSE.c:132:9: note: ------>vectorizing SLP node starting from: <retval>.layers = layers_33;
PULSE.c:132:9: note: vect_is_simple_use: operand MODEL_PARAMETHERS_123 = PHI <MODEL_PARAMETHERS_38(37), MODEL_PARAMETHERS_122(13)>, type of def: internal
PULSE.c:132:9: note: conflicting alias set types.
PULSE.c:132:9: note: transform store. ncopies = 1
PULSE.c:132:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.layers
PULSE.c:132:9: note: created &<retval>
PULSE.c:132:9: note: add new stmt: MEM <vector(2) long unsigned int> [(void *)&<retval>] = _196;
PULSE.c:132:9: note: vectorizing stmts using SLP.
PULSE.c:132:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:132:9: note: Vectorizing SLP tree:
PULSE.c:132:9: note: node 0x4fd8c88 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:132:9: note: op template: <retval>.io = 0B;
PULSE.c:132:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:132:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:132:9: note: 	children 0x4fd8d08
PULSE.c:132:9: note: node (constant) 0x4fd8d08 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:132:9: note: 	{ 0B, 0B }
PULSE.c:132:9: note: ------>vectorizing SLP node starting from: <retval>.io = 0B;
PULSE.c:132:9: note: vect_is_simple_use: operand 0B, type of def: constant
PULSE.c:132:9: note: transform store. ncopies = 1
PULSE.c:132:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.io
PULSE.c:132:9: note: created &<retval>.io
PULSE.c:132:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval> + 16B] = { 0, 0 };
PULSE.c:132:9: note: vectorizing stmts using SLP.
PULSE.c:132:9: note: ***** The result for vector mode V32QI would be the same
PULSE.c:144:1: note: ***** Analysis failed with vector mode VOID
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:4:18: note: vectorized 0 loops in function.
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_4, inputs_15, _3);
PULSE.c:8:2: missed: statement clobbers memory: _5 (layer_17);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_12, pretmp_26, _9);
PULSE.c:13:28: note: ***** Analysis failed with vector mode V8SI
PULSE.c:13:28: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:22:2: missed: statement clobbers memory: _1 (layer_10(D));
PULSE.c:22:2: missed: statement clobbers memory: _14 (_2);
PULSE.c:22:2: missed: statement clobbers memory: _21 (_15);
PULSE.c:22:2: missed: statement clobbers memory: _28 (_22);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_29);
PULSE.c:26:3: missed: statement clobbers memory: memset (_34, 0, _32);
PULSE.c:26:3: missed: statement clobbers memory: memset (_27, 0, _25);
PULSE.c:26:3: missed: statement clobbers memory: memset (_20, 0, _18);
PULSE.c:26:3: missed: statement clobbers memory: memset (_7, 0, _5);
PULSE.c:28:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:28:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:32:6: note: vectorized 1 loops in function.
PULSE.c:34:8: missed: statement clobbers memory: _1 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_2);
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _9 = rand ();
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8SI
PULSE.c:32:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:61:16: missed: couldn't vectorize loop
PULSE.c:61:16: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:64:17: missed: couldn't vectorize loop
PULSE.c:64:17: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:73:19: missed: couldn't vectorize loop
PULSE.c:73:19: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _69 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:53:22: missed: couldn't vectorize loop
PULSE.c:53:22: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:47:6: note: vectorized 1 loops in function.
PULSE.c:48:1: missed: statement clobbers memory: saved_stack.7_39 = __builtin_stack_save ();
PULSE.c:49:46: missed: statement clobbers memory: _1 = PULSE_GetLossFunctionPtr (loss_function_40(D));
PULSE.c:60:10: missed: statement clobbers memory: random.4_46 = __builtin_alloca_with_align (_4, 32);
PULSE.c:34:8: missed: statement clobbers memory: _61 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_62);
PULSE.c:39:29: missed: statement clobbers memory: _69 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _72 = rand ();
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_98, inputs_93, _97);
PULSE.c:8:2: missed: statement clobbers memory: _99 (layer_94);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_105, _104, _103);
PULSE.c:67:11: missed: statement clobbers memory: loss_55 = PULSE_GetLoss_42 (_17, _16, _12, _11);
PULSE.c:22:2: missed: statement clobbers memory: _86 (output_34);
PULSE.c:22:2: missed: statement clobbers memory: _108 (_87);
PULSE.c:22:2: missed: statement clobbers memory: _115 (_109);
PULSE.c:22:2: missed: statement clobbers memory: _122 (_116);
PULSE.c:22:2: missed: statement clobbers memory: _129 (_123);
PULSE.c:22:2: missed: statement clobbers memory: _136 (_130);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_137);
PULSE.c:26:3: missed: statement clobbers memory: memset (_142, 0, _140);
PULSE.c:26:3: missed: statement clobbers memory: memset (_135, 0, _133);
PULSE.c:26:3: missed: statement clobbers memory: memset (_128, 0, _126);
PULSE.c:26:3: missed: statement clobbers memory: memset (_121, 0, _119);
PULSE.c:26:3: missed: statement clobbers memory: memset (_114, 0, _112);
PULSE.c:26:3: missed: statement clobbers memory: memset (_92, 0, _90);
PULSE.c:75:6: missed: statement clobbers memory: _21 (current_191, args);
PULSE.c:81:4: missed: statement clobbers memory: printf ("Epoch: %d | Item: %d | Loss: %.10f | Batch Loss: %.10f\r", i_188, j_189, _25, _24);
PULSE.c:47:6: missed: statement clobbers memory: __builtin_stack_restore (saved_stack.7_39);
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4DI
PULSE.c:47:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:91:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:91:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:119:20: missed: couldn't vectorize loop
PULSE.c:119:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:119:20: missed: couldn't vectorize loop
PULSE.c:119:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:102:20: missed: couldn't vectorize loop
PULSE.c:102:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:102:20: missed: couldn't vectorize loop
PULSE.c:102:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:93:13: note: vectorized 0 loops in function.
PULSE.c:96:39: missed: statement clobbers memory: layers_33 = malloc (_3);
PULSE.c:99:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:114:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:115:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:116:44: missed: statement clobbers memory: MODEL_122 = malloc (0);
PULSE.c:114:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:115:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:116:44: missed: statement clobbers memory: MODEL_38 = malloc (_11);
PULSE.c:125:62: missed: not vectorized: more than one data ref in stmt: D.6468 = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.106_66];
PULSE.c:125:17: missed: statement clobbers memory: *_14 = PULSE_CreateDenseLayer (D.6468); [return slot optimization]
PULSE.c:125:62: missed: not vectorized: more than one data ref in stmt: D.6468 = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.106_181];
PULSE.c:125:17: missed: statement clobbers memory: *_175 = PULSE_CreateDenseLayer (D.6468); [return slot optimization]
PULSE.c:131:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:132:9: note: ***** Analysis succeeded with vector mode V4DI
PULSE.c:132:9: note: SLPing BB part
PULSE.c:132:9: note: Costing subgraph: 
PULSE.c:132:9: note: node 0x31adae8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:132:9: note: op template: <retval>.layers = layers_33;
PULSE.c:132:9: note: 	stmt 0 <retval>.layers = layers_33;
PULSE.c:132:9: note: 	stmt 1 <retval>.paramethers = MODEL_123;
PULSE.c:132:9: note: 	children 0x31adbe8
PULSE.c:132:9: note: node (external) 0x31adbe8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:132:9: note: 	{ layers_33, MODEL_123 }
PULSE.c:132:9: note: Cost model analysis: 
PULSE.c:132:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:132:9: note: Costing subgraph: 
PULSE.c:132:9: note: node 0x31adc68 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:132:9: note: op template: <retval>.io = 0B;
PULSE.c:132:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:132:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:132:9: note: 	children 0x31adce8
PULSE.c:132:9: note: node (constant) 0x31adce8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:132:9: note: 	{ 0B, 0B }
PULSE.c:132:9: note: Cost model analysis: 
PULSE.c:132:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:132:9: note: Basic block will be vectorized using SLP
PULSE.c:132:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:132:9: note: Vectorizing SLP tree:
PULSE.c:132:9: note: node 0x31adae8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:132:9: note: op template: <retval>.layers = layers_33;
PULSE.c:132:9: note: 	stmt 0 <retval>.layers = layers_33;
PULSE.c:132:9: note: 	stmt 1 <retval>.paramethers = MODEL_123;
PULSE.c:132:9: note: 	children 0x31adbe8
PULSE.c:132:9: note: node (external) 0x31adbe8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:132:9: note: 	{ layers_33, MODEL_123 }
PULSE.c:132:9: note: ------>vectorizing SLP node starting from: <retval>.layers = layers_33;
PULSE.c:132:9: note: vect_is_simple_use: operand MODEL_123 = PHI <MODEL_38(37), MODEL_122(13)>, type of def: internal
PULSE.c:132:9: note: conflicting alias set types.
PULSE.c:132:9: note: transform store. ncopies = 1
PULSE.c:132:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.layers
PULSE.c:132:9: note: created &<retval>
PULSE.c:132:9: note: add new stmt: MEM <vector(2) long unsigned int> [(void *)&<retval>] = _196;
PULSE.c:132:9: note: vectorizing stmts using SLP.
PULSE.c:132:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:132:9: note: Vectorizing SLP tree:
PULSE.c:132:9: note: node 0x31adc68 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:132:9: note: op template: <retval>.io = 0B;
PULSE.c:132:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:132:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:132:9: note: 	children 0x31adce8
PULSE.c:132:9: note: node (constant) 0x31adce8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:132:9: note: 	{ 0B, 0B }
PULSE.c:132:9: note: ------>vectorizing SLP node starting from: <retval>.io = 0B;
PULSE.c:132:9: note: vect_is_simple_use: operand 0B, type of def: constant
PULSE.c:132:9: note: transform store. ncopies = 1
PULSE.c:132:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.io
PULSE.c:132:9: note: created &<retval>.io
PULSE.c:132:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval> + 16B] = { 0, 0 };
PULSE.c:132:9: note: vectorizing stmts using SLP.
PULSE.c:132:9: note: ***** The result for vector mode V32QI would be the same
PULSE.c:144:1: note: ***** Analysis failed with vector mode VOID
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _28 (pretmp_91, _51, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:19: missed: not vectorized: no vectype for stmt: _22 = *_21;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:170:10: optimized: loop vectorized using 32 byte vectors
Dense.c:170:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:170:10: optimized: loop vectorized using 16 byte vectors
Dense.c:161:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
Dense.c:154:10: optimized: loop vectorized using 32 byte vectors
Dense.c:154:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:154:10: optimized: loop vectorized using 16 byte vectors
Dense.c:145:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
Dense.c:136:13: note: vectorized 2 loops in function.
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_28, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
Dense.c:136:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:136:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:105:24: missed: couldn't vectorize loop
Dense.c:105:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:108:17: missed: couldn't vectorize loop
Dense.c:108:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:122:24: missed: couldn't vectorize loop
Dense.c:122:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:125:17: missed: couldn't vectorize loop
Dense.c:125:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:90:15: missed: couldn't vectorize loop
Dense.c:90:15: missed: not vectorized: number of iterations cannot be computed.
Dense.c:83:13: note: vectorized 0 loops in function.
Dense.c:86:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:110:17: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:110:17: note: SLPing BB part
Dense.c:127:17: note: Costing subgraph: 
Dense.c:127:17: note: node 0x8be9d08 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:127:17: note: op template: _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 0 _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 1 _198 = (sizetype) wi_137;
Dense.c:127:17: note: 	children 0x8be9e08
Dense.c:127:17: note: node (external) 0x8be9e08 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:127:17: note: 	{ j_135, wi_137 }
Dense.c:127:17: note: Cost model analysis: 
Dense.c:127:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:127:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:127:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: Costing subgraph: 
Dense.c:110:17: note: node 0x8be9f08 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:110:17: note: op template: _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 0 _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 1 _217 = (sizetype) wi_136;
Dense.c:110:17: note: 	children 0x8bea008
Dense.c:110:17: note: node (external) 0x8bea008 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:110:17: note: 	{ j_134, wi_136 }
Dense.c:110:17: note: Cost model analysis: 
Dense.c:110:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:110:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:110:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: ***** The result for vector mode V32QI would be the same
Dense.c:110:17: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:110:17: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:110:17: note: SLPing BB part
Dense.c:127:17: note: Costing subgraph: 
Dense.c:127:17: note: node 0x8be9f08 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:127:17: note: op template: _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 0 _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 1 _198 = (sizetype) wi_137;
Dense.c:127:17: note: 	children 0x8be9d08
Dense.c:127:17: note: node (external) 0x8be9d08 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:127:17: note: 	{ j_135, wi_137 }
Dense.c:127:17: note: Cost model analysis: 
Dense.c:127:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:127:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:127:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: Costing subgraph: 
Dense.c:110:17: note: node 0x8be9e88 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:110:17: note: op template: _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 0 _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 1 _217 = (sizetype) wi_136;
Dense.c:110:17: note: 	children 0x8be9f88
Dense.c:110:17: note: node (external) 0x8be9f88 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:110:17: note: 	{ j_134, wi_136 }
Dense.c:110:17: note: Cost model analysis: 
Dense.c:110:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:110:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:110:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:105:51: note: ***** Analysis failed with vector mode V8QI
Dense.c:105:51: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:105:51: note: ***** Analysis failed with vector mode V4QI
Dense.c:62:15: missed: couldn't vectorize loop
Dense.c:62:15: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:75:11: optimized: loop vectorized using 32 byte vectors
Dense.c:75:11: optimized: loop vectorized using 16 byte vectors
Dense.c:66:11: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:881:10: missed: not vectorized: no vectype for stmt: _53 = MEM[(__m256 * {ref-all})w_ptr_82];
 scalar_type: __m256
Dense.c:54:13: note: vectorized 1 loops in function.
Dense.c:57:2: missed: statement clobbers memory: memcpy (_4, dense$baiases_63, _3);
Dense.c:79:2: missed: statement clobbers memory: _27 (pretmp_163, _68, 0);
Dense.c:54:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:54:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:182:2: missed: statement clobbers memory: free (dense$weights_7);
Dense.c:183:2: missed: statement clobbers memory: free (dense$baiases_8);
Dense.c:184:2: missed: statement clobbers memory: free (dense$gradients_10);
Dense.c:185:2: missed: statement clobbers memory: free (dense$deltas_9);
Dense.c:186:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:186:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:202:19: missed: couldn't vectorize loop
Dense.c:202:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:190:13: note: vectorized 0 loops in function.
Dense.c:198:37: missed: statement clobbers memory: _9 = aligned_alloc (64, _8);
Dense.c:199:34: missed: statement clobbers memory: _10 = aligned_alloc (64, _5);
Dense.c:200:35: missed: statement clobbers memory: _11 = aligned_alloc (64, _5);
Dense.c:203:38: missed: statement clobbers memory: _12 = rand ();
Dense.c:203:72: missed: statement clobbers memory: _57 = sqrt (_19);
Dense.c:215:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:216:4: missed: statement clobbers memory: exit (1);
Dense.c:212:4: missed: statement clobbers memory: layer = PULSE_CreateLayer (args$n_inputs_36, args$n_outputs_47, 0, args$8_48, _SIMD_FeedDense, _SIMD_BackDense, _SIMD_FixDense, _DestroyDense, 1); [return slot optimization]
Dense.c:209:12: missed: statement clobbers memory: layer = PULSE_CreateLayer (args$n_inputs_36, args$n_outputs_47, 0, args$8_48, _FeedDense, _BackDense, _FixDense, _DestroyDense, 0); [return slot optimization]
Dense.c:221:9: missed: not vectorized: more than one data ref in stmt: <retval> = layer;
Dense.c:221:9: note: ***** Analysis succeeded with vector mode V8SI
Dense.c:221:9: note: SLPing BB part
Dense.c:221:9: note: Costing subgraph: 
Dense.c:221:9: note: node 0x8aeee68 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:221:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = MODEL_31(D);
Dense.c:221:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = MODEL_31(D);
Dense.c:221:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&layer + 96B] = MODEL_32;
Dense.c:221:9: note: 	stmt 2 MEM <float *> [(struct PULSE_Layer *)&layer + 104B] = _10;
Dense.c:221:9: note: 	stmt 3 MEM <float *> [(struct PULSE_Layer *)&layer + 112B] = _11;
Dense.c:221:9: note: 	children 0x8aeeee8
Dense.c:221:9: note: node (external) 0x8aeeee8 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:221:9: note: 	{ MODEL_31(D), MODEL_32, _10, _11 }
Dense.c:221:9: note: Cost model analysis: 
Dense.c:221:9: note: Cost model analysis for part in loop 0:
  Vector cost: 64
  Scalar cost: 64
Dense.c:221:9: note: Basic block will be vectorized using SLP
Dense.c:221:9: optimized: basic block part vectorized using 32 byte vectors
Dense.c:221:9: note: Vectorizing SLP tree:
Dense.c:221:9: note: node 0x8aeee68 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:221:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = MODEL_31(D);
Dense.c:221:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = MODEL_31(D);
Dense.c:221:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&layer + 96B] = MODEL_32;
Dense.c:221:9: note: 	stmt 2 MEM <float *> [(struct PULSE_Layer *)&layer + 104B] = _10;
Dense.c:221:9: note: 	stmt 3 MEM <float *> [(struct PULSE_Layer *)&layer + 112B] = _11;
Dense.c:221:9: note: 	children 0x8aeeee8
Dense.c:221:9: note: node (external) 0x8aeeee8 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:221:9: note: 	{ MODEL_31(D), MODEL_32, _10, _11 }
Dense.c:221:9: note: ------>vectorizing SLP node starting from: MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = MODEL_31(D);
Dense.c:221:9: note: vect_is_simple_use: operand MODEL_31(D) + _3, type of def: internal
Dense.c:221:9: note: vect_is_simple_use: operand aligned_alloc (64, _5), type of def: internal
Dense.c:221:9: note: vect_is_simple_use: operand aligned_alloc (64, _5), type of def: internal
Dense.c:221:9: note: transform store. ncopies = 1
Dense.c:221:9: note: create vector_type-pointer variable to type: vector(4) long unsigned int  vectorizing a pointer ref: MEM <float *> [(struct PULSE_Layer *)&layer + 88B]
Dense.c:221:9: note: created &MEM <float *> [(struct PULSE_Layer *)&layer + 88B]
Dense.c:221:9: note: add new stmt: MEM <vector(4) long unsigned int> [(struct PULSE_Layer *)&layer + 88B] = _61;
Dense.c:221:9: note: vectorizing stmts using SLP.
Dense.c:221:9: note: ***** The result for vector mode V32QI would be the same
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:4:18: note: vectorized 0 loops in function.
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_4, inputs_15, _3);
PULSE.c:8:2: missed: statement clobbers memory: _5 (layer_17);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_12, pretmp_26, _9);
PULSE.c:13:28: note: ***** Analysis failed with vector mode V8SI
PULSE.c:13:28: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:22:2: missed: statement clobbers memory: _1 (layer_10(D));
PULSE.c:22:2: missed: statement clobbers memory: _14 (_2);
PULSE.c:22:2: missed: statement clobbers memory: _21 (_15);
PULSE.c:22:2: missed: statement clobbers memory: _28 (_22);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_29);
PULSE.c:26:3: missed: statement clobbers memory: memset (_34, 0, _32);
PULSE.c:26:3: missed: statement clobbers memory: memset (_27, 0, _25);
PULSE.c:26:3: missed: statement clobbers memory: memset (_20, 0, _18);
PULSE.c:26:3: missed: statement clobbers memory: memset (_7, 0, _5);
PULSE.c:28:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:28:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:32:6: note: vectorized 1 loops in function.
PULSE.c:34:8: missed: statement clobbers memory: _1 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_2);
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _9 = rand ();
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8SI
PULSE.c:32:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:61:16: missed: couldn't vectorize loop
PULSE.c:61:16: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:64:17: missed: couldn't vectorize loop
PULSE.c:64:17: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:73:19: missed: couldn't vectorize loop
PULSE.c:73:19: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _69 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:53:22: missed: couldn't vectorize loop
PULSE.c:53:22: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:47:6: note: vectorized 1 loops in function.
PULSE.c:48:1: missed: statement clobbers memory: saved_stack.7_39 = __builtin_stack_save ();
PULSE.c:49:46: missed: statement clobbers memory: _1 = PULSE_GetLossFunctionPtr (loss_function_40(D));
PULSE.c:60:10: missed: statement clobbers memory: random.4_46 = __builtin_alloca_with_align (_4, 32);
PULSE.c:34:8: missed: statement clobbers memory: _61 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_62);
PULSE.c:39:29: missed: statement clobbers memory: _69 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _72 = rand ();
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_98, inputs_93, _97);
PULSE.c:8:2: missed: statement clobbers memory: _99 (layer_94);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_105, _104, _103);
PULSE.c:67:11: missed: statement clobbers memory: loss_55 = PULSE_GetLoss_42 (_17, _16, _12, _11);
PULSE.c:22:2: missed: statement clobbers memory: _86 (output_34);
PULSE.c:22:2: missed: statement clobbers memory: _108 (_87);
PULSE.c:22:2: missed: statement clobbers memory: _115 (_109);
PULSE.c:22:2: missed: statement clobbers memory: _122 (_116);
PULSE.c:22:2: missed: statement clobbers memory: _129 (_123);
PULSE.c:22:2: missed: statement clobbers memory: _136 (_130);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_137);
PULSE.c:26:3: missed: statement clobbers memory: memset (_142, 0, _140);
PULSE.c:26:3: missed: statement clobbers memory: memset (_135, 0, _133);
PULSE.c:26:3: missed: statement clobbers memory: memset (_128, 0, _126);
PULSE.c:26:3: missed: statement clobbers memory: memset (_121, 0, _119);
PULSE.c:26:3: missed: statement clobbers memory: memset (_114, 0, _112);
PULSE.c:26:3: missed: statement clobbers memory: memset (_92, 0, _90);
PULSE.c:75:6: missed: statement clobbers memory: _21 (current_191, args);
PULSE.c:81:4: missed: statement clobbers memory: printf ("Epoch: %d | Item: %d | Loss: %.10f | Batch Loss: %.10f\r", i_188, j_189, _25, _24);
PULSE.c:47:6: missed: statement clobbers memory: __builtin_stack_restore (saved_stack.7_39);
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4DI
PULSE.c:47:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:91:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:91:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:119:20: missed: couldn't vectorize loop
PULSE.c:119:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:119:20: missed: couldn't vectorize loop
PULSE.c:119:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:102:20: missed: couldn't vectorize loop
PULSE.c:102:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:102:20: missed: couldn't vectorize loop
PULSE.c:102:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:93:13: note: vectorized 0 loops in function.
PULSE.c:96:39: missed: statement clobbers memory: layers_33 = malloc (_3);
PULSE.c:99:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:114:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:115:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:116:44: missed: statement clobbers memory: MODEL_122 = malloc (0);
PULSE.c:114:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:115:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:116:44: missed: statement clobbers memory: MODEL_38 = malloc (_11);
PULSE.c:125:62: missed: not vectorized: more than one data ref in stmt: D.6469 = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.106_66];
PULSE.c:125:17: missed: statement clobbers memory: *_14 = PULSE_CreateDenseLayer (D.6469, MODEL_38); [return slot optimization]
PULSE.c:125:62: missed: not vectorized: more than one data ref in stmt: D.6469 = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.106_181];
PULSE.c:125:17: missed: statement clobbers memory: *_175 = PULSE_CreateDenseLayer (D.6469, MODEL_38); [return slot optimization]
PULSE.c:131:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:132:9: note: ***** Analysis succeeded with vector mode V4DI
PULSE.c:132:9: note: SLPing BB part
PULSE.c:132:9: note: Costing subgraph: 
PULSE.c:132:9: note: node 0x4fd0b08 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:132:9: note: op template: <retval>.layers = layers_33;
PULSE.c:132:9: note: 	stmt 0 <retval>.layers = layers_33;
PULSE.c:132:9: note: 	stmt 1 <retval>.paramethers = MODEL_123;
PULSE.c:132:9: note: 	children 0x4fd0c08
PULSE.c:132:9: note: node (external) 0x4fd0c08 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:132:9: note: 	{ layers_33, MODEL_123 }
PULSE.c:132:9: note: Cost model analysis: 
PULSE.c:132:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:132:9: note: Costing subgraph: 
PULSE.c:132:9: note: node 0x4fd0c88 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:132:9: note: op template: <retval>.io = 0B;
PULSE.c:132:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:132:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:132:9: note: 	children 0x4fd0d08
PULSE.c:132:9: note: node (constant) 0x4fd0d08 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:132:9: note: 	{ 0B, 0B }
PULSE.c:132:9: note: Cost model analysis: 
PULSE.c:132:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:132:9: note: Basic block will be vectorized using SLP
PULSE.c:132:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:132:9: note: Vectorizing SLP tree:
PULSE.c:132:9: note: node 0x4fd0b08 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:132:9: note: op template: <retval>.layers = layers_33;
PULSE.c:132:9: note: 	stmt 0 <retval>.layers = layers_33;
PULSE.c:132:9: note: 	stmt 1 <retval>.paramethers = MODEL_123;
PULSE.c:132:9: note: 	children 0x4fd0c08
PULSE.c:132:9: note: node (external) 0x4fd0c08 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:132:9: note: 	{ layers_33, MODEL_123 }
PULSE.c:132:9: note: ------>vectorizing SLP node starting from: <retval>.layers = layers_33;
PULSE.c:132:9: note: vect_is_simple_use: operand MODEL_123 = PHI <MODEL_38(37), MODEL_122(13)>, type of def: internal
PULSE.c:132:9: note: conflicting alias set types.
PULSE.c:132:9: note: transform store. ncopies = 1
PULSE.c:132:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.layers
PULSE.c:132:9: note: created &<retval>
PULSE.c:132:9: note: add new stmt: MEM <vector(2) long unsigned int> [(void *)&<retval>] = _196;
PULSE.c:132:9: note: vectorizing stmts using SLP.
PULSE.c:132:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:132:9: note: Vectorizing SLP tree:
PULSE.c:132:9: note: node 0x4fd0c88 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:132:9: note: op template: <retval>.io = 0B;
PULSE.c:132:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:132:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:132:9: note: 	children 0x4fd0d08
PULSE.c:132:9: note: node (constant) 0x4fd0d08 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:132:9: note: 	{ 0B, 0B }
PULSE.c:132:9: note: ------>vectorizing SLP node starting from: <retval>.io = 0B;
PULSE.c:132:9: note: vect_is_simple_use: operand 0B, type of def: constant
PULSE.c:132:9: note: transform store. ncopies = 1
PULSE.c:132:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.io
PULSE.c:132:9: note: created &<retval>.io
PULSE.c:132:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval> + 16B] = { 0, 0 };
PULSE.c:132:9: note: vectorizing stmts using SLP.
PULSE.c:132:9: note: ***** The result for vector mode V32QI would be the same
PULSE.c:144:1: note: ***** Analysis failed with vector mode VOID
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _28 (pretmp_91, _51, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:19: missed: not vectorized: no vectype for stmt: _22 = *_21;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:170:10: optimized: loop vectorized using 32 byte vectors
Dense.c:170:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:170:10: optimized: loop vectorized using 16 byte vectors
Dense.c:161:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
Dense.c:154:10: optimized: loop vectorized using 32 byte vectors
Dense.c:154:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:154:10: optimized: loop vectorized using 16 byte vectors
Dense.c:145:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
Dense.c:136:13: note: vectorized 2 loops in function.
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_28, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
Dense.c:136:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:136:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:105:24: missed: couldn't vectorize loop
Dense.c:105:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:108:17: missed: couldn't vectorize loop
Dense.c:108:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:122:24: missed: couldn't vectorize loop
Dense.c:122:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:125:17: missed: couldn't vectorize loop
Dense.c:125:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:90:15: missed: couldn't vectorize loop
Dense.c:90:15: missed: not vectorized: number of iterations cannot be computed.
Dense.c:83:13: note: vectorized 0 loops in function.
Dense.c:86:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:110:17: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:110:17: note: SLPing BB part
Dense.c:127:17: note: Costing subgraph: 
Dense.c:127:17: note: node 0x767c138 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:127:17: note: op template: _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 0 _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 1 _198 = (sizetype) wi_137;
Dense.c:127:17: note: 	children 0x767c238
Dense.c:127:17: note: node (external) 0x767c238 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:127:17: note: 	{ j_135, wi_137 }
Dense.c:127:17: note: Cost model analysis: 
Dense.c:127:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:127:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:127:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: Costing subgraph: 
Dense.c:110:17: note: node 0x767c338 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:110:17: note: op template: _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 0 _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 1 _217 = (sizetype) wi_136;
Dense.c:110:17: note: 	children 0x767c438
Dense.c:110:17: note: node (external) 0x767c438 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:110:17: note: 	{ j_134, wi_136 }
Dense.c:110:17: note: Cost model analysis: 
Dense.c:110:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:110:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:110:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: ***** The result for vector mode V32QI would be the same
Dense.c:110:17: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:110:17: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:110:17: note: SLPing BB part
Dense.c:127:17: note: Costing subgraph: 
Dense.c:127:17: note: node 0x767c338 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:127:17: note: op template: _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 0 _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 1 _198 = (sizetype) wi_137;
Dense.c:127:17: note: 	children 0x767c138
Dense.c:127:17: note: node (external) 0x767c138 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:127:17: note: 	{ j_135, wi_137 }
Dense.c:127:17: note: Cost model analysis: 
Dense.c:127:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:127:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:127:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: Costing subgraph: 
Dense.c:110:17: note: node 0x767c2b8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:110:17: note: op template: _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 0 _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 1 _217 = (sizetype) wi_136;
Dense.c:110:17: note: 	children 0x767c3b8
Dense.c:110:17: note: node (external) 0x767c3b8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:110:17: note: 	{ j_134, wi_136 }
Dense.c:110:17: note: Cost model analysis: 
Dense.c:110:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:110:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:110:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:105:51: note: ***** Analysis failed with vector mode V8QI
Dense.c:105:51: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:105:51: note: ***** Analysis failed with vector mode V4QI
Dense.c:62:15: missed: couldn't vectorize loop
Dense.c:62:15: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:75:11: optimized: loop vectorized using 32 byte vectors
Dense.c:75:11: optimized: loop vectorized using 16 byte vectors
Dense.c:66:11: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:881:10: missed: not vectorized: no vectype for stmt: _53 = MEM[(__m256 * {ref-all})w_ptr_82];
 scalar_type: __m256
Dense.c:54:13: note: vectorized 1 loops in function.
Dense.c:57:2: missed: statement clobbers memory: memcpy (_4, dense$baiases_63, _3);
Dense.c:79:2: missed: statement clobbers memory: _27 (pretmp_163, _68, 0);
Dense.c:54:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:54:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:182:2: missed: statement clobbers memory: free (dense$weights_7);
Dense.c:183:2: missed: statement clobbers memory: free (dense$baiases_8);
Dense.c:184:2: missed: statement clobbers memory: free (dense$gradients_10);
Dense.c:185:2: missed: statement clobbers memory: free (dense$deltas_9);
Dense.c:186:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:186:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:202:19: missed: couldn't vectorize loop
Dense.c:202:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:190:13: note: vectorized 0 loops in function.
Dense.c:198:37: missed: statement clobbers memory: _12 = aligned_alloc (64, _11);
Dense.c:199:34: missed: statement clobbers memory: _13 = aligned_alloc (64, _7);
Dense.c:200:35: missed: statement clobbers memory: _14 = aligned_alloc (64, _7);
Dense.c:203:38: missed: statement clobbers memory: _15 = rand ();
Dense.c:203:72: missed: statement clobbers memory: _60 = sqrt (_22);
Dense.c:215:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:216:4: missed: statement clobbers memory: exit (1);
Dense.c:212:4: missed: statement clobbers memory: layer = PULSE_CreateLayer (args$n_inputs_39, args$n_outputs_50, 0, args$8_51, _SIMD_FeedDense, _SIMD_BackDense, _SIMD_FixDense, _DestroyDense, 1); [return slot optimization]
Dense.c:209:12: missed: statement clobbers memory: layer = PULSE_CreateLayer (args$n_inputs_39, args$n_outputs_50, 0, args$8_51, _FeedDense, _BackDense, _FixDense, _DestroyDense, 0); [return slot optimization]
Dense.c:221:9: missed: not vectorized: more than one data ref in stmt: <retval> = layer;
Dense.c:221:9: note: ***** Analysis succeeded with vector mode V8SI
Dense.c:221:9: note: SLPing BB part
Dense.c:221:9: note: Costing subgraph: 
Dense.c:221:9: note: node 0x7584c48 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:221:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = _1;
Dense.c:221:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = _1;
Dense.c:221:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&layer + 96B] = _5;
Dense.c:221:9: note: 	stmt 2 MEM <float *> [(struct PULSE_Layer *)&layer + 104B] = _13;
Dense.c:221:9: note: 	stmt 3 MEM <float *> [(struct PULSE_Layer *)&layer + 112B] = _14;
Dense.c:221:9: note: 	children 0x7584d48
Dense.c:221:9: note: node (external) 0x7584d48 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:221:9: note: 	{ _1, _5, _13, _14 }
Dense.c:221:9: note: Cost model analysis: 
Dense.c:221:9: note: Cost model analysis for part in loop 0:
  Vector cost: 58
  Scalar cost: 64
Dense.c:221:9: note: Basic block will be vectorized using SLP
Dense.c:221:9: optimized: basic block part vectorized using 32 byte vectors
Dense.c:221:9: note: Vectorizing SLP tree:
Dense.c:221:9: note: node 0x7584c48 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:221:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = _1;
Dense.c:221:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = _1;
Dense.c:221:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&layer + 96B] = _5;
Dense.c:221:9: note: 	stmt 2 MEM <float *> [(struct PULSE_Layer *)&layer + 104B] = _13;
Dense.c:221:9: note: 	stmt 3 MEM <float *> [(struct PULSE_Layer *)&layer + 112B] = _14;
Dense.c:221:9: note: 	children 0x7584d48
Dense.c:221:9: note: node (external) 0x7584d48 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:221:9: note: 	{ _1, _5, _13, _14 }
Dense.c:221:9: note: ------>vectorizing SLP node starting from: MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = _1;
Dense.c:221:9: note: vect_is_simple_use: operand _1 + _4, type of def: internal
Dense.c:221:9: note: vect_is_simple_use: operand aligned_alloc (64, _7), type of def: internal
Dense.c:221:9: note: vect_is_simple_use: operand aligned_alloc (64, _7), type of def: internal
Dense.c:221:9: note: transform store. ncopies = 1
Dense.c:221:9: note: create vector_type-pointer variable to type: vector(4) long unsigned int  vectorizing a pointer ref: MEM <float *> [(struct PULSE_Layer *)&layer + 88B]
Dense.c:221:9: note: created &MEM <float *> [(struct PULSE_Layer *)&layer + 88B]
Dense.c:221:9: note: add new stmt: MEM <vector(4) long unsigned int> [(struct PULSE_Layer *)&layer + 88B] = _64;
Dense.c:221:9: note: vectorizing stmts using SLP.
Dense.c:221:9: note: ***** The result for vector mode V32QI would be the same
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:4:18: note: vectorized 0 loops in function.
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_4, inputs_15, _3);
PULSE.c:8:2: missed: statement clobbers memory: _5 (layer_17);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_12, pretmp_26, _9);
PULSE.c:13:28: note: ***** Analysis failed with vector mode V8SI
PULSE.c:13:28: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:22:2: missed: statement clobbers memory: _1 (layer_10(D));
PULSE.c:22:2: missed: statement clobbers memory: _14 (_2);
PULSE.c:22:2: missed: statement clobbers memory: _21 (_15);
PULSE.c:22:2: missed: statement clobbers memory: _28 (_22);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_29);
PULSE.c:26:3: missed: statement clobbers memory: memset (_34, 0, _32);
PULSE.c:26:3: missed: statement clobbers memory: memset (_27, 0, _25);
PULSE.c:26:3: missed: statement clobbers memory: memset (_20, 0, _18);
PULSE.c:26:3: missed: statement clobbers memory: memset (_7, 0, _5);
PULSE.c:28:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:28:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:32:6: note: vectorized 1 loops in function.
PULSE.c:34:8: missed: statement clobbers memory: _1 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_2);
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _9 = rand ();
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8SI
PULSE.c:32:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:61:16: missed: couldn't vectorize loop
PULSE.c:61:16: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:64:17: missed: couldn't vectorize loop
PULSE.c:64:17: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:73:19: missed: couldn't vectorize loop
PULSE.c:73:19: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _69 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:53:22: missed: couldn't vectorize loop
PULSE.c:53:22: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:47:6: note: vectorized 1 loops in function.
PULSE.c:48:1: missed: statement clobbers memory: saved_stack.7_39 = __builtin_stack_save ();
PULSE.c:49:46: missed: statement clobbers memory: _1 = PULSE_GetLossFunctionPtr (loss_function_40(D));
PULSE.c:60:10: missed: statement clobbers memory: random.4_46 = __builtin_alloca_with_align (_4, 32);
PULSE.c:34:8: missed: statement clobbers memory: _61 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_62);
PULSE.c:39:29: missed: statement clobbers memory: _69 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _72 = rand ();
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_98, inputs_93, _97);
PULSE.c:8:2: missed: statement clobbers memory: _99 (layer_94);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_105, _104, _103);
PULSE.c:67:11: missed: statement clobbers memory: loss_55 = PULSE_GetLoss_42 (_17, _16, _12, _11);
PULSE.c:22:2: missed: statement clobbers memory: _86 (output_34);
PULSE.c:22:2: missed: statement clobbers memory: _108 (_87);
PULSE.c:22:2: missed: statement clobbers memory: _115 (_109);
PULSE.c:22:2: missed: statement clobbers memory: _122 (_116);
PULSE.c:22:2: missed: statement clobbers memory: _129 (_123);
PULSE.c:22:2: missed: statement clobbers memory: _136 (_130);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_137);
PULSE.c:26:3: missed: statement clobbers memory: memset (_142, 0, _140);
PULSE.c:26:3: missed: statement clobbers memory: memset (_135, 0, _133);
PULSE.c:26:3: missed: statement clobbers memory: memset (_128, 0, _126);
PULSE.c:26:3: missed: statement clobbers memory: memset (_121, 0, _119);
PULSE.c:26:3: missed: statement clobbers memory: memset (_114, 0, _112);
PULSE.c:26:3: missed: statement clobbers memory: memset (_92, 0, _90);
PULSE.c:75:6: missed: statement clobbers memory: _21 (current_191, args);
PULSE.c:81:4: missed: statement clobbers memory: printf ("Epoch: %d | Item: %d | Loss: %.10f | Batch Loss: %.10f\r", i_188, j_189, _25, _24);
PULSE.c:47:6: missed: statement clobbers memory: __builtin_stack_restore (saved_stack.7_39);
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4DI
PULSE.c:47:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:91:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:91:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:119:20: missed: couldn't vectorize loop
PULSE.c:119:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:119:20: missed: couldn't vectorize loop
PULSE.c:119:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:102:20: missed: couldn't vectorize loop
PULSE.c:102:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:102:20: missed: couldn't vectorize loop
PULSE.c:102:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:93:13: note: vectorized 0 loops in function.
PULSE.c:96:39: missed: statement clobbers memory: layers_33 = malloc (_3);
PULSE.c:99:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:114:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:115:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:116:44: missed: statement clobbers memory: MODEL_128 = malloc (0);
PULSE.c:114:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:115:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:116:44: missed: statement clobbers memory: MODEL_38 = malloc (_11);
PULSE.c:125:62: missed: not vectorized: more than one data ref in stmt: D.6469 = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.106_68];
PULSE.c:125:17: missed: statement clobbers memory: *_14 = PULSE_CreateDenseLayer (D.6469, &MODEL_PTR); [return slot optimization]
PULSE.c:125:62: missed: not vectorized: more than one data ref in stmt: D.6469 = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.106_183];
PULSE.c:125:17: missed: statement clobbers memory: *_177 = PULSE_CreateDenseLayer (D.6469, &MODEL_PTR); [return slot optimization]
PULSE.c:131:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:132:9: note: ***** Analysis succeeded with vector mode V4DI
PULSE.c:132:9: note: SLPing BB part
PULSE.c:132:9: note: Costing subgraph: 
PULSE.c:132:9: note: node 0x46b8af8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:132:9: note: op template: <retval>.layers = layers_33;
PULSE.c:132:9: note: 	stmt 0 <retval>.layers = layers_33;
PULSE.c:132:9: note: 	stmt 1 <retval>.paramethers = MODEL_134;
PULSE.c:132:9: note: 	children 0x46b8bf8
PULSE.c:132:9: note: node (external) 0x46b8bf8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:132:9: note: 	{ layers_33, MODEL_134 }
PULSE.c:132:9: note: Cost model analysis: 
PULSE.c:132:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:132:9: note: Costing subgraph: 
PULSE.c:132:9: note: node 0x46b8c78 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:132:9: note: op template: <retval>.io = 0B;
PULSE.c:132:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:132:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:132:9: note: 	children 0x46b8cf8
PULSE.c:132:9: note: node (constant) 0x46b8cf8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:132:9: note: 	{ 0B, 0B }
PULSE.c:132:9: note: Cost model analysis: 
PULSE.c:132:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:132:9: note: Basic block will be vectorized using SLP
PULSE.c:132:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:132:9: note: Vectorizing SLP tree:
PULSE.c:132:9: note: node 0x46b8af8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:132:9: note: op template: <retval>.layers = layers_33;
PULSE.c:132:9: note: 	stmt 0 <retval>.layers = layers_33;
PULSE.c:132:9: note: 	stmt 1 <retval>.paramethers = MODEL_134;
PULSE.c:132:9: note: 	children 0x46b8bf8
PULSE.c:132:9: note: node (external) 0x46b8bf8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:132:9: note: 	{ layers_33, MODEL_134 }
PULSE.c:132:9: note: ------>vectorizing SLP node starting from: <retval>.layers = layers_33;
PULSE.c:132:9: note: vect_is_simple_use: operand MODEL_134 = PHI <MODEL_38(37), MODEL_128(27)>, type of def: internal
PULSE.c:132:9: note: conflicting alias set types.
PULSE.c:132:9: note: transform store. ncopies = 1
PULSE.c:132:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.layers
PULSE.c:132:9: note: created &<retval>
PULSE.c:132:9: note: add new stmt: MEM <vector(2) long unsigned int> [(void *)&<retval>] = _198;
PULSE.c:132:9: note: vectorizing stmts using SLP.
PULSE.c:132:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:132:9: note: Vectorizing SLP tree:
PULSE.c:132:9: note: node 0x46b8c78 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:132:9: note: op template: <retval>.io = 0B;
PULSE.c:132:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:132:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:132:9: note: 	children 0x46b8cf8
PULSE.c:132:9: note: node (constant) 0x46b8cf8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:132:9: note: 	{ 0B, 0B }
PULSE.c:132:9: note: ------>vectorizing SLP node starting from: <retval>.io = 0B;
PULSE.c:132:9: note: vect_is_simple_use: operand 0B, type of def: constant
PULSE.c:132:9: note: transform store. ncopies = 1
PULSE.c:132:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.io
PULSE.c:132:9: note: created &<retval>.io
PULSE.c:132:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval> + 16B] = { 0, 0 };
PULSE.c:132:9: note: vectorizing stmts using SLP.
PULSE.c:132:9: note: ***** The result for vector mode V32QI would be the same
PULSE.c:144:1: note: ***** Analysis failed with vector mode VOID
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _28 (pretmp_91, _51, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:19: missed: not vectorized: no vectype for stmt: _22 = *_21;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:170:10: optimized: loop vectorized using 32 byte vectors
Dense.c:170:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:170:10: optimized: loop vectorized using 16 byte vectors
Dense.c:161:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
Dense.c:154:10: optimized: loop vectorized using 32 byte vectors
Dense.c:154:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:154:10: optimized: loop vectorized using 16 byte vectors
Dense.c:145:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
Dense.c:136:13: note: vectorized 2 loops in function.
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_28, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
Dense.c:136:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:136:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:105:24: missed: couldn't vectorize loop
Dense.c:105:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:108:17: missed: couldn't vectorize loop
Dense.c:108:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:122:24: missed: couldn't vectorize loop
Dense.c:122:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:125:17: missed: couldn't vectorize loop
Dense.c:125:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:90:15: missed: couldn't vectorize loop
Dense.c:90:15: missed: not vectorized: number of iterations cannot be computed.
Dense.c:83:13: note: vectorized 0 loops in function.
Dense.c:86:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:110:17: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:110:17: note: SLPing BB part
Dense.c:127:17: note: Costing subgraph: 
Dense.c:127:17: note: node 0x7d781d8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:127:17: note: op template: _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 0 _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 1 _198 = (sizetype) wi_137;
Dense.c:127:17: note: 	children 0x7d782d8
Dense.c:127:17: note: node (external) 0x7d782d8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:127:17: note: 	{ j_135, wi_137 }
Dense.c:127:17: note: Cost model analysis: 
Dense.c:127:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:127:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:127:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: Costing subgraph: 
Dense.c:110:17: note: node 0x7d783d8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:110:17: note: op template: _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 0 _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 1 _217 = (sizetype) wi_136;
Dense.c:110:17: note: 	children 0x7d784d8
Dense.c:110:17: note: node (external) 0x7d784d8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:110:17: note: 	{ j_134, wi_136 }
Dense.c:110:17: note: Cost model analysis: 
Dense.c:110:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:110:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:110:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: ***** The result for vector mode V32QI would be the same
Dense.c:110:17: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:110:17: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:110:17: note: SLPing BB part
Dense.c:127:17: note: Costing subgraph: 
Dense.c:127:17: note: node 0x7d783d8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:127:17: note: op template: _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 0 _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 1 _198 = (sizetype) wi_137;
Dense.c:127:17: note: 	children 0x7d781d8
Dense.c:127:17: note: node (external) 0x7d781d8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:127:17: note: 	{ j_135, wi_137 }
Dense.c:127:17: note: Cost model analysis: 
Dense.c:127:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:127:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:127:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: Costing subgraph: 
Dense.c:110:17: note: node 0x7d78358 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:110:17: note: op template: _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 0 _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 1 _217 = (sizetype) wi_136;
Dense.c:110:17: note: 	children 0x7d78458
Dense.c:110:17: note: node (external) 0x7d78458 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:110:17: note: 	{ j_134, wi_136 }
Dense.c:110:17: note: Cost model analysis: 
Dense.c:110:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:110:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:110:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:105:51: note: ***** Analysis failed with vector mode V8QI
Dense.c:105:51: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:105:51: note: ***** Analysis failed with vector mode V4QI
Dense.c:62:15: missed: couldn't vectorize loop
Dense.c:62:15: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:75:11: optimized: loop vectorized using 32 byte vectors
Dense.c:75:11: optimized: loop vectorized using 16 byte vectors
Dense.c:66:11: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:881:10: missed: not vectorized: no vectype for stmt: _53 = MEM[(__m256 * {ref-all})w_ptr_82];
 scalar_type: __m256
Dense.c:54:13: note: vectorized 1 loops in function.
Dense.c:57:2: missed: statement clobbers memory: memcpy (_4, dense$baiases_63, _3);
Dense.c:79:2: missed: statement clobbers memory: _27 (pretmp_163, _68, 0);
Dense.c:54:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:54:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:182:2: missed: statement clobbers memory: free (dense$weights_7);
Dense.c:183:2: missed: statement clobbers memory: free (dense$baiases_8);
Dense.c:184:2: missed: statement clobbers memory: free (dense$gradients_10);
Dense.c:185:2: missed: statement clobbers memory: free (dense$deltas_9);
Dense.c:186:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:186:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:202:19: missed: couldn't vectorize loop
Dense.c:202:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:190:13: note: vectorized 0 loops in function.
Dense.c:198:37: missed: statement clobbers memory: _9 = aligned_alloc (64, _8);
Dense.c:199:34: missed: statement clobbers memory: _10 = aligned_alloc (64, _5);
Dense.c:200:35: missed: statement clobbers memory: _11 = aligned_alloc (64, _5);
Dense.c:203:38: missed: statement clobbers memory: _12 = rand ();
Dense.c:203:72: missed: statement clobbers memory: _57 = sqrt (_19);
Dense.c:215:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:216:4: missed: statement clobbers memory: exit (1);
Dense.c:212:4: missed: statement clobbers memory: layer = PULSE_CreateLayer (args$n_inputs_36, args$n_outputs_47, 0, args$8_48, _SIMD_FeedDense, _SIMD_BackDense, _SIMD_FixDense, _DestroyDense, 1); [return slot optimization]
Dense.c:209:12: missed: statement clobbers memory: layer = PULSE_CreateLayer (args$n_inputs_36, args$n_outputs_47, 0, args$8_48, _FeedDense, _BackDense, _FixDense, _DestroyDense, 0); [return slot optimization]
Dense.c:221:9: missed: not vectorized: more than one data ref in stmt: <retval> = layer;
Dense.c:221:9: note: ***** Analysis succeeded with vector mode V8SI
Dense.c:221:9: note: SLPing BB part
Dense.c:221:9: note: Costing subgraph: 
Dense.c:221:9: note: node 0x7c54fa8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:221:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = MODEL_31(D);
Dense.c:221:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = MODEL_31(D);
Dense.c:221:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&layer + 96B] = MODEL_32;
Dense.c:221:9: note: 	stmt 2 MEM <float *> [(struct PULSE_Layer *)&layer + 104B] = _10;
Dense.c:221:9: note: 	stmt 3 MEM <float *> [(struct PULSE_Layer *)&layer + 112B] = _11;
Dense.c:221:9: note: 	children 0x7c55028
Dense.c:221:9: note: node (external) 0x7c55028 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:221:9: note: 	{ MODEL_31(D), MODEL_32, _10, _11 }
Dense.c:221:9: note: Cost model analysis: 
Dense.c:221:9: note: Cost model analysis for part in loop 0:
  Vector cost: 64
  Scalar cost: 64
Dense.c:221:9: note: Basic block will be vectorized using SLP
Dense.c:221:9: optimized: basic block part vectorized using 32 byte vectors
Dense.c:221:9: note: Vectorizing SLP tree:
Dense.c:221:9: note: node 0x7c54fa8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:221:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = MODEL_31(D);
Dense.c:221:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = MODEL_31(D);
Dense.c:221:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&layer + 96B] = MODEL_32;
Dense.c:221:9: note: 	stmt 2 MEM <float *> [(struct PULSE_Layer *)&layer + 104B] = _10;
Dense.c:221:9: note: 	stmt 3 MEM <float *> [(struct PULSE_Layer *)&layer + 112B] = _11;
Dense.c:221:9: note: 	children 0x7c55028
Dense.c:221:9: note: node (external) 0x7c55028 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:221:9: note: 	{ MODEL_31(D), MODEL_32, _10, _11 }
Dense.c:221:9: note: ------>vectorizing SLP node starting from: MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = MODEL_31(D);
Dense.c:221:9: note: vect_is_simple_use: operand MODEL_31(D) + _3, type of def: internal
Dense.c:221:9: note: vect_is_simple_use: operand aligned_alloc (64, _5), type of def: internal
Dense.c:221:9: note: vect_is_simple_use: operand aligned_alloc (64, _5), type of def: internal
Dense.c:221:9: note: transform store. ncopies = 1
Dense.c:221:9: note: create vector_type-pointer variable to type: vector(4) long unsigned int  vectorizing a pointer ref: MEM <float *> [(struct PULSE_Layer *)&layer + 88B]
Dense.c:221:9: note: created &MEM <float *> [(struct PULSE_Layer *)&layer + 88B]
Dense.c:221:9: note: add new stmt: MEM <vector(4) long unsigned int> [(struct PULSE_Layer *)&layer + 88B] = _61;
Dense.c:221:9: note: vectorizing stmts using SLP.
Dense.c:221:9: note: ***** The result for vector mode V32QI would be the same
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:4:18: note: vectorized 0 loops in function.
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_4, inputs_15, _3);
PULSE.c:8:2: missed: statement clobbers memory: _5 (layer_17);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_12, pretmp_26, _9);
PULSE.c:13:28: note: ***** Analysis failed with vector mode V8SI
PULSE.c:13:28: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:22:2: missed: statement clobbers memory: _1 (layer_10(D));
PULSE.c:22:2: missed: statement clobbers memory: _14 (_2);
PULSE.c:22:2: missed: statement clobbers memory: _21 (_15);
PULSE.c:22:2: missed: statement clobbers memory: _28 (_22);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_29);
PULSE.c:26:3: missed: statement clobbers memory: memset (_34, 0, _32);
PULSE.c:26:3: missed: statement clobbers memory: memset (_27, 0, _25);
PULSE.c:26:3: missed: statement clobbers memory: memset (_20, 0, _18);
PULSE.c:26:3: missed: statement clobbers memory: memset (_7, 0, _5);
PULSE.c:28:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:28:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:32:6: note: vectorized 1 loops in function.
PULSE.c:34:8: missed: statement clobbers memory: _1 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_2);
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _9 = rand ();
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8SI
PULSE.c:32:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:61:16: missed: couldn't vectorize loop
PULSE.c:61:16: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:64:17: missed: couldn't vectorize loop
PULSE.c:64:17: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:73:19: missed: couldn't vectorize loop
PULSE.c:73:19: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _69 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:53:22: missed: couldn't vectorize loop
PULSE.c:53:22: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:47:6: note: vectorized 1 loops in function.
PULSE.c:48:1: missed: statement clobbers memory: saved_stack.7_39 = __builtin_stack_save ();
PULSE.c:49:46: missed: statement clobbers memory: _1 = PULSE_GetLossFunctionPtr (loss_function_40(D));
PULSE.c:60:10: missed: statement clobbers memory: random.4_46 = __builtin_alloca_with_align (_4, 32);
PULSE.c:34:8: missed: statement clobbers memory: _61 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_62);
PULSE.c:39:29: missed: statement clobbers memory: _69 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _72 = rand ();
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_98, inputs_93, _97);
PULSE.c:8:2: missed: statement clobbers memory: _99 (layer_94);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_105, _104, _103);
PULSE.c:67:11: missed: statement clobbers memory: loss_55 = PULSE_GetLoss_42 (_17, _16, _12, _11);
PULSE.c:22:2: missed: statement clobbers memory: _86 (output_34);
PULSE.c:22:2: missed: statement clobbers memory: _108 (_87);
PULSE.c:22:2: missed: statement clobbers memory: _115 (_109);
PULSE.c:22:2: missed: statement clobbers memory: _122 (_116);
PULSE.c:22:2: missed: statement clobbers memory: _129 (_123);
PULSE.c:22:2: missed: statement clobbers memory: _136 (_130);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_137);
PULSE.c:26:3: missed: statement clobbers memory: memset (_142, 0, _140);
PULSE.c:26:3: missed: statement clobbers memory: memset (_135, 0, _133);
PULSE.c:26:3: missed: statement clobbers memory: memset (_128, 0, _126);
PULSE.c:26:3: missed: statement clobbers memory: memset (_121, 0, _119);
PULSE.c:26:3: missed: statement clobbers memory: memset (_114, 0, _112);
PULSE.c:26:3: missed: statement clobbers memory: memset (_92, 0, _90);
PULSE.c:75:6: missed: statement clobbers memory: _21 (current_191, args);
PULSE.c:81:4: missed: statement clobbers memory: printf ("Epoch: %d | Item: %d | Loss: %.10f | Batch Loss: %.10f\r", i_188, j_189, _25, _24);
PULSE.c:47:6: missed: statement clobbers memory: __builtin_stack_restore (saved_stack.7_39);
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4DI
PULSE.c:47:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:91:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:91:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:119:20: missed: couldn't vectorize loop
PULSE.c:119:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:119:20: missed: couldn't vectorize loop
PULSE.c:119:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:102:20: missed: couldn't vectorize loop
PULSE.c:102:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:102:20: missed: couldn't vectorize loop
PULSE.c:102:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:93:13: note: vectorized 0 loops in function.
PULSE.c:96:39: missed: statement clobbers memory: layers_41 = malloc (_3);
PULSE.c:99:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:114:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:115:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:116:44: missed: statement clobbers memory: MODEL_110 = malloc (0);
PULSE.c:114:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:115:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:116:44: missed: statement clobbers memory: MODEL_46 = malloc (_11);
PULSE.c:125:62: missed: not vectorized: more than one data ref in stmt: D.6469 = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.106_75];
PULSE.c:125:17: missed: statement clobbers memory: *_14 = PULSE_CreateDenseLayer (D.6469, MODEL_PTR_126); [return slot optimization]
PULSE.c:125:62: missed: not vectorized: more than one data ref in stmt: D.6469 = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.106_189];
PULSE.c:125:17: missed: statement clobbers memory: *_183 = PULSE_CreateDenseLayer (D.6469, MODEL_PTR_194); [return slot optimization]
PULSE.c:132:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:133:9: note: ***** Analysis succeeded with vector mode V4DI
PULSE.c:133:9: note: SLPing BB part
PULSE.c:133:9: note: Costing subgraph: 
PULSE.c:133:9: note: node 0x47adb48 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:133:9: note: op template: <retval>.layers = layers_41;
PULSE.c:133:9: note: 	stmt 0 <retval>.layers = layers_41;
PULSE.c:133:9: note: 	stmt 1 <retval>.paramethers = MODEL_132;
PULSE.c:133:9: note: 	children 0x47adc48
PULSE.c:133:9: note: node (external) 0x47adc48 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:133:9: note: 	{ layers_41, MODEL_132 }
PULSE.c:133:9: note: Cost model analysis: 
PULSE.c:133:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:133:9: note: Costing subgraph: 
PULSE.c:133:9: note: node 0x47adcc8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:133:9: note: op template: <retval>.io = 0B;
PULSE.c:133:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:133:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:133:9: note: 	children 0x47add48
PULSE.c:133:9: note: node (constant) 0x47add48 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:133:9: note: 	{ 0B, 0B }
PULSE.c:133:9: note: Cost model analysis: 
PULSE.c:133:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:133:9: note: Basic block will be vectorized using SLP
PULSE.c:133:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:133:9: note: Vectorizing SLP tree:
PULSE.c:133:9: note: node 0x47adb48 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:133:9: note: op template: <retval>.layers = layers_41;
PULSE.c:133:9: note: 	stmt 0 <retval>.layers = layers_41;
PULSE.c:133:9: note: 	stmt 1 <retval>.paramethers = MODEL_132;
PULSE.c:133:9: note: 	children 0x47adc48
PULSE.c:133:9: note: node (external) 0x47adc48 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:133:9: note: 	{ layers_41, MODEL_132 }
PULSE.c:133:9: note: ------>vectorizing SLP node starting from: <retval>.layers = layers_41;
PULSE.c:133:9: note: vect_is_simple_use: operand MODEL_132 = PHI <MODEL_46(36), MODEL_110(13)>, type of def: internal
PULSE.c:133:9: note: conflicting alias set types.
PULSE.c:133:9: note: transform store. ncopies = 1
PULSE.c:133:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.layers
PULSE.c:133:9: note: created &<retval>
PULSE.c:133:9: note: add new stmt: MEM <vector(2) long unsigned int> [(void *)&<retval>] = _217;
PULSE.c:133:9: note: vectorizing stmts using SLP.
PULSE.c:133:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:133:9: note: Vectorizing SLP tree:
PULSE.c:133:9: note: node 0x47adcc8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:133:9: note: op template: <retval>.io = 0B;
PULSE.c:133:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:133:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:133:9: note: 	children 0x47add48
PULSE.c:133:9: note: node (constant) 0x47add48 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:133:9: note: 	{ 0B, 0B }
PULSE.c:133:9: note: ------>vectorizing SLP node starting from: <retval>.io = 0B;
PULSE.c:133:9: note: vect_is_simple_use: operand 0B, type of def: constant
PULSE.c:133:9: note: transform store. ncopies = 1
PULSE.c:133:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.io
PULSE.c:133:9: note: created &<retval>.io
PULSE.c:133:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval> + 16B] = { 0, 0 };
PULSE.c:133:9: note: vectorizing stmts using SLP.
PULSE.c:133:9: note: ***** The result for vector mode V32QI would be the same
PULSE.c:145:1: note: ***** Analysis failed with vector mode VOID
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _28 (pretmp_91, _51, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:19: missed: not vectorized: no vectype for stmt: _22 = *_21;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:170:10: optimized: loop vectorized using 32 byte vectors
Dense.c:170:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:170:10: optimized: loop vectorized using 16 byte vectors
Dense.c:161:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
Dense.c:154:10: optimized: loop vectorized using 32 byte vectors
Dense.c:154:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:154:10: optimized: loop vectorized using 16 byte vectors
Dense.c:145:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
Dense.c:136:13: note: vectorized 2 loops in function.
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_28, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
Dense.c:136:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:136:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:105:24: missed: couldn't vectorize loop
Dense.c:105:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:108:17: missed: couldn't vectorize loop
Dense.c:108:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:122:24: missed: couldn't vectorize loop
Dense.c:122:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:125:17: missed: couldn't vectorize loop
Dense.c:125:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:90:15: missed: couldn't vectorize loop
Dense.c:90:15: missed: not vectorized: number of iterations cannot be computed.
Dense.c:83:13: note: vectorized 0 loops in function.
Dense.c:86:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:110:17: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:110:17: note: SLPing BB part
Dense.c:127:17: note: Costing subgraph: 
Dense.c:127:17: note: node 0x8668888 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:127:17: note: op template: _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 0 _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 1 _198 = (sizetype) wi_137;
Dense.c:127:17: note: 	children 0x8668988
Dense.c:127:17: note: node (external) 0x8668988 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:127:17: note: 	{ j_135, wi_137 }
Dense.c:127:17: note: Cost model analysis: 
Dense.c:127:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:127:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:127:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: Costing subgraph: 
Dense.c:110:17: note: node 0x8668a88 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:110:17: note: op template: _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 0 _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 1 _217 = (sizetype) wi_136;
Dense.c:110:17: note: 	children 0x8668b88
Dense.c:110:17: note: node (external) 0x8668b88 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:110:17: note: 	{ j_134, wi_136 }
Dense.c:110:17: note: Cost model analysis: 
Dense.c:110:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:110:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:110:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: ***** The result for vector mode V32QI would be the same
Dense.c:110:17: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:110:17: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:110:17: note: SLPing BB part
Dense.c:127:17: note: Costing subgraph: 
Dense.c:127:17: note: node 0x8668a88 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:127:17: note: op template: _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 0 _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 1 _198 = (sizetype) wi_137;
Dense.c:127:17: note: 	children 0x8668888
Dense.c:127:17: note: node (external) 0x8668888 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:127:17: note: 	{ j_135, wi_137 }
Dense.c:127:17: note: Cost model analysis: 
Dense.c:127:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:127:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:127:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: Costing subgraph: 
Dense.c:110:17: note: node 0x8668a08 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:110:17: note: op template: _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 0 _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 1 _217 = (sizetype) wi_136;
Dense.c:110:17: note: 	children 0x8668b08
Dense.c:110:17: note: node (external) 0x8668b08 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:110:17: note: 	{ j_134, wi_136 }
Dense.c:110:17: note: Cost model analysis: 
Dense.c:110:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:110:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:110:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:105:51: note: ***** Analysis failed with vector mode V8QI
Dense.c:105:51: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:105:51: note: ***** Analysis failed with vector mode V4QI
Dense.c:62:15: missed: couldn't vectorize loop
Dense.c:62:15: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:75:11: optimized: loop vectorized using 32 byte vectors
Dense.c:75:11: optimized: loop vectorized using 16 byte vectors
Dense.c:66:11: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:881:10: missed: not vectorized: no vectype for stmt: _53 = MEM[(__m256 * {ref-all})w_ptr_82];
 scalar_type: __m256
Dense.c:54:13: note: vectorized 1 loops in function.
Dense.c:57:2: missed: statement clobbers memory: memcpy (_4, dense$baiases_63, _3);
Dense.c:79:2: missed: statement clobbers memory: _27 (pretmp_163, _68, 0);
Dense.c:54:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:54:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:182:2: missed: statement clobbers memory: free (dense$weights_7);
Dense.c:183:2: missed: statement clobbers memory: free (dense$baiases_8);
Dense.c:184:2: missed: statement clobbers memory: free (dense$gradients_10);
Dense.c:185:2: missed: statement clobbers memory: free (dense$deltas_9);
Dense.c:186:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:186:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:200:19: missed: couldn't vectorize loop
Dense.c:200:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:190:13: note: vectorized 0 loops in function.
Dense.c:196:37: missed: statement clobbers memory: _9 = aligned_alloc (64, _8);
Dense.c:197:34: missed: statement clobbers memory: _11 = aligned_alloc (64, _10);
Dense.c:198:35: missed: statement clobbers memory: _12 = aligned_alloc (64, _10);
Dense.c:201:38: missed: statement clobbers memory: _13 = rand ();
Dense.c:201:72: missed: statement clobbers memory: _57 = sqrt (_20);
Dense.c:213:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:214:4: missed: statement clobbers memory: exit (1);
Dense.c:210:4: missed: statement clobbers memory: layer = PULSE_CreateLayer (args$n_inputs_36, args$n_outputs_47, 0, args$8_48, _SIMD_FeedDense, _SIMD_BackDense, _SIMD_FixDense, _DestroyDense, 1); [return slot optimization]
Dense.c:207:12: missed: statement clobbers memory: layer = PULSE_CreateLayer (args$n_inputs_36, args$n_outputs_47, 0, args$8_48, _FeedDense, _BackDense, _FixDense, _DestroyDense, 0); [return slot optimization]
Dense.c:219:9: missed: not vectorized: more than one data ref in stmt: <retval> = layer;
Dense.c:219:9: note: ***** Analysis succeeded with vector mode V8SI
Dense.c:219:9: note: SLPing BB part
Dense.c:219:9: note: Costing subgraph: 
Dense.c:219:9: note: node 0x86488e8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:219:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = MODEL_32(D);
Dense.c:219:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = MODEL_32(D);
Dense.c:219:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&layer + 96B] = _4;
Dense.c:219:9: note: 	stmt 2 MEM <float *> [(struct PULSE_Layer *)&layer + 104B] = _11;
Dense.c:219:9: note: 	stmt 3 MEM <float *> [(struct PULSE_Layer *)&layer + 112B] = _12;
Dense.c:219:9: note: 	children 0x8648968
Dense.c:219:9: note: node (external) 0x8648968 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:219:9: note: 	{ MODEL_32(D), _4, _11, _12 }
Dense.c:219:9: note: Cost model analysis: 
Dense.c:219:9: note: Cost model analysis for part in loop 0:
  Vector cost: 64
  Scalar cost: 64
Dense.c:219:9: note: Basic block will be vectorized using SLP
Dense.c:219:9: optimized: basic block part vectorized using 32 byte vectors
Dense.c:219:9: note: Vectorizing SLP tree:
Dense.c:219:9: note: node 0x86488e8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:219:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = MODEL_32(D);
Dense.c:219:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = MODEL_32(D);
Dense.c:219:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&layer + 96B] = _4;
Dense.c:219:9: note: 	stmt 2 MEM <float *> [(struct PULSE_Layer *)&layer + 104B] = _11;
Dense.c:219:9: note: 	stmt 3 MEM <float *> [(struct PULSE_Layer *)&layer + 112B] = _12;
Dense.c:219:9: note: 	children 0x8648968
Dense.c:219:9: note: node (external) 0x8648968 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:219:9: note: 	{ MODEL_32(D), _4, _11, _12 }
Dense.c:219:9: note: ------>vectorizing SLP node starting from: MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = MODEL_32(D);
Dense.c:219:9: note: vect_is_simple_use: operand MODEL_32(D) + _3, type of def: internal
Dense.c:219:9: note: vect_is_simple_use: operand aligned_alloc (64, _10), type of def: internal
Dense.c:219:9: note: vect_is_simple_use: operand aligned_alloc (64, _10), type of def: internal
Dense.c:219:9: note: transform store. ncopies = 1
Dense.c:219:9: note: create vector_type-pointer variable to type: vector(4) long unsigned int  vectorizing a pointer ref: MEM <float *> [(struct PULSE_Layer *)&layer + 88B]
Dense.c:219:9: note: created &MEM <float *> [(struct PULSE_Layer *)&layer + 88B]
Dense.c:219:9: note: add new stmt: MEM <vector(4) long unsigned int> [(struct PULSE_Layer *)&layer + 88B] = _61;
Dense.c:219:9: note: vectorizing stmts using SLP.
Dense.c:219:9: note: ***** The result for vector mode V32QI would be the same
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:4:18: note: vectorized 0 loops in function.
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_4, inputs_15, _3);
PULSE.c:8:2: missed: statement clobbers memory: _5 (layer_17);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_12, pretmp_26, _9);
PULSE.c:13:28: note: ***** Analysis failed with vector mode V8SI
PULSE.c:13:28: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:22:2: missed: statement clobbers memory: _1 (layer_10(D));
PULSE.c:22:2: missed: statement clobbers memory: _14 (_2);
PULSE.c:22:2: missed: statement clobbers memory: _21 (_15);
PULSE.c:22:2: missed: statement clobbers memory: _28 (_22);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_29);
PULSE.c:26:3: missed: statement clobbers memory: memset (_34, 0, _32);
PULSE.c:26:3: missed: statement clobbers memory: memset (_27, 0, _25);
PULSE.c:26:3: missed: statement clobbers memory: memset (_20, 0, _18);
PULSE.c:26:3: missed: statement clobbers memory: memset (_7, 0, _5);
PULSE.c:28:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:28:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:32:6: note: vectorized 1 loops in function.
PULSE.c:34:8: missed: statement clobbers memory: _1 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_2);
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _9 = rand ();
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8SI
PULSE.c:32:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:61:16: missed: couldn't vectorize loop
PULSE.c:61:16: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:64:17: missed: couldn't vectorize loop
PULSE.c:64:17: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:73:19: missed: couldn't vectorize loop
PULSE.c:73:19: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _69 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:53:22: missed: couldn't vectorize loop
PULSE.c:53:22: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:47:6: note: vectorized 1 loops in function.
PULSE.c:48:1: missed: statement clobbers memory: saved_stack.7_39 = __builtin_stack_save ();
PULSE.c:49:46: missed: statement clobbers memory: _1 = PULSE_GetLossFunctionPtr (loss_function_40(D));
PULSE.c:60:10: missed: statement clobbers memory: random.4_46 = __builtin_alloca_with_align (_4, 32);
PULSE.c:34:8: missed: statement clobbers memory: _61 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_62);
PULSE.c:39:29: missed: statement clobbers memory: _69 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _72 = rand ();
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_98, inputs_93, _97);
PULSE.c:8:2: missed: statement clobbers memory: _99 (layer_94);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_105, _104, _103);
PULSE.c:67:11: missed: statement clobbers memory: loss_55 = PULSE_GetLoss_42 (_17, _16, _12, _11);
PULSE.c:22:2: missed: statement clobbers memory: _86 (output_34);
PULSE.c:22:2: missed: statement clobbers memory: _108 (_87);
PULSE.c:22:2: missed: statement clobbers memory: _115 (_109);
PULSE.c:22:2: missed: statement clobbers memory: _122 (_116);
PULSE.c:22:2: missed: statement clobbers memory: _129 (_123);
PULSE.c:22:2: missed: statement clobbers memory: _136 (_130);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_137);
PULSE.c:26:3: missed: statement clobbers memory: memset (_142, 0, _140);
PULSE.c:26:3: missed: statement clobbers memory: memset (_135, 0, _133);
PULSE.c:26:3: missed: statement clobbers memory: memset (_128, 0, _126);
PULSE.c:26:3: missed: statement clobbers memory: memset (_121, 0, _119);
PULSE.c:26:3: missed: statement clobbers memory: memset (_114, 0, _112);
PULSE.c:26:3: missed: statement clobbers memory: memset (_92, 0, _90);
PULSE.c:75:6: missed: statement clobbers memory: _21 (current_191, args);
PULSE.c:81:4: missed: statement clobbers memory: printf ("Epoch: %d | Item: %d | Loss: %.10f | Batch Loss: %.10f\r", i_188, j_189, _25, _24);
PULSE.c:47:6: missed: statement clobbers memory: __builtin_stack_restore (saved_stack.7_39);
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4DI
PULSE.c:47:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:91:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:91:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:119:20: missed: couldn't vectorize loop
PULSE.c:119:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:119:20: missed: couldn't vectorize loop
PULSE.c:119:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:102:20: missed: couldn't vectorize loop
PULSE.c:102:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:102:20: missed: couldn't vectorize loop
PULSE.c:102:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:93:13: note: vectorized 0 loops in function.
PULSE.c:96:39: missed: statement clobbers memory: layers_44 = malloc (_3);
PULSE.c:99:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:114:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:115:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:116:44: missed: statement clobbers memory: MODEL_113 = malloc (0);
PULSE.c:114:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:115:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:116:44: missed: statement clobbers memory: MODEL_49 = malloc (_11);
PULSE.c:125:62: missed: not vectorized: more than one data ref in stmt: D.6469 = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.106_78];
PULSE.c:125:17: missed: statement clobbers memory: *_17 = PULSE_CreateDenseLayer (D.6469, _148); [return slot optimization]
PULSE.c:125:62: missed: not vectorized: more than one data ref in stmt: D.6469 = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.106_199];
PULSE.c:125:17: missed: statement clobbers memory: *_192 = PULSE_CreateDenseLayer (D.6469, _198); [return slot optimization]
PULSE.c:132:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:133:9: note: ***** Analysis succeeded with vector mode V4DI
PULSE.c:133:9: note: SLPing BB part
PULSE.c:133:9: note: Costing subgraph: 
PULSE.c:133:9: note: node 0x474bb48 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:133:9: note: op template: <retval>.layers = layers_44;
PULSE.c:133:9: note: 	stmt 0 <retval>.layers = layers_44;
PULSE.c:133:9: note: 	stmt 1 <retval>.paramethers = MODEL_135;
PULSE.c:133:9: note: 	children 0x474bc48
PULSE.c:133:9: note: node (external) 0x474bc48 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:133:9: note: 	{ layers_44, MODEL_135 }
PULSE.c:133:9: note: Cost model analysis: 
PULSE.c:133:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:133:9: note: Costing subgraph: 
PULSE.c:133:9: note: node 0x474bcc8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:133:9: note: op template: <retval>.io = 0B;
PULSE.c:133:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:133:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:133:9: note: 	children 0x474bd48
PULSE.c:133:9: note: node (constant) 0x474bd48 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:133:9: note: 	{ 0B, 0B }
PULSE.c:133:9: note: Cost model analysis: 
PULSE.c:133:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:133:9: note: Basic block will be vectorized using SLP
PULSE.c:133:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:133:9: note: Vectorizing SLP tree:
PULSE.c:133:9: note: node 0x474bb48 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:133:9: note: op template: <retval>.layers = layers_44;
PULSE.c:133:9: note: 	stmt 0 <retval>.layers = layers_44;
PULSE.c:133:9: note: 	stmt 1 <retval>.paramethers = MODEL_135;
PULSE.c:133:9: note: 	children 0x474bc48
PULSE.c:133:9: note: node (external) 0x474bc48 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:133:9: note: 	{ layers_44, MODEL_135 }
PULSE.c:133:9: note: ------>vectorizing SLP node starting from: <retval>.layers = layers_44;
PULSE.c:133:9: note: vect_is_simple_use: operand MODEL_135 = PHI <MODEL_49(36), MODEL_113(13)>, type of def: internal
PULSE.c:133:9: note: conflicting alias set types.
PULSE.c:133:9: note: transform store. ncopies = 1
PULSE.c:133:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.layers
PULSE.c:133:9: note: created &<retval>
PULSE.c:133:9: note: add new stmt: MEM <vector(2) long unsigned int> [(void *)&<retval>] = _247;
PULSE.c:133:9: note: vectorizing stmts using SLP.
PULSE.c:133:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:133:9: note: Vectorizing SLP tree:
PULSE.c:133:9: note: node 0x474bcc8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:133:9: note: op template: <retval>.io = 0B;
PULSE.c:133:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:133:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:133:9: note: 	children 0x474bd48
PULSE.c:133:9: note: node (constant) 0x474bd48 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:133:9: note: 	{ 0B, 0B }
PULSE.c:133:9: note: ------>vectorizing SLP node starting from: <retval>.io = 0B;
PULSE.c:133:9: note: vect_is_simple_use: operand 0B, type of def: constant
PULSE.c:133:9: note: transform store. ncopies = 1
PULSE.c:133:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.io
PULSE.c:133:9: note: created &<retval>.io
PULSE.c:133:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval> + 16B] = { 0, 0 };
PULSE.c:133:9: note: vectorizing stmts using SLP.
PULSE.c:133:9: note: ***** The result for vector mode V32QI would be the same
PULSE.c:145:1: note: ***** Analysis failed with vector mode VOID
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:4:18: note: vectorized 0 loops in function.
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_4, inputs_15, _3);
PULSE.c:8:2: missed: statement clobbers memory: _5 (layer_17);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_12, pretmp_26, _9);
PULSE.c:13:28: note: ***** Analysis failed with vector mode V8SI
PULSE.c:13:28: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:22:2: missed: statement clobbers memory: _1 (layer_10(D));
PULSE.c:22:2: missed: statement clobbers memory: _14 (_2);
PULSE.c:22:2: missed: statement clobbers memory: _21 (_15);
PULSE.c:22:2: missed: statement clobbers memory: _28 (_22);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_29);
PULSE.c:26:3: missed: statement clobbers memory: memset (_34, 0, _32);
PULSE.c:26:3: missed: statement clobbers memory: memset (_27, 0, _25);
PULSE.c:26:3: missed: statement clobbers memory: memset (_20, 0, _18);
PULSE.c:26:3: missed: statement clobbers memory: memset (_7, 0, _5);
PULSE.c:28:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:28:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:32:6: note: vectorized 1 loops in function.
PULSE.c:34:8: missed: statement clobbers memory: _1 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_2);
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _9 = rand ();
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8SI
PULSE.c:32:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:61:16: missed: couldn't vectorize loop
PULSE.c:61:16: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:64:17: missed: couldn't vectorize loop
PULSE.c:64:17: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:73:19: missed: couldn't vectorize loop
PULSE.c:73:19: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _69 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:53:22: missed: couldn't vectorize loop
PULSE.c:53:22: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:47:6: note: vectorized 1 loops in function.
PULSE.c:48:1: missed: statement clobbers memory: saved_stack.7_39 = __builtin_stack_save ();
PULSE.c:49:46: missed: statement clobbers memory: _1 = PULSE_GetLossFunctionPtr (loss_function_40(D));
PULSE.c:60:10: missed: statement clobbers memory: random.4_46 = __builtin_alloca_with_align (_4, 32);
PULSE.c:34:8: missed: statement clobbers memory: _61 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_62);
PULSE.c:39:29: missed: statement clobbers memory: _69 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _72 = rand ();
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_98, inputs_93, _97);
PULSE.c:8:2: missed: statement clobbers memory: _99 (layer_94);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_105, _104, _103);
PULSE.c:67:11: missed: statement clobbers memory: loss_55 = PULSE_GetLoss_42 (_17, _16, _12, _11);
PULSE.c:22:2: missed: statement clobbers memory: _86 (output_34);
PULSE.c:22:2: missed: statement clobbers memory: _108 (_87);
PULSE.c:22:2: missed: statement clobbers memory: _115 (_109);
PULSE.c:22:2: missed: statement clobbers memory: _122 (_116);
PULSE.c:22:2: missed: statement clobbers memory: _129 (_123);
PULSE.c:22:2: missed: statement clobbers memory: _136 (_130);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_137);
PULSE.c:26:3: missed: statement clobbers memory: memset (_142, 0, _140);
PULSE.c:26:3: missed: statement clobbers memory: memset (_135, 0, _133);
PULSE.c:26:3: missed: statement clobbers memory: memset (_128, 0, _126);
PULSE.c:26:3: missed: statement clobbers memory: memset (_121, 0, _119);
PULSE.c:26:3: missed: statement clobbers memory: memset (_114, 0, _112);
PULSE.c:26:3: missed: statement clobbers memory: memset (_92, 0, _90);
PULSE.c:75:6: missed: statement clobbers memory: _21 (current_191, args);
PULSE.c:81:4: missed: statement clobbers memory: printf ("Epoch: %d | Item: %d | Loss: %.10f | Batch Loss: %.10f\r", i_188, j_189, _25, _24);
PULSE.c:47:6: missed: statement clobbers memory: __builtin_stack_restore (saved_stack.7_39);
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4DI
PULSE.c:47:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:91:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:91:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:119:20: missed: couldn't vectorize loop
PULSE.c:119:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:119:20: missed: couldn't vectorize loop
PULSE.c:119:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:102:20: missed: couldn't vectorize loop
PULSE.c:102:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:102:20: missed: couldn't vectorize loop
PULSE.c:102:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:93:13: note: vectorized 0 loops in function.
PULSE.c:96:39: missed: statement clobbers memory: layers_45 = malloc (_3);
PULSE.c:99:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:114:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:115:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:116:44: missed: statement clobbers memory: MODEL_115 = malloc (0);
PULSE.c:114:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:115:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:116:44: missed: statement clobbers memory: MODEL_50 = malloc (_11);
PULSE.c:125:62: missed: not vectorized: more than one data ref in stmt: D.6469 = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.106_80];
PULSE.c:125:17: missed: statement clobbers memory: *_17 = PULSE_CreateDenseLayer (D.6469, _150);
PULSE.c:127:5: missed: statement clobbers memory: printf ("%d\n", _24);
PULSE.c:125:62: missed: not vectorized: more than one data ref in stmt: D.6469 = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.106_201];
PULSE.c:125:17: missed: statement clobbers memory: *_194 = PULSE_CreateDenseLayer (D.6469, _200);
PULSE.c:127:5: missed: statement clobbers memory: printf ("%d\n", _185);
PULSE.c:133:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:134:9: note: ***** Analysis succeeded with vector mode V4DI
PULSE.c:134:9: note: SLPing BB part
PULSE.c:134:9: note: Costing subgraph: 
PULSE.c:134:9: note: node 0x4e3bb68 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:134:9: note: op template: <retval>.layers = layers_45;
PULSE.c:134:9: note: 	stmt 0 <retval>.layers = layers_45;
PULSE.c:134:9: note: 	stmt 1 <retval>.paramethers = MODEL_137;
PULSE.c:134:9: note: 	children 0x4e3bc68
PULSE.c:134:9: note: node (external) 0x4e3bc68 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:134:9: note: 	{ layers_45, MODEL_137 }
PULSE.c:134:9: note: Cost model analysis: 
PULSE.c:134:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:134:9: note: Costing subgraph: 
PULSE.c:134:9: note: node 0x4e3bce8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:134:9: note: op template: <retval>.io = 0B;
PULSE.c:134:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:134:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:134:9: note: 	children 0x4e3bd68
PULSE.c:134:9: note: node (constant) 0x4e3bd68 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:134:9: note: 	{ 0B, 0B }
PULSE.c:134:9: note: Cost model analysis: 
PULSE.c:134:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:134:9: note: Basic block will be vectorized using SLP
PULSE.c:134:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:134:9: note: Vectorizing SLP tree:
PULSE.c:134:9: note: node 0x4e3bb68 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:134:9: note: op template: <retval>.layers = layers_45;
PULSE.c:134:9: note: 	stmt 0 <retval>.layers = layers_45;
PULSE.c:134:9: note: 	stmt 1 <retval>.paramethers = MODEL_137;
PULSE.c:134:9: note: 	children 0x4e3bc68
PULSE.c:134:9: note: node (external) 0x4e3bc68 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:134:9: note: 	{ layers_45, MODEL_137 }
PULSE.c:134:9: note: ------>vectorizing SLP node starting from: <retval>.layers = layers_45;
PULSE.c:134:9: note: vect_is_simple_use: operand MODEL_137 = PHI <MODEL_50(36), MODEL_115(13)>, type of def: internal
PULSE.c:134:9: note: conflicting alias set types.
PULSE.c:134:9: note: transform store. ncopies = 1
PULSE.c:134:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.layers
PULSE.c:134:9: note: created &<retval>
PULSE.c:134:9: note: add new stmt: MEM <vector(2) long unsigned int> [(void *)&<retval>] = _251;
PULSE.c:134:9: note: vectorizing stmts using SLP.
PULSE.c:134:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:134:9: note: Vectorizing SLP tree:
PULSE.c:134:9: note: node 0x4e3bce8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:134:9: note: op template: <retval>.io = 0B;
PULSE.c:134:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:134:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:134:9: note: 	children 0x4e3bd68
PULSE.c:134:9: note: node (constant) 0x4e3bd68 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:134:9: note: 	{ 0B, 0B }
PULSE.c:134:9: note: ------>vectorizing SLP node starting from: <retval>.io = 0B;
PULSE.c:134:9: note: vect_is_simple_use: operand 0B, type of def: constant
PULSE.c:134:9: note: transform store. ncopies = 1
PULSE.c:134:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.io
PULSE.c:134:9: note: created &<retval>.io
PULSE.c:134:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval> + 16B] = { 0, 0 };
PULSE.c:134:9: note: vectorizing stmts using SLP.
PULSE.c:134:9: note: ***** The result for vector mode V32QI would be the same
PULSE.c:146:1: note: ***** Analysis failed with vector mode VOID
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:4:18: note: vectorized 0 loops in function.
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_4, inputs_15, _3);
PULSE.c:8:2: missed: statement clobbers memory: _5 (layer_17);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_12, pretmp_26, _9);
PULSE.c:13:28: note: ***** Analysis failed with vector mode V8SI
PULSE.c:13:28: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:22:2: missed: statement clobbers memory: _1 (layer_10(D));
PULSE.c:22:2: missed: statement clobbers memory: _14 (_2);
PULSE.c:22:2: missed: statement clobbers memory: _21 (_15);
PULSE.c:22:2: missed: statement clobbers memory: _28 (_22);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_29);
PULSE.c:26:3: missed: statement clobbers memory: memset (_34, 0, _32);
PULSE.c:26:3: missed: statement clobbers memory: memset (_27, 0, _25);
PULSE.c:26:3: missed: statement clobbers memory: memset (_20, 0, _18);
PULSE.c:26:3: missed: statement clobbers memory: memset (_7, 0, _5);
PULSE.c:28:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:28:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:32:6: note: vectorized 1 loops in function.
PULSE.c:34:8: missed: statement clobbers memory: _1 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_2);
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _9 = rand ();
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8SI
PULSE.c:32:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:61:16: missed: couldn't vectorize loop
PULSE.c:61:16: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:64:17: missed: couldn't vectorize loop
PULSE.c:64:17: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:73:19: missed: couldn't vectorize loop
PULSE.c:73:19: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _69 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:53:22: missed: couldn't vectorize loop
PULSE.c:53:22: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:47:6: note: vectorized 1 loops in function.
PULSE.c:48:1: missed: statement clobbers memory: saved_stack.7_39 = __builtin_stack_save ();
PULSE.c:49:46: missed: statement clobbers memory: _1 = PULSE_GetLossFunctionPtr (loss_function_40(D));
PULSE.c:60:10: missed: statement clobbers memory: random.4_46 = __builtin_alloca_with_align (_4, 32);
PULSE.c:34:8: missed: statement clobbers memory: _61 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_62);
PULSE.c:39:29: missed: statement clobbers memory: _69 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _72 = rand ();
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_98, inputs_93, _97);
PULSE.c:8:2: missed: statement clobbers memory: _99 (layer_94);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_105, _104, _103);
PULSE.c:67:11: missed: statement clobbers memory: loss_55 = PULSE_GetLoss_42 (_17, _16, _12, _11);
PULSE.c:22:2: missed: statement clobbers memory: _86 (output_34);
PULSE.c:22:2: missed: statement clobbers memory: _108 (_87);
PULSE.c:22:2: missed: statement clobbers memory: _115 (_109);
PULSE.c:22:2: missed: statement clobbers memory: _122 (_116);
PULSE.c:22:2: missed: statement clobbers memory: _129 (_123);
PULSE.c:22:2: missed: statement clobbers memory: _136 (_130);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_137);
PULSE.c:26:3: missed: statement clobbers memory: memset (_142, 0, _140);
PULSE.c:26:3: missed: statement clobbers memory: memset (_135, 0, _133);
PULSE.c:26:3: missed: statement clobbers memory: memset (_128, 0, _126);
PULSE.c:26:3: missed: statement clobbers memory: memset (_121, 0, _119);
PULSE.c:26:3: missed: statement clobbers memory: memset (_114, 0, _112);
PULSE.c:26:3: missed: statement clobbers memory: memset (_92, 0, _90);
PULSE.c:75:6: missed: statement clobbers memory: _21 (current_191, args);
PULSE.c:81:4: missed: statement clobbers memory: printf ("Epoch: %d | Item: %d | Loss: %.10f | Batch Loss: %.10f\r", i_188, j_189, _25, _24);
PULSE.c:47:6: missed: statement clobbers memory: __builtin_stack_restore (saved_stack.7_39);
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4DI
PULSE.c:47:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:91:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:91:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:119:20: missed: couldn't vectorize loop
PULSE.c:119:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:119:20: missed: couldn't vectorize loop
PULSE.c:119:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:102:20: missed: couldn't vectorize loop
PULSE.c:102:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:102:20: missed: couldn't vectorize loop
PULSE.c:102:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:93:13: note: vectorized 0 loops in function.
PULSE.c:96:39: missed: statement clobbers memory: layers_45 = malloc (_3);
PULSE.c:99:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:114:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:115:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:116:44: missed: statement clobbers memory: MODEL_115 = malloc (0);
PULSE.c:114:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:115:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:116:44: missed: statement clobbers memory: MODEL_50 = malloc (_11);
PULSE.c:125:62: missed: not vectorized: more than one data ref in stmt: D.6469 = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.106_80];
PULSE.c:125:17: missed: statement clobbers memory: *_17 = PULSE_CreateDenseLayer (D.6469, _150);
PULSE.c:127:5: missed: statement clobbers memory: printf ("\nPOS: %d\n", _24);
PULSE.c:125:62: missed: not vectorized: more than one data ref in stmt: D.6469 = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.106_201];
PULSE.c:125:17: missed: statement clobbers memory: *_194 = PULSE_CreateDenseLayer (D.6469, _200);
PULSE.c:127:5: missed: statement clobbers memory: printf ("\nPOS: %d\n", _185);
PULSE.c:133:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:134:9: note: ***** Analysis succeeded with vector mode V4DI
PULSE.c:134:9: note: SLPing BB part
PULSE.c:134:9: note: Costing subgraph: 
PULSE.c:134:9: note: node 0x4e47b68 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:134:9: note: op template: <retval>.layers = layers_45;
PULSE.c:134:9: note: 	stmt 0 <retval>.layers = layers_45;
PULSE.c:134:9: note: 	stmt 1 <retval>.paramethers = MODEL_137;
PULSE.c:134:9: note: 	children 0x4e47c68
PULSE.c:134:9: note: node (external) 0x4e47c68 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:134:9: note: 	{ layers_45, MODEL_137 }
PULSE.c:134:9: note: Cost model analysis: 
PULSE.c:134:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:134:9: note: Costing subgraph: 
PULSE.c:134:9: note: node 0x4e47ce8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:134:9: note: op template: <retval>.io = 0B;
PULSE.c:134:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:134:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:134:9: note: 	children 0x4e47d68
PULSE.c:134:9: note: node (constant) 0x4e47d68 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:134:9: note: 	{ 0B, 0B }
PULSE.c:134:9: note: Cost model analysis: 
PULSE.c:134:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:134:9: note: Basic block will be vectorized using SLP
PULSE.c:134:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:134:9: note: Vectorizing SLP tree:
PULSE.c:134:9: note: node 0x4e47b68 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:134:9: note: op template: <retval>.layers = layers_45;
PULSE.c:134:9: note: 	stmt 0 <retval>.layers = layers_45;
PULSE.c:134:9: note: 	stmt 1 <retval>.paramethers = MODEL_137;
PULSE.c:134:9: note: 	children 0x4e47c68
PULSE.c:134:9: note: node (external) 0x4e47c68 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:134:9: note: 	{ layers_45, MODEL_137 }
PULSE.c:134:9: note: ------>vectorizing SLP node starting from: <retval>.layers = layers_45;
PULSE.c:134:9: note: vect_is_simple_use: operand MODEL_137 = PHI <MODEL_50(36), MODEL_115(13)>, type of def: internal
PULSE.c:134:9: note: conflicting alias set types.
PULSE.c:134:9: note: transform store. ncopies = 1
PULSE.c:134:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.layers
PULSE.c:134:9: note: created &<retval>
PULSE.c:134:9: note: add new stmt: MEM <vector(2) long unsigned int> [(void *)&<retval>] = _251;
PULSE.c:134:9: note: vectorizing stmts using SLP.
PULSE.c:134:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:134:9: note: Vectorizing SLP tree:
PULSE.c:134:9: note: node 0x4e47ce8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:134:9: note: op template: <retval>.io = 0B;
PULSE.c:134:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:134:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:134:9: note: 	children 0x4e47d68
PULSE.c:134:9: note: node (constant) 0x4e47d68 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:134:9: note: 	{ 0B, 0B }
PULSE.c:134:9: note: ------>vectorizing SLP node starting from: <retval>.io = 0B;
PULSE.c:134:9: note: vect_is_simple_use: operand 0B, type of def: constant
PULSE.c:134:9: note: transform store. ncopies = 1
PULSE.c:134:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.io
PULSE.c:134:9: note: created &<retval>.io
PULSE.c:134:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval> + 16B] = { 0, 0 };
PULSE.c:134:9: note: vectorizing stmts using SLP.
PULSE.c:134:9: note: ***** The result for vector mode V32QI would be the same
PULSE.c:146:1: note: ***** Analysis failed with vector mode VOID
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:4:18: note: vectorized 0 loops in function.
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_4, inputs_15, _3);
PULSE.c:8:2: missed: statement clobbers memory: _5 (layer_17);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_12, pretmp_26, _9);
PULSE.c:13:28: note: ***** Analysis failed with vector mode V8SI
PULSE.c:13:28: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:22:2: missed: statement clobbers memory: _1 (layer_10(D));
PULSE.c:22:2: missed: statement clobbers memory: _14 (_2);
PULSE.c:22:2: missed: statement clobbers memory: _21 (_15);
PULSE.c:22:2: missed: statement clobbers memory: _28 (_22);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_29);
PULSE.c:26:3: missed: statement clobbers memory: memset (_34, 0, _32);
PULSE.c:26:3: missed: statement clobbers memory: memset (_27, 0, _25);
PULSE.c:26:3: missed: statement clobbers memory: memset (_20, 0, _18);
PULSE.c:26:3: missed: statement clobbers memory: memset (_7, 0, _5);
PULSE.c:28:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:28:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:32:6: note: vectorized 1 loops in function.
PULSE.c:34:8: missed: statement clobbers memory: _1 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_2);
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _9 = rand ();
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8SI
PULSE.c:32:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:61:16: missed: couldn't vectorize loop
PULSE.c:61:16: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:64:17: missed: couldn't vectorize loop
PULSE.c:64:17: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:73:19: missed: couldn't vectorize loop
PULSE.c:73:19: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _69 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:53:22: missed: couldn't vectorize loop
PULSE.c:53:22: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:47:6: note: vectorized 1 loops in function.
PULSE.c:48:1: missed: statement clobbers memory: saved_stack.7_39 = __builtin_stack_save ();
PULSE.c:49:46: missed: statement clobbers memory: _1 = PULSE_GetLossFunctionPtr (loss_function_40(D));
PULSE.c:60:10: missed: statement clobbers memory: random.4_46 = __builtin_alloca_with_align (_4, 32);
PULSE.c:34:8: missed: statement clobbers memory: _61 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_62);
PULSE.c:39:29: missed: statement clobbers memory: _69 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _72 = rand ();
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_98, inputs_93, _97);
PULSE.c:8:2: missed: statement clobbers memory: _99 (layer_94);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_105, _104, _103);
PULSE.c:67:11: missed: statement clobbers memory: loss_55 = PULSE_GetLoss_42 (_17, _16, _12, _11);
PULSE.c:22:2: missed: statement clobbers memory: _86 (output_34);
PULSE.c:22:2: missed: statement clobbers memory: _108 (_87);
PULSE.c:22:2: missed: statement clobbers memory: _115 (_109);
PULSE.c:22:2: missed: statement clobbers memory: _122 (_116);
PULSE.c:22:2: missed: statement clobbers memory: _129 (_123);
PULSE.c:22:2: missed: statement clobbers memory: _136 (_130);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_137);
PULSE.c:26:3: missed: statement clobbers memory: memset (_142, 0, _140);
PULSE.c:26:3: missed: statement clobbers memory: memset (_135, 0, _133);
PULSE.c:26:3: missed: statement clobbers memory: memset (_128, 0, _126);
PULSE.c:26:3: missed: statement clobbers memory: memset (_121, 0, _119);
PULSE.c:26:3: missed: statement clobbers memory: memset (_114, 0, _112);
PULSE.c:26:3: missed: statement clobbers memory: memset (_92, 0, _90);
PULSE.c:75:6: missed: statement clobbers memory: _21 (current_191, args);
PULSE.c:81:4: missed: statement clobbers memory: printf ("Epoch: %d | Item: %d | Loss: %.10f | Batch Loss: %.10f\r", i_188, j_189, _25, _24);
PULSE.c:47:6: missed: statement clobbers memory: __builtin_stack_restore (saved_stack.7_39);
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4DI
PULSE.c:47:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:91:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:91:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:119:20: missed: couldn't vectorize loop
PULSE.c:119:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:119:20: missed: couldn't vectorize loop
PULSE.c:119:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:102:20: missed: couldn't vectorize loop
PULSE.c:102:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:102:20: missed: couldn't vectorize loop
PULSE.c:102:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:93:13: note: vectorized 0 loops in function.
PULSE.c:96:39: missed: statement clobbers memory: layers_45 = malloc (_3);
PULSE.c:99:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:114:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:115:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:116:44: missed: statement clobbers memory: MODEL_115 = malloc (0);
PULSE.c:114:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:115:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:116:44: missed: statement clobbers memory: MODEL_50 = malloc (_11);
PULSE.c:125:62: missed: not vectorized: more than one data ref in stmt: D.6469 = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.106_80];
PULSE.c:125:17: missed: statement clobbers memory: *_17 = PULSE_CreateDenseLayer (D.6469, _150);
PULSE.c:127:5: missed: statement clobbers memory: printf ("\nPOS: %d\n", _24);
PULSE.c:125:62: missed: not vectorized: more than one data ref in stmt: D.6469 = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.106_201];
PULSE.c:125:17: missed: statement clobbers memory: *_194 = PULSE_CreateDenseLayer (D.6469, _200);
PULSE.c:127:5: missed: statement clobbers memory: printf ("\nPOS: %d\n", _185);
PULSE.c:133:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:134:9: note: ***** Analysis succeeded with vector mode V4DI
PULSE.c:134:9: note: SLPing BB part
PULSE.c:134:9: note: Costing subgraph: 
PULSE.c:134:9: note: node 0x33f4ba8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:134:9: note: op template: <retval>.layers = layers_45;
PULSE.c:134:9: note: 	stmt 0 <retval>.layers = layers_45;
PULSE.c:134:9: note: 	stmt 1 <retval>.paramethers = MODEL_137;
PULSE.c:134:9: note: 	children 0x33f4ca8
PULSE.c:134:9: note: node (external) 0x33f4ca8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:134:9: note: 	{ layers_45, MODEL_137 }
PULSE.c:134:9: note: Cost model analysis: 
PULSE.c:134:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:134:9: note: Costing subgraph: 
PULSE.c:134:9: note: node 0x33f4d28 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:134:9: note: op template: <retval>.io = 0B;
PULSE.c:134:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:134:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:134:9: note: 	children 0x33f4da8
PULSE.c:134:9: note: node (constant) 0x33f4da8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:134:9: note: 	{ 0B, 0B }
PULSE.c:134:9: note: Cost model analysis: 
PULSE.c:134:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:134:9: note: Basic block will be vectorized using SLP
PULSE.c:134:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:134:9: note: Vectorizing SLP tree:
PULSE.c:134:9: note: node 0x33f4ba8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:134:9: note: op template: <retval>.layers = layers_45;
PULSE.c:134:9: note: 	stmt 0 <retval>.layers = layers_45;
PULSE.c:134:9: note: 	stmt 1 <retval>.paramethers = MODEL_137;
PULSE.c:134:9: note: 	children 0x33f4ca8
PULSE.c:134:9: note: node (external) 0x33f4ca8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:134:9: note: 	{ layers_45, MODEL_137 }
PULSE.c:134:9: note: ------>vectorizing SLP node starting from: <retval>.layers = layers_45;
PULSE.c:134:9: note: vect_is_simple_use: operand MODEL_137 = PHI <MODEL_50(36), MODEL_115(13)>, type of def: internal
PULSE.c:134:9: note: conflicting alias set types.
PULSE.c:134:9: note: transform store. ncopies = 1
PULSE.c:134:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.layers
PULSE.c:134:9: note: created &<retval>
PULSE.c:134:9: note: add new stmt: MEM <vector(2) long unsigned int> [(void *)&<retval>] = _243;
PULSE.c:134:9: note: vectorizing stmts using SLP.
PULSE.c:134:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:134:9: note: Vectorizing SLP tree:
PULSE.c:134:9: note: node 0x33f4d28 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:134:9: note: op template: <retval>.io = 0B;
PULSE.c:134:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:134:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:134:9: note: 	children 0x33f4da8
PULSE.c:134:9: note: node (constant) 0x33f4da8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:134:9: note: 	{ 0B, 0B }
PULSE.c:134:9: note: ------>vectorizing SLP node starting from: <retval>.io = 0B;
PULSE.c:134:9: note: vect_is_simple_use: operand 0B, type of def: constant
PULSE.c:134:9: note: transform store. ncopies = 1
PULSE.c:134:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.io
PULSE.c:134:9: note: created &<retval>.io
PULSE.c:134:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval> + 16B] = { 0, 0 };
PULSE.c:134:9: note: vectorizing stmts using SLP.
PULSE.c:134:9: note: ***** The result for vector mode V32QI would be the same
PULSE.c:146:1: note: ***** Analysis failed with vector mode VOID
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:4:18: note: vectorized 0 loops in function.
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_4, inputs_15, _3);
PULSE.c:8:2: missed: statement clobbers memory: _5 (layer_17);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_12, pretmp_26, _9);
PULSE.c:13:28: note: ***** Analysis failed with vector mode V8SI
PULSE.c:13:28: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:22:2: missed: statement clobbers memory: _1 (layer_10(D));
PULSE.c:22:2: missed: statement clobbers memory: _14 (_2);
PULSE.c:22:2: missed: statement clobbers memory: _21 (_15);
PULSE.c:22:2: missed: statement clobbers memory: _28 (_22);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_29);
PULSE.c:26:3: missed: statement clobbers memory: memset (_34, 0, _32);
PULSE.c:26:3: missed: statement clobbers memory: memset (_27, 0, _25);
PULSE.c:26:3: missed: statement clobbers memory: memset (_20, 0, _18);
PULSE.c:26:3: missed: statement clobbers memory: memset (_7, 0, _5);
PULSE.c:28:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:28:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:32:6: note: vectorized 1 loops in function.
PULSE.c:34:8: missed: statement clobbers memory: _1 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_2);
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _9 = rand ();
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8SI
PULSE.c:32:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:61:16: missed: couldn't vectorize loop
PULSE.c:61:16: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:64:17: missed: couldn't vectorize loop
PULSE.c:64:17: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:73:19: missed: couldn't vectorize loop
PULSE.c:73:19: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _69 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:53:22: missed: couldn't vectorize loop
PULSE.c:53:22: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:47:6: note: vectorized 1 loops in function.
PULSE.c:48:1: missed: statement clobbers memory: saved_stack.7_39 = __builtin_stack_save ();
PULSE.c:49:46: missed: statement clobbers memory: _1 = PULSE_GetLossFunctionPtr (loss_function_40(D));
PULSE.c:60:10: missed: statement clobbers memory: random.4_46 = __builtin_alloca_with_align (_4, 32);
PULSE.c:34:8: missed: statement clobbers memory: _61 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_62);
PULSE.c:39:29: missed: statement clobbers memory: _69 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _72 = rand ();
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_98, inputs_93, _97);
PULSE.c:8:2: missed: statement clobbers memory: _99 (layer_94);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_105, _104, _103);
PULSE.c:67:11: missed: statement clobbers memory: loss_55 = PULSE_GetLoss_42 (_17, _16, _12, _11);
PULSE.c:22:2: missed: statement clobbers memory: _86 (output_34);
PULSE.c:22:2: missed: statement clobbers memory: _108 (_87);
PULSE.c:22:2: missed: statement clobbers memory: _115 (_109);
PULSE.c:22:2: missed: statement clobbers memory: _122 (_116);
PULSE.c:22:2: missed: statement clobbers memory: _129 (_123);
PULSE.c:22:2: missed: statement clobbers memory: _136 (_130);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_137);
PULSE.c:26:3: missed: statement clobbers memory: memset (_142, 0, _140);
PULSE.c:26:3: missed: statement clobbers memory: memset (_135, 0, _133);
PULSE.c:26:3: missed: statement clobbers memory: memset (_128, 0, _126);
PULSE.c:26:3: missed: statement clobbers memory: memset (_121, 0, _119);
PULSE.c:26:3: missed: statement clobbers memory: memset (_114, 0, _112);
PULSE.c:26:3: missed: statement clobbers memory: memset (_92, 0, _90);
PULSE.c:75:6: missed: statement clobbers memory: _21 (current_191, args);
PULSE.c:81:4: missed: statement clobbers memory: printf ("Epoch: %d | Item: %d | Loss: %.10f | Batch Loss: %.10f\r", i_188, j_189, _25, _24);
PULSE.c:47:6: missed: statement clobbers memory: __builtin_stack_restore (saved_stack.7_39);
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4DI
PULSE.c:47:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:91:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:91:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:119:20: missed: couldn't vectorize loop
PULSE.c:119:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:119:20: missed: couldn't vectorize loop
PULSE.c:119:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:102:20: missed: couldn't vectorize loop
PULSE.c:102:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:102:20: missed: couldn't vectorize loop
PULSE.c:102:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:93:13: note: vectorized 0 loops in function.
PULSE.c:96:39: missed: statement clobbers memory: layers_46 = malloc (_3);
PULSE.c:99:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:114:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:115:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:116:44: missed: statement clobbers memory: MODEL_117 = malloc (0);
PULSE.c:114:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:115:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:116:44: missed: statement clobbers memory: MODEL_51 = malloc (_11);
PULSE.c:125:62: missed: not vectorized: more than one data ref in stmt: D.6469 = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.106_82];
PULSE.c:125:17: missed: statement clobbers memory: *_17 = PULSE_CreateDenseLayer (D.6469, _152);
PULSE.c:127:5: missed: statement clobbers memory: printf ("\nPOS: %d\n", _24);
PULSE.c:128:5: missed: statement clobbers memory: printf ("\nPOS: %d\n", _25);
PULSE.c:125:62: missed: not vectorized: more than one data ref in stmt: D.6469 = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.106_203];
PULSE.c:125:17: missed: statement clobbers memory: *_196 = PULSE_CreateDenseLayer (D.6469, _202);
PULSE.c:127:5: missed: statement clobbers memory: printf ("\nPOS: %d\n", _187);
PULSE.c:128:5: missed: statement clobbers memory: printf ("\nPOS: %d\n", _185);
PULSE.c:134:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:135:9: note: ***** Analysis succeeded with vector mode V4DI
PULSE.c:135:9: note: SLPing BB part
PULSE.c:135:9: note: Costing subgraph: 
PULSE.c:135:9: note: node 0x462fbc8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:135:9: note: op template: <retval>.layers = layers_46;
PULSE.c:135:9: note: 	stmt 0 <retval>.layers = layers_46;
PULSE.c:135:9: note: 	stmt 1 <retval>.paramethers = MODEL_139;
PULSE.c:135:9: note: 	children 0x462fcc8
PULSE.c:135:9: note: node (external) 0x462fcc8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:135:9: note: 	{ layers_46, MODEL_139 }
PULSE.c:135:9: note: Cost model analysis: 
PULSE.c:135:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:135:9: note: Costing subgraph: 
PULSE.c:135:9: note: node 0x462fd48 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:135:9: note: op template: <retval>.io = 0B;
PULSE.c:135:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:135:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:135:9: note: 	children 0x462fdc8
PULSE.c:135:9: note: node (constant) 0x462fdc8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:135:9: note: 	{ 0B, 0B }
PULSE.c:135:9: note: Cost model analysis: 
PULSE.c:135:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:135:9: note: Basic block will be vectorized using SLP
PULSE.c:135:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:135:9: note: Vectorizing SLP tree:
PULSE.c:135:9: note: node 0x462fbc8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:135:9: note: op template: <retval>.layers = layers_46;
PULSE.c:135:9: note: 	stmt 0 <retval>.layers = layers_46;
PULSE.c:135:9: note: 	stmt 1 <retval>.paramethers = MODEL_139;
PULSE.c:135:9: note: 	children 0x462fcc8
PULSE.c:135:9: note: node (external) 0x462fcc8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:135:9: note: 	{ layers_46, MODEL_139 }
PULSE.c:135:9: note: ------>vectorizing SLP node starting from: <retval>.layers = layers_46;
PULSE.c:135:9: note: vect_is_simple_use: operand MODEL_139 = PHI <MODEL_51(36), MODEL_117(13)>, type of def: internal
PULSE.c:135:9: note: conflicting alias set types.
PULSE.c:135:9: note: transform store. ncopies = 1
PULSE.c:135:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.layers
PULSE.c:135:9: note: created &<retval>
PULSE.c:135:9: note: add new stmt: MEM <vector(2) long unsigned int> [(void *)&<retval>] = _247;
PULSE.c:135:9: note: vectorizing stmts using SLP.
PULSE.c:135:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:135:9: note: Vectorizing SLP tree:
PULSE.c:135:9: note: node 0x462fd48 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:135:9: note: op template: <retval>.io = 0B;
PULSE.c:135:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:135:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:135:9: note: 	children 0x462fdc8
PULSE.c:135:9: note: node (constant) 0x462fdc8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:135:9: note: 	{ 0B, 0B }
PULSE.c:135:9: note: ------>vectorizing SLP node starting from: <retval>.io = 0B;
PULSE.c:135:9: note: vect_is_simple_use: operand 0B, type of def: constant
PULSE.c:135:9: note: transform store. ncopies = 1
PULSE.c:135:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.io
PULSE.c:135:9: note: created &<retval>.io
PULSE.c:135:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval> + 16B] = { 0, 0 };
PULSE.c:135:9: note: vectorizing stmts using SLP.
PULSE.c:135:9: note: ***** The result for vector mode V32QI would be the same
PULSE.c:147:1: note: ***** Analysis failed with vector mode VOID
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:4:18: note: vectorized 0 loops in function.
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_4, inputs_15, _3);
PULSE.c:8:2: missed: statement clobbers memory: _5 (layer_17);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_12, pretmp_26, _9);
PULSE.c:13:28: note: ***** Analysis failed with vector mode V8SI
PULSE.c:13:28: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:22:2: missed: statement clobbers memory: _1 (layer_10(D));
PULSE.c:22:2: missed: statement clobbers memory: _14 (_2);
PULSE.c:22:2: missed: statement clobbers memory: _21 (_15);
PULSE.c:22:2: missed: statement clobbers memory: _28 (_22);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_29);
PULSE.c:26:3: missed: statement clobbers memory: memset (_34, 0, _32);
PULSE.c:26:3: missed: statement clobbers memory: memset (_27, 0, _25);
PULSE.c:26:3: missed: statement clobbers memory: memset (_20, 0, _18);
PULSE.c:26:3: missed: statement clobbers memory: memset (_7, 0, _5);
PULSE.c:28:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:28:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:32:6: note: vectorized 1 loops in function.
PULSE.c:34:8: missed: statement clobbers memory: _1 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_2);
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _9 = rand ();
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8SI
PULSE.c:32:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:61:16: missed: couldn't vectorize loop
PULSE.c:61:16: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:64:17: missed: couldn't vectorize loop
PULSE.c:64:17: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:73:19: missed: couldn't vectorize loop
PULSE.c:73:19: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _69 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:53:22: missed: couldn't vectorize loop
PULSE.c:53:22: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:47:6: note: vectorized 1 loops in function.
PULSE.c:48:1: missed: statement clobbers memory: saved_stack.7_39 = __builtin_stack_save ();
PULSE.c:49:46: missed: statement clobbers memory: _1 = PULSE_GetLossFunctionPtr (loss_function_40(D));
PULSE.c:60:10: missed: statement clobbers memory: random.4_46 = __builtin_alloca_with_align (_4, 32);
PULSE.c:34:8: missed: statement clobbers memory: _61 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_62);
PULSE.c:39:29: missed: statement clobbers memory: _69 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _72 = rand ();
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_98, inputs_93, _97);
PULSE.c:8:2: missed: statement clobbers memory: _99 (layer_94);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_105, _104, _103);
PULSE.c:67:11: missed: statement clobbers memory: loss_55 = PULSE_GetLoss_42 (_17, _16, _12, _11);
PULSE.c:22:2: missed: statement clobbers memory: _86 (output_34);
PULSE.c:22:2: missed: statement clobbers memory: _108 (_87);
PULSE.c:22:2: missed: statement clobbers memory: _115 (_109);
PULSE.c:22:2: missed: statement clobbers memory: _122 (_116);
PULSE.c:22:2: missed: statement clobbers memory: _129 (_123);
PULSE.c:22:2: missed: statement clobbers memory: _136 (_130);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_137);
PULSE.c:26:3: missed: statement clobbers memory: memset (_142, 0, _140);
PULSE.c:26:3: missed: statement clobbers memory: memset (_135, 0, _133);
PULSE.c:26:3: missed: statement clobbers memory: memset (_128, 0, _126);
PULSE.c:26:3: missed: statement clobbers memory: memset (_121, 0, _119);
PULSE.c:26:3: missed: statement clobbers memory: memset (_114, 0, _112);
PULSE.c:26:3: missed: statement clobbers memory: memset (_92, 0, _90);
PULSE.c:75:6: missed: statement clobbers memory: _21 (current_191, args);
PULSE.c:81:4: missed: statement clobbers memory: printf ("Epoch: %d | Item: %d | Loss: %.10f | Batch Loss: %.10f\r", i_188, j_189, _25, _24);
PULSE.c:47:6: missed: statement clobbers memory: __builtin_stack_restore (saved_stack.7_39);
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4DI
PULSE.c:47:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:91:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:91:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:119:20: missed: couldn't vectorize loop
PULSE.c:119:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:119:20: missed: couldn't vectorize loop
PULSE.c:119:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:102:20: missed: couldn't vectorize loop
PULSE.c:102:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:102:20: missed: couldn't vectorize loop
PULSE.c:102:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:93:13: note: vectorized 0 loops in function.
PULSE.c:96:39: missed: statement clobbers memory: layers_40 = malloc (_3);
PULSE.c:99:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:114:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:115:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:116:44: missed: statement clobbers memory: MODEL_111 = malloc (0);
PULSE.c:114:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:115:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:116:44: missed: statement clobbers memory: MODEL_45 = malloc (_9);
PULSE.c:125:62: missed: not vectorized: more than one data ref in stmt: D.6469 = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.106_76];
PULSE.c:125:17: missed: statement clobbers memory: *_15 = PULSE_CreateDenseLayer (D.6469, _146);
PULSE.c:127:5: missed: statement clobbers memory: printf ("\nPOS: %d\n", _19);
PULSE.c:128:5: missed: statement clobbers memory: printf ("\nPOS: %d\n", MODEL_POS_57);
PULSE.c:125:62: missed: not vectorized: more than one data ref in stmt: D.6469 = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.106_183];
PULSE.c:125:17: missed: statement clobbers memory: *_176 = PULSE_CreateDenseLayer (D.6469, _182);
PULSE.c:127:5: missed: statement clobbers memory: printf ("\nPOS: %d\n", _171);
PULSE.c:128:5: missed: statement clobbers memory: printf ("\nPOS: %d\n", MODEL_POS_170);
PULSE.c:134:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:135:9: note: ***** Analysis succeeded with vector mode V4DI
PULSE.c:135:9: note: SLPing BB part
PULSE.c:135:9: note: Costing subgraph: 
PULSE.c:135:9: note: node 0x30b4bc8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:135:9: note: op template: <retval>.layers = layers_40;
PULSE.c:135:9: note: 	stmt 0 <retval>.layers = layers_40;
PULSE.c:135:9: note: 	stmt 1 <retval>.paramethers = MODEL_133;
PULSE.c:135:9: note: 	children 0x30b4cc8
PULSE.c:135:9: note: node (external) 0x30b4cc8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:135:9: note: 	{ layers_40, MODEL_133 }
PULSE.c:135:9: note: Cost model analysis: 
PULSE.c:135:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:135:9: note: Costing subgraph: 
PULSE.c:135:9: note: node 0x30b4d48 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:135:9: note: op template: <retval>.io = 0B;
PULSE.c:135:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:135:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:135:9: note: 	children 0x30b4dc8
PULSE.c:135:9: note: node (constant) 0x30b4dc8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:135:9: note: 	{ 0B, 0B }
PULSE.c:135:9: note: Cost model analysis: 
PULSE.c:135:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:135:9: note: Basic block will be vectorized using SLP
PULSE.c:135:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:135:9: note: Vectorizing SLP tree:
PULSE.c:135:9: note: node 0x30b4bc8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:135:9: note: op template: <retval>.layers = layers_40;
PULSE.c:135:9: note: 	stmt 0 <retval>.layers = layers_40;
PULSE.c:135:9: note: 	stmt 1 <retval>.paramethers = MODEL_133;
PULSE.c:135:9: note: 	children 0x30b4cc8
PULSE.c:135:9: note: node (external) 0x30b4cc8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:135:9: note: 	{ layers_40, MODEL_133 }
PULSE.c:135:9: note: ------>vectorizing SLP node starting from: <retval>.layers = layers_40;
PULSE.c:135:9: note: vect_is_simple_use: operand MODEL_133 = PHI <MODEL_45(36), MODEL_111(13)>, type of def: internal
PULSE.c:135:9: note: conflicting alias set types.
PULSE.c:135:9: note: transform store. ncopies = 1
PULSE.c:135:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.layers
PULSE.c:135:9: note: created &<retval>
PULSE.c:135:9: note: add new stmt: MEM <vector(2) long unsigned int> [(void *)&<retval>] = _23;
PULSE.c:135:9: note: vectorizing stmts using SLP.
PULSE.c:135:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:135:9: note: Vectorizing SLP tree:
PULSE.c:135:9: note: node 0x30b4d48 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:135:9: note: op template: <retval>.io = 0B;
PULSE.c:135:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:135:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:135:9: note: 	children 0x30b4dc8
PULSE.c:135:9: note: node (constant) 0x30b4dc8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:135:9: note: 	{ 0B, 0B }
PULSE.c:135:9: note: ------>vectorizing SLP node starting from: <retval>.io = 0B;
PULSE.c:135:9: note: vect_is_simple_use: operand 0B, type of def: constant
PULSE.c:135:9: note: transform store. ncopies = 1
PULSE.c:135:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.io
PULSE.c:135:9: note: created &<retval>.io
PULSE.c:135:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval> + 16B] = { 0, 0 };
PULSE.c:135:9: note: vectorizing stmts using SLP.
PULSE.c:135:9: note: ***** The result for vector mode V32QI would be the same
PULSE.c:147:1: note: ***** Analysis failed with vector mode VOID
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:4:18: note: vectorized 0 loops in function.
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_4, inputs_15, _3);
PULSE.c:8:2: missed: statement clobbers memory: _5 (layer_17);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_12, pretmp_26, _9);
PULSE.c:13:28: note: ***** Analysis failed with vector mode V8SI
PULSE.c:13:28: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:22:2: missed: statement clobbers memory: _1 (layer_10(D));
PULSE.c:22:2: missed: statement clobbers memory: _14 (_2);
PULSE.c:22:2: missed: statement clobbers memory: _21 (_15);
PULSE.c:22:2: missed: statement clobbers memory: _28 (_22);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_29);
PULSE.c:26:3: missed: statement clobbers memory: memset (_34, 0, _32);
PULSE.c:26:3: missed: statement clobbers memory: memset (_27, 0, _25);
PULSE.c:26:3: missed: statement clobbers memory: memset (_20, 0, _18);
PULSE.c:26:3: missed: statement clobbers memory: memset (_7, 0, _5);
PULSE.c:28:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:28:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:32:6: note: vectorized 1 loops in function.
PULSE.c:34:8: missed: statement clobbers memory: _1 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_2);
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _9 = rand ();
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8SI
PULSE.c:32:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:61:16: missed: couldn't vectorize loop
PULSE.c:61:16: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:64:17: missed: couldn't vectorize loop
PULSE.c:64:17: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:73:19: missed: couldn't vectorize loop
PULSE.c:73:19: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _69 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:53:22: missed: couldn't vectorize loop
PULSE.c:53:22: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:47:6: note: vectorized 1 loops in function.
PULSE.c:48:1: missed: statement clobbers memory: saved_stack.7_39 = __builtin_stack_save ();
PULSE.c:49:46: missed: statement clobbers memory: _1 = PULSE_GetLossFunctionPtr (loss_function_40(D));
PULSE.c:60:10: missed: statement clobbers memory: random.4_46 = __builtin_alloca_with_align (_4, 32);
PULSE.c:34:8: missed: statement clobbers memory: _61 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_62);
PULSE.c:39:29: missed: statement clobbers memory: _69 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _72 = rand ();
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_98, inputs_93, _97);
PULSE.c:8:2: missed: statement clobbers memory: _99 (layer_94);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_105, _104, _103);
PULSE.c:67:11: missed: statement clobbers memory: loss_55 = PULSE_GetLoss_42 (_17, _16, _12, _11);
PULSE.c:22:2: missed: statement clobbers memory: _86 (output_34);
PULSE.c:22:2: missed: statement clobbers memory: _108 (_87);
PULSE.c:22:2: missed: statement clobbers memory: _115 (_109);
PULSE.c:22:2: missed: statement clobbers memory: _122 (_116);
PULSE.c:22:2: missed: statement clobbers memory: _129 (_123);
PULSE.c:22:2: missed: statement clobbers memory: _136 (_130);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_137);
PULSE.c:26:3: missed: statement clobbers memory: memset (_142, 0, _140);
PULSE.c:26:3: missed: statement clobbers memory: memset (_135, 0, _133);
PULSE.c:26:3: missed: statement clobbers memory: memset (_128, 0, _126);
PULSE.c:26:3: missed: statement clobbers memory: memset (_121, 0, _119);
PULSE.c:26:3: missed: statement clobbers memory: memset (_114, 0, _112);
PULSE.c:26:3: missed: statement clobbers memory: memset (_92, 0, _90);
PULSE.c:75:6: missed: statement clobbers memory: _21 (current_191, args);
PULSE.c:81:4: missed: statement clobbers memory: printf ("Epoch: %d | Item: %d | Loss: %.10f | Batch Loss: %.10f\r", i_188, j_189, _25, _24);
PULSE.c:47:6: missed: statement clobbers memory: __builtin_stack_restore (saved_stack.7_39);
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4DI
PULSE.c:47:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:91:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:91:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:119:20: missed: couldn't vectorize loop
PULSE.c:119:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:119:20: missed: couldn't vectorize loop
PULSE.c:119:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:102:20: missed: couldn't vectorize loop
PULSE.c:102:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:102:20: missed: couldn't vectorize loop
PULSE.c:102:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:93:13: note: vectorized 0 loops in function.
PULSE.c:96:39: missed: statement clobbers memory: layers_40 = malloc (_3);
PULSE.c:99:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:114:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:115:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:116:44: missed: statement clobbers memory: MODEL_109 = malloc (0);
PULSE.c:114:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:115:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:116:44: missed: statement clobbers memory: MODEL_45 = malloc (_9);
PULSE.c:125:62: missed: not vectorized: more than one data ref in stmt: D.6469 = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.106_74];
PULSE.c:125:17: missed: statement clobbers memory: *_15 = PULSE_CreateDenseLayer (D.6469, _144); [return slot optimization]
PULSE.c:125:62: missed: not vectorized: more than one data ref in stmt: D.6469 = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.106_181];
PULSE.c:125:17: missed: statement clobbers memory: *_174 = PULSE_CreateDenseLayer (D.6469, _180); [return slot optimization]
PULSE.c:132:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:133:9: note: ***** Analysis succeeded with vector mode V4DI
PULSE.c:133:9: note: SLPing BB part
PULSE.c:133:9: note: Costing subgraph: 
PULSE.c:133:9: note: node 0x4c0db48 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:133:9: note: op template: <retval>.layers = layers_40;
PULSE.c:133:9: note: 	stmt 0 <retval>.layers = layers_40;
PULSE.c:133:9: note: 	stmt 1 <retval>.paramethers = MODEL_131;
PULSE.c:133:9: note: 	children 0x4c0dc48
PULSE.c:133:9: note: node (external) 0x4c0dc48 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:133:9: note: 	{ layers_40, MODEL_131 }
PULSE.c:133:9: note: Cost model analysis: 
PULSE.c:133:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:133:9: note: Costing subgraph: 
PULSE.c:133:9: note: node 0x4c0dcc8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:133:9: note: op template: <retval>.io = 0B;
PULSE.c:133:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:133:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:133:9: note: 	children 0x4c0dd48
PULSE.c:133:9: note: node (constant) 0x4c0dd48 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:133:9: note: 	{ 0B, 0B }
PULSE.c:133:9: note: Cost model analysis: 
PULSE.c:133:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:133:9: note: Basic block will be vectorized using SLP
PULSE.c:133:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:133:9: note: Vectorizing SLP tree:
PULSE.c:133:9: note: node 0x4c0db48 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:133:9: note: op template: <retval>.layers = layers_40;
PULSE.c:133:9: note: 	stmt 0 <retval>.layers = layers_40;
PULSE.c:133:9: note: 	stmt 1 <retval>.paramethers = MODEL_131;
PULSE.c:133:9: note: 	children 0x4c0dc48
PULSE.c:133:9: note: node (external) 0x4c0dc48 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:133:9: note: 	{ layers_40, MODEL_131 }
PULSE.c:133:9: note: ------>vectorizing SLP node starting from: <retval>.layers = layers_40;
PULSE.c:133:9: note: vect_is_simple_use: operand MODEL_131 = PHI <MODEL_45(36), MODEL_109(13)>, type of def: internal
PULSE.c:133:9: note: conflicting alias set types.
PULSE.c:133:9: note: transform store. ncopies = 1
PULSE.c:133:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.layers
PULSE.c:133:9: note: created &<retval>
PULSE.c:133:9: note: add new stmt: MEM <vector(2) long unsigned int> [(void *)&<retval>] = _23;
PULSE.c:133:9: note: vectorizing stmts using SLP.
PULSE.c:133:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:133:9: note: Vectorizing SLP tree:
PULSE.c:133:9: note: node 0x4c0dcc8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:133:9: note: op template: <retval>.io = 0B;
PULSE.c:133:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:133:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:133:9: note: 	children 0x4c0dd48
PULSE.c:133:9: note: node (constant) 0x4c0dd48 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:133:9: note: 	{ 0B, 0B }
PULSE.c:133:9: note: ------>vectorizing SLP node starting from: <retval>.io = 0B;
PULSE.c:133:9: note: vect_is_simple_use: operand 0B, type of def: constant
PULSE.c:133:9: note: transform store. ncopies = 1
PULSE.c:133:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.io
PULSE.c:133:9: note: created &<retval>.io
PULSE.c:133:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval> + 16B] = { 0, 0 };
PULSE.c:133:9: note: vectorizing stmts using SLP.
PULSE.c:133:9: note: ***** The result for vector mode V32QI would be the same
PULSE.c:145:1: note: ***** Analysis failed with vector mode VOID
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:4:18: note: vectorized 0 loops in function.
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_4, inputs_15, _3);
PULSE.c:8:2: missed: statement clobbers memory: _5 (layer_17);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_12, pretmp_26, _9);
PULSE.c:13:28: note: ***** Analysis failed with vector mode V8SI
PULSE.c:13:28: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:22:2: missed: statement clobbers memory: _1 (layer_10(D));
PULSE.c:22:2: missed: statement clobbers memory: _14 (_2);
PULSE.c:22:2: missed: statement clobbers memory: _21 (_15);
PULSE.c:22:2: missed: statement clobbers memory: _28 (_22);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_29);
PULSE.c:26:3: missed: statement clobbers memory: memset (_34, 0, _32);
PULSE.c:26:3: missed: statement clobbers memory: memset (_27, 0, _25);
PULSE.c:26:3: missed: statement clobbers memory: memset (_20, 0, _18);
PULSE.c:26:3: missed: statement clobbers memory: memset (_7, 0, _5);
PULSE.c:28:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:28:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:32:6: note: vectorized 1 loops in function.
PULSE.c:34:8: missed: statement clobbers memory: _1 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_2);
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _9 = rand ();
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8SI
PULSE.c:32:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:61:16: missed: couldn't vectorize loop
PULSE.c:61:16: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:64:17: missed: couldn't vectorize loop
PULSE.c:64:17: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:73:19: missed: couldn't vectorize loop
PULSE.c:73:19: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _69 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:53:22: missed: couldn't vectorize loop
PULSE.c:53:22: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:47:6: note: vectorized 1 loops in function.
PULSE.c:48:1: missed: statement clobbers memory: saved_stack.7_39 = __builtin_stack_save ();
PULSE.c:49:46: missed: statement clobbers memory: _1 = PULSE_GetLossFunctionPtr (loss_function_40(D));
PULSE.c:60:10: missed: statement clobbers memory: random.4_46 = __builtin_alloca_with_align (_4, 32);
PULSE.c:34:8: missed: statement clobbers memory: _61 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_62);
PULSE.c:39:29: missed: statement clobbers memory: _69 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _72 = rand ();
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_98, inputs_93, _97);
PULSE.c:8:2: missed: statement clobbers memory: _99 (layer_94);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_105, _104, _103);
PULSE.c:67:11: missed: statement clobbers memory: loss_55 = PULSE_GetLoss_42 (_17, _16, _12, _11);
PULSE.c:22:2: missed: statement clobbers memory: _86 (output_34);
PULSE.c:22:2: missed: statement clobbers memory: _108 (_87);
PULSE.c:22:2: missed: statement clobbers memory: _115 (_109);
PULSE.c:22:2: missed: statement clobbers memory: _122 (_116);
PULSE.c:22:2: missed: statement clobbers memory: _129 (_123);
PULSE.c:22:2: missed: statement clobbers memory: _136 (_130);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_137);
PULSE.c:26:3: missed: statement clobbers memory: memset (_142, 0, _140);
PULSE.c:26:3: missed: statement clobbers memory: memset (_135, 0, _133);
PULSE.c:26:3: missed: statement clobbers memory: memset (_128, 0, _126);
PULSE.c:26:3: missed: statement clobbers memory: memset (_121, 0, _119);
PULSE.c:26:3: missed: statement clobbers memory: memset (_114, 0, _112);
PULSE.c:26:3: missed: statement clobbers memory: memset (_92, 0, _90);
PULSE.c:75:6: missed: statement clobbers memory: _21 (current_191, args);
PULSE.c:81:4: missed: statement clobbers memory: printf ("Epoch: %d | Item: %d | Loss: %.10f | Batch Loss: %.10f\r", i_188, j_189, _25, _24);
PULSE.c:47:6: missed: statement clobbers memory: __builtin_stack_restore (saved_stack.7_39);
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4DI
PULSE.c:47:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:91:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:91:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:119:20: missed: couldn't vectorize loop
PULSE.c:119:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:119:20: missed: couldn't vectorize loop
PULSE.c:119:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:102:20: missed: couldn't vectorize loop
PULSE.c:102:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:102:20: missed: couldn't vectorize loop
PULSE.c:102:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:93:13: note: vectorized 0 loops in function.
PULSE.c:96:39: missed: statement clobbers memory: layers_40 = malloc (_3);
PULSE.c:99:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:114:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:115:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:116:46: missed: statement clobbers memory: WEIGHTS_109 = malloc (0);
PULSE.c:114:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:115:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:116:46: missed: statement clobbers memory: WEIGHTS_45 = malloc (_9);
PULSE.c:125:62: missed: not vectorized: more than one data ref in stmt: D.6471 = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.106_74];
PULSE.c:125:17: missed: statement clobbers memory: *_15 = PULSE_CreateDenseLayer (D.6471, _144); [return slot optimization]
PULSE.c:125:62: missed: not vectorized: more than one data ref in stmt: D.6471 = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.106_181];
PULSE.c:125:17: missed: statement clobbers memory: *_174 = PULSE_CreateDenseLayer (D.6471, _180); [return slot optimization]
PULSE.c:132:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:133:9: note: ***** Analysis succeeded with vector mode V4DI
PULSE.c:133:9: note: SLPing BB part
PULSE.c:133:9: note: Costing subgraph: 
PULSE.c:133:9: note: node 0x49cab58 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:133:9: note: op template: <retval>.layers = layers_40;
PULSE.c:133:9: note: 	stmt 0 <retval>.layers = layers_40;
PULSE.c:133:9: note: 	stmt 1 <retval>.paramethers = WEIGHTS_131;
PULSE.c:133:9: note: 	children 0x49cac58
PULSE.c:133:9: note: node (external) 0x49cac58 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:133:9: note: 	{ layers_40, WEIGHTS_131 }
PULSE.c:133:9: note: Cost model analysis: 
PULSE.c:133:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:133:9: note: Costing subgraph: 
PULSE.c:133:9: note: node 0x49cacd8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:133:9: note: op template: <retval>.io = 0B;
PULSE.c:133:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:133:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:133:9: note: 	children 0x49cad58
PULSE.c:133:9: note: node (constant) 0x49cad58 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:133:9: note: 	{ 0B, 0B }
PULSE.c:133:9: note: Cost model analysis: 
PULSE.c:133:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:133:9: note: Basic block will be vectorized using SLP
PULSE.c:133:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:133:9: note: Vectorizing SLP tree:
PULSE.c:133:9: note: node 0x49cab58 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:133:9: note: op template: <retval>.layers = layers_40;
PULSE.c:133:9: note: 	stmt 0 <retval>.layers = layers_40;
PULSE.c:133:9: note: 	stmt 1 <retval>.paramethers = WEIGHTS_131;
PULSE.c:133:9: note: 	children 0x49cac58
PULSE.c:133:9: note: node (external) 0x49cac58 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:133:9: note: 	{ layers_40, WEIGHTS_131 }
PULSE.c:133:9: note: ------>vectorizing SLP node starting from: <retval>.layers = layers_40;
PULSE.c:133:9: note: vect_is_simple_use: operand WEIGHTS_131 = PHI <WEIGHTS_45(36), WEIGHTS_109(13)>, type of def: internal
PULSE.c:133:9: note: conflicting alias set types.
PULSE.c:133:9: note: transform store. ncopies = 1
PULSE.c:133:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.layers
PULSE.c:133:9: note: created &<retval>
PULSE.c:133:9: note: add new stmt: MEM <vector(2) long unsigned int> [(void *)&<retval>] = _23;
PULSE.c:133:9: note: vectorizing stmts using SLP.
PULSE.c:133:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:133:9: note: Vectorizing SLP tree:
PULSE.c:133:9: note: node 0x49cacd8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:133:9: note: op template: <retval>.io = 0B;
PULSE.c:133:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:133:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:133:9: note: 	children 0x49cad58
PULSE.c:133:9: note: node (constant) 0x49cad58 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:133:9: note: 	{ 0B, 0B }
PULSE.c:133:9: note: ------>vectorizing SLP node starting from: <retval>.io = 0B;
PULSE.c:133:9: note: vect_is_simple_use: operand 0B, type of def: constant
PULSE.c:133:9: note: transform store. ncopies = 1
PULSE.c:133:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.io
PULSE.c:133:9: note: created &<retval>.io
PULSE.c:133:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval> + 16B] = { 0, 0 };
PULSE.c:133:9: note: vectorizing stmts using SLP.
PULSE.c:133:9: note: ***** The result for vector mode V32QI would be the same
PULSE.c:145:1: note: ***** Analysis failed with vector mode VOID
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _28 (pretmp_91, _51, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:19: missed: not vectorized: no vectype for stmt: _22 = *_21;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:170:10: optimized: loop vectorized using 32 byte vectors
Dense.c:170:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:170:10: optimized: loop vectorized using 16 byte vectors
Dense.c:161:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
Dense.c:154:10: optimized: loop vectorized using 32 byte vectors
Dense.c:154:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:154:10: optimized: loop vectorized using 16 byte vectors
Dense.c:145:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
Dense.c:136:13: note: vectorized 2 loops in function.
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_28, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
Dense.c:136:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:136:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:105:24: missed: couldn't vectorize loop
Dense.c:105:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:108:17: missed: couldn't vectorize loop
Dense.c:108:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:122:24: missed: couldn't vectorize loop
Dense.c:122:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:125:17: missed: couldn't vectorize loop
Dense.c:125:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:90:15: missed: couldn't vectorize loop
Dense.c:90:15: missed: not vectorized: number of iterations cannot be computed.
Dense.c:83:13: note: vectorized 0 loops in function.
Dense.c:86:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:110:17: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:110:17: note: SLPing BB part
Dense.c:127:17: note: Costing subgraph: 
Dense.c:127:17: note: node 0x884e2a8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:127:17: note: op template: _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 0 _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 1 _198 = (sizetype) wi_137;
Dense.c:127:17: note: 	children 0x884e3a8
Dense.c:127:17: note: node (external) 0x884e3a8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:127:17: note: 	{ j_135, wi_137 }
Dense.c:127:17: note: Cost model analysis: 
Dense.c:127:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:127:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:127:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: Costing subgraph: 
Dense.c:110:17: note: node 0x884e4a8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:110:17: note: op template: _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 0 _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 1 _217 = (sizetype) wi_136;
Dense.c:110:17: note: 	children 0x884e5a8
Dense.c:110:17: note: node (external) 0x884e5a8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:110:17: note: 	{ j_134, wi_136 }
Dense.c:110:17: note: Cost model analysis: 
Dense.c:110:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:110:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:110:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: ***** The result for vector mode V32QI would be the same
Dense.c:110:17: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:110:17: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:110:17: note: SLPing BB part
Dense.c:127:17: note: Costing subgraph: 
Dense.c:127:17: note: node 0x884e4a8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:127:17: note: op template: _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 0 _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 1 _198 = (sizetype) wi_137;
Dense.c:127:17: note: 	children 0x884e2a8
Dense.c:127:17: note: node (external) 0x884e2a8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:127:17: note: 	{ j_135, wi_137 }
Dense.c:127:17: note: Cost model analysis: 
Dense.c:127:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:127:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:127:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: Costing subgraph: 
Dense.c:110:17: note: node 0x884e428 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:110:17: note: op template: _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 0 _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 1 _217 = (sizetype) wi_136;
Dense.c:110:17: note: 	children 0x884e528
Dense.c:110:17: note: node (external) 0x884e528 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:110:17: note: 	{ j_134, wi_136 }
Dense.c:110:17: note: Cost model analysis: 
Dense.c:110:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:110:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:110:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:105:51: note: ***** Analysis failed with vector mode V8QI
Dense.c:105:51: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:105:51: note: ***** Analysis failed with vector mode V4QI
Dense.c:62:15: missed: couldn't vectorize loop
Dense.c:62:15: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:75:11: optimized: loop vectorized using 32 byte vectors
Dense.c:75:11: optimized: loop vectorized using 16 byte vectors
Dense.c:66:11: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:881:10: missed: not vectorized: no vectype for stmt: _53 = MEM[(__m256 * {ref-all})w_ptr_82];
 scalar_type: __m256
Dense.c:54:13: note: vectorized 1 loops in function.
Dense.c:57:2: missed: statement clobbers memory: memcpy (_4, dense$baiases_63, _3);
Dense.c:79:2: missed: statement clobbers memory: _27 (pretmp_163, _68, 0);
Dense.c:54:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:54:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:181:2: missed: statement clobbers memory: free (dense$weights_7);
Dense.c:182:2: missed: statement clobbers memory: free (dense$baiases_8);
Dense.c:183:2: missed: statement clobbers memory: free (dense$gradients_10);
Dense.c:184:2: missed: statement clobbers memory: free (dense$deltas_9);
Dense.c:185:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:185:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:200:19: missed: couldn't vectorize loop
Dense.c:200:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:190:13: note: vectorized 0 loops in function.
Dense.c:196:37: missed: statement clobbers memory: _9 = aligned_alloc (64, _8);
Dense.c:197:34: missed: statement clobbers memory: _11 = aligned_alloc (64, _10);
Dense.c:198:35: missed: statement clobbers memory: _12 = aligned_alloc (64, _10);
Dense.c:201:38: missed: statement clobbers memory: _13 = rand ();
Dense.c:201:72: missed: statement clobbers memory: _57 = sqrt (_20);
Dense.c:213:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:214:4: missed: statement clobbers memory: exit (1);
Dense.c:210:4: missed: statement clobbers memory: layer = PULSE_CreateLayer (args$n_inputs_36, args$n_outputs_47, 0, args$8_48, _SIMD_FeedDense, _SIMD_BackDense, _SIMD_FixDense, _DestroyDense, 1); [return slot optimization]
Dense.c:207:12: missed: statement clobbers memory: layer = PULSE_CreateLayer (args$n_inputs_36, args$n_outputs_47, 0, args$8_48, _FeedDense, _BackDense, _FixDense, _DestroyDense, 0); [return slot optimization]
Dense.c:219:9: missed: not vectorized: more than one data ref in stmt: <retval> = layer;
Dense.c:219:9: note: ***** Analysis succeeded with vector mode V8SI
Dense.c:219:9: note: SLPing BB part
Dense.c:219:9: note: Costing subgraph: 
Dense.c:219:9: note: node 0x87342f8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:219:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = MODEL_32(D);
Dense.c:219:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = MODEL_32(D);
Dense.c:219:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&layer + 96B] = _4;
Dense.c:219:9: note: 	stmt 2 MEM <float *> [(struct PULSE_Layer *)&layer + 104B] = _11;
Dense.c:219:9: note: 	stmt 3 MEM <float *> [(struct PULSE_Layer *)&layer + 112B] = _12;
Dense.c:219:9: note: 	children 0x8734378
Dense.c:219:9: note: node (external) 0x8734378 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:219:9: note: 	{ MODEL_32(D), _4, _11, _12 }
Dense.c:219:9: note: Cost model analysis: 
Dense.c:219:9: note: Cost model analysis for part in loop 0:
  Vector cost: 64
  Scalar cost: 64
Dense.c:219:9: note: Basic block will be vectorized using SLP
Dense.c:219:9: optimized: basic block part vectorized using 32 byte vectors
Dense.c:219:9: note: Vectorizing SLP tree:
Dense.c:219:9: note: node 0x87342f8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:219:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = MODEL_32(D);
Dense.c:219:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = MODEL_32(D);
Dense.c:219:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&layer + 96B] = _4;
Dense.c:219:9: note: 	stmt 2 MEM <float *> [(struct PULSE_Layer *)&layer + 104B] = _11;
Dense.c:219:9: note: 	stmt 3 MEM <float *> [(struct PULSE_Layer *)&layer + 112B] = _12;
Dense.c:219:9: note: 	children 0x8734378
Dense.c:219:9: note: node (external) 0x8734378 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:219:9: note: 	{ MODEL_32(D), _4, _11, _12 }
Dense.c:219:9: note: ------>vectorizing SLP node starting from: MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = MODEL_32(D);
Dense.c:219:9: note: vect_is_simple_use: operand MODEL_32(D) + _3, type of def: internal
Dense.c:219:9: note: vect_is_simple_use: operand aligned_alloc (64, _10), type of def: internal
Dense.c:219:9: note: vect_is_simple_use: operand aligned_alloc (64, _10), type of def: internal
Dense.c:219:9: note: transform store. ncopies = 1
Dense.c:219:9: note: create vector_type-pointer variable to type: vector(4) long unsigned int  vectorizing a pointer ref: MEM <float *> [(struct PULSE_Layer *)&layer + 88B]
Dense.c:219:9: note: created &MEM <float *> [(struct PULSE_Layer *)&layer + 88B]
Dense.c:219:9: note: add new stmt: MEM <vector(4) long unsigned int> [(struct PULSE_Layer *)&layer + 88B] = _61;
Dense.c:219:9: note: vectorizing stmts using SLP.
Dense.c:219:9: note: ***** The result for vector mode V32QI would be the same
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:4:18: note: vectorized 0 loops in function.
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_4, inputs_15, _3);
PULSE.c:8:2: missed: statement clobbers memory: _5 (layer_17);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_12, pretmp_26, _9);
PULSE.c:13:28: note: ***** Analysis failed with vector mode V8SI
PULSE.c:13:28: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:22:2: missed: statement clobbers memory: _1 (layer_10(D));
PULSE.c:22:2: missed: statement clobbers memory: _14 (_2);
PULSE.c:22:2: missed: statement clobbers memory: _21 (_15);
PULSE.c:22:2: missed: statement clobbers memory: _28 (_22);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_29);
PULSE.c:26:3: missed: statement clobbers memory: memset (_34, 0, _32);
PULSE.c:26:3: missed: statement clobbers memory: memset (_27, 0, _25);
PULSE.c:26:3: missed: statement clobbers memory: memset (_20, 0, _18);
PULSE.c:26:3: missed: statement clobbers memory: memset (_7, 0, _5);
PULSE.c:28:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:28:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:32:6: note: vectorized 1 loops in function.
PULSE.c:34:8: missed: statement clobbers memory: _1 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_2);
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _9 = rand ();
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8SI
PULSE.c:32:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:61:16: missed: couldn't vectorize loop
PULSE.c:61:16: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:64:17: missed: couldn't vectorize loop
PULSE.c:64:17: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:73:19: missed: couldn't vectorize loop
PULSE.c:73:19: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _69 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:53:22: missed: couldn't vectorize loop
PULSE.c:53:22: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:47:6: note: vectorized 1 loops in function.
PULSE.c:48:1: missed: statement clobbers memory: saved_stack.7_39 = __builtin_stack_save ();
PULSE.c:49:46: missed: statement clobbers memory: _1 = PULSE_GetLossFunctionPtr (loss_function_40(D));
PULSE.c:60:10: missed: statement clobbers memory: random.4_46 = __builtin_alloca_with_align (_4, 32);
PULSE.c:34:8: missed: statement clobbers memory: _61 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_62);
PULSE.c:39:29: missed: statement clobbers memory: _69 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _72 = rand ();
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_98, inputs_93, _97);
PULSE.c:8:2: missed: statement clobbers memory: _99 (layer_94);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_105, _104, _103);
PULSE.c:67:11: missed: statement clobbers memory: loss_55 = PULSE_GetLoss_42 (_17, _16, _12, _11);
PULSE.c:22:2: missed: statement clobbers memory: _86 (output_34);
PULSE.c:22:2: missed: statement clobbers memory: _108 (_87);
PULSE.c:22:2: missed: statement clobbers memory: _115 (_109);
PULSE.c:22:2: missed: statement clobbers memory: _122 (_116);
PULSE.c:22:2: missed: statement clobbers memory: _129 (_123);
PULSE.c:22:2: missed: statement clobbers memory: _136 (_130);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_137);
PULSE.c:26:3: missed: statement clobbers memory: memset (_142, 0, _140);
PULSE.c:26:3: missed: statement clobbers memory: memset (_135, 0, _133);
PULSE.c:26:3: missed: statement clobbers memory: memset (_128, 0, _126);
PULSE.c:26:3: missed: statement clobbers memory: memset (_121, 0, _119);
PULSE.c:26:3: missed: statement clobbers memory: memset (_114, 0, _112);
PULSE.c:26:3: missed: statement clobbers memory: memset (_92, 0, _90);
PULSE.c:75:6: missed: statement clobbers memory: _21 (current_191, args);
PULSE.c:81:4: missed: statement clobbers memory: printf ("Epoch: %d | Item: %d | Loss: %.10f | Batch Loss: %.10f\r", i_188, j_189, _25, _24);
PULSE.c:47:6: missed: statement clobbers memory: __builtin_stack_restore (saved_stack.7_39);
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4DI
PULSE.c:47:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:91:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:91:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:119:20: missed: couldn't vectorize loop
PULSE.c:119:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:119:20: missed: couldn't vectorize loop
PULSE.c:119:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:102:20: missed: couldn't vectorize loop
PULSE.c:102:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:102:20: missed: couldn't vectorize loop
PULSE.c:102:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:93:13: note: vectorized 0 loops in function.
PULSE.c:96:39: missed: statement clobbers memory: layers_32 = malloc (_3);
PULSE.c:99:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:114:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:115:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:116:46: missed: statement clobbers memory: WEIGHTS_47 = malloc (0);
PULSE.c:108:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.104_69];
PULSE.c:109:21: missed: statement clobbers memory: _58 = PULSE_GetWeightsLayerSize (args);
PULSE.c:108:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})_181];
PULSE.c:109:21: missed: statement clobbers memory: _172 = PULSE_GetWeightsLayerSize (args);
PULSE.c:114:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:115:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:116:46: missed: statement clobbers memory: WEIGHTS_37 = malloc (_6);
PULSE.c:125:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.106_71];
PULSE.c:126:17: missed: statement clobbers memory: *_9 = PULSE_CreateDenseLayer (args, WEIGHTS_PTR_122); [return slot optimization]
PULSE.c:127:20: missed: statement clobbers memory: _50 = PULSE_GetWeightsLayerSize (args);
PULSE.c:125:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.106_114];
PULSE.c:126:17: missed: statement clobbers memory: *_99 = PULSE_CreateDenseLayer (args, WEIGHTS_PTR_136); [return slot optimization]
PULSE.c:127:20: missed: statement clobbers memory: _77 = PULSE_GetWeightsLayerSize (args);
PULSE.c:133:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:134:9: note: ***** Analysis succeeded with vector mode V8SI
PULSE.c:134:9: note: SLPing BB part
PULSE.c:134:9: note: Costing subgraph: 
PULSE.c:134:9: note: node 0x3b13b38 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:134:9: note: op template: <retval>.layers = layers_32;
PULSE.c:134:9: note: 	stmt 0 <retval>.layers = layers_32;
PULSE.c:134:9: note: 	stmt 1 <retval>.paramethers = WEIGHTS_81;
PULSE.c:134:9: note: 	children 0x3b13c38
PULSE.c:134:9: note: node (external) 0x3b13c38 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:134:9: note: 	{ layers_32, WEIGHTS_81 }
PULSE.c:134:9: note: Cost model analysis: 
PULSE.c:134:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:134:9: note: Costing subgraph: 
PULSE.c:134:9: note: node 0x3b13cb8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:134:9: note: op template: <retval>.io = 0B;
PULSE.c:134:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:134:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:134:9: note: 	children 0x3b13d38
PULSE.c:134:9: note: node (constant) 0x3b13d38 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:134:9: note: 	{ 0B, 0B }
PULSE.c:134:9: note: Cost model analysis: 
PULSE.c:134:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:134:9: note: Basic block will be vectorized using SLP
PULSE.c:134:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:134:9: note: Vectorizing SLP tree:
PULSE.c:134:9: note: node 0x3b13b38 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:134:9: note: op template: <retval>.layers = layers_32;
PULSE.c:134:9: note: 	stmt 0 <retval>.layers = layers_32;
PULSE.c:134:9: note: 	stmt 1 <retval>.paramethers = WEIGHTS_81;
PULSE.c:134:9: note: 	children 0x3b13c38
PULSE.c:134:9: note: node (external) 0x3b13c38 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:134:9: note: 	{ layers_32, WEIGHTS_81 }
PULSE.c:134:9: note: ------>vectorizing SLP node starting from: <retval>.layers = layers_32;
PULSE.c:134:9: note: vect_is_simple_use: operand WEIGHTS_81 = PHI <WEIGHTS_37(36), WEIGHTS_47(12)>, type of def: internal
PULSE.c:134:9: note: conflicting alias set types.
PULSE.c:134:9: note: transform store. ncopies = 1
PULSE.c:134:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.layers
PULSE.c:134:9: note: created &<retval>
PULSE.c:134:9: note: add new stmt: MEM <vector(2) long unsigned int> [(void *)&<retval>] = _14;
PULSE.c:134:9: note: vectorizing stmts using SLP.
PULSE.c:134:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:134:9: note: Vectorizing SLP tree:
PULSE.c:134:9: note: node 0x3b13cb8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:134:9: note: op template: <retval>.io = 0B;
PULSE.c:134:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:134:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:134:9: note: 	children 0x3b13d38
PULSE.c:134:9: note: node (constant) 0x3b13d38 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:134:9: note: 	{ 0B, 0B }
PULSE.c:134:9: note: ------>vectorizing SLP node starting from: <retval>.io = 0B;
PULSE.c:134:9: note: vect_is_simple_use: operand 0B, type of def: constant
PULSE.c:134:9: note: transform store. ncopies = 1
PULSE.c:134:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.io
PULSE.c:134:9: note: created &<retval>.io
PULSE.c:134:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval> + 16B] = { 0, 0 };
PULSE.c:134:9: note: vectorizing stmts using SLP.
PULSE.c:134:9: note: ***** The result for vector mode V32QI would be the same
PULSE.c:146:1: note: ***** Analysis failed with vector mode VOID
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _28 (pretmp_91, _51, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:19: missed: not vectorized: no vectype for stmt: _22 = *_21;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:170:10: optimized: loop vectorized using 32 byte vectors
Dense.c:170:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:170:10: optimized: loop vectorized using 16 byte vectors
Dense.c:161:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
Dense.c:154:10: optimized: loop vectorized using 32 byte vectors
Dense.c:154:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:154:10: optimized: loop vectorized using 16 byte vectors
Dense.c:145:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
Dense.c:136:13: note: vectorized 2 loops in function.
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_28, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
Dense.c:136:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:136:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:105:24: missed: couldn't vectorize loop
Dense.c:105:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:108:17: missed: couldn't vectorize loop
Dense.c:108:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:122:24: missed: couldn't vectorize loop
Dense.c:122:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:125:17: missed: couldn't vectorize loop
Dense.c:125:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:90:15: missed: couldn't vectorize loop
Dense.c:90:15: missed: not vectorized: number of iterations cannot be computed.
Dense.c:83:13: note: vectorized 0 loops in function.
Dense.c:86:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:110:17: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:110:17: note: SLPing BB part
Dense.c:127:17: note: Costing subgraph: 
Dense.c:127:17: note: node 0x7382358 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:127:17: note: op template: _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 0 _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 1 _198 = (sizetype) wi_137;
Dense.c:127:17: note: 	children 0x7382458
Dense.c:127:17: note: node (external) 0x7382458 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:127:17: note: 	{ j_135, wi_137 }
Dense.c:127:17: note: Cost model analysis: 
Dense.c:127:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:127:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:127:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: Costing subgraph: 
Dense.c:110:17: note: node 0x7382558 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:110:17: note: op template: _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 0 _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 1 _217 = (sizetype) wi_136;
Dense.c:110:17: note: 	children 0x7382658
Dense.c:110:17: note: node (external) 0x7382658 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:110:17: note: 	{ j_134, wi_136 }
Dense.c:110:17: note: Cost model analysis: 
Dense.c:110:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:110:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:110:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: ***** The result for vector mode V32QI would be the same
Dense.c:110:17: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:110:17: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:110:17: note: SLPing BB part
Dense.c:127:17: note: Costing subgraph: 
Dense.c:127:17: note: node 0x7382558 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:127:17: note: op template: _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 0 _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 1 _198 = (sizetype) wi_137;
Dense.c:127:17: note: 	children 0x7382358
Dense.c:127:17: note: node (external) 0x7382358 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:127:17: note: 	{ j_135, wi_137 }
Dense.c:127:17: note: Cost model analysis: 
Dense.c:127:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:127:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:127:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: Costing subgraph: 
Dense.c:110:17: note: node 0x73824d8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:110:17: note: op template: _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 0 _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 1 _217 = (sizetype) wi_136;
Dense.c:110:17: note: 	children 0x73825d8
Dense.c:110:17: note: node (external) 0x73825d8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:110:17: note: 	{ j_134, wi_136 }
Dense.c:110:17: note: Cost model analysis: 
Dense.c:110:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:110:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:110:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:105:51: note: ***** Analysis failed with vector mode V8QI
Dense.c:105:51: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:105:51: note: ***** Analysis failed with vector mode V4QI
Dense.c:62:15: missed: couldn't vectorize loop
Dense.c:62:15: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:75:11: optimized: loop vectorized using 32 byte vectors
Dense.c:75:11: optimized: loop vectorized using 16 byte vectors
Dense.c:66:11: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:881:10: missed: not vectorized: no vectype for stmt: _53 = MEM[(__m256 * {ref-all})w_ptr_82];
 scalar_type: __m256
Dense.c:54:13: note: vectorized 1 loops in function.
Dense.c:57:2: missed: statement clobbers memory: memcpy (_4, dense$baiases_63, _3);
Dense.c:79:2: missed: statement clobbers memory: _27 (pretmp_163, _68, 0);
Dense.c:54:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:54:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:181:2: missed: statement clobbers memory: free (dense$weights_7);
Dense.c:182:2: missed: statement clobbers memory: free (dense$baiases_8);
Dense.c:183:2: missed: statement clobbers memory: free (dense$gradients_10);
Dense.c:184:2: missed: statement clobbers memory: free (dense$deltas_9);
Dense.c:185:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:185:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:200:19: missed: couldn't vectorize loop
Dense.c:200:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:190:13: note: vectorized 0 loops in function.
Dense.c:196:37: missed: statement clobbers memory: _9 = aligned_alloc (64, _8);
Dense.c:197:34: missed: statement clobbers memory: _11 = aligned_alloc (64, _10);
Dense.c:198:35: missed: statement clobbers memory: _12 = aligned_alloc (64, _10);
Dense.c:201:38: missed: statement clobbers memory: _13 = rand ();
Dense.c:201:72: missed: statement clobbers memory: _57 = sqrt (_20);
Dense.c:213:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:214:4: missed: statement clobbers memory: exit (1);
Dense.c:210:4: missed: statement clobbers memory: layer = PULSE_CreateLayer (args$n_inputs_36, args$n_outputs_47, 0, args$8_48, _SIMD_FeedDense, _SIMD_BackDense, _SIMD_FixDense, _DestroyDense, 1); [return slot optimization]
Dense.c:207:12: missed: statement clobbers memory: layer = PULSE_CreateLayer (args$n_inputs_36, args$n_outputs_47, 0, args$8_48, _FeedDense, _BackDense, _FixDense, _DestroyDense, 0); [return slot optimization]
Dense.c:219:9: missed: not vectorized: more than one data ref in stmt: <retval> = layer;
Dense.c:219:9: note: ***** Analysis succeeded with vector mode V8SI
Dense.c:219:9: note: SLPing BB part
Dense.c:219:9: note: Costing subgraph: 
Dense.c:219:9: note: node 0x719ed68 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:219:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = MODEL_32(D);
Dense.c:219:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = MODEL_32(D);
Dense.c:219:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&layer + 96B] = _4;
Dense.c:219:9: note: 	stmt 2 MEM <float *> [(struct PULSE_Layer *)&layer + 104B] = _11;
Dense.c:219:9: note: 	stmt 3 MEM <float *> [(struct PULSE_Layer *)&layer + 112B] = _12;
Dense.c:219:9: note: 	children 0x719ede8
Dense.c:219:9: note: node (external) 0x719ede8 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:219:9: note: 	{ MODEL_32(D), _4, _11, _12 }
Dense.c:219:9: note: Cost model analysis: 
Dense.c:219:9: note: Cost model analysis for part in loop 0:
  Vector cost: 64
  Scalar cost: 64
Dense.c:219:9: note: Basic block will be vectorized using SLP
Dense.c:219:9: optimized: basic block part vectorized using 32 byte vectors
Dense.c:219:9: note: Vectorizing SLP tree:
Dense.c:219:9: note: node 0x719ed68 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:219:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = MODEL_32(D);
Dense.c:219:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = MODEL_32(D);
Dense.c:219:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&layer + 96B] = _4;
Dense.c:219:9: note: 	stmt 2 MEM <float *> [(struct PULSE_Layer *)&layer + 104B] = _11;
Dense.c:219:9: note: 	stmt 3 MEM <float *> [(struct PULSE_Layer *)&layer + 112B] = _12;
Dense.c:219:9: note: 	children 0x719ede8
Dense.c:219:9: note: node (external) 0x719ede8 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:219:9: note: 	{ MODEL_32(D), _4, _11, _12 }
Dense.c:219:9: note: ------>vectorizing SLP node starting from: MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = MODEL_32(D);
Dense.c:219:9: note: vect_is_simple_use: operand MODEL_32(D) + _3, type of def: internal
Dense.c:219:9: note: vect_is_simple_use: operand aligned_alloc (64, _10), type of def: internal
Dense.c:219:9: note: vect_is_simple_use: operand aligned_alloc (64, _10), type of def: internal
Dense.c:219:9: note: transform store. ncopies = 1
Dense.c:219:9: note: create vector_type-pointer variable to type: vector(4) long unsigned int  vectorizing a pointer ref: MEM <float *> [(struct PULSE_Layer *)&layer + 88B]
Dense.c:219:9: note: created &MEM <float *> [(struct PULSE_Layer *)&layer + 88B]
Dense.c:219:9: note: add new stmt: MEM <vector(4) long unsigned int> [(struct PULSE_Layer *)&layer + 88B] = _61;
Dense.c:219:9: note: vectorizing stmts using SLP.
Dense.c:219:9: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _28 (pretmp_91, _51, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:19: missed: not vectorized: no vectype for stmt: _22 = *_21;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:170:10: optimized: loop vectorized using 32 byte vectors
Dense.c:170:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:170:10: optimized: loop vectorized using 16 byte vectors
Dense.c:161:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
Dense.c:154:10: optimized: loop vectorized using 32 byte vectors
Dense.c:154:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:154:10: optimized: loop vectorized using 16 byte vectors
Dense.c:145:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
Dense.c:136:13: note: vectorized 2 loops in function.
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_28, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
Dense.c:136:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:136:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:105:24: missed: couldn't vectorize loop
Dense.c:105:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:108:17: missed: couldn't vectorize loop
Dense.c:108:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:122:24: missed: couldn't vectorize loop
Dense.c:122:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:125:17: missed: couldn't vectorize loop
Dense.c:125:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:90:15: missed: couldn't vectorize loop
Dense.c:90:15: missed: not vectorized: number of iterations cannot be computed.
Dense.c:83:13: note: vectorized 0 loops in function.
Dense.c:86:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:110:17: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:110:17: note: SLPing BB part
Dense.c:127:17: note: Costing subgraph: 
Dense.c:127:17: note: node 0x6f2cc38 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:127:17: note: op template: _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 0 _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 1 _198 = (sizetype) wi_137;
Dense.c:127:17: note: 	children 0x6f2cd38
Dense.c:127:17: note: node (external) 0x6f2cd38 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:127:17: note: 	{ j_135, wi_137 }
Dense.c:127:17: note: Cost model analysis: 
Dense.c:127:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:127:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:127:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: Costing subgraph: 
Dense.c:110:17: note: node 0x6f2ce38 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:110:17: note: op template: _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 0 _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 1 _217 = (sizetype) wi_136;
Dense.c:110:17: note: 	children 0x6f2cf38
Dense.c:110:17: note: node (external) 0x6f2cf38 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:110:17: note: 	{ j_134, wi_136 }
Dense.c:110:17: note: Cost model analysis: 
Dense.c:110:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:110:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:110:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: ***** The result for vector mode V32QI would be the same
Dense.c:110:17: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:110:17: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:110:17: note: SLPing BB part
Dense.c:127:17: note: Costing subgraph: 
Dense.c:127:17: note: node 0x6f2ce38 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:127:17: note: op template: _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 0 _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 1 _198 = (sizetype) wi_137;
Dense.c:127:17: note: 	children 0x6f2cc38
Dense.c:127:17: note: node (external) 0x6f2cc38 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:127:17: note: 	{ j_135, wi_137 }
Dense.c:127:17: note: Cost model analysis: 
Dense.c:127:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:127:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:127:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: Costing subgraph: 
Dense.c:110:17: note: node 0x6f2cdb8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:110:17: note: op template: _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 0 _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 1 _217 = (sizetype) wi_136;
Dense.c:110:17: note: 	children 0x6f2ceb8
Dense.c:110:17: note: node (external) 0x6f2ceb8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:110:17: note: 	{ j_134, wi_136 }
Dense.c:110:17: note: Cost model analysis: 
Dense.c:110:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:110:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:110:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:105:51: note: ***** Analysis failed with vector mode V8QI
Dense.c:105:51: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:105:51: note: ***** Analysis failed with vector mode V4QI
Dense.c:62:15: missed: couldn't vectorize loop
Dense.c:62:15: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:75:11: optimized: loop vectorized using 32 byte vectors
Dense.c:75:11: optimized: loop vectorized using 16 byte vectors
Dense.c:66:11: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:881:10: missed: not vectorized: no vectype for stmt: _53 = MEM[(__m256 * {ref-all})w_ptr_82];
 scalar_type: __m256
Dense.c:54:13: note: vectorized 1 loops in function.
Dense.c:57:2: missed: statement clobbers memory: memcpy (_4, dense$baiases_63, _3);
Dense.c:79:2: missed: statement clobbers memory: _27 (pretmp_163, _68, 0);
Dense.c:54:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:54:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:181:2: missed: statement clobbers memory: free (dense$weights_7);
Dense.c:182:2: missed: statement clobbers memory: free (dense$baiases_8);
Dense.c:183:2: missed: statement clobbers memory: free (dense$gradients_10);
Dense.c:184:2: missed: statement clobbers memory: free (dense$deltas_9);
Dense.c:185:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:185:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:200:19: missed: couldn't vectorize loop
Dense.c:200:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:190:13: note: vectorized 0 loops in function.
Dense.c:196:37: missed: statement clobbers memory: _9 = aligned_alloc (64, _8);
Dense.c:197:34: missed: statement clobbers memory: _11 = aligned_alloc (64, _10);
Dense.c:198:35: missed: statement clobbers memory: _12 = aligned_alloc (64, _10);
Dense.c:201:38: missed: statement clobbers memory: _13 = rand ();
Dense.c:201:72: missed: statement clobbers memory: _57 = sqrt (_20);
Dense.c:213:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:214:4: missed: statement clobbers memory: exit (1);
Dense.c:210:4: missed: statement clobbers memory: layer = PULSE_CreateLayer (args$n_inputs_36, args$n_outputs_47, 0, args$8_48, _SIMD_FeedDense, _SIMD_BackDense, _SIMD_FixDense, _DestroyDense, 1); [return slot optimization]
Dense.c:207:12: missed: statement clobbers memory: layer = PULSE_CreateLayer (args$n_inputs_36, args$n_outputs_47, 0, args$8_48, _FeedDense, _BackDense, _FixDense, _DestroyDense, 0); [return slot optimization]
Dense.c:219:9: missed: not vectorized: more than one data ref in stmt: <retval> = layer;
Dense.c:219:9: note: ***** Analysis succeeded with vector mode V8SI
Dense.c:219:9: note: SLPing BB part
Dense.c:219:9: note: Costing subgraph: 
Dense.c:219:9: note: node 0x6d51548 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:219:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = MODEL_32(D);
Dense.c:219:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = MODEL_32(D);
Dense.c:219:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&layer + 96B] = _4;
Dense.c:219:9: note: 	stmt 2 MEM <float *> [(struct PULSE_Layer *)&layer + 104B] = _11;
Dense.c:219:9: note: 	stmt 3 MEM <float *> [(struct PULSE_Layer *)&layer + 112B] = _12;
Dense.c:219:9: note: 	children 0x6d515c8
Dense.c:219:9: note: node (external) 0x6d515c8 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:219:9: note: 	{ MODEL_32(D), _4, _11, _12 }
Dense.c:219:9: note: Cost model analysis: 
Dense.c:219:9: note: Cost model analysis for part in loop 0:
  Vector cost: 64
  Scalar cost: 64
Dense.c:219:9: note: Basic block will be vectorized using SLP
Dense.c:219:9: optimized: basic block part vectorized using 32 byte vectors
Dense.c:219:9: note: Vectorizing SLP tree:
Dense.c:219:9: note: node 0x6d51548 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:219:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = MODEL_32(D);
Dense.c:219:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = MODEL_32(D);
Dense.c:219:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&layer + 96B] = _4;
Dense.c:219:9: note: 	stmt 2 MEM <float *> [(struct PULSE_Layer *)&layer + 104B] = _11;
Dense.c:219:9: note: 	stmt 3 MEM <float *> [(struct PULSE_Layer *)&layer + 112B] = _12;
Dense.c:219:9: note: 	children 0x6d515c8
Dense.c:219:9: note: node (external) 0x6d515c8 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:219:9: note: 	{ MODEL_32(D), _4, _11, _12 }
Dense.c:219:9: note: ------>vectorizing SLP node starting from: MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = MODEL_32(D);
Dense.c:219:9: note: vect_is_simple_use: operand MODEL_32(D) + _3, type of def: internal
Dense.c:219:9: note: vect_is_simple_use: operand aligned_alloc (64, _10), type of def: internal
Dense.c:219:9: note: vect_is_simple_use: operand aligned_alloc (64, _10), type of def: internal
Dense.c:219:9: note: transform store. ncopies = 1
Dense.c:219:9: note: create vector_type-pointer variable to type: vector(4) long unsigned int  vectorizing a pointer ref: MEM <float *> [(struct PULSE_Layer *)&layer + 88B]
Dense.c:219:9: note: created &MEM <float *> [(struct PULSE_Layer *)&layer + 88B]
Dense.c:219:9: note: add new stmt: MEM <vector(4) long unsigned int> [(struct PULSE_Layer *)&layer + 88B] = _61;
Dense.c:219:9: note: vectorizing stmts using SLP.
Dense.c:219:9: note: ***** The result for vector mode V32QI would be the same
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:4:18: note: vectorized 0 loops in function.
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_4, inputs_15, _3);
PULSE.c:8:2: missed: statement clobbers memory: _5 (layer_17);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_12, pretmp_26, _9);
PULSE.c:13:28: note: ***** Analysis failed with vector mode V8SI
PULSE.c:13:28: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:22:2: missed: statement clobbers memory: _1 (layer_10(D));
PULSE.c:22:2: missed: statement clobbers memory: _14 (_2);
PULSE.c:22:2: missed: statement clobbers memory: _21 (_15);
PULSE.c:22:2: missed: statement clobbers memory: _28 (_22);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_29);
PULSE.c:26:3: missed: statement clobbers memory: memset (_34, 0, _32);
PULSE.c:26:3: missed: statement clobbers memory: memset (_27, 0, _25);
PULSE.c:26:3: missed: statement clobbers memory: memset (_20, 0, _18);
PULSE.c:26:3: missed: statement clobbers memory: memset (_7, 0, _5);
PULSE.c:28:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:28:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:32:6: note: vectorized 1 loops in function.
PULSE.c:34:8: missed: statement clobbers memory: _1 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_2);
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _9 = rand ();
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8SI
PULSE.c:32:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:61:16: missed: couldn't vectorize loop
PULSE.c:61:16: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:64:17: missed: couldn't vectorize loop
PULSE.c:64:17: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:73:19: missed: couldn't vectorize loop
PULSE.c:73:19: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _69 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:53:22: missed: couldn't vectorize loop
PULSE.c:53:22: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:47:6: note: vectorized 1 loops in function.
PULSE.c:48:1: missed: statement clobbers memory: saved_stack.7_39 = __builtin_stack_save ();
PULSE.c:49:46: missed: statement clobbers memory: _1 = PULSE_GetLossFunctionPtr (loss_function_40(D));
PULSE.c:60:10: missed: statement clobbers memory: random.4_46 = __builtin_alloca_with_align (_4, 32);
PULSE.c:34:8: missed: statement clobbers memory: _61 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_62);
PULSE.c:39:29: missed: statement clobbers memory: _69 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _72 = rand ();
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_98, inputs_93, _97);
PULSE.c:8:2: missed: statement clobbers memory: _99 (layer_94);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_105, _104, _103);
PULSE.c:67:11: missed: statement clobbers memory: loss_55 = PULSE_GetLoss_42 (_17, _16, _12, _11);
PULSE.c:22:2: missed: statement clobbers memory: _86 (output_34);
PULSE.c:22:2: missed: statement clobbers memory: _108 (_87);
PULSE.c:22:2: missed: statement clobbers memory: _115 (_109);
PULSE.c:22:2: missed: statement clobbers memory: _122 (_116);
PULSE.c:22:2: missed: statement clobbers memory: _129 (_123);
PULSE.c:22:2: missed: statement clobbers memory: _136 (_130);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_137);
PULSE.c:26:3: missed: statement clobbers memory: memset (_142, 0, _140);
PULSE.c:26:3: missed: statement clobbers memory: memset (_135, 0, _133);
PULSE.c:26:3: missed: statement clobbers memory: memset (_128, 0, _126);
PULSE.c:26:3: missed: statement clobbers memory: memset (_121, 0, _119);
PULSE.c:26:3: missed: statement clobbers memory: memset (_114, 0, _112);
PULSE.c:26:3: missed: statement clobbers memory: memset (_92, 0, _90);
PULSE.c:75:6: missed: statement clobbers memory: _21 (current_191, args);
PULSE.c:81:4: missed: statement clobbers memory: printf ("Epoch: %d | Item: %d | Loss: %.10f | Batch Loss: %.10f\r", i_188, j_189, _25, _24);
PULSE.c:47:6: missed: statement clobbers memory: __builtin_stack_restore (saved_stack.7_39);
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4DI
PULSE.c:47:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:91:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:91:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:119:20: missed: couldn't vectorize loop
PULSE.c:119:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:119:20: missed: couldn't vectorize loop
PULSE.c:119:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:102:20: missed: couldn't vectorize loop
PULSE.c:102:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:102:20: missed: couldn't vectorize loop
PULSE.c:102:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:93:13: note: vectorized 0 loops in function.
PULSE.c:96:39: missed: statement clobbers memory: layers_31 = malloc (_3);
PULSE.c:99:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:114:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:115:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:116:46: missed: statement clobbers memory: WEIGHTS_46 = malloc (0);
PULSE.c:108:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.104_68];
PULSE.c:109:21: missed: statement clobbers memory: _57 = PULSE_GetDenseWeightsSize (args);
PULSE.c:108:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})_179];
PULSE.c:109:21: missed: statement clobbers memory: _171 = PULSE_GetDenseWeightsSize (args);
PULSE.c:114:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:115:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:116:46: missed: statement clobbers memory: WEIGHTS_36 = malloc (_5);
PULSE.c:125:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.106_70];
PULSE.c:126:17: missed: statement clobbers memory: *_8 = PULSE_CreateDenseLayer (args, WEIGHTS_PTR_121); [return slot optimization]
PULSE.c:127:20: missed: statement clobbers memory: _49 = PULSE_GetDenseWeightsSize (args);
PULSE.c:125:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.106_113];
PULSE.c:126:17: missed: statement clobbers memory: *_98 = PULSE_CreateDenseLayer (args, WEIGHTS_PTR_135); [return slot optimization]
PULSE.c:127:20: missed: statement clobbers memory: _76 = PULSE_GetDenseWeightsSize (args);
PULSE.c:133:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:134:9: note: ***** Analysis succeeded with vector mode V8SI
PULSE.c:134:9: note: SLPing BB part
PULSE.c:134:9: note: Costing subgraph: 
PULSE.c:134:9: note: node 0x44e5b38 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:134:9: note: op template: <retval>.layers = layers_31;
PULSE.c:134:9: note: 	stmt 0 <retval>.layers = layers_31;
PULSE.c:134:9: note: 	stmt 1 <retval>.paramethers = WEIGHTS_80;
PULSE.c:134:9: note: 	children 0x44e5c38
PULSE.c:134:9: note: node (external) 0x44e5c38 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:134:9: note: 	{ layers_31, WEIGHTS_80 }
PULSE.c:134:9: note: Cost model analysis: 
PULSE.c:134:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:134:9: note: Costing subgraph: 
PULSE.c:134:9: note: node 0x44e5cb8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:134:9: note: op template: <retval>.io = 0B;
PULSE.c:134:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:134:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:134:9: note: 	children 0x44e5d38
PULSE.c:134:9: note: node (constant) 0x44e5d38 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:134:9: note: 	{ 0B, 0B }
PULSE.c:134:9: note: Cost model analysis: 
PULSE.c:134:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:134:9: note: Basic block will be vectorized using SLP
PULSE.c:134:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:134:9: note: Vectorizing SLP tree:
PULSE.c:134:9: note: node 0x44e5b38 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:134:9: note: op template: <retval>.layers = layers_31;
PULSE.c:134:9: note: 	stmt 0 <retval>.layers = layers_31;
PULSE.c:134:9: note: 	stmt 1 <retval>.paramethers = WEIGHTS_80;
PULSE.c:134:9: note: 	children 0x44e5c38
PULSE.c:134:9: note: node (external) 0x44e5c38 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:134:9: note: 	{ layers_31, WEIGHTS_80 }
PULSE.c:134:9: note: ------>vectorizing SLP node starting from: <retval>.layers = layers_31;
PULSE.c:134:9: note: vect_is_simple_use: operand WEIGHTS_80 = PHI <WEIGHTS_36(36), WEIGHTS_46(12)>, type of def: internal
PULSE.c:134:9: note: conflicting alias set types.
PULSE.c:134:9: note: transform store. ncopies = 1
PULSE.c:134:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.layers
PULSE.c:134:9: note: created &<retval>
PULSE.c:134:9: note: add new stmt: MEM <vector(2) long unsigned int> [(void *)&<retval>] = _11;
PULSE.c:134:9: note: vectorizing stmts using SLP.
PULSE.c:134:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:134:9: note: Vectorizing SLP tree:
PULSE.c:134:9: note: node 0x44e5cb8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:134:9: note: op template: <retval>.io = 0B;
PULSE.c:134:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:134:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:134:9: note: 	children 0x44e5d38
PULSE.c:134:9: note: node (constant) 0x44e5d38 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:134:9: note: 	{ 0B, 0B }
PULSE.c:134:9: note: ------>vectorizing SLP node starting from: <retval>.io = 0B;
PULSE.c:134:9: note: vect_is_simple_use: operand 0B, type of def: constant
PULSE.c:134:9: note: transform store. ncopies = 1
PULSE.c:134:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.io
PULSE.c:134:9: note: created &<retval>.io
PULSE.c:134:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval> + 16B] = { 0, 0 };
PULSE.c:134:9: note: vectorizing stmts using SLP.
PULSE.c:134:9: note: ***** The result for vector mode V32QI would be the same
PULSE.c:146:1: note: ***** Analysis failed with vector mode VOID
Layer.c:7:34: missed: statement clobbers memory: _3 = aligned_alloc (64, _2);
Layer.c:8:35: missed: statement clobbers memory: _6 = aligned_alloc (64, _5);
Layer.c:9:34: missed: statement clobbers memory: _7 = aligned_alloc (64, _5);
Layer.c:14:19: missed: statement clobbers memory: _8 = PULSE_GetActivationFunctionPtr (activation_function_18(D));
Layer.c:19:9: note: ***** Analysis succeeded with vector mode V8SI
Layer.c:19:9: note: SLPing BB part
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x44cd3c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: op template: MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: 	stmt 0 MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: 	stmt 1 MEM <unsigned int> [(struct PULSE_Layer *)&<retval> + 4B] = optimization_type_17(D);
Layer.c:19:9: note: 	children 0x44cd448
Layer.c:19:9: note: node (external) 0x44cd448 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ type_16(D), optimization_type_17(D) }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x44cd4c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: op template: <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: 	stmt 0 <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: 	stmt 1 <retval>.n_outputs = n_outputs_12(D);
Layer.c:19:9: note: 	children 0x44cd548
Layer.c:19:9: note: node (external) 0x44cd548 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ n_inputs_11(D), n_outputs_12(D) }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x44cd648 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: <retval>.inputs = _3;
Layer.c:19:9: note: 	stmt 0 <retval>.inputs = _3;
Layer.c:19:9: note: 	stmt 1 <retval>.outputs = _6;
Layer.c:19:9: note: 	children 0x44cd6c8
Layer.c:19:9: note: node (external) 0x44cd6c8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: 	stmt 0 _3 = aligned_alloc (64, _2);
Layer.c:19:9: note: 	stmt 1 _6 = aligned_alloc (64, _5);
Layer.c:19:9: note: 	children 0x44cd748 0x44cd7c8
Layer.c:19:9: note: node (constant) 0x44cd748 (max_nunits=1, refcnt=1)
Layer.c:19:9: note: 	{ 64, 64 }
Layer.c:19:9: note: node 0x44cd7c8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: patt_38 = n_inputs_11(D) w* 4;
Layer.c:19:9: note: 	stmt 0 patt_38 = n_inputs_11(D) w* 4;
Layer.c:19:9: note: 	stmt 1 patt_39 = n_outputs_12(D) w* 4;
Layer.c:19:9: note: 	children 0x44cd848 0x44cd8c8
Layer.c:19:9: note: node (external) 0x44cd848 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ n_inputs_11(D), n_outputs_12(D) }
Layer.c:19:9: note: node (constant) 0x44cd8c8 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ 4, 4 }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x44cd9c8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Layer.c:19:9: note: op template: <retval>.feed = feed_20(D);
Layer.c:19:9: note: 	stmt 0 <retval>.feed = feed_20(D);
Layer.c:19:9: note: 	stmt 1 <retval>.back = back_21(D);
Layer.c:19:9: note: 	stmt 2 <retval>.fix = fix_22(D);
Layer.c:19:9: note: 	stmt 3 <retval>.destroy = destroy_23(D);
Layer.c:19:9: note: 	children 0x44cda48
Layer.c:19:9: note: node (external) 0x44cda48 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Layer.c:19:9: note: 	{ feed_20(D), back_21(D), fix_22(D), destroy_23(D) }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 64
  Scalar cost: 64
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x44cdac8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: <retval>.parent = 0B;
Layer.c:19:9: note: 	stmt 0 <retval>.parent = 0B;
Layer.c:19:9: note: 	stmt 1 <retval>.child = 0B;
Layer.c:19:9: note: 	children 0x44cdb48
Layer.c:19:9: note: node (constant) 0x44cdb48 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: 	{ 0B, 0B }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Layer.c:19:9: note: Basic block will be vectorized using SLP
Layer.c:19:9: optimized: basic block part vectorized using 8 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x44cd3c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: op template: MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: 	stmt 0 MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: 	stmt 1 MEM <unsigned int> [(struct PULSE_Layer *)&<retval> + 4B] = optimization_type_17(D);
Layer.c:19:9: note: 	children 0x44cd448
Layer.c:19:9: note: node (external) 0x44cd448 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ type_16(D), optimization_type_17(D) }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: vect_is_simple_use: operand optimization_type_17(D), type of def: external
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: MEM <unsigned int> [(struct PULSE_Layer *)&<retval>]
Layer.c:19:9: note: created &MEM <unsigned int> [(struct PULSE_Layer *)&<retval>]
Layer.c:19:9: note: add new stmt: MEM <vector(2) unsigned int> [(struct PULSE_Layer *)&<retval>] = _40;
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: optimized: basic block part vectorized using 8 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x44cd4c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: op template: <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: 	stmt 0 <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: 	stmt 1 <retval>.n_outputs = n_outputs_12(D);
Layer.c:19:9: note: 	children 0x44cd548
Layer.c:19:9: note: node (external) 0x44cd548 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ n_inputs_11(D), n_outputs_12(D) }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: vect_is_simple_use: operand n_outputs_12(D), type of def: external
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: <retval>.n_inputs
Layer.c:19:9: note: created &<retval>.n_inputs
Layer.c:19:9: note: add new stmt: MEM <vector(2) unsigned int> [(unsigned int *)&<retval> + 128B] = _42;
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: optimized: basic block part vectorized using 16 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x44cd648 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: <retval>.inputs = _3;
Layer.c:19:9: note: 	stmt 0 <retval>.inputs = _3;
Layer.c:19:9: note: 	stmt 1 <retval>.outputs = _6;
Layer.c:19:9: note: 	children 0x44cd6c8
Layer.c:19:9: note: node (external) 0x44cd6c8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: 	stmt 0 _3 = aligned_alloc (64, _2);
Layer.c:19:9: note: 	stmt 1 _6 = aligned_alloc (64, _5);
Layer.c:19:9: note: 	children 0x44cd748 0x44cd7c8
Layer.c:19:9: note: node (constant) 0x44cd748 (max_nunits=1, refcnt=1)
Layer.c:19:9: note: 	{ 64, 64 }
Layer.c:19:9: note: node 0x44cd7c8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: patt_38 = n_inputs_11(D) w* 4;
Layer.c:19:9: note: 	stmt 0 patt_38 = n_inputs_11(D) w* 4;
Layer.c:19:9: note: 	stmt 1 patt_39 = n_outputs_12(D) w* 4;
Layer.c:19:9: note: 	children 0x44cd848 0x44cd8c8
Layer.c:19:9: note: node (external) 0x44cd848 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ n_inputs_11(D), n_outputs_12(D) }
Layer.c:19:9: note: node (constant) 0x44cd8c8 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ 4, 4 }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: <retval>.inputs = _3;
Layer.c:19:9: note: vect_is_simple_use: operand aligned_alloc (64, _5), type of def: internal
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.inputs
Layer.c:19:9: note: created &<retval>.inputs
Layer.c:19:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval> + 8B] = _46;
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: optimized: basic block part vectorized using 32 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x44cd9c8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Layer.c:19:9: note: op template: <retval>.feed = feed_20(D);
Layer.c:19:9: note: 	stmt 0 <retval>.feed = feed_20(D);
Layer.c:19:9: note: 	stmt 1 <retval>.back = back_21(D);
Layer.c:19:9: note: 	stmt 2 <retval>.fix = fix_22(D);
Layer.c:19:9: note: 	stmt 3 <retval>.destroy = destroy_23(D);
Layer.c:19:9: note: 	children 0x44cda48
Layer.c:19:9: note: node (external) 0x44cda48 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Layer.c:19:9: note: 	{ feed_20(D), back_21(D), fix_22(D), destroy_23(D) }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: <retval>.feed = feed_20(D);
Layer.c:19:9: note: vect_is_simple_use: operand back_21(D), type of def: external
Layer.c:19:9: note: vect_is_simple_use: operand fix_22(D), type of def: external
Layer.c:19:9: note: vect_is_simple_use: operand destroy_23(D), type of def: external
Layer.c:19:9: note: conflicting alias set types.
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(4) long unsigned int  vectorizing a pointer ref: <retval>.feed
Layer.c:19:9: note: created &<retval>.feed
Layer.c:19:9: note: add new stmt: MEM <vector(4) long unsigned int> [(void *)&<retval> + 32B] = _52;
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: optimized: basic block part vectorized using 16 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x44cdac8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: <retval>.parent = 0B;
Layer.c:19:9: note: 	stmt 0 <retval>.parent = 0B;
Layer.c:19:9: note: 	stmt 1 <retval>.child = 0B;
Layer.c:19:9: note: 	children 0x44cdb48
Layer.c:19:9: note: node (constant) 0x44cdb48 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: 	{ 0B, 0B }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: <retval>.parent = 0B;
Layer.c:19:9: note: vect_is_simple_use: operand 0B, type of def: constant
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.parent
Layer.c:19:9: note: created &<retval>.parent
Layer.c:19:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer * *)&<retval> + 72B] = { 0, 0 };
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: note: ***** The result for vector mode V32QI would be the same
Layer.c:24:2: missed: statement clobbers memory: free (_1);
Layer.c:25:2: missed: statement clobbers memory: free (_2);
Layer.c:26:2: missed: statement clobbers memory: free (_3);
Layer.c:27:1: note: ***** Analysis failed with vector mode V4DI
Layer.c:27:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _28 (pretmp_91, _51, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:19: missed: not vectorized: no vectype for stmt: _22 = *_21;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:170:10: optimized: loop vectorized using 32 byte vectors
Dense.c:170:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:170:10: optimized: loop vectorized using 16 byte vectors
Dense.c:161:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
Dense.c:154:10: optimized: loop vectorized using 32 byte vectors
Dense.c:154:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:154:10: optimized: loop vectorized using 16 byte vectors
Dense.c:145:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
Dense.c:136:13: note: vectorized 2 loops in function.
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_28, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
Dense.c:136:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:136:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:105:24: missed: couldn't vectorize loop
Dense.c:105:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:108:17: missed: couldn't vectorize loop
Dense.c:108:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:122:24: missed: couldn't vectorize loop
Dense.c:122:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:125:17: missed: couldn't vectorize loop
Dense.c:125:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:90:15: missed: couldn't vectorize loop
Dense.c:90:15: missed: not vectorized: number of iterations cannot be computed.
Dense.c:83:13: note: vectorized 0 loops in function.
Dense.c:86:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:110:17: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:110:17: note: SLPing BB part
Dense.c:127:17: note: Costing subgraph: 
Dense.c:127:17: note: node 0x7a082b8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:127:17: note: op template: _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 0 _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 1 _198 = (sizetype) wi_137;
Dense.c:127:17: note: 	children 0x7a083b8
Dense.c:127:17: note: node (external) 0x7a083b8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:127:17: note: 	{ j_135, wi_137 }
Dense.c:127:17: note: Cost model analysis: 
Dense.c:127:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:127:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:127:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: Costing subgraph: 
Dense.c:110:17: note: node 0x7a084b8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:110:17: note: op template: _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 0 _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 1 _217 = (sizetype) wi_136;
Dense.c:110:17: note: 	children 0x7a085b8
Dense.c:110:17: note: node (external) 0x7a085b8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:110:17: note: 	{ j_134, wi_136 }
Dense.c:110:17: note: Cost model analysis: 
Dense.c:110:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:110:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:110:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: ***** The result for vector mode V32QI would be the same
Dense.c:110:17: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:110:17: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:110:17: note: SLPing BB part
Dense.c:127:17: note: Costing subgraph: 
Dense.c:127:17: note: node 0x7a084b8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:127:17: note: op template: _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 0 _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 1 _198 = (sizetype) wi_137;
Dense.c:127:17: note: 	children 0x7a082b8
Dense.c:127:17: note: node (external) 0x7a082b8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:127:17: note: 	{ j_135, wi_137 }
Dense.c:127:17: note: Cost model analysis: 
Dense.c:127:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:127:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:127:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: Costing subgraph: 
Dense.c:110:17: note: node 0x7a08438 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:110:17: note: op template: _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 0 _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 1 _217 = (sizetype) wi_136;
Dense.c:110:17: note: 	children 0x7a08538
Dense.c:110:17: note: node (external) 0x7a08538 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:110:17: note: 	{ j_134, wi_136 }
Dense.c:110:17: note: Cost model analysis: 
Dense.c:110:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:110:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:110:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:105:51: note: ***** Analysis failed with vector mode V8QI
Dense.c:105:51: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:105:51: note: ***** Analysis failed with vector mode V4QI
Dense.c:62:15: missed: couldn't vectorize loop
Dense.c:62:15: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:75:11: optimized: loop vectorized using 32 byte vectors
Dense.c:75:11: optimized: loop vectorized using 16 byte vectors
Dense.c:66:11: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:881:10: missed: not vectorized: no vectype for stmt: _53 = MEM[(__m256 * {ref-all})w_ptr_82];
 scalar_type: __m256
Dense.c:54:13: note: vectorized 1 loops in function.
Dense.c:57:2: missed: statement clobbers memory: memcpy (_4, dense$baiases_63, _3);
Dense.c:79:2: missed: statement clobbers memory: _27 (pretmp_163, _68, 0);
Dense.c:54:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:54:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:181:2: missed: statement clobbers memory: free (dense$weights_7);
Dense.c:182:2: missed: statement clobbers memory: free (dense$baiases_8);
Dense.c:183:2: missed: statement clobbers memory: free (dense$gradients_10);
Dense.c:184:2: missed: statement clobbers memory: free (dense$deltas_9);
Dense.c:185:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:185:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:200:19: missed: couldn't vectorize loop
Dense.c:200:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:190:13: note: vectorized 0 loops in function.
Dense.c:196:37: missed: statement clobbers memory: _9 = aligned_alloc (64, _8);
Dense.c:197:34: missed: statement clobbers memory: _11 = aligned_alloc (64, _10);
Dense.c:198:35: missed: statement clobbers memory: _12 = aligned_alloc (64, _10);
Dense.c:201:38: missed: statement clobbers memory: _13 = rand ();
Dense.c:201:72: missed: statement clobbers memory: _57 = sqrt (_20);
Dense.c:213:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:214:4: missed: statement clobbers memory: exit (1);
Dense.c:210:4: missed: statement clobbers memory: layer = PULSE_CreateLayer (args$n_inputs_36, args$n_outputs_47, 0, args$8_48, _SIMD_FeedDense, _SIMD_BackDense, _SIMD_FixDense, _DestroyDense, 1); [return slot optimization]
Dense.c:207:12: missed: statement clobbers memory: layer = PULSE_CreateLayer (args$n_inputs_36, args$n_outputs_47, 0, args$8_48, _FeedDense, _BackDense, _FixDense, _DestroyDense, 0); [return slot optimization]
Dense.c:219:9: missed: not vectorized: more than one data ref in stmt: <retval> = layer;
Dense.c:219:9: note: ***** Analysis succeeded with vector mode V8SI
Dense.c:219:9: note: SLPing BB part
Dense.c:219:9: note: Costing subgraph: 
Dense.c:219:9: note: node 0x7846868 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:219:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = MODEL_32(D);
Dense.c:219:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = MODEL_32(D);
Dense.c:219:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&layer + 96B] = _4;
Dense.c:219:9: note: 	stmt 2 MEM <float *> [(struct PULSE_Layer *)&layer + 104B] = _11;
Dense.c:219:9: note: 	stmt 3 MEM <float *> [(struct PULSE_Layer *)&layer + 112B] = _12;
Dense.c:219:9: note: 	children 0x78468e8
Dense.c:219:9: note: node (external) 0x78468e8 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:219:9: note: 	{ MODEL_32(D), _4, _11, _12 }
Dense.c:219:9: note: Cost model analysis: 
Dense.c:219:9: note: Cost model analysis for part in loop 0:
  Vector cost: 64
  Scalar cost: 64
Dense.c:219:9: note: Basic block will be vectorized using SLP
Dense.c:219:9: optimized: basic block part vectorized using 32 byte vectors
Dense.c:219:9: note: Vectorizing SLP tree:
Dense.c:219:9: note: node 0x7846868 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:219:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = MODEL_32(D);
Dense.c:219:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = MODEL_32(D);
Dense.c:219:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&layer + 96B] = _4;
Dense.c:219:9: note: 	stmt 2 MEM <float *> [(struct PULSE_Layer *)&layer + 104B] = _11;
Dense.c:219:9: note: 	stmt 3 MEM <float *> [(struct PULSE_Layer *)&layer + 112B] = _12;
Dense.c:219:9: note: 	children 0x78468e8
Dense.c:219:9: note: node (external) 0x78468e8 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:219:9: note: 	{ MODEL_32(D), _4, _11, _12 }
Dense.c:219:9: note: ------>vectorizing SLP node starting from: MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = MODEL_32(D);
Dense.c:219:9: note: vect_is_simple_use: operand MODEL_32(D) + _3, type of def: internal
Dense.c:219:9: note: vect_is_simple_use: operand aligned_alloc (64, _10), type of def: internal
Dense.c:219:9: note: vect_is_simple_use: operand aligned_alloc (64, _10), type of def: internal
Dense.c:219:9: note: transform store. ncopies = 1
Dense.c:219:9: note: create vector_type-pointer variable to type: vector(4) long unsigned int  vectorizing a pointer ref: MEM <float *> [(struct PULSE_Layer *)&layer + 88B]
Dense.c:219:9: note: created &MEM <float *> [(struct PULSE_Layer *)&layer + 88B]
Dense.c:219:9: note: add new stmt: MEM <vector(4) long unsigned int> [(struct PULSE_Layer *)&layer + 88B] = _61;
Dense.c:219:9: note: vectorizing stmts using SLP.
Dense.c:219:9: note: ***** The result for vector mode V32QI would be the same
Activations.c:17:19: optimized: loop vectorized using 32 byte vectors
Activations.c:17:19: optimized: loop vectorized using 16 byte vectors
Activations.c:17:19: optimized: loop vectorized using 32 byte vectors
Activations.c:17:19: optimized: loop vectorized using 16 byte vectors
Activations.c:14:13: note: vectorized 2 loops in function.
Activations.c:14:13: note: ***** Analysis failed with vector mode V8SF
Activations.c:14:13: note: ***** The result for vector mode V32QI would be the same
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V16QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V16QI
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V8QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V8QI
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V4QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V4QI
Activations.c:28:19: missed: couldn't vectorize loop
Activations.c:28:19: missed: not vectorized: unsupported control flow in loop.
Activations.c:28:19: optimized: loop vectorized using 32 byte vectors
Activations.c:28:19: optimized: loop vectorized using 16 byte vectors
Activations.c:25:13: note: vectorized 1 loops in function.
Activations.c:25:13: note: ***** Analysis failed with vector mode V8SF
Activations.c:25:13: note: ***** The result for vector mode V32QI would be the same
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V16QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V16QI
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V8QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V8QI
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V4QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V4QI
Activations.c:38:1: note: ***** Analysis failed with vector mode VOID
Activations.c:5:19: missed: couldn't vectorize loop
Activations.c:9:21: missed: statement clobbers memory: _36 = expf (_18);
Activations.c:5:19: missed: couldn't vectorize loop
Activations.c:9:21: missed: statement clobbers memory: _12 = expf (_11);
Activations.c:3:13: note: vectorized 0 loops in function.
Activations.c:9:21: missed: statement clobbers memory: _12 = expf (_11);
Activations.c:9:21: missed: statement clobbers memory: _36 = expf (_18);
Activations.c:11:1: note: ***** Analysis failed with vector mode V8SF
Activations.c:11:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SF
Activations.c:44:2: note: ***** Analysis failed with vector mode VOID
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:4:18: note: vectorized 0 loops in function.
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_4, inputs_15, _3);
PULSE.c:8:2: missed: statement clobbers memory: _5 (layer_17);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_12, pretmp_26, _9);
PULSE.c:13:28: note: ***** Analysis failed with vector mode V8SI
PULSE.c:13:28: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:22:2: missed: statement clobbers memory: _1 (layer_10(D));
PULSE.c:22:2: missed: statement clobbers memory: _14 (_2);
PULSE.c:22:2: missed: statement clobbers memory: _21 (_15);
PULSE.c:22:2: missed: statement clobbers memory: _28 (_22);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_29);
PULSE.c:26:3: missed: statement clobbers memory: memset (_34, 0, _32);
PULSE.c:26:3: missed: statement clobbers memory: memset (_27, 0, _25);
PULSE.c:26:3: missed: statement clobbers memory: memset (_20, 0, _18);
PULSE.c:26:3: missed: statement clobbers memory: memset (_7, 0, _5);
PULSE.c:28:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:28:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:32:6: note: vectorized 1 loops in function.
PULSE.c:34:8: missed: statement clobbers memory: _1 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_2);
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _9 = rand ();
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8SI
PULSE.c:32:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:61:16: missed: couldn't vectorize loop
PULSE.c:61:16: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:64:17: missed: couldn't vectorize loop
PULSE.c:64:17: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:73:19: missed: couldn't vectorize loop
PULSE.c:73:19: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _69 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:53:22: missed: couldn't vectorize loop
PULSE.c:53:22: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:47:6: note: vectorized 1 loops in function.
PULSE.c:48:1: missed: statement clobbers memory: saved_stack.7_39 = __builtin_stack_save ();
PULSE.c:49:46: missed: statement clobbers memory: _1 = PULSE_GetLossFunctionPtr (loss_function_40(D));
PULSE.c:60:10: missed: statement clobbers memory: random.4_46 = __builtin_alloca_with_align (_4, 32);
PULSE.c:34:8: missed: statement clobbers memory: _61 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_62);
PULSE.c:39:29: missed: statement clobbers memory: _69 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _72 = rand ();
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_98, inputs_93, _97);
PULSE.c:8:2: missed: statement clobbers memory: _99 (layer_94);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_105, _104, _103);
PULSE.c:67:11: missed: statement clobbers memory: loss_55 = PULSE_GetLoss_42 (_17, _16, _12, _11);
PULSE.c:22:2: missed: statement clobbers memory: _86 (output_34);
PULSE.c:22:2: missed: statement clobbers memory: _108 (_87);
PULSE.c:22:2: missed: statement clobbers memory: _115 (_109);
PULSE.c:22:2: missed: statement clobbers memory: _122 (_116);
PULSE.c:22:2: missed: statement clobbers memory: _129 (_123);
PULSE.c:22:2: missed: statement clobbers memory: _136 (_130);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_137);
PULSE.c:26:3: missed: statement clobbers memory: memset (_142, 0, _140);
PULSE.c:26:3: missed: statement clobbers memory: memset (_135, 0, _133);
PULSE.c:26:3: missed: statement clobbers memory: memset (_128, 0, _126);
PULSE.c:26:3: missed: statement clobbers memory: memset (_121, 0, _119);
PULSE.c:26:3: missed: statement clobbers memory: memset (_114, 0, _112);
PULSE.c:26:3: missed: statement clobbers memory: memset (_92, 0, _90);
PULSE.c:75:6: missed: statement clobbers memory: _21 (current_191, args);
PULSE.c:81:4: missed: statement clobbers memory: printf ("Epoch: %d | Item: %d | Loss: %.10f | Batch Loss: %.10f\r", i_188, j_189, _25, _24);
PULSE.c:47:6: missed: statement clobbers memory: __builtin_stack_restore (saved_stack.7_39);
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4DI
PULSE.c:47:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:91:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:91:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:119:20: missed: couldn't vectorize loop
PULSE.c:119:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:119:20: missed: couldn't vectorize loop
PULSE.c:119:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:102:20: missed: couldn't vectorize loop
PULSE.c:102:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:102:20: missed: couldn't vectorize loop
PULSE.c:102:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:93:13: note: vectorized 0 loops in function.
PULSE.c:96:39: missed: statement clobbers memory: layers_31 = malloc (_3);
PULSE.c:99:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:114:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:115:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:116:46: missed: statement clobbers memory: WEIGHTS_46 = malloc (0);
PULSE.c:108:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.104_68];
PULSE.c:109:21: missed: statement clobbers memory: _57 = PULSE_GetDenseWeightsSize (args);
PULSE.c:108:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})_179];
PULSE.c:109:21: missed: statement clobbers memory: _171 = PULSE_GetDenseWeightsSize (args);
PULSE.c:114:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:115:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:116:46: missed: statement clobbers memory: WEIGHTS_36 = malloc (_5);
PULSE.c:125:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.106_70];
PULSE.c:126:17: missed: statement clobbers memory: *_8 = PULSE_CreateDenseLayer (args, WEIGHTS_PTR_121); [return slot optimization]
PULSE.c:127:20: missed: statement clobbers memory: _49 = PULSE_GetDenseWeightsSize (args);
PULSE.c:125:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.106_113];
PULSE.c:126:17: missed: statement clobbers memory: *_98 = PULSE_CreateDenseLayer (args, WEIGHTS_PTR_135); [return slot optimization]
PULSE.c:127:20: missed: statement clobbers memory: _76 = PULSE_GetDenseWeightsSize (args);
PULSE.c:133:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:134:9: note: ***** Analysis succeeded with vector mode V8SI
PULSE.c:134:9: note: SLPing BB part
PULSE.c:134:9: note: Costing subgraph: 
PULSE.c:134:9: note: node 0x4978808 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:134:9: note: op template: <retval>.layers = layers_31;
PULSE.c:134:9: note: 	stmt 0 <retval>.layers = layers_31;
PULSE.c:134:9: note: 	stmt 1 <retval>.paramethers = WEIGHTS_80;
PULSE.c:134:9: note: 	children 0x4978908
PULSE.c:134:9: note: node (external) 0x4978908 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:134:9: note: 	{ layers_31, WEIGHTS_80 }
PULSE.c:134:9: note: Cost model analysis: 
PULSE.c:134:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:134:9: note: Costing subgraph: 
PULSE.c:134:9: note: node 0x4978988 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:134:9: note: op template: <retval>.io = 0B;
PULSE.c:134:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:134:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:134:9: note: 	children 0x4978a08
PULSE.c:134:9: note: node (constant) 0x4978a08 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:134:9: note: 	{ 0B, 0B }
PULSE.c:134:9: note: Cost model analysis: 
PULSE.c:134:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:134:9: note: Basic block will be vectorized using SLP
PULSE.c:134:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:134:9: note: Vectorizing SLP tree:
PULSE.c:134:9: note: node 0x4978808 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:134:9: note: op template: <retval>.layers = layers_31;
PULSE.c:134:9: note: 	stmt 0 <retval>.layers = layers_31;
PULSE.c:134:9: note: 	stmt 1 <retval>.paramethers = WEIGHTS_80;
PULSE.c:134:9: note: 	children 0x4978908
PULSE.c:134:9: note: node (external) 0x4978908 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:134:9: note: 	{ layers_31, WEIGHTS_80 }
PULSE.c:134:9: note: ------>vectorizing SLP node starting from: <retval>.layers = layers_31;
PULSE.c:134:9: note: vect_is_simple_use: operand WEIGHTS_80 = PHI <WEIGHTS_36(36), WEIGHTS_46(12)>, type of def: internal
PULSE.c:134:9: note: conflicting alias set types.
PULSE.c:134:9: note: transform store. ncopies = 1
PULSE.c:134:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.layers
PULSE.c:134:9: note: created &<retval>
PULSE.c:134:9: note: add new stmt: MEM <vector(2) long unsigned int> [(void *)&<retval>] = _11;
PULSE.c:134:9: note: vectorizing stmts using SLP.
PULSE.c:134:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:134:9: note: Vectorizing SLP tree:
PULSE.c:134:9: note: node 0x4978988 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:134:9: note: op template: <retval>.io = 0B;
PULSE.c:134:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:134:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:134:9: note: 	children 0x4978a08
PULSE.c:134:9: note: node (constant) 0x4978a08 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:134:9: note: 	{ 0B, 0B }
PULSE.c:134:9: note: ------>vectorizing SLP node starting from: <retval>.io = 0B;
PULSE.c:134:9: note: vect_is_simple_use: operand 0B, type of def: constant
PULSE.c:134:9: note: transform store. ncopies = 1
PULSE.c:134:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.io
PULSE.c:134:9: note: created &<retval>.io
PULSE.c:134:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval> + 16B] = { 0, 0 };
PULSE.c:134:9: note: vectorizing stmts using SLP.
PULSE.c:134:9: note: ***** The result for vector mode V32QI would be the same
PULSE.c:146:1: note: ***** Analysis failed with vector mode VOID
Loss.c:15:19: optimized: loop vectorized using 32 byte vectors
Loss.c:15:19: optimized:  loop versioned for vectorization because of possible aliasing
Loss.c:15:19: optimized: loop vectorized using 16 byte vectors
Loss.c:12:23: note: vectorized 1 loops in function.
Loss.c:12:23: note: ***** Analysis failed with vector mode V8SF
Loss.c:12:23: note: ***** The result for vector mode V32QI would be the same
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V16QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V16QI
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V8QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V8QI
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V4QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V4QI
Loss.c:7:19: missed: couldn't vectorize loop
Loss.c:8:38: missed: not vectorized: unsupported use in stmt.
Loss.c:4:23: note: vectorized 0 loops in function.
Loss.c:9:13: note: ***** Analysis failed with vector mode V8SF
Loss.c:9:13: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SF
Loss.c:23:2: note: ***** Analysis failed with vector mode VOID
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _28 (pretmp_91, _51, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:19: missed: not vectorized: no vectype for stmt: _22 = *_21;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:170:10: optimized: loop vectorized using 32 byte vectors
Dense.c:170:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:170:10: optimized: loop vectorized using 16 byte vectors
Dense.c:161:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
Dense.c:154:10: optimized: loop vectorized using 32 byte vectors
Dense.c:154:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:154:10: optimized: loop vectorized using 16 byte vectors
Dense.c:145:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
Dense.c:136:13: note: vectorized 2 loops in function.
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_28, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
Dense.c:136:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:136:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:105:24: missed: couldn't vectorize loop
Dense.c:105:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:108:17: missed: couldn't vectorize loop
Dense.c:108:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:122:24: missed: couldn't vectorize loop
Dense.c:122:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:125:17: missed: couldn't vectorize loop
Dense.c:125:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:90:15: missed: couldn't vectorize loop
Dense.c:90:15: missed: not vectorized: number of iterations cannot be computed.
Dense.c:83:13: note: vectorized 0 loops in function.
Dense.c:86:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:110:17: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:110:17: note: SLPing BB part
Dense.c:127:17: note: Costing subgraph: 
Dense.c:127:17: note: node 0x83f4518 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:127:17: note: op template: _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 0 _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 1 _198 = (sizetype) wi_137;
Dense.c:127:17: note: 	children 0x83f4618
Dense.c:127:17: note: node (external) 0x83f4618 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:127:17: note: 	{ j_135, wi_137 }
Dense.c:127:17: note: Cost model analysis: 
Dense.c:127:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:127:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:127:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: Costing subgraph: 
Dense.c:110:17: note: node 0x83f4718 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:110:17: note: op template: _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 0 _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 1 _217 = (sizetype) wi_136;
Dense.c:110:17: note: 	children 0x83f4818
Dense.c:110:17: note: node (external) 0x83f4818 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:110:17: note: 	{ j_134, wi_136 }
Dense.c:110:17: note: Cost model analysis: 
Dense.c:110:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:110:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:110:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: ***** The result for vector mode V32QI would be the same
Dense.c:110:17: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:110:17: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:110:17: note: SLPing BB part
Dense.c:127:17: note: Costing subgraph: 
Dense.c:127:17: note: node 0x83f4718 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:127:17: note: op template: _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 0 _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 1 _198 = (sizetype) wi_137;
Dense.c:127:17: note: 	children 0x83f4518
Dense.c:127:17: note: node (external) 0x83f4518 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:127:17: note: 	{ j_135, wi_137 }
Dense.c:127:17: note: Cost model analysis: 
Dense.c:127:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:127:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:127:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: Costing subgraph: 
Dense.c:110:17: note: node 0x83f4698 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:110:17: note: op template: _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 0 _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 1 _217 = (sizetype) wi_136;
Dense.c:110:17: note: 	children 0x83f4798
Dense.c:110:17: note: node (external) 0x83f4798 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:110:17: note: 	{ j_134, wi_136 }
Dense.c:110:17: note: Cost model analysis: 
Dense.c:110:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:110:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:110:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:105:51: note: ***** Analysis failed with vector mode V8QI
Dense.c:105:51: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:105:51: note: ***** Analysis failed with vector mode V4QI
Dense.c:62:15: missed: couldn't vectorize loop
Dense.c:62:15: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:75:11: optimized: loop vectorized using 32 byte vectors
Dense.c:75:11: optimized: loop vectorized using 16 byte vectors
Dense.c:66:11: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:881:10: missed: not vectorized: no vectype for stmt: _53 = MEM[(__m256 * {ref-all})w_ptr_82];
 scalar_type: __m256
Dense.c:54:13: note: vectorized 1 loops in function.
Dense.c:57:2: missed: statement clobbers memory: memcpy (_4, dense$baiases_63, _3);
Dense.c:79:2: missed: statement clobbers memory: _27 (pretmp_163, _68, 0);
Dense.c:54:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:54:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:181:2: missed: statement clobbers memory: free (dense$weights_7);
Dense.c:182:2: missed: statement clobbers memory: free (dense$baiases_8);
Dense.c:183:2: missed: statement clobbers memory: free (dense$gradients_10);
Dense.c:184:2: missed: statement clobbers memory: free (dense$deltas_9);
Dense.c:185:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:185:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:188:107: note: ***** Analysis failed with vector mode V8SI
Dense.c:188:107: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
Dense.c:200:19: missed: couldn't vectorize loop
Dense.c:200:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:190:13: note: vectorized 0 loops in function.
Dense.c:196:37: missed: statement clobbers memory: _9 = aligned_alloc (64, _8);
Dense.c:197:34: missed: statement clobbers memory: _11 = aligned_alloc (64, _10);
Dense.c:198:35: missed: statement clobbers memory: _12 = aligned_alloc (64, _10);
Dense.c:201:38: missed: statement clobbers memory: _13 = rand ();
Dense.c:201:72: missed: statement clobbers memory: _57 = sqrt (_20);
Dense.c:213:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:214:4: missed: statement clobbers memory: exit (1);
Dense.c:210:4: missed: statement clobbers memory: layer = PULSE_CreateLayer (args$n_inputs_36, args$n_outputs_47, 0, args$8_48, _SIMD_FeedDense, _SIMD_BackDense, _SIMD_FixDense, _DestroyDense, 1); [return slot optimization]
Dense.c:207:12: missed: statement clobbers memory: layer = PULSE_CreateLayer (args$n_inputs_36, args$n_outputs_47, 0, args$8_48, _FeedDense, _BackDense, _FixDense, _DestroyDense, 0); [return slot optimization]
Dense.c:219:9: missed: not vectorized: more than one data ref in stmt: <retval> = layer;
Dense.c:219:9: note: ***** Analysis succeeded with vector mode V8SI
Dense.c:219:9: note: SLPing BB part
Dense.c:219:9: note: Costing subgraph: 
Dense.c:219:9: note: node 0x822abe8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:219:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = MODEL_32(D);
Dense.c:219:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = MODEL_32(D);
Dense.c:219:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&layer + 96B] = _4;
Dense.c:219:9: note: 	stmt 2 MEM <float *> [(struct PULSE_Layer *)&layer + 104B] = _11;
Dense.c:219:9: note: 	stmt 3 MEM <float *> [(struct PULSE_Layer *)&layer + 112B] = _12;
Dense.c:219:9: note: 	children 0x822ac68
Dense.c:219:9: note: node (external) 0x822ac68 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:219:9: note: 	{ MODEL_32(D), _4, _11, _12 }
Dense.c:219:9: note: Cost model analysis: 
Dense.c:219:9: note: Cost model analysis for part in loop 0:
  Vector cost: 64
  Scalar cost: 64
Dense.c:219:9: note: Basic block will be vectorized using SLP
Dense.c:219:9: optimized: basic block part vectorized using 32 byte vectors
Dense.c:219:9: note: Vectorizing SLP tree:
Dense.c:219:9: note: node 0x822abe8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:219:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = MODEL_32(D);
Dense.c:219:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = MODEL_32(D);
Dense.c:219:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&layer + 96B] = _4;
Dense.c:219:9: note: 	stmt 2 MEM <float *> [(struct PULSE_Layer *)&layer + 104B] = _11;
Dense.c:219:9: note: 	stmt 3 MEM <float *> [(struct PULSE_Layer *)&layer + 112B] = _12;
Dense.c:219:9: note: 	children 0x822ac68
Dense.c:219:9: note: node (external) 0x822ac68 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:219:9: note: 	{ MODEL_32(D), _4, _11, _12 }
Dense.c:219:9: note: ------>vectorizing SLP node starting from: MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = MODEL_32(D);
Dense.c:219:9: note: vect_is_simple_use: operand MODEL_32(D) + _3, type of def: internal
Dense.c:219:9: note: vect_is_simple_use: operand aligned_alloc (64, _10), type of def: internal
Dense.c:219:9: note: vect_is_simple_use: operand aligned_alloc (64, _10), type of def: internal
Dense.c:219:9: note: transform store. ncopies = 1
Dense.c:219:9: note: create vector_type-pointer variable to type: vector(4) long unsigned int  vectorizing a pointer ref: MEM <float *> [(struct PULSE_Layer *)&layer + 88B]
Dense.c:219:9: note: created &MEM <float *> [(struct PULSE_Layer *)&layer + 88B]
Dense.c:219:9: note: add new stmt: MEM <vector(4) long unsigned int> [(struct PULSE_Layer *)&layer + 88B] = _61;
Dense.c:219:9: note: vectorizing stmts using SLP.
Dense.c:219:9: note: ***** The result for vector mode V32QI would be the same
Layer.c:7:34: missed: statement clobbers memory: _3 = aligned_alloc (64, _2);
Layer.c:8:35: missed: statement clobbers memory: _6 = aligned_alloc (64, _5);
Layer.c:9:34: missed: statement clobbers memory: _7 = aligned_alloc (64, _5);
Layer.c:14:19: missed: statement clobbers memory: _8 = PULSE_GetActivationFunctionPtr (activation_function_18(D));
Layer.c:19:9: note: ***** Analysis succeeded with vector mode V8SI
Layer.c:19:9: note: SLPing BB part
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x4ee73c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: op template: MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: 	stmt 0 MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: 	stmt 1 MEM <unsigned int> [(struct PULSE_Layer *)&<retval> + 4B] = optimization_type_17(D);
Layer.c:19:9: note: 	children 0x4ee7448
Layer.c:19:9: note: node (external) 0x4ee7448 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ type_16(D), optimization_type_17(D) }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x4ee74c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: op template: <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: 	stmt 0 <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: 	stmt 1 <retval>.n_outputs = n_outputs_12(D);
Layer.c:19:9: note: 	children 0x4ee7548
Layer.c:19:9: note: node (external) 0x4ee7548 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ n_inputs_11(D), n_outputs_12(D) }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x4ee7648 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: <retval>.inputs = _3;
Layer.c:19:9: note: 	stmt 0 <retval>.inputs = _3;
Layer.c:19:9: note: 	stmt 1 <retval>.outputs = _6;
Layer.c:19:9: note: 	children 0x4ee76c8
Layer.c:19:9: note: node (external) 0x4ee76c8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: 	stmt 0 _3 = aligned_alloc (64, _2);
Layer.c:19:9: note: 	stmt 1 _6 = aligned_alloc (64, _5);
Layer.c:19:9: note: 	children 0x4ee7748 0x4ee77c8
Layer.c:19:9: note: node (constant) 0x4ee7748 (max_nunits=1, refcnt=1)
Layer.c:19:9: note: 	{ 64, 64 }
Layer.c:19:9: note: node 0x4ee77c8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: patt_38 = n_inputs_11(D) w* 4;
Layer.c:19:9: note: 	stmt 0 patt_38 = n_inputs_11(D) w* 4;
Layer.c:19:9: note: 	stmt 1 patt_39 = n_outputs_12(D) w* 4;
Layer.c:19:9: note: 	children 0x4ee7848 0x4ee78c8
Layer.c:19:9: note: node (external) 0x4ee7848 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ n_inputs_11(D), n_outputs_12(D) }
Layer.c:19:9: note: node (constant) 0x4ee78c8 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ 4, 4 }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x4ee79c8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Layer.c:19:9: note: op template: <retval>.feed = feed_20(D);
Layer.c:19:9: note: 	stmt 0 <retval>.feed = feed_20(D);
Layer.c:19:9: note: 	stmt 1 <retval>.back = back_21(D);
Layer.c:19:9: note: 	stmt 2 <retval>.fix = fix_22(D);
Layer.c:19:9: note: 	stmt 3 <retval>.destroy = destroy_23(D);
Layer.c:19:9: note: 	children 0x4ee7a48
Layer.c:19:9: note: node (external) 0x4ee7a48 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Layer.c:19:9: note: 	{ feed_20(D), back_21(D), fix_22(D), destroy_23(D) }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 64
  Scalar cost: 64
Layer.c:19:9: note: Costing subgraph: 
Layer.c:19:9: note: node 0x4ee7ac8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: <retval>.parent = 0B;
Layer.c:19:9: note: 	stmt 0 <retval>.parent = 0B;
Layer.c:19:9: note: 	stmt 1 <retval>.child = 0B;
Layer.c:19:9: note: 	children 0x4ee7b48
Layer.c:19:9: note: node (constant) 0x4ee7b48 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: 	{ 0B, 0B }
Layer.c:19:9: note: Cost model analysis: 
Layer.c:19:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Layer.c:19:9: note: Basic block will be vectorized using SLP
Layer.c:19:9: optimized: basic block part vectorized using 8 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x4ee73c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: op template: MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: 	stmt 0 MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: 	stmt 1 MEM <unsigned int> [(struct PULSE_Layer *)&<retval> + 4B] = optimization_type_17(D);
Layer.c:19:9: note: 	children 0x4ee7448
Layer.c:19:9: note: node (external) 0x4ee7448 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ type_16(D), optimization_type_17(D) }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: MEM <unsigned int> [(struct PULSE_Layer *)&<retval>] = type_16(D);
Layer.c:19:9: note: vect_is_simple_use: operand optimization_type_17(D), type of def: external
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: MEM <unsigned int> [(struct PULSE_Layer *)&<retval>]
Layer.c:19:9: note: created &MEM <unsigned int> [(struct PULSE_Layer *)&<retval>]
Layer.c:19:9: note: add new stmt: MEM <vector(2) unsigned int> [(struct PULSE_Layer *)&<retval>] = _40;
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: optimized: basic block part vectorized using 8 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x4ee74c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: op template: <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: 	stmt 0 <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: 	stmt 1 <retval>.n_outputs = n_outputs_12(D);
Layer.c:19:9: note: 	children 0x4ee7548
Layer.c:19:9: note: node (external) 0x4ee7548 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ n_inputs_11(D), n_outputs_12(D) }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: <retval>.n_inputs = n_inputs_11(D);
Layer.c:19:9: note: vect_is_simple_use: operand n_outputs_12(D), type of def: external
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: <retval>.n_inputs
Layer.c:19:9: note: created &<retval>.n_inputs
Layer.c:19:9: note: add new stmt: MEM <vector(2) unsigned int> [(unsigned int *)&<retval> + 128B] = _42;
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: optimized: basic block part vectorized using 16 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x4ee7648 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: <retval>.inputs = _3;
Layer.c:19:9: note: 	stmt 0 <retval>.inputs = _3;
Layer.c:19:9: note: 	stmt 1 <retval>.outputs = _6;
Layer.c:19:9: note: 	children 0x4ee76c8
Layer.c:19:9: note: node (external) 0x4ee76c8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: 	stmt 0 _3 = aligned_alloc (64, _2);
Layer.c:19:9: note: 	stmt 1 _6 = aligned_alloc (64, _5);
Layer.c:19:9: note: 	children 0x4ee7748 0x4ee77c8
Layer.c:19:9: note: node (constant) 0x4ee7748 (max_nunits=1, refcnt=1)
Layer.c:19:9: note: 	{ 64, 64 }
Layer.c:19:9: note: node 0x4ee77c8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: patt_38 = n_inputs_11(D) w* 4;
Layer.c:19:9: note: 	stmt 0 patt_38 = n_inputs_11(D) w* 4;
Layer.c:19:9: note: 	stmt 1 patt_39 = n_outputs_12(D) w* 4;
Layer.c:19:9: note: 	children 0x4ee7848 0x4ee78c8
Layer.c:19:9: note: node (external) 0x4ee7848 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ n_inputs_11(D), n_outputs_12(D) }
Layer.c:19:9: note: node (constant) 0x4ee78c8 (max_nunits=1, refcnt=1) vector(2) unsigned int
Layer.c:19:9: note: 	{ 4, 4 }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: <retval>.inputs = _3;
Layer.c:19:9: note: vect_is_simple_use: operand aligned_alloc (64, _5), type of def: internal
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.inputs
Layer.c:19:9: note: created &<retval>.inputs
Layer.c:19:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval> + 8B] = _46;
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: optimized: basic block part vectorized using 32 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x4ee79c8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Layer.c:19:9: note: op template: <retval>.feed = feed_20(D);
Layer.c:19:9: note: 	stmt 0 <retval>.feed = feed_20(D);
Layer.c:19:9: note: 	stmt 1 <retval>.back = back_21(D);
Layer.c:19:9: note: 	stmt 2 <retval>.fix = fix_22(D);
Layer.c:19:9: note: 	stmt 3 <retval>.destroy = destroy_23(D);
Layer.c:19:9: note: 	children 0x4ee7a48
Layer.c:19:9: note: node (external) 0x4ee7a48 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Layer.c:19:9: note: 	{ feed_20(D), back_21(D), fix_22(D), destroy_23(D) }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: <retval>.feed = feed_20(D);
Layer.c:19:9: note: vect_is_simple_use: operand back_21(D), type of def: external
Layer.c:19:9: note: vect_is_simple_use: operand fix_22(D), type of def: external
Layer.c:19:9: note: vect_is_simple_use: operand destroy_23(D), type of def: external
Layer.c:19:9: note: conflicting alias set types.
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(4) long unsigned int  vectorizing a pointer ref: <retval>.feed
Layer.c:19:9: note: created &<retval>.feed
Layer.c:19:9: note: add new stmt: MEM <vector(4) long unsigned int> [(void *)&<retval> + 32B] = _52;
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: optimized: basic block part vectorized using 16 byte vectors
Layer.c:19:9: note: Vectorizing SLP tree:
Layer.c:19:9: note: node 0x4ee7ac8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: op template: <retval>.parent = 0B;
Layer.c:19:9: note: 	stmt 0 <retval>.parent = 0B;
Layer.c:19:9: note: 	stmt 1 <retval>.child = 0B;
Layer.c:19:9: note: 	children 0x4ee7b48
Layer.c:19:9: note: node (constant) 0x4ee7b48 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Layer.c:19:9: note: 	{ 0B, 0B }
Layer.c:19:9: note: ------>vectorizing SLP node starting from: <retval>.parent = 0B;
Layer.c:19:9: note: vect_is_simple_use: operand 0B, type of def: constant
Layer.c:19:9: note: transform store. ncopies = 1
Layer.c:19:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.parent
Layer.c:19:9: note: created &<retval>.parent
Layer.c:19:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer * *)&<retval> + 72B] = { 0, 0 };
Layer.c:19:9: note: vectorizing stmts using SLP.
Layer.c:19:9: note: ***** The result for vector mode V32QI would be the same
Layer.c:24:2: missed: statement clobbers memory: free (_1);
Layer.c:25:2: missed: statement clobbers memory: free (_2);
Layer.c:26:2: missed: statement clobbers memory: free (_3);
Layer.c:27:1: note: ***** Analysis failed with vector mode V4DI
Layer.c:27:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _28 (pretmp_91, _51, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:19: missed: not vectorized: no vectype for stmt: _22 = *_21;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:170:10: optimized: loop vectorized using 32 byte vectors
Dense.c:170:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:170:10: optimized: loop vectorized using 16 byte vectors
Dense.c:161:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
Dense.c:154:10: optimized: loop vectorized using 32 byte vectors
Dense.c:154:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:154:10: optimized: loop vectorized using 16 byte vectors
Dense.c:145:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
Dense.c:136:13: note: vectorized 2 loops in function.
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_28, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
Dense.c:136:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:136:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:105:24: missed: couldn't vectorize loop
Dense.c:105:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:108:17: missed: couldn't vectorize loop
Dense.c:108:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:122:24: missed: couldn't vectorize loop
Dense.c:122:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:125:17: missed: couldn't vectorize loop
Dense.c:125:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:90:15: missed: couldn't vectorize loop
Dense.c:90:15: missed: not vectorized: number of iterations cannot be computed.
Dense.c:83:13: note: vectorized 0 loops in function.
Dense.c:86:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:110:17: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:110:17: note: SLPing BB part
Dense.c:127:17: note: Costing subgraph: 
Dense.c:127:17: note: node 0x877c6c8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:127:17: note: op template: _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 0 _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 1 _198 = (sizetype) wi_137;
Dense.c:127:17: note: 	children 0x877c7c8
Dense.c:127:17: note: node (external) 0x877c7c8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:127:17: note: 	{ j_135, wi_137 }
Dense.c:127:17: note: Cost model analysis: 
Dense.c:127:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:127:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:127:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: Costing subgraph: 
Dense.c:110:17: note: node 0x877c8c8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:110:17: note: op template: _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 0 _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 1 _217 = (sizetype) wi_136;
Dense.c:110:17: note: 	children 0x877c9c8
Dense.c:110:17: note: node (external) 0x877c9c8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:110:17: note: 	{ j_134, wi_136 }
Dense.c:110:17: note: Cost model analysis: 
Dense.c:110:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:110:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:110:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: ***** The result for vector mode V32QI would be the same
Dense.c:110:17: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:110:17: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:110:17: note: SLPing BB part
Dense.c:127:17: note: Costing subgraph: 
Dense.c:127:17: note: node 0x877c8c8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:127:17: note: op template: _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 0 _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 1 _198 = (sizetype) wi_137;
Dense.c:127:17: note: 	children 0x877c6c8
Dense.c:127:17: note: node (external) 0x877c6c8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:127:17: note: 	{ j_135, wi_137 }
Dense.c:127:17: note: Cost model analysis: 
Dense.c:127:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:127:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:127:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: Costing subgraph: 
Dense.c:110:17: note: node 0x877c848 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:110:17: note: op template: _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 0 _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 1 _217 = (sizetype) wi_136;
Dense.c:110:17: note: 	children 0x877c948
Dense.c:110:17: note: node (external) 0x877c948 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:110:17: note: 	{ j_134, wi_136 }
Dense.c:110:17: note: Cost model analysis: 
Dense.c:110:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:110:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:110:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:105:51: note: ***** Analysis failed with vector mode V8QI
Dense.c:105:51: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:105:51: note: ***** Analysis failed with vector mode V4QI
Dense.c:62:15: missed: couldn't vectorize loop
Dense.c:62:15: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:75:11: optimized: loop vectorized using 32 byte vectors
Dense.c:75:11: optimized: loop vectorized using 16 byte vectors
Dense.c:66:11: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:881:10: missed: not vectorized: no vectype for stmt: _53 = MEM[(__m256 * {ref-all})w_ptr_82];
 scalar_type: __m256
Dense.c:54:13: note: vectorized 1 loops in function.
Dense.c:57:2: missed: statement clobbers memory: memcpy (_4, dense$baiases_63, _3);
Dense.c:79:2: missed: statement clobbers memory: _27 (pretmp_163, _68, 0);
Dense.c:54:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:54:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:181:2: missed: statement clobbers memory: free (dense$weights_7);
Dense.c:182:2: missed: statement clobbers memory: free (dense$baiases_8);
Dense.c:183:2: missed: statement clobbers memory: free (dense$gradients_10);
Dense.c:184:2: missed: statement clobbers memory: free (dense$deltas_9);
Dense.c:185:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:185:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:188:107: note: ***** Analysis failed with vector mode V8SI
Dense.c:188:107: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
Dense.c:200:19: missed: couldn't vectorize loop
Dense.c:200:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:190:13: note: vectorized 0 loops in function.
Dense.c:196:37: missed: statement clobbers memory: _9 = aligned_alloc (64, _8);
Dense.c:197:34: missed: statement clobbers memory: _11 = aligned_alloc (64, _10);
Dense.c:198:35: missed: statement clobbers memory: _12 = aligned_alloc (64, _10);
Dense.c:201:38: missed: statement clobbers memory: _13 = rand ();
Dense.c:201:72: missed: statement clobbers memory: _57 = sqrt (_20);
Dense.c:213:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:214:4: missed: statement clobbers memory: exit (1);
Dense.c:210:4: missed: statement clobbers memory: layer = PULSE_CreateLayer (args$n_inputs_36, args$n_outputs_47, 0, args$8_48, _SIMD_FeedDense, _SIMD_BackDense, _SIMD_FixDense, _DestroyDense, 1); [return slot optimization]
Dense.c:207:12: missed: statement clobbers memory: layer = PULSE_CreateLayer (args$n_inputs_36, args$n_outputs_47, 0, args$8_48, _FeedDense, _BackDense, _FixDense, _DestroyDense, 0); [return slot optimization]
Dense.c:219:9: missed: not vectorized: more than one data ref in stmt: <retval> = layer;
Dense.c:219:9: note: ***** Analysis succeeded with vector mode V8SI
Dense.c:219:9: note: SLPing BB part
Dense.c:219:9: note: Costing subgraph: 
Dense.c:219:9: note: node 0x8664038 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:219:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = MODEL_32(D);
Dense.c:219:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = MODEL_32(D);
Dense.c:219:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&layer + 96B] = _4;
Dense.c:219:9: note: 	stmt 2 MEM <float *> [(struct PULSE_Layer *)&layer + 104B] = _11;
Dense.c:219:9: note: 	stmt 3 MEM <float *> [(struct PULSE_Layer *)&layer + 112B] = _12;
Dense.c:219:9: note: 	children 0x86640b8
Dense.c:219:9: note: node (external) 0x86640b8 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:219:9: note: 	{ MODEL_32(D), _4, _11, _12 }
Dense.c:219:9: note: Cost model analysis: 
Dense.c:219:9: note: Cost model analysis for part in loop 0:
  Vector cost: 64
  Scalar cost: 64
Dense.c:219:9: note: Basic block will be vectorized using SLP
Dense.c:219:9: optimized: basic block part vectorized using 32 byte vectors
Dense.c:219:9: note: Vectorizing SLP tree:
Dense.c:219:9: note: node 0x8664038 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:219:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = MODEL_32(D);
Dense.c:219:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = MODEL_32(D);
Dense.c:219:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&layer + 96B] = _4;
Dense.c:219:9: note: 	stmt 2 MEM <float *> [(struct PULSE_Layer *)&layer + 104B] = _11;
Dense.c:219:9: note: 	stmt 3 MEM <float *> [(struct PULSE_Layer *)&layer + 112B] = _12;
Dense.c:219:9: note: 	children 0x86640b8
Dense.c:219:9: note: node (external) 0x86640b8 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:219:9: note: 	{ MODEL_32(D), _4, _11, _12 }
Dense.c:219:9: note: ------>vectorizing SLP node starting from: MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = MODEL_32(D);
Dense.c:219:9: note: vect_is_simple_use: operand MODEL_32(D) + _3, type of def: internal
Dense.c:219:9: note: vect_is_simple_use: operand aligned_alloc (64, _10), type of def: internal
Dense.c:219:9: note: vect_is_simple_use: operand aligned_alloc (64, _10), type of def: internal
Dense.c:219:9: note: transform store. ncopies = 1
Dense.c:219:9: note: create vector_type-pointer variable to type: vector(4) long unsigned int  vectorizing a pointer ref: MEM <float *> [(struct PULSE_Layer *)&layer + 88B]
Dense.c:219:9: note: created &MEM <float *> [(struct PULSE_Layer *)&layer + 88B]
Dense.c:219:9: note: add new stmt: MEM <vector(4) long unsigned int> [(struct PULSE_Layer *)&layer + 88B] = _61;
Dense.c:219:9: note: vectorizing stmts using SLP.
Dense.c:219:9: note: ***** The result for vector mode V32QI would be the same
Activations.c:17:19: optimized: loop vectorized using 32 byte vectors
Activations.c:17:19: optimized: loop vectorized using 16 byte vectors
Activations.c:17:19: optimized: loop vectorized using 32 byte vectors
Activations.c:17:19: optimized: loop vectorized using 16 byte vectors
Activations.c:14:13: note: vectorized 2 loops in function.
Activations.c:14:13: note: ***** Analysis failed with vector mode V8SF
Activations.c:14:13: note: ***** The result for vector mode V32QI would be the same
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V16QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V16QI
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V8QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V8QI
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V4QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V4QI
Activations.c:28:19: missed: couldn't vectorize loop
Activations.c:28:19: missed: not vectorized: unsupported control flow in loop.
Activations.c:28:19: optimized: loop vectorized using 32 byte vectors
Activations.c:28:19: optimized: loop vectorized using 16 byte vectors
Activations.c:25:13: note: vectorized 1 loops in function.
Activations.c:25:13: note: ***** Analysis failed with vector mode V8SF
Activations.c:25:13: note: ***** The result for vector mode V32QI would be the same
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V16QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V16QI
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V8QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V8QI
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V4QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V4QI
Activations.c:38:1: note: ***** Analysis failed with vector mode VOID
Activations.c:5:19: missed: couldn't vectorize loop
Activations.c:9:21: missed: statement clobbers memory: _36 = expf (_18);
Activations.c:5:19: missed: couldn't vectorize loop
Activations.c:9:21: missed: statement clobbers memory: _12 = expf (_11);
Activations.c:3:13: note: vectorized 0 loops in function.
Activations.c:9:21: missed: statement clobbers memory: _12 = expf (_11);
Activations.c:9:21: missed: statement clobbers memory: _36 = expf (_18);
Activations.c:11:1: note: ***** Analysis failed with vector mode V8SF
Activations.c:11:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SF
Activations.c:44:2: note: ***** Analysis failed with vector mode VOID
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:4:18: note: vectorized 0 loops in function.
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_4, inputs_15, _3);
PULSE.c:8:2: missed: statement clobbers memory: _5 (layer_17);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_12, pretmp_26, _9);
PULSE.c:13:28: note: ***** Analysis failed with vector mode V8SI
PULSE.c:13:28: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:22:2: missed: statement clobbers memory: _1 (layer_10(D));
PULSE.c:22:2: missed: statement clobbers memory: _14 (_2);
PULSE.c:22:2: missed: statement clobbers memory: _21 (_15);
PULSE.c:22:2: missed: statement clobbers memory: _28 (_22);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_29);
PULSE.c:26:3: missed: statement clobbers memory: memset (_34, 0, _32);
PULSE.c:26:3: missed: statement clobbers memory: memset (_27, 0, _25);
PULSE.c:26:3: missed: statement clobbers memory: memset (_20, 0, _18);
PULSE.c:26:3: missed: statement clobbers memory: memset (_7, 0, _5);
PULSE.c:28:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:28:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:32:6: note: vectorized 1 loops in function.
PULSE.c:34:8: missed: statement clobbers memory: _1 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_2);
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _9 = rand ();
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8SI
PULSE.c:32:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:61:16: missed: couldn't vectorize loop
PULSE.c:61:16: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:64:17: missed: couldn't vectorize loop
PULSE.c:64:17: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:73:19: missed: couldn't vectorize loop
PULSE.c:73:19: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _69 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:53:22: missed: couldn't vectorize loop
PULSE.c:53:22: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:47:6: note: vectorized 1 loops in function.
PULSE.c:48:1: missed: statement clobbers memory: saved_stack.7_39 = __builtin_stack_save ();
PULSE.c:49:46: missed: statement clobbers memory: _1 = PULSE_GetLossFunctionPtr (loss_function_40(D));
PULSE.c:60:10: missed: statement clobbers memory: random.4_46 = __builtin_alloca_with_align (_4, 32);
PULSE.c:34:8: missed: statement clobbers memory: _61 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_62);
PULSE.c:39:29: missed: statement clobbers memory: _69 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _72 = rand ();
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_98, inputs_93, _97);
PULSE.c:8:2: missed: statement clobbers memory: _99 (layer_94);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_105, _104, _103);
PULSE.c:67:11: missed: statement clobbers memory: loss_55 = PULSE_GetLoss_42 (_17, _16, _12, _11);
PULSE.c:22:2: missed: statement clobbers memory: _86 (output_34);
PULSE.c:22:2: missed: statement clobbers memory: _108 (_87);
PULSE.c:22:2: missed: statement clobbers memory: _115 (_109);
PULSE.c:22:2: missed: statement clobbers memory: _122 (_116);
PULSE.c:22:2: missed: statement clobbers memory: _129 (_123);
PULSE.c:22:2: missed: statement clobbers memory: _136 (_130);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_137);
PULSE.c:26:3: missed: statement clobbers memory: memset (_142, 0, _140);
PULSE.c:26:3: missed: statement clobbers memory: memset (_135, 0, _133);
PULSE.c:26:3: missed: statement clobbers memory: memset (_128, 0, _126);
PULSE.c:26:3: missed: statement clobbers memory: memset (_121, 0, _119);
PULSE.c:26:3: missed: statement clobbers memory: memset (_114, 0, _112);
PULSE.c:26:3: missed: statement clobbers memory: memset (_92, 0, _90);
PULSE.c:75:6: missed: statement clobbers memory: _21 (current_191, args);
PULSE.c:81:4: missed: statement clobbers memory: printf ("Epoch: %d | Item: %d | Loss: %.10f | Batch Loss: %.10f\r", i_188, j_189, _25, _24);
PULSE.c:47:6: missed: statement clobbers memory: __builtin_stack_restore (saved_stack.7_39);
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4DI
PULSE.c:47:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:91:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:91:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:119:20: missed: couldn't vectorize loop
PULSE.c:119:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:119:20: missed: couldn't vectorize loop
PULSE.c:119:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:102:20: missed: couldn't vectorize loop
PULSE.c:102:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:102:20: missed: couldn't vectorize loop
PULSE.c:102:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:93:13: note: vectorized 0 loops in function.
PULSE.c:96:39: missed: statement clobbers memory: layers_31 = malloc (_3);
PULSE.c:99:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:114:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:115:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:116:46: missed: statement clobbers memory: WEIGHTS_46 = malloc (0);
PULSE.c:108:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.104_68];
PULSE.c:109:21: missed: statement clobbers memory: _57 = PULSE_GetDenseWeightsSize (args);
PULSE.c:108:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})_179];
PULSE.c:109:21: missed: statement clobbers memory: _171 = PULSE_GetDenseWeightsSize (args);
PULSE.c:114:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:115:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:116:46: missed: statement clobbers memory: WEIGHTS_36 = malloc (_5);
PULSE.c:125:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.106_70];
PULSE.c:126:17: missed: statement clobbers memory: *_8 = PULSE_CreateDenseLayer (args, WEIGHTS_PTR_121); [return slot optimization]
PULSE.c:127:20: missed: statement clobbers memory: _49 = PULSE_GetDenseWeightsSize (args);
PULSE.c:125:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.106_113];
PULSE.c:126:17: missed: statement clobbers memory: *_98 = PULSE_CreateDenseLayer (args, WEIGHTS_PTR_135); [return slot optimization]
PULSE.c:127:20: missed: statement clobbers memory: _76 = PULSE_GetDenseWeightsSize (args);
PULSE.c:133:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:134:9: note: ***** Analysis succeeded with vector mode V8SI
PULSE.c:134:9: note: SLPing BB part
PULSE.c:134:9: note: Costing subgraph: 
PULSE.c:134:9: note: node 0x49b2b38 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:134:9: note: op template: <retval>.layers = layers_31;
PULSE.c:134:9: note: 	stmt 0 <retval>.layers = layers_31;
PULSE.c:134:9: note: 	stmt 1 <retval>.paramethers = WEIGHTS_80;
PULSE.c:134:9: note: 	children 0x49b2c38
PULSE.c:134:9: note: node (external) 0x49b2c38 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:134:9: note: 	{ layers_31, WEIGHTS_80 }
PULSE.c:134:9: note: Cost model analysis: 
PULSE.c:134:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:134:9: note: Costing subgraph: 
PULSE.c:134:9: note: node 0x49b2cb8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:134:9: note: op template: <retval>.io = 0B;
PULSE.c:134:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:134:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:134:9: note: 	children 0x49b2d38
PULSE.c:134:9: note: node (constant) 0x49b2d38 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:134:9: note: 	{ 0B, 0B }
PULSE.c:134:9: note: Cost model analysis: 
PULSE.c:134:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:134:9: note: Basic block will be vectorized using SLP
PULSE.c:134:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:134:9: note: Vectorizing SLP tree:
PULSE.c:134:9: note: node 0x49b2b38 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:134:9: note: op template: <retval>.layers = layers_31;
PULSE.c:134:9: note: 	stmt 0 <retval>.layers = layers_31;
PULSE.c:134:9: note: 	stmt 1 <retval>.paramethers = WEIGHTS_80;
PULSE.c:134:9: note: 	children 0x49b2c38
PULSE.c:134:9: note: node (external) 0x49b2c38 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:134:9: note: 	{ layers_31, WEIGHTS_80 }
PULSE.c:134:9: note: ------>vectorizing SLP node starting from: <retval>.layers = layers_31;
PULSE.c:134:9: note: vect_is_simple_use: operand WEIGHTS_80 = PHI <WEIGHTS_36(36), WEIGHTS_46(12)>, type of def: internal
PULSE.c:134:9: note: conflicting alias set types.
PULSE.c:134:9: note: transform store. ncopies = 1
PULSE.c:134:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.layers
PULSE.c:134:9: note: created &<retval>
PULSE.c:134:9: note: add new stmt: MEM <vector(2) long unsigned int> [(void *)&<retval>] = _11;
PULSE.c:134:9: note: vectorizing stmts using SLP.
PULSE.c:134:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:134:9: note: Vectorizing SLP tree:
PULSE.c:134:9: note: node 0x49b2cb8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:134:9: note: op template: <retval>.io = 0B;
PULSE.c:134:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:134:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:134:9: note: 	children 0x49b2d38
PULSE.c:134:9: note: node (constant) 0x49b2d38 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:134:9: note: 	{ 0B, 0B }
PULSE.c:134:9: note: ------>vectorizing SLP node starting from: <retval>.io = 0B;
PULSE.c:134:9: note: vect_is_simple_use: operand 0B, type of def: constant
PULSE.c:134:9: note: transform store. ncopies = 1
PULSE.c:134:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.io
PULSE.c:134:9: note: created &<retval>.io
PULSE.c:134:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval> + 16B] = { 0, 0 };
PULSE.c:134:9: note: vectorizing stmts using SLP.
PULSE.c:134:9: note: ***** The result for vector mode V32QI would be the same
PULSE.c:146:1: note: ***** Analysis failed with vector mode VOID
Loss.c:15:19: optimized: loop vectorized using 32 byte vectors
Loss.c:15:19: optimized:  loop versioned for vectorization because of possible aliasing
Loss.c:15:19: optimized: loop vectorized using 16 byte vectors
Loss.c:12:23: note: vectorized 1 loops in function.
Loss.c:12:23: note: ***** Analysis failed with vector mode V8SF
Loss.c:12:23: note: ***** The result for vector mode V32QI would be the same
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V16QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V16QI
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V8QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V8QI
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V4QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V4QI
Loss.c:7:19: missed: couldn't vectorize loop
Loss.c:8:38: missed: not vectorized: unsupported use in stmt.
Loss.c:4:23: note: vectorized 0 loops in function.
Loss.c:9:13: note: ***** Analysis failed with vector mode V8SF
Loss.c:9:13: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SF
Loss.c:23:2: note: ***** Analysis failed with vector mode VOID
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _28 (pretmp_91, _51, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:19: missed: not vectorized: no vectype for stmt: _22 = *_21;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:170:10: optimized: loop vectorized using 32 byte vectors
Dense.c:170:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:170:10: optimized: loop vectorized using 16 byte vectors
Dense.c:161:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
Dense.c:154:10: optimized: loop vectorized using 32 byte vectors
Dense.c:154:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:154:10: optimized: loop vectorized using 16 byte vectors
Dense.c:145:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
Dense.c:136:13: note: vectorized 2 loops in function.
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_28, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
Dense.c:136:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:136:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:105:24: missed: couldn't vectorize loop
Dense.c:105:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:108:17: missed: couldn't vectorize loop
Dense.c:108:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:122:24: missed: couldn't vectorize loop
Dense.c:122:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:125:17: missed: couldn't vectorize loop
Dense.c:125:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:90:15: missed: couldn't vectorize loop
Dense.c:90:15: missed: not vectorized: number of iterations cannot be computed.
Dense.c:83:13: note: vectorized 0 loops in function.
Dense.c:86:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:110:17: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:110:17: note: SLPing BB part
Dense.c:127:17: note: Costing subgraph: 
Dense.c:127:17: note: node 0x76fe588 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:127:17: note: op template: _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 0 _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 1 _198 = (sizetype) wi_137;
Dense.c:127:17: note: 	children 0x76fe688
Dense.c:127:17: note: node (external) 0x76fe688 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:127:17: note: 	{ j_135, wi_137 }
Dense.c:127:17: note: Cost model analysis: 
Dense.c:127:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:127:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:127:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: Costing subgraph: 
Dense.c:110:17: note: node 0x76fe788 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:110:17: note: op template: _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 0 _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 1 _217 = (sizetype) wi_136;
Dense.c:110:17: note: 	children 0x76fe888
Dense.c:110:17: note: node (external) 0x76fe888 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:110:17: note: 	{ j_134, wi_136 }
Dense.c:110:17: note: Cost model analysis: 
Dense.c:110:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:110:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:110:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: ***** The result for vector mode V32QI would be the same
Dense.c:110:17: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:110:17: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:110:17: note: SLPing BB part
Dense.c:127:17: note: Costing subgraph: 
Dense.c:127:17: note: node 0x76fe788 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:127:17: note: op template: _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 0 _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 1 _198 = (sizetype) wi_137;
Dense.c:127:17: note: 	children 0x76fe588
Dense.c:127:17: note: node (external) 0x76fe588 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:127:17: note: 	{ j_135, wi_137 }
Dense.c:127:17: note: Cost model analysis: 
Dense.c:127:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:127:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:127:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: Costing subgraph: 
Dense.c:110:17: note: node 0x76fe708 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:110:17: note: op template: _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 0 _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 1 _217 = (sizetype) wi_136;
Dense.c:110:17: note: 	children 0x76fe808
Dense.c:110:17: note: node (external) 0x76fe808 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:110:17: note: 	{ j_134, wi_136 }
Dense.c:110:17: note: Cost model analysis: 
Dense.c:110:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:110:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:110:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:105:51: note: ***** Analysis failed with vector mode V8QI
Dense.c:105:51: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:105:51: note: ***** Analysis failed with vector mode V4QI
Dense.c:62:15: missed: couldn't vectorize loop
Dense.c:62:15: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:75:11: optimized: loop vectorized using 32 byte vectors
Dense.c:75:11: optimized: loop vectorized using 16 byte vectors
Dense.c:66:11: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:881:10: missed: not vectorized: no vectype for stmt: _53 = MEM[(__m256 * {ref-all})w_ptr_82];
 scalar_type: __m256
Dense.c:54:13: note: vectorized 1 loops in function.
Dense.c:57:2: missed: statement clobbers memory: memcpy (_4, dense$baiases_63, _3);
Dense.c:79:2: missed: statement clobbers memory: _27 (pretmp_163, _68, 0);
Dense.c:54:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:54:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:181:2: missed: statement clobbers memory: free (dense$weights_7);
Dense.c:182:2: missed: statement clobbers memory: free (dense$baiases_8);
Dense.c:183:2: missed: statement clobbers memory: free (dense$gradients_10);
Dense.c:184:2: missed: statement clobbers memory: free (dense$deltas_9);
Dense.c:185:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:185:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:188:107: note: ***** Analysis failed with vector mode V8SI
Dense.c:188:107: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
Dense.c:200:19: missed: couldn't vectorize loop
Dense.c:200:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:190:13: note: vectorized 0 loops in function.
Dense.c:196:37: missed: statement clobbers memory: _9 = aligned_alloc (64, _8);
Dense.c:197:34: missed: statement clobbers memory: _11 = aligned_alloc (64, _10);
Dense.c:198:35: missed: statement clobbers memory: _12 = aligned_alloc (64, _10);
Dense.c:201:38: missed: statement clobbers memory: _13 = rand ();
Dense.c:201:72: missed: statement clobbers memory: _57 = sqrt (_20);
Dense.c:213:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:214:4: missed: statement clobbers memory: exit (1);
Dense.c:210:4: missed: statement clobbers memory: layer = PULSE_CreateLayer (args$n_inputs_36, args$n_outputs_47, 0, args$8_48, _SIMD_FeedDense, _SIMD_BackDense, _SIMD_FixDense, _DestroyDense, 1); [return slot optimization]
Dense.c:207:12: missed: statement clobbers memory: layer = PULSE_CreateLayer (args$n_inputs_36, args$n_outputs_47, 0, args$8_48, _FeedDense, _BackDense, _FixDense, _DestroyDense, 0); [return slot optimization]
Dense.c:219:9: missed: not vectorized: more than one data ref in stmt: <retval> = layer;
Dense.c:219:9: note: ***** Analysis succeeded with vector mode V8SI
Dense.c:219:9: note: SLPing BB part
Dense.c:219:9: note: Costing subgraph: 
Dense.c:219:9: note: node 0x753d248 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:219:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = MODEL_32(D);
Dense.c:219:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = MODEL_32(D);
Dense.c:219:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&layer + 96B] = _4;
Dense.c:219:9: note: 	stmt 2 MEM <float *> [(struct PULSE_Layer *)&layer + 104B] = _11;
Dense.c:219:9: note: 	stmt 3 MEM <float *> [(struct PULSE_Layer *)&layer + 112B] = _12;
Dense.c:219:9: note: 	children 0x753d2c8
Dense.c:219:9: note: node (external) 0x753d2c8 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:219:9: note: 	{ MODEL_32(D), _4, _11, _12 }
Dense.c:219:9: note: Cost model analysis: 
Dense.c:219:9: note: Cost model analysis for part in loop 0:
  Vector cost: 64
  Scalar cost: 64
Dense.c:219:9: note: Basic block will be vectorized using SLP
Dense.c:219:9: optimized: basic block part vectorized using 32 byte vectors
Dense.c:219:9: note: Vectorizing SLP tree:
Dense.c:219:9: note: node 0x753d248 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:219:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = MODEL_32(D);
Dense.c:219:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = MODEL_32(D);
Dense.c:219:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&layer + 96B] = _4;
Dense.c:219:9: note: 	stmt 2 MEM <float *> [(struct PULSE_Layer *)&layer + 104B] = _11;
Dense.c:219:9: note: 	stmt 3 MEM <float *> [(struct PULSE_Layer *)&layer + 112B] = _12;
Dense.c:219:9: note: 	children 0x753d2c8
Dense.c:219:9: note: node (external) 0x753d2c8 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:219:9: note: 	{ MODEL_32(D), _4, _11, _12 }
Dense.c:219:9: note: ------>vectorizing SLP node starting from: MEM <float *> [(struct PULSE_Layer *)&layer + 88B] = MODEL_32(D);
Dense.c:219:9: note: vect_is_simple_use: operand MODEL_32(D) + _3, type of def: internal
Dense.c:219:9: note: vect_is_simple_use: operand aligned_alloc (64, _10), type of def: internal
Dense.c:219:9: note: vect_is_simple_use: operand aligned_alloc (64, _10), type of def: internal
Dense.c:219:9: note: transform store. ncopies = 1
Dense.c:219:9: note: create vector_type-pointer variable to type: vector(4) long unsigned int  vectorizing a pointer ref: MEM <float *> [(struct PULSE_Layer *)&layer + 88B]
Dense.c:219:9: note: created &MEM <float *> [(struct PULSE_Layer *)&layer + 88B]
Dense.c:219:9: note: add new stmt: MEM <vector(4) long unsigned int> [(struct PULSE_Layer *)&layer + 88B] = _61;
Dense.c:219:9: note: vectorizing stmts using SLP.
Dense.c:219:9: note: ***** The result for vector mode V32QI would be the same
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:4:18: note: vectorized 0 loops in function.
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_4, inputs_15, _3);
PULSE.c:8:2: missed: statement clobbers memory: _5 (layer_17);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_12, pretmp_26, _9);
PULSE.c:13:28: note: ***** Analysis failed with vector mode V8SI
PULSE.c:13:28: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:22:2: missed: statement clobbers memory: _1 (layer_10(D));
PULSE.c:22:2: missed: statement clobbers memory: _14 (_2);
PULSE.c:22:2: missed: statement clobbers memory: _21 (_15);
PULSE.c:22:2: missed: statement clobbers memory: _28 (_22);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_29);
PULSE.c:26:3: missed: statement clobbers memory: memset (_34, 0, _32);
PULSE.c:26:3: missed: statement clobbers memory: memset (_27, 0, _25);
PULSE.c:26:3: missed: statement clobbers memory: memset (_20, 0, _18);
PULSE.c:26:3: missed: statement clobbers memory: memset (_7, 0, _5);
PULSE.c:28:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:28:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:32:6: note: vectorized 1 loops in function.
PULSE.c:34:8: missed: statement clobbers memory: _1 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_2);
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _9 = rand ();
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8SI
PULSE.c:32:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:61:16: missed: couldn't vectorize loop
PULSE.c:61:16: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:64:17: missed: couldn't vectorize loop
PULSE.c:64:17: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:73:19: missed: couldn't vectorize loop
PULSE.c:73:19: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _69 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:53:22: missed: couldn't vectorize loop
PULSE.c:53:22: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:47:6: note: vectorized 1 loops in function.
PULSE.c:48:1: missed: statement clobbers memory: saved_stack.7_39 = __builtin_stack_save ();
PULSE.c:49:46: missed: statement clobbers memory: _1 = PULSE_GetLossFunctionPtr (loss_function_40(D));
PULSE.c:60:10: missed: statement clobbers memory: random.4_46 = __builtin_alloca_with_align (_4, 32);
PULSE.c:34:8: missed: statement clobbers memory: _61 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_62);
PULSE.c:39:29: missed: statement clobbers memory: _69 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _72 = rand ();
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_98, inputs_93, _97);
PULSE.c:8:2: missed: statement clobbers memory: _99 (layer_94);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_105, _104, _103);
PULSE.c:67:11: missed: statement clobbers memory: loss_55 = PULSE_GetLoss_42 (_17, _16, _12, _11);
PULSE.c:22:2: missed: statement clobbers memory: _86 (output_34);
PULSE.c:22:2: missed: statement clobbers memory: _108 (_87);
PULSE.c:22:2: missed: statement clobbers memory: _115 (_109);
PULSE.c:22:2: missed: statement clobbers memory: _122 (_116);
PULSE.c:22:2: missed: statement clobbers memory: _129 (_123);
PULSE.c:22:2: missed: statement clobbers memory: _136 (_130);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_137);
PULSE.c:26:3: missed: statement clobbers memory: memset (_142, 0, _140);
PULSE.c:26:3: missed: statement clobbers memory: memset (_135, 0, _133);
PULSE.c:26:3: missed: statement clobbers memory: memset (_128, 0, _126);
PULSE.c:26:3: missed: statement clobbers memory: memset (_121, 0, _119);
PULSE.c:26:3: missed: statement clobbers memory: memset (_114, 0, _112);
PULSE.c:26:3: missed: statement clobbers memory: memset (_92, 0, _90);
PULSE.c:75:6: missed: statement clobbers memory: _21 (current_191, args);
PULSE.c:81:4: missed: statement clobbers memory: printf ("Epoch: %d | Item: %d | Loss: %.10f | Batch Loss: %.10f\r", i_188, j_189, _25, _24);
PULSE.c:47:6: missed: statement clobbers memory: __builtin_stack_restore (saved_stack.7_39);
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4DI
PULSE.c:47:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:91:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:91:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:119:20: missed: couldn't vectorize loop
PULSE.c:119:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:119:20: missed: couldn't vectorize loop
PULSE.c:119:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:102:20: missed: couldn't vectorize loop
PULSE.c:102:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:102:20: missed: couldn't vectorize loop
PULSE.c:102:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:93:13: note: vectorized 0 loops in function.
PULSE.c:96:39: missed: statement clobbers memory: layers_31 = malloc (_3);
PULSE.c:99:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:114:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:115:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:116:46: missed: statement clobbers memory: WEIGHTS_105 = aligned_alloc (64, 0);
PULSE.c:108:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.104_68];
PULSE.c:109:21: missed: statement clobbers memory: _57 = PULSE_GetDenseWeightsSize (args);
PULSE.c:108:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})_179];
PULSE.c:109:21: missed: statement clobbers memory: _171 = PULSE_GetDenseWeightsSize (args);
PULSE.c:114:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:115:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:116:46: missed: statement clobbers memory: WEIGHTS_36 = aligned_alloc (64, _5);
PULSE.c:125:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.106_70];
PULSE.c:126:17: missed: statement clobbers memory: *_8 = PULSE_CreateDenseLayer (args, WEIGHTS_PTR_121); [return slot optimization]
PULSE.c:127:20: missed: statement clobbers memory: _49 = PULSE_GetDenseWeightsSize (args);
PULSE.c:125:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.106_124];
PULSE.c:126:17: missed: statement clobbers memory: *_98 = PULSE_CreateDenseLayer (args, WEIGHTS_PTR_135); [return slot optimization]
PULSE.c:127:20: missed: statement clobbers memory: _80 = PULSE_GetDenseWeightsSize (args);
PULSE.c:133:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:134:9: note: ***** Analysis succeeded with vector mode V8SI
PULSE.c:134:9: note: SLPing BB part
PULSE.c:134:9: note: Costing subgraph: 
PULSE.c:134:9: note: node 0x3548258 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:134:9: note: op template: <retval>.layers = layers_31;
PULSE.c:134:9: note: 	stmt 0 <retval>.layers = layers_31;
PULSE.c:134:9: note: 	stmt 1 <retval>.weights = WEIGHTS_131;
PULSE.c:134:9: note: 	children 0x3548358
PULSE.c:134:9: note: node (external) 0x3548358 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:134:9: note: 	{ layers_31, WEIGHTS_131 }
PULSE.c:134:9: note: Cost model analysis: 
PULSE.c:134:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:134:9: note: Costing subgraph: 
PULSE.c:134:9: note: node 0x35483d8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:134:9: note: op template: <retval>.io = 0B;
PULSE.c:134:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:134:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:134:9: note: 	children 0x3548458
PULSE.c:134:9: note: node (constant) 0x3548458 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:134:9: note: 	{ 0B, 0B }
PULSE.c:134:9: note: Cost model analysis: 
PULSE.c:134:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:134:9: note: Basic block will be vectorized using SLP
PULSE.c:134:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:134:9: note: Vectorizing SLP tree:
PULSE.c:134:9: note: node 0x3548258 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:134:9: note: op template: <retval>.layers = layers_31;
PULSE.c:134:9: note: 	stmt 0 <retval>.layers = layers_31;
PULSE.c:134:9: note: 	stmt 1 <retval>.weights = WEIGHTS_131;
PULSE.c:134:9: note: 	children 0x3548358
PULSE.c:134:9: note: node (external) 0x3548358 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:134:9: note: 	{ layers_31, WEIGHTS_131 }
PULSE.c:134:9: note: ------>vectorizing SLP node starting from: <retval>.layers = layers_31;
PULSE.c:134:9: note: vect_is_simple_use: operand WEIGHTS_131 = PHI <WEIGHTS_36(36), WEIGHTS_105(25)>, type of def: internal
PULSE.c:134:9: note: conflicting alias set types.
PULSE.c:134:9: note: transform store. ncopies = 1
PULSE.c:134:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.layers
PULSE.c:134:9: note: created &<retval>
PULSE.c:134:9: note: add new stmt: MEM <vector(2) long unsigned int> [(void *)&<retval>] = _11;
PULSE.c:134:9: note: vectorizing stmts using SLP.
PULSE.c:134:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:134:9: note: Vectorizing SLP tree:
PULSE.c:134:9: note: node 0x35483d8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:134:9: note: op template: <retval>.io = 0B;
PULSE.c:134:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:134:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:134:9: note: 	children 0x3548458
PULSE.c:134:9: note: node (constant) 0x3548458 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:134:9: note: 	{ 0B, 0B }
PULSE.c:134:9: note: ------>vectorizing SLP node starting from: <retval>.io = 0B;
PULSE.c:134:9: note: vect_is_simple_use: operand 0B, type of def: constant
PULSE.c:134:9: note: transform store. ncopies = 1
PULSE.c:134:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.io
PULSE.c:134:9: note: created &<retval>.io
PULSE.c:134:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval> + 16B] = { 0, 0 };
PULSE.c:134:9: note: vectorizing stmts using SLP.
PULSE.c:134:9: note: ***** The result for vector mode V32QI would be the same
PULSE.c:146:1: note: ***** Analysis failed with vector mode VOID
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _28 (pretmp_91, _51, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:19: missed: not vectorized: no vectype for stmt: _22 = *_21;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:170:10: optimized: loop vectorized using 32 byte vectors
Dense.c:170:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:170:10: optimized: loop vectorized using 16 byte vectors
Dense.c:161:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
Dense.c:154:10: optimized: loop vectorized using 32 byte vectors
Dense.c:154:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:154:10: optimized: loop vectorized using 16 byte vectors
Dense.c:145:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
Dense.c:136:13: note: vectorized 2 loops in function.
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_28, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
Dense.c:136:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:136:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:105:24: missed: couldn't vectorize loop
Dense.c:105:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:108:17: missed: couldn't vectorize loop
Dense.c:108:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:122:24: missed: couldn't vectorize loop
Dense.c:122:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:125:17: missed: couldn't vectorize loop
Dense.c:125:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:90:15: missed: couldn't vectorize loop
Dense.c:90:15: missed: not vectorized: number of iterations cannot be computed.
Dense.c:83:13: note: vectorized 0 loops in function.
Dense.c:86:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:110:17: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:110:17: note: SLPing BB part
Dense.c:127:17: note: Costing subgraph: 
Dense.c:127:17: note: node 0x7678f08 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:127:17: note: op template: _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 0 _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 1 _198 = (sizetype) wi_137;
Dense.c:127:17: note: 	children 0x7679008
Dense.c:127:17: note: node (external) 0x7679008 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:127:17: note: 	{ j_135, wi_137 }
Dense.c:127:17: note: Cost model analysis: 
Dense.c:127:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:127:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:127:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: Costing subgraph: 
Dense.c:110:17: note: node 0x7679108 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:110:17: note: op template: _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 0 _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 1 _217 = (sizetype) wi_136;
Dense.c:110:17: note: 	children 0x7679208
Dense.c:110:17: note: node (external) 0x7679208 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:110:17: note: 	{ j_134, wi_136 }
Dense.c:110:17: note: Cost model analysis: 
Dense.c:110:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:110:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:110:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: ***** The result for vector mode V32QI would be the same
Dense.c:110:17: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:110:17: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:110:17: note: SLPing BB part
Dense.c:127:17: note: Costing subgraph: 
Dense.c:127:17: note: node 0x7679108 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:127:17: note: op template: _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 0 _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 1 _198 = (sizetype) wi_137;
Dense.c:127:17: note: 	children 0x7678f08
Dense.c:127:17: note: node (external) 0x7678f08 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:127:17: note: 	{ j_135, wi_137 }
Dense.c:127:17: note: Cost model analysis: 
Dense.c:127:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:127:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:127:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: Costing subgraph: 
Dense.c:110:17: note: node 0x7679088 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:110:17: note: op template: _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 0 _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 1 _217 = (sizetype) wi_136;
Dense.c:110:17: note: 	children 0x7679188
Dense.c:110:17: note: node (external) 0x7679188 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:110:17: note: 	{ j_134, wi_136 }
Dense.c:110:17: note: Cost model analysis: 
Dense.c:110:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:110:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:110:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:105:51: note: ***** Analysis failed with vector mode V8QI
Dense.c:105:51: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:105:51: note: ***** Analysis failed with vector mode V4QI
Dense.c:62:15: missed: couldn't vectorize loop
Dense.c:62:15: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:75:11: optimized: loop vectorized using 32 byte vectors
Dense.c:75:11: optimized: loop vectorized using 16 byte vectors
Dense.c:66:11: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:881:10: missed: not vectorized: no vectype for stmt: _53 = MEM[(__m256 * {ref-all})w_ptr_82];
 scalar_type: __m256
Dense.c:54:13: note: vectorized 1 loops in function.
Dense.c:57:2: missed: statement clobbers memory: memcpy (_4, dense$baiases_63, _3);
Dense.c:79:2: missed: statement clobbers memory: _27 (pretmp_163, _68, 0);
Dense.c:54:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:54:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:188:107: note: ***** Analysis failed with vector mode V8SI
Dense.c:188:107: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
Dense.c:189:85: note: ***** Analysis failed with vector mode V8SI
Dense.c:189:85: note: ***** The result for vector mode V32QI would be the same
Dense.c:189:85: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:189:85: note: ***** Analysis failed with vector mode V16QI
Dense.c:189:85: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:189:85: note: ***** Analysis failed with vector mode V8QI
Dense.c:189:85: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:189:85: note: ***** Analysis failed with vector mode V4QI
Dense.c:201:19: missed: couldn't vectorize loop
Dense.c:201:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:191:13: note: vectorized 0 loops in function.
Dense.c:197:37: missed: statement clobbers memory: _9 = aligned_alloc (64, _8);
Dense.c:198:34: missed: statement clobbers memory: _11 = aligned_alloc (64, _10);
Dense.c:199:35: missed: statement clobbers memory: _12 = aligned_alloc (64, _10);
Dense.c:202:38: missed: statement clobbers memory: _13 = rand ();
Dense.c:202:72: missed: statement clobbers memory: _77 = sqrt (_20);
Dense.c:215:19: missed: statement clobbers memory: _31 = PULSE_GetActivationFunctionPtr (_30);
Dense.c:231:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:232:4: missed: statement clobbers memory: exit (1);
Dense.c:236:9: note: ***** Analysis succeeded with vector mode V8SI
Dense.c:236:9: note: SLPing BB part
Dense.c:236:9: note: Costing subgraph: 
Dense.c:236:9: note: node 0x7581458 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:236:9: note: op template: <retval>.n_inputs = args$n_inputs_51;
Dense.c:236:9: note: 	stmt 0 <retval>.n_inputs = args$n_inputs_51;
Dense.c:236:9: note: 	stmt 1 <retval>.n_outputs = args$n_outputs_52;
Dense.c:236:9: note: 	children 0x75814d8
Dense.c:236:9: note: node 0x75814d8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:236:9: note: op template: args$n_inputs_51 = args.n_inputs;
Dense.c:236:9: note: 	stmt 0 args$n_inputs_51 = args.n_inputs;
Dense.c:236:9: note: 	stmt 1 args$n_outputs_52 = args.n_outputs;
Dense.c:236:9: note: Cost model analysis: 
Dense.c:236:9: note: Cost model analysis for part in loop 0:
  Vector cost: 40
  Scalar cost: 56
Dense.c:236:9: note: Costing subgraph: 
Dense.c:236:9: note: node 0x75815d8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:236:9: note: op template: <retval>.inputs = IO_41(D);
Dense.c:236:9: note: 	stmt 0 <retval>.inputs = IO_41(D);
Dense.c:236:9: note: 	stmt 1 <retval>.outputs = _29;
Dense.c:236:9: note: 	children 0x7581658
Dense.c:236:9: note: node (external) 0x7581658 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:236:9: note: 	{ IO_41(D), _29 }
Dense.c:236:9: note: Cost model analysis: 
Dense.c:236:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:236:9: note: Costing subgraph: 
Dense.c:236:9: note: node 0x7581758 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:236:9: note: op template: <retval>.feed = layer$feed_40;
Dense.c:236:9: note: 	stmt 0 <retval>.feed = layer$feed_40;
Dense.c:236:9: note: 	stmt 1 <retval>.back = layer$back_49;
Dense.c:236:9: note: 	children 0x7581958
Dense.c:236:9: note: node (external) 0x7581958 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:236:9: note: 	{ layer$feed_40, layer$back_49 }
Dense.c:236:9: note: Cost model analysis: 
Dense.c:236:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:236:9: note: Costing subgraph: 
Dense.c:236:9: note: node 0x7581858 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:236:9: note: op template: <retval>.parent = 0B;
Dense.c:236:9: note: 	stmt 0 <retval>.parent = 0B;
Dense.c:236:9: note: 	stmt 1 <retval>.child = 0B;
Dense.c:236:9: note: 	children 0x75819d8
Dense.c:236:9: note: node (constant) 0x75819d8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:236:9: note: 	{ 0B, 0B }
Dense.c:236:9: note: Cost model analysis: 
Dense.c:236:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:236:9: note: Costing subgraph: 
Dense.c:236:9: note: node 0x7581ad8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:236:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_36(D);
Dense.c:236:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_36(D);
Dense.c:236:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&<retval> + 96B] = _4;
Dense.c:236:9: note: 	stmt 2 MEM <float *> [(struct PULSE_Layer *)&<retval> + 104B] = _11;
Dense.c:236:9: note: 	stmt 3 MEM <float *> [(struct PULSE_Layer *)&<retval> + 112B] = _12;
Dense.c:236:9: note: 	children 0x7581b58
Dense.c:236:9: note: node (external) 0x7581b58 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:236:9: note: 	{ MODEL_36(D), _4, _11, _12 }
Dense.c:236:9: note: Cost model analysis: 
Dense.c:236:9: note: Cost model analysis for part in loop 0:
  Vector cost: 64
  Scalar cost: 64
Dense.c:236:9: note: Basic block will be vectorized using SLP
Dense.c:236:9: optimized: basic block part vectorized using 8 byte vectors
Dense.c:236:9: note: Vectorizing SLP tree:
Dense.c:236:9: note: node 0x7581458 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:236:9: note: op template: <retval>.n_inputs = args$n_inputs_51;
Dense.c:236:9: note: 	stmt 0 <retval>.n_inputs = args$n_inputs_51;
Dense.c:236:9: note: 	stmt 1 <retval>.n_outputs = args$n_outputs_52;
Dense.c:236:9: note: 	children 0x75814d8
Dense.c:236:9: note: node 0x75814d8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:236:9: note: op template: args$n_inputs_51 = args.n_inputs;
Dense.c:236:9: note: 	stmt 0 args$n_inputs_51 = args.n_inputs;
Dense.c:236:9: note: 	stmt 1 args$n_outputs_52 = args.n_outputs;
Dense.c:236:9: note: ------>vectorizing SLP node starting from: args$n_inputs_51 = args.n_inputs;
Dense.c:236:9: note: transform load. ncopies = 1
Dense.c:236:9: note: conflicting alias set types.
Dense.c:236:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: args.n_inputs
Dense.c:236:9: note: created &args
Dense.c:236:9: note: add new stmt: vect_args_n_inputs_51.504_86 = MEM <vector(2) unsigned int> [(void *)&args];
Dense.c:236:9: note: extracting lane for live stmt args$n_inputs_51 = args.n_inputs;
Dense.c:236:9: note: extracting lane for live stmt args$n_outputs_52 = args.n_outputs;
Dense.c:236:9: note: ------>vectorizing SLP node starting from: <retval>.n_inputs = args$n_inputs_51;
Dense.c:236:9: note: vect_is_simple_use: operand args.n_inputs, type of def: internal
Dense.c:236:9: note: vect_is_simple_use: operand args.n_outputs, type of def: internal
Dense.c:236:9: note: transform store. ncopies = 1
Dense.c:236:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: <retval>.n_inputs
Dense.c:236:9: note: created &<retval>.n_inputs
Dense.c:236:9: note: add new stmt: MEM <vector(2) unsigned int> [(unsigned int *)&<retval> + 128B] = vect_args_n_inputs_51.504_86;
Dense.c:236:9: note: vectorizing stmts using SLP.
Dense.c:236:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:236:9: note: Vectorizing SLP tree:
Dense.c:236:9: note: node 0x75815d8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:236:9: note: op template: <retval>.inputs = IO_41(D);
Dense.c:236:9: note: 	stmt 0 <retval>.inputs = IO_41(D);
Dense.c:236:9: note: 	stmt 1 <retval>.outputs = _29;
Dense.c:236:9: note: 	children 0x7581658
Dense.c:236:9: note: node (external) 0x7581658 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:236:9: note: 	{ IO_41(D), _29 }
Dense.c:236:9: note: ------>vectorizing SLP node starting from: <retval>.inputs = IO_41(D);
Dense.c:236:9: note: vect_is_simple_use: operand IO_41(D) + _28, type of def: internal
Dense.c:236:9: note: transform store. ncopies = 1
Dense.c:236:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.inputs
Dense.c:236:9: note: created &<retval>
Dense.c:236:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval>] = _92;
Dense.c:236:9: note: vectorizing stmts using SLP.
Dense.c:236:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:236:9: note: Vectorizing SLP tree:
Dense.c:236:9: note: node 0x7581758 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:236:9: note: op template: <retval>.feed = layer$feed_40;
Dense.c:236:9: note: 	stmt 0 <retval>.feed = layer$feed_40;
Dense.c:236:9: note: 	stmt 1 <retval>.back = layer$back_49;
Dense.c:236:9: note: 	children 0x7581958
Dense.c:236:9: note: node (external) 0x7581958 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:236:9: note: 	{ layer$feed_40, layer$back_49 }
Dense.c:236:9: note: ------>vectorizing SLP node starting from: <retval>.feed = layer$feed_40;
Dense.c:236:9: note: vect_is_simple_use: operand layer$back_49 = PHI <layer$back_55(D)(11), _BackDense(9), _SIMD_BackDense(15)>, type of def: internal
Dense.c:236:9: note: transform store. ncopies = 1
Dense.c:236:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.feed
Dense.c:236:9: note: created &<retval>.feed
Dense.c:236:9: note: add new stmt: MEM <vector(2) long unsigned int> [(void (*<T921>) (struct PULSE_Layer *) *)&<retval> + 32B] = _96;
Dense.c:236:9: note: vectorizing stmts using SLP.
Dense.c:236:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:236:9: note: Vectorizing SLP tree:
Dense.c:236:9: note: node 0x7581858 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:236:9: note: op template: <retval>.parent = 0B;
Dense.c:236:9: note: 	stmt 0 <retval>.parent = 0B;
Dense.c:236:9: note: 	stmt 1 <retval>.child = 0B;
Dense.c:236:9: note: 	children 0x75819d8
Dense.c:236:9: note: node (constant) 0x75819d8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:236:9: note: 	{ 0B, 0B }
Dense.c:236:9: note: ------>vectorizing SLP node starting from: <retval>.parent = 0B;
Dense.c:236:9: note: vect_is_simple_use: operand 0B, type of def: constant
Dense.c:236:9: note: transform store. ncopies = 1
Dense.c:236:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.parent
Dense.c:236:9: note: created &<retval>.parent
Dense.c:236:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer * *)&<retval> + 72B] = { 0, 0 };
Dense.c:236:9: note: vectorizing stmts using SLP.
Dense.c:236:9: optimized: basic block part vectorized using 32 byte vectors
Dense.c:236:9: note: Vectorizing SLP tree:
Dense.c:236:9: note: node 0x7581ad8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:236:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_36(D);
Dense.c:236:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_36(D);
Dense.c:236:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&<retval> + 96B] = _4;
Dense.c:236:9: note: 	stmt 2 MEM <float *> [(struct PULSE_Layer *)&<retval> + 104B] = _11;
Dense.c:236:9: note: 	stmt 3 MEM <float *> [(struct PULSE_Layer *)&<retval> + 112B] = _12;
Dense.c:236:9: note: 	children 0x7581b58
Dense.c:236:9: note: node (external) 0x7581b58 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:236:9: note: 	{ MODEL_36(D), _4, _11, _12 }
Dense.c:236:9: note: ------>vectorizing SLP node starting from: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_36(D);
Dense.c:236:9: note: vect_is_simple_use: operand MODEL_36(D) + _3, type of def: internal
Dense.c:236:9: note: vect_is_simple_use: operand aligned_alloc (64, _10), type of def: internal
Dense.c:236:9: note: vect_is_simple_use: operand aligned_alloc (64, _10), type of def: internal
Dense.c:236:9: note: transform store. ncopies = 1
Dense.c:236:9: note: create vector_type-pointer variable to type: vector(4) long unsigned int  vectorizing a pointer ref: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B]
Dense.c:236:9: note: created &MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B]
Dense.c:236:9: note: add new stmt: MEM <vector(4) long unsigned int> [(struct PULSE_Layer *)&<retval> + 88B] = _103;
Dense.c:236:9: note: vectorizing stmts using SLP.
Dense.c:236:9: note: ***** The result for vector mode V32QI would be the same
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:4:18: note: vectorized 0 loops in function.
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_4, inputs_15, _3);
PULSE.c:8:2: missed: statement clobbers memory: _5 (layer_17);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_12, pretmp_26, _9);
PULSE.c:13:28: note: ***** Analysis failed with vector mode V8SI
PULSE.c:13:28: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:22:2: missed: statement clobbers memory: _1 (layer_10(D));
PULSE.c:22:2: missed: statement clobbers memory: _14 (_2);
PULSE.c:22:2: missed: statement clobbers memory: _21 (_15);
PULSE.c:22:2: missed: statement clobbers memory: _28 (_22);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_29);
PULSE.c:26:3: missed: statement clobbers memory: memset (_34, 0, _32);
PULSE.c:26:3: missed: statement clobbers memory: memset (_27, 0, _25);
PULSE.c:26:3: missed: statement clobbers memory: memset (_20, 0, _18);
PULSE.c:26:3: missed: statement clobbers memory: memset (_7, 0, _5);
PULSE.c:28:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:28:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:32:6: note: vectorized 1 loops in function.
PULSE.c:34:8: missed: statement clobbers memory: _1 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_2);
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _9 = rand ();
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8SI
PULSE.c:32:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:61:16: missed: couldn't vectorize loop
PULSE.c:61:16: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:64:17: missed: couldn't vectorize loop
PULSE.c:64:17: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:73:19: missed: couldn't vectorize loop
PULSE.c:73:19: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _69 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:53:22: missed: couldn't vectorize loop
PULSE.c:53:22: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:47:6: note: vectorized 1 loops in function.
PULSE.c:48:1: missed: statement clobbers memory: saved_stack.7_39 = __builtin_stack_save ();
PULSE.c:49:46: missed: statement clobbers memory: _1 = PULSE_GetLossFunctionPtr (loss_function_40(D));
PULSE.c:60:10: missed: statement clobbers memory: random.4_46 = __builtin_alloca_with_align (_4, 32);
PULSE.c:34:8: missed: statement clobbers memory: _61 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_62);
PULSE.c:39:29: missed: statement clobbers memory: _69 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _72 = rand ();
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_98, inputs_93, _97);
PULSE.c:8:2: missed: statement clobbers memory: _99 (layer_94);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_105, _104, _103);
PULSE.c:67:11: missed: statement clobbers memory: loss_55 = PULSE_GetLoss_42 (_17, _16, _12, _11);
PULSE.c:22:2: missed: statement clobbers memory: _86 (output_34);
PULSE.c:22:2: missed: statement clobbers memory: _108 (_87);
PULSE.c:22:2: missed: statement clobbers memory: _115 (_109);
PULSE.c:22:2: missed: statement clobbers memory: _122 (_116);
PULSE.c:22:2: missed: statement clobbers memory: _129 (_123);
PULSE.c:22:2: missed: statement clobbers memory: _136 (_130);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_137);
PULSE.c:26:3: missed: statement clobbers memory: memset (_142, 0, _140);
PULSE.c:26:3: missed: statement clobbers memory: memset (_135, 0, _133);
PULSE.c:26:3: missed: statement clobbers memory: memset (_128, 0, _126);
PULSE.c:26:3: missed: statement clobbers memory: memset (_121, 0, _119);
PULSE.c:26:3: missed: statement clobbers memory: memset (_114, 0, _112);
PULSE.c:26:3: missed: statement clobbers memory: memset (_92, 0, _90);
PULSE.c:75:6: missed: statement clobbers memory: _21 (current_191, args);
PULSE.c:81:4: missed: statement clobbers memory: printf ("Epoch: %d | Item: %d | Loss: %.10f | Batch Loss: %.10f\r", i_188, j_189, _25, _24);
PULSE.c:47:6: missed: statement clobbers memory: __builtin_stack_restore (saved_stack.7_39);
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4DI
PULSE.c:47:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:91:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:91:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:123:20: missed: couldn't vectorize loop
PULSE.c:123:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:123:20: missed: couldn't vectorize loop
PULSE.c:123:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:103:20: missed: couldn't vectorize loop
PULSE.c:103:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:103:20: missed: couldn't vectorize loop
PULSE.c:103:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:93:13: note: vectorized 0 loops in function.
PULSE.c:96:39: missed: statement clobbers memory: layers_35 = malloc (_3);
PULSE.c:99:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:116:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:117:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:118:46: missed: statement clobbers memory: WEIGHTS_128 = aligned_alloc (64, 0);
PULSE.c:109:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.104_79];
PULSE.c:110:21: missed: statement clobbers memory: _64 = PULSE_GetDenseWeightsSize (args);
PULSE.c:111:16: missed: statement clobbers memory: _67 = PULSE_GetDenseIOSize (args);
PULSE.c:109:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})_195];
PULSE.c:110:21: missed: statement clobbers memory: _184 = PULSE_GetDenseWeightsSize (args);
PULSE.c:111:16: missed: statement clobbers memory: _187 = PULSE_GetDenseIOSize (args);
PULSE.c:116:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:117:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:118:46: missed: statement clobbers memory: WEIGHTS_40 = aligned_alloc (64, _5);
PULSE.c:119:41: missed: statement clobbers memory: IO_42 = aligned_alloc (64, _7);
PULSE.c:129:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.106_81];
PULSE.c:130:17: missed: statement clobbers memory: *_10 = PULSE_CreateDenseLayer (args, WEIGHTS_PTR_134, IO_42); [return slot optimization]
PULSE.c:131:20: missed: statement clobbers memory: _55 = PULSE_GetDenseWeightsSize (args);
PULSE.c:132:15: missed: statement clobbers memory: PULSE_GetDenseIOSize (args);
PULSE.c:129:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.106_132];
PULSE.c:130:17: missed: statement clobbers memory: *_112 = PULSE_CreateDenseLayer (args, WEIGHTS_PTR_143, IO_42); [return slot optimization]
PULSE.c:131:20: missed: statement clobbers memory: _96 = PULSE_GetDenseWeightsSize (args);
PULSE.c:132:15: missed: statement clobbers memory: PULSE_GetDenseIOSize (args);
PULSE.c:138:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:139:9: note: ***** Analysis succeeded with vector mode V8SI
PULSE.c:139:9: note: SLPing BB part
PULSE.c:139:9: note: Costing subgraph: 
PULSE.c:139:9: note: node 0x4280368 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:139:9: note: op template: <retval>.layers = layers_35;
PULSE.c:139:9: note: 	stmt 0 <retval>.layers = layers_35;
PULSE.c:139:9: note: 	stmt 1 <retval>.weights = WEIGHTS_147;
PULSE.c:139:9: note: 	children 0x4280468
PULSE.c:139:9: note: node (external) 0x4280468 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:139:9: note: 	{ layers_35, WEIGHTS_147 }
PULSE.c:139:9: note: Cost model analysis: 
PULSE.c:139:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:139:9: note: Costing subgraph: 
PULSE.c:139:9: note: node 0x42804e8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:139:9: note: op template: <retval>.io = 0B;
PULSE.c:139:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:139:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:139:9: note: 	children 0x4280568
PULSE.c:139:9: note: node (constant) 0x4280568 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:139:9: note: 	{ 0B, 0B }
PULSE.c:139:9: note: Cost model analysis: 
PULSE.c:139:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:139:9: note: Basic block will be vectorized using SLP
PULSE.c:139:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:139:9: note: Vectorizing SLP tree:
PULSE.c:139:9: note: node 0x4280368 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:139:9: note: op template: <retval>.layers = layers_35;
PULSE.c:139:9: note: 	stmt 0 <retval>.layers = layers_35;
PULSE.c:139:9: note: 	stmt 1 <retval>.weights = WEIGHTS_147;
PULSE.c:139:9: note: 	children 0x4280468
PULSE.c:139:9: note: node (external) 0x4280468 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:139:9: note: 	{ layers_35, WEIGHTS_147 }
PULSE.c:139:9: note: ------>vectorizing SLP node starting from: <retval>.layers = layers_35;
PULSE.c:139:9: note: vect_is_simple_use: operand WEIGHTS_147 = PHI <WEIGHTS_40(36), WEIGHTS_128(25)>, type of def: internal
PULSE.c:139:9: note: conflicting alias set types.
PULSE.c:139:9: note: transform store. ncopies = 1
PULSE.c:139:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.layers
PULSE.c:139:9: note: created &<retval>
PULSE.c:139:9: note: add new stmt: MEM <vector(2) long unsigned int> [(void *)&<retval>] = _15;
PULSE.c:139:9: note: vectorizing stmts using SLP.
PULSE.c:139:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:139:9: note: Vectorizing SLP tree:
PULSE.c:139:9: note: node 0x42804e8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:139:9: note: op template: <retval>.io = 0B;
PULSE.c:139:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:139:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:139:9: note: 	children 0x4280568
PULSE.c:139:9: note: node (constant) 0x4280568 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:139:9: note: 	{ 0B, 0B }
PULSE.c:139:9: note: ------>vectorizing SLP node starting from: <retval>.io = 0B;
PULSE.c:139:9: note: vect_is_simple_use: operand 0B, type of def: constant
PULSE.c:139:9: note: transform store. ncopies = 1
PULSE.c:139:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.io
PULSE.c:139:9: note: created &<retval>.io
PULSE.c:139:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval> + 16B] = { 0, 0 };
PULSE.c:139:9: note: vectorizing stmts using SLP.
PULSE.c:139:9: note: ***** The result for vector mode V32QI would be the same
PULSE.c:151:1: note: ***** Analysis failed with vector mode VOID
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _28 (pretmp_91, _51, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:19: missed: not vectorized: no vectype for stmt: _22 = *_21;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:170:10: optimized: loop vectorized using 32 byte vectors
Dense.c:170:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:170:10: optimized: loop vectorized using 16 byte vectors
Dense.c:161:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
Dense.c:154:10: optimized: loop vectorized using 32 byte vectors
Dense.c:154:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:154:10: optimized: loop vectorized using 16 byte vectors
Dense.c:145:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
Dense.c:136:13: note: vectorized 2 loops in function.
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_28, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
Dense.c:136:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:136:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:105:24: missed: couldn't vectorize loop
Dense.c:105:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:108:17: missed: couldn't vectorize loop
Dense.c:108:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:122:24: missed: couldn't vectorize loop
Dense.c:122:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:125:17: missed: couldn't vectorize loop
Dense.c:125:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:90:15: missed: couldn't vectorize loop
Dense.c:90:15: missed: not vectorized: number of iterations cannot be computed.
Dense.c:83:13: note: vectorized 0 loops in function.
Dense.c:86:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:110:17: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:110:17: note: SLPing BB part
Dense.c:127:17: note: Costing subgraph: 
Dense.c:127:17: note: node 0x83a34b8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:127:17: note: op template: _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 0 _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 1 _198 = (sizetype) wi_137;
Dense.c:127:17: note: 	children 0x83a35b8
Dense.c:127:17: note: node (external) 0x83a35b8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:127:17: note: 	{ j_135, wi_137 }
Dense.c:127:17: note: Cost model analysis: 
Dense.c:127:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:127:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:127:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: Costing subgraph: 
Dense.c:110:17: note: node 0x83a36b8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:110:17: note: op template: _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 0 _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 1 _217 = (sizetype) wi_136;
Dense.c:110:17: note: 	children 0x83a37b8
Dense.c:110:17: note: node (external) 0x83a37b8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:110:17: note: 	{ j_134, wi_136 }
Dense.c:110:17: note: Cost model analysis: 
Dense.c:110:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:110:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:110:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: ***** The result for vector mode V32QI would be the same
Dense.c:110:17: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:110:17: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:110:17: note: SLPing BB part
Dense.c:127:17: note: Costing subgraph: 
Dense.c:127:17: note: node 0x83a36b8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:127:17: note: op template: _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 0 _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 1 _198 = (sizetype) wi_137;
Dense.c:127:17: note: 	children 0x83a34b8
Dense.c:127:17: note: node (external) 0x83a34b8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:127:17: note: 	{ j_135, wi_137 }
Dense.c:127:17: note: Cost model analysis: 
Dense.c:127:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:127:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:127:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: Costing subgraph: 
Dense.c:110:17: note: node 0x83a3638 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:110:17: note: op template: _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 0 _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 1 _217 = (sizetype) wi_136;
Dense.c:110:17: note: 	children 0x83a3738
Dense.c:110:17: note: node (external) 0x83a3738 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:110:17: note: 	{ j_134, wi_136 }
Dense.c:110:17: note: Cost model analysis: 
Dense.c:110:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:110:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:110:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:105:51: note: ***** Analysis failed with vector mode V8QI
Dense.c:105:51: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:105:51: note: ***** Analysis failed with vector mode V4QI
Dense.c:62:15: missed: couldn't vectorize loop
Dense.c:62:15: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:75:11: optimized: loop vectorized using 32 byte vectors
Dense.c:75:11: optimized: loop vectorized using 16 byte vectors
Dense.c:66:11: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:881:10: missed: not vectorized: no vectype for stmt: _53 = MEM[(__m256 * {ref-all})w_ptr_82];
 scalar_type: __m256
Dense.c:54:13: note: vectorized 1 loops in function.
Dense.c:57:2: missed: statement clobbers memory: memcpy (_4, dense$baiases_63, _3);
Dense.c:79:2: missed: statement clobbers memory: _27 (pretmp_163, _68, 0);
Dense.c:54:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:54:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:179:107: note: ***** Analysis failed with vector mode V8SI
Dense.c:179:107: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
Dense.c:180:85: note: ***** Analysis failed with vector mode V8SI
Dense.c:180:85: note: ***** The result for vector mode V32QI would be the same
Dense.c:180:85: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:180:85: note: ***** Analysis failed with vector mode V16QI
Dense.c:180:85: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:180:85: note: ***** Analysis failed with vector mode V8QI
Dense.c:180:85: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:180:85: note: ***** Analysis failed with vector mode V4QI
Dense.c:192:19: missed: couldn't vectorize loop
Dense.c:192:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:182:13: note: vectorized 0 loops in function.
Dense.c:188:37: missed: statement clobbers memory: _9 = aligned_alloc (64, _8);
Dense.c:189:34: missed: statement clobbers memory: _11 = aligned_alloc (64, _10);
Dense.c:190:35: missed: statement clobbers memory: _12 = aligned_alloc (64, _10);
Dense.c:193:38: missed: statement clobbers memory: _13 = rand ();
Dense.c:193:72: missed: statement clobbers memory: _77 = sqrt (_20);
Dense.c:206:19: missed: statement clobbers memory: _31 = PULSE_GetActivationFunctionPtr (_30);
Dense.c:222:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:223:4: missed: statement clobbers memory: exit (1);
Dense.c:227:9: note: ***** Analysis succeeded with vector mode V8SI
Dense.c:227:9: note: SLPing BB part
Dense.c:227:9: note: Costing subgraph: 
Dense.c:227:9: note: node 0x8229468 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:227:9: note: op template: <retval>.n_inputs = args$n_inputs_51;
Dense.c:227:9: note: 	stmt 0 <retval>.n_inputs = args$n_inputs_51;
Dense.c:227:9: note: 	stmt 1 <retval>.n_outputs = args$n_outputs_52;
Dense.c:227:9: note: 	children 0x82294e8
Dense.c:227:9: note: node 0x82294e8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:227:9: note: op template: args$n_inputs_51 = args.n_inputs;
Dense.c:227:9: note: 	stmt 0 args$n_inputs_51 = args.n_inputs;
Dense.c:227:9: note: 	stmt 1 args$n_outputs_52 = args.n_outputs;
Dense.c:227:9: note: Cost model analysis: 
Dense.c:227:9: note: Cost model analysis for part in loop 0:
  Vector cost: 40
  Scalar cost: 56
Dense.c:227:9: note: Costing subgraph: 
Dense.c:227:9: note: node 0x82295e8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:227:9: note: op template: <retval>.inputs = IO_41(D);
Dense.c:227:9: note: 	stmt 0 <retval>.inputs = IO_41(D);
Dense.c:227:9: note: 	stmt 1 <retval>.outputs = _29;
Dense.c:227:9: note: 	children 0x8229668
Dense.c:227:9: note: node (external) 0x8229668 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:227:9: note: 	{ IO_41(D), _29 }
Dense.c:227:9: note: Cost model analysis: 
Dense.c:227:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:227:9: note: Costing subgraph: 
Dense.c:227:9: note: node 0x8229768 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:227:9: note: op template: <retval>.feed = layer$feed_40;
Dense.c:227:9: note: 	stmt 0 <retval>.feed = layer$feed_40;
Dense.c:227:9: note: 	stmt 1 <retval>.back = layer$back_49;
Dense.c:227:9: note: 	children 0x8229968
Dense.c:227:9: note: node (external) 0x8229968 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:227:9: note: 	{ layer$feed_40, layer$back_49 }
Dense.c:227:9: note: Cost model analysis: 
Dense.c:227:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:227:9: note: Costing subgraph: 
Dense.c:227:9: note: node 0x8229868 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:227:9: note: op template: <retval>.parent = 0B;
Dense.c:227:9: note: 	stmt 0 <retval>.parent = 0B;
Dense.c:227:9: note: 	stmt 1 <retval>.child = 0B;
Dense.c:227:9: note: 	children 0x82299e8
Dense.c:227:9: note: node (constant) 0x82299e8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:227:9: note: 	{ 0B, 0B }
Dense.c:227:9: note: Cost model analysis: 
Dense.c:227:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:227:9: note: Costing subgraph: 
Dense.c:227:9: note: node 0x8229ae8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:227:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_36(D);
Dense.c:227:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_36(D);
Dense.c:227:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&<retval> + 96B] = _4;
Dense.c:227:9: note: 	stmt 2 MEM <float *> [(struct PULSE_Layer *)&<retval> + 104B] = _11;
Dense.c:227:9: note: 	stmt 3 MEM <float *> [(struct PULSE_Layer *)&<retval> + 112B] = _12;
Dense.c:227:9: note: 	children 0x8229b68
Dense.c:227:9: note: node (external) 0x8229b68 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:227:9: note: 	{ MODEL_36(D), _4, _11, _12 }
Dense.c:227:9: note: Cost model analysis: 
Dense.c:227:9: note: Cost model analysis for part in loop 0:
  Vector cost: 64
  Scalar cost: 64
Dense.c:227:9: note: Basic block will be vectorized using SLP
Dense.c:227:9: optimized: basic block part vectorized using 8 byte vectors
Dense.c:227:9: note: Vectorizing SLP tree:
Dense.c:227:9: note: node 0x8229468 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:227:9: note: op template: <retval>.n_inputs = args$n_inputs_51;
Dense.c:227:9: note: 	stmt 0 <retval>.n_inputs = args$n_inputs_51;
Dense.c:227:9: note: 	stmt 1 <retval>.n_outputs = args$n_outputs_52;
Dense.c:227:9: note: 	children 0x82294e8
Dense.c:227:9: note: node 0x82294e8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:227:9: note: op template: args$n_inputs_51 = args.n_inputs;
Dense.c:227:9: note: 	stmt 0 args$n_inputs_51 = args.n_inputs;
Dense.c:227:9: note: 	stmt 1 args$n_outputs_52 = args.n_outputs;
Dense.c:227:9: note: ------>vectorizing SLP node starting from: args$n_inputs_51 = args.n_inputs;
Dense.c:227:9: note: transform load. ncopies = 1
Dense.c:227:9: note: conflicting alias set types.
Dense.c:227:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: args.n_inputs
Dense.c:227:9: note: created &args
Dense.c:227:9: note: add new stmt: vect_args_n_inputs_51.504_86 = MEM <vector(2) unsigned int> [(void *)&args];
Dense.c:227:9: note: extracting lane for live stmt args$n_inputs_51 = args.n_inputs;
Dense.c:227:9: note: extracting lane for live stmt args$n_outputs_52 = args.n_outputs;
Dense.c:227:9: note: ------>vectorizing SLP node starting from: <retval>.n_inputs = args$n_inputs_51;
Dense.c:227:9: note: vect_is_simple_use: operand args.n_inputs, type of def: internal
Dense.c:227:9: note: vect_is_simple_use: operand args.n_outputs, type of def: internal
Dense.c:227:9: note: transform store. ncopies = 1
Dense.c:227:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: <retval>.n_inputs
Dense.c:227:9: note: created &<retval>.n_inputs
Dense.c:227:9: note: add new stmt: MEM <vector(2) unsigned int> [(unsigned int *)&<retval> + 128B] = vect_args_n_inputs_51.504_86;
Dense.c:227:9: note: vectorizing stmts using SLP.
Dense.c:227:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:227:9: note: Vectorizing SLP tree:
Dense.c:227:9: note: node 0x82295e8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:227:9: note: op template: <retval>.inputs = IO_41(D);
Dense.c:227:9: note: 	stmt 0 <retval>.inputs = IO_41(D);
Dense.c:227:9: note: 	stmt 1 <retval>.outputs = _29;
Dense.c:227:9: note: 	children 0x8229668
Dense.c:227:9: note: node (external) 0x8229668 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:227:9: note: 	{ IO_41(D), _29 }
Dense.c:227:9: note: ------>vectorizing SLP node starting from: <retval>.inputs = IO_41(D);
Dense.c:227:9: note: vect_is_simple_use: operand IO_41(D) + _28, type of def: internal
Dense.c:227:9: note: transform store. ncopies = 1
Dense.c:227:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.inputs
Dense.c:227:9: note: created &<retval>
Dense.c:227:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval>] = _92;
Dense.c:227:9: note: vectorizing stmts using SLP.
Dense.c:227:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:227:9: note: Vectorizing SLP tree:
Dense.c:227:9: note: node 0x8229768 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:227:9: note: op template: <retval>.feed = layer$feed_40;
Dense.c:227:9: note: 	stmt 0 <retval>.feed = layer$feed_40;
Dense.c:227:9: note: 	stmt 1 <retval>.back = layer$back_49;
Dense.c:227:9: note: 	children 0x8229968
Dense.c:227:9: note: node (external) 0x8229968 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:227:9: note: 	{ layer$feed_40, layer$back_49 }
Dense.c:227:9: note: ------>vectorizing SLP node starting from: <retval>.feed = layer$feed_40;
Dense.c:227:9: note: vect_is_simple_use: operand layer$back_49 = PHI <layer$back_55(D)(11), _BackDense(9), _SIMD_BackDense(15)>, type of def: internal
Dense.c:227:9: note: transform store. ncopies = 1
Dense.c:227:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.feed
Dense.c:227:9: note: created &<retval>.feed
Dense.c:227:9: note: add new stmt: MEM <vector(2) long unsigned int> [(void (*<T921>) (struct PULSE_Layer *) *)&<retval> + 32B] = _96;
Dense.c:227:9: note: vectorizing stmts using SLP.
Dense.c:227:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:227:9: note: Vectorizing SLP tree:
Dense.c:227:9: note: node 0x8229868 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:227:9: note: op template: <retval>.parent = 0B;
Dense.c:227:9: note: 	stmt 0 <retval>.parent = 0B;
Dense.c:227:9: note: 	stmt 1 <retval>.child = 0B;
Dense.c:227:9: note: 	children 0x82299e8
Dense.c:227:9: note: node (constant) 0x82299e8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:227:9: note: 	{ 0B, 0B }
Dense.c:227:9: note: ------>vectorizing SLP node starting from: <retval>.parent = 0B;
Dense.c:227:9: note: vect_is_simple_use: operand 0B, type of def: constant
Dense.c:227:9: note: transform store. ncopies = 1
Dense.c:227:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.parent
Dense.c:227:9: note: created &<retval>.parent
Dense.c:227:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer * *)&<retval> + 72B] = { 0, 0 };
Dense.c:227:9: note: vectorizing stmts using SLP.
Dense.c:227:9: optimized: basic block part vectorized using 32 byte vectors
Dense.c:227:9: note: Vectorizing SLP tree:
Dense.c:227:9: note: node 0x8229ae8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:227:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_36(D);
Dense.c:227:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_36(D);
Dense.c:227:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&<retval> + 96B] = _4;
Dense.c:227:9: note: 	stmt 2 MEM <float *> [(struct PULSE_Layer *)&<retval> + 104B] = _11;
Dense.c:227:9: note: 	stmt 3 MEM <float *> [(struct PULSE_Layer *)&<retval> + 112B] = _12;
Dense.c:227:9: note: 	children 0x8229b68
Dense.c:227:9: note: node (external) 0x8229b68 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:227:9: note: 	{ MODEL_36(D), _4, _11, _12 }
Dense.c:227:9: note: ------>vectorizing SLP node starting from: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_36(D);
Dense.c:227:9: note: vect_is_simple_use: operand MODEL_36(D) + _3, type of def: internal
Dense.c:227:9: note: vect_is_simple_use: operand aligned_alloc (64, _10), type of def: internal
Dense.c:227:9: note: vect_is_simple_use: operand aligned_alloc (64, _10), type of def: internal
Dense.c:227:9: note: transform store. ncopies = 1
Dense.c:227:9: note: create vector_type-pointer variable to type: vector(4) long unsigned int  vectorizing a pointer ref: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B]
Dense.c:227:9: note: created &MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B]
Dense.c:227:9: note: add new stmt: MEM <vector(4) long unsigned int> [(struct PULSE_Layer *)&<retval> + 88B] = _103;
Dense.c:227:9: note: vectorizing stmts using SLP.
Dense.c:227:9: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _28 (pretmp_91, _51, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:19: missed: not vectorized: no vectype for stmt: _22 = *_21;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:170:10: optimized: loop vectorized using 32 byte vectors
Dense.c:170:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:170:10: optimized: loop vectorized using 16 byte vectors
Dense.c:161:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
Dense.c:154:10: optimized: loop vectorized using 32 byte vectors
Dense.c:154:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:154:10: optimized: loop vectorized using 16 byte vectors
Dense.c:145:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
Dense.c:136:13: note: vectorized 2 loops in function.
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_28, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
Dense.c:136:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:136:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:105:24: missed: couldn't vectorize loop
Dense.c:105:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:108:17: missed: couldn't vectorize loop
Dense.c:108:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:122:24: missed: couldn't vectorize loop
Dense.c:122:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:125:17: missed: couldn't vectorize loop
Dense.c:125:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:90:15: missed: couldn't vectorize loop
Dense.c:90:15: missed: not vectorized: number of iterations cannot be computed.
Dense.c:83:13: note: vectorized 0 loops in function.
Dense.c:86:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:110:17: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:110:17: note: SLPing BB part
Dense.c:127:17: note: Costing subgraph: 
Dense.c:127:17: note: node 0x868d448 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:127:17: note: op template: _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 0 _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 1 _198 = (sizetype) wi_137;
Dense.c:127:17: note: 	children 0x868d548
Dense.c:127:17: note: node (external) 0x868d548 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:127:17: note: 	{ j_135, wi_137 }
Dense.c:127:17: note: Cost model analysis: 
Dense.c:127:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:127:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:127:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: Costing subgraph: 
Dense.c:110:17: note: node 0x868d648 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:110:17: note: op template: _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 0 _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 1 _217 = (sizetype) wi_136;
Dense.c:110:17: note: 	children 0x868d748
Dense.c:110:17: note: node (external) 0x868d748 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:110:17: note: 	{ j_134, wi_136 }
Dense.c:110:17: note: Cost model analysis: 
Dense.c:110:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:110:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:110:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: ***** The result for vector mode V32QI would be the same
Dense.c:110:17: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:110:17: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:110:17: note: SLPing BB part
Dense.c:127:17: note: Costing subgraph: 
Dense.c:127:17: note: node 0x868d648 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:127:17: note: op template: _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 0 _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 1 _198 = (sizetype) wi_137;
Dense.c:127:17: note: 	children 0x868d448
Dense.c:127:17: note: node (external) 0x868d448 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:127:17: note: 	{ j_135, wi_137 }
Dense.c:127:17: note: Cost model analysis: 
Dense.c:127:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:127:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:127:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: Costing subgraph: 
Dense.c:110:17: note: node 0x868d5c8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:110:17: note: op template: _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 0 _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 1 _217 = (sizetype) wi_136;
Dense.c:110:17: note: 	children 0x868d6c8
Dense.c:110:17: note: node (external) 0x868d6c8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:110:17: note: 	{ j_134, wi_136 }
Dense.c:110:17: note: Cost model analysis: 
Dense.c:110:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:110:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:110:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:105:51: note: ***** Analysis failed with vector mode V8QI
Dense.c:105:51: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:105:51: note: ***** Analysis failed with vector mode V4QI
Dense.c:62:15: missed: couldn't vectorize loop
Dense.c:62:15: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:75:11: optimized: loop vectorized using 32 byte vectors
Dense.c:75:11: optimized: loop vectorized using 16 byte vectors
Dense.c:66:11: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:881:10: missed: not vectorized: no vectype for stmt: _53 = MEM[(__m256 * {ref-all})w_ptr_82];
 scalar_type: __m256
Dense.c:54:13: note: vectorized 1 loops in function.
Dense.c:57:2: missed: statement clobbers memory: memcpy (_4, dense$baiases_63, _3);
Dense.c:79:2: missed: statement clobbers memory: _27 (pretmp_163, _68, 0);
Dense.c:54:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:54:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:179:107: note: ***** Analysis failed with vector mode V8SI
Dense.c:179:107: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
Dense.c:180:85: note: ***** Analysis failed with vector mode V8SI
Dense.c:180:85: note: ***** The result for vector mode V32QI would be the same
Dense.c:180:85: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:180:85: note: ***** Analysis failed with vector mode V16QI
Dense.c:180:85: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:180:85: note: ***** Analysis failed with vector mode V8QI
Dense.c:180:85: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:180:85: note: ***** Analysis failed with vector mode V4QI
Dense.c:192:19: missed: couldn't vectorize loop
Dense.c:192:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:182:13: note: vectorized 0 loops in function.
Dense.c:188:37: missed: statement clobbers memory: _9 = aligned_alloc (64, _8);
Dense.c:189:34: missed: statement clobbers memory: _11 = aligned_alloc (64, _10);
Dense.c:190:35: missed: statement clobbers memory: _12 = aligned_alloc (64, _10);
Dense.c:193:38: missed: statement clobbers memory: _13 = rand ();
Dense.c:193:72: missed: statement clobbers memory: _77 = sqrt (_20);
Dense.c:206:19: missed: statement clobbers memory: _31 = PULSE_GetActivationFunctionPtr (_30);
Dense.c:222:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:223:4: missed: statement clobbers memory: exit (1);
Dense.c:227:9: note: ***** Analysis succeeded with vector mode V8SI
Dense.c:227:9: note: SLPing BB part
Dense.c:227:9: note: Costing subgraph: 
Dense.c:227:9: note: node 0x855e458 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:227:9: note: op template: <retval>.n_inputs = args$n_inputs_51;
Dense.c:227:9: note: 	stmt 0 <retval>.n_inputs = args$n_inputs_51;
Dense.c:227:9: note: 	stmt 1 <retval>.n_outputs = args$n_outputs_52;
Dense.c:227:9: note: 	children 0x855e4d8
Dense.c:227:9: note: node 0x855e4d8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:227:9: note: op template: args$n_inputs_51 = args.n_inputs;
Dense.c:227:9: note: 	stmt 0 args$n_inputs_51 = args.n_inputs;
Dense.c:227:9: note: 	stmt 1 args$n_outputs_52 = args.n_outputs;
Dense.c:227:9: note: Cost model analysis: 
Dense.c:227:9: note: Cost model analysis for part in loop 0:
  Vector cost: 40
  Scalar cost: 56
Dense.c:227:9: note: Costing subgraph: 
Dense.c:227:9: note: node 0x855e5d8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:227:9: note: op template: <retval>.inputs = IO_41(D);
Dense.c:227:9: note: 	stmt 0 <retval>.inputs = IO_41(D);
Dense.c:227:9: note: 	stmt 1 <retval>.outputs = _29;
Dense.c:227:9: note: 	children 0x855e658
Dense.c:227:9: note: node (external) 0x855e658 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:227:9: note: 	{ IO_41(D), _29 }
Dense.c:227:9: note: Cost model analysis: 
Dense.c:227:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:227:9: note: Costing subgraph: 
Dense.c:227:9: note: node 0x855e758 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:227:9: note: op template: <retval>.feed = layer$feed_40;
Dense.c:227:9: note: 	stmt 0 <retval>.feed = layer$feed_40;
Dense.c:227:9: note: 	stmt 1 <retval>.back = layer$back_49;
Dense.c:227:9: note: 	children 0x855e958
Dense.c:227:9: note: node (external) 0x855e958 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:227:9: note: 	{ layer$feed_40, layer$back_49 }
Dense.c:227:9: note: Cost model analysis: 
Dense.c:227:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:227:9: note: Costing subgraph: 
Dense.c:227:9: note: node 0x855e858 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:227:9: note: op template: <retval>.parent = 0B;
Dense.c:227:9: note: 	stmt 0 <retval>.parent = 0B;
Dense.c:227:9: note: 	stmt 1 <retval>.child = 0B;
Dense.c:227:9: note: 	children 0x855e9d8
Dense.c:227:9: note: node (constant) 0x855e9d8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:227:9: note: 	{ 0B, 0B }
Dense.c:227:9: note: Cost model analysis: 
Dense.c:227:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:227:9: note: Costing subgraph: 
Dense.c:227:9: note: node 0x855ead8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:227:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_36(D);
Dense.c:227:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_36(D);
Dense.c:227:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&<retval> + 96B] = _4;
Dense.c:227:9: note: 	stmt 2 MEM <float *> [(struct PULSE_Layer *)&<retval> + 104B] = _11;
Dense.c:227:9: note: 	stmt 3 MEM <float *> [(struct PULSE_Layer *)&<retval> + 112B] = _12;
Dense.c:227:9: note: 	children 0x855eb58
Dense.c:227:9: note: node (external) 0x855eb58 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:227:9: note: 	{ MODEL_36(D), _4, _11, _12 }
Dense.c:227:9: note: Cost model analysis: 
Dense.c:227:9: note: Cost model analysis for part in loop 0:
  Vector cost: 64
  Scalar cost: 64
Dense.c:227:9: note: Basic block will be vectorized using SLP
Dense.c:227:9: optimized: basic block part vectorized using 8 byte vectors
Dense.c:227:9: note: Vectorizing SLP tree:
Dense.c:227:9: note: node 0x855e458 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:227:9: note: op template: <retval>.n_inputs = args$n_inputs_51;
Dense.c:227:9: note: 	stmt 0 <retval>.n_inputs = args$n_inputs_51;
Dense.c:227:9: note: 	stmt 1 <retval>.n_outputs = args$n_outputs_52;
Dense.c:227:9: note: 	children 0x855e4d8
Dense.c:227:9: note: node 0x855e4d8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:227:9: note: op template: args$n_inputs_51 = args.n_inputs;
Dense.c:227:9: note: 	stmt 0 args$n_inputs_51 = args.n_inputs;
Dense.c:227:9: note: 	stmt 1 args$n_outputs_52 = args.n_outputs;
Dense.c:227:9: note: ------>vectorizing SLP node starting from: args$n_inputs_51 = args.n_inputs;
Dense.c:227:9: note: transform load. ncopies = 1
Dense.c:227:9: note: conflicting alias set types.
Dense.c:227:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: args.n_inputs
Dense.c:227:9: note: created &args
Dense.c:227:9: note: add new stmt: vect_args_n_inputs_51.504_86 = MEM <vector(2) unsigned int> [(void *)&args];
Dense.c:227:9: note: extracting lane for live stmt args$n_inputs_51 = args.n_inputs;
Dense.c:227:9: note: extracting lane for live stmt args$n_outputs_52 = args.n_outputs;
Dense.c:227:9: note: ------>vectorizing SLP node starting from: <retval>.n_inputs = args$n_inputs_51;
Dense.c:227:9: note: vect_is_simple_use: operand args.n_inputs, type of def: internal
Dense.c:227:9: note: vect_is_simple_use: operand args.n_outputs, type of def: internal
Dense.c:227:9: note: transform store. ncopies = 1
Dense.c:227:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: <retval>.n_inputs
Dense.c:227:9: note: created &<retval>.n_inputs
Dense.c:227:9: note: add new stmt: MEM <vector(2) unsigned int> [(unsigned int *)&<retval> + 128B] = vect_args_n_inputs_51.504_86;
Dense.c:227:9: note: vectorizing stmts using SLP.
Dense.c:227:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:227:9: note: Vectorizing SLP tree:
Dense.c:227:9: note: node 0x855e5d8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:227:9: note: op template: <retval>.inputs = IO_41(D);
Dense.c:227:9: note: 	stmt 0 <retval>.inputs = IO_41(D);
Dense.c:227:9: note: 	stmt 1 <retval>.outputs = _29;
Dense.c:227:9: note: 	children 0x855e658
Dense.c:227:9: note: node (external) 0x855e658 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:227:9: note: 	{ IO_41(D), _29 }
Dense.c:227:9: note: ------>vectorizing SLP node starting from: <retval>.inputs = IO_41(D);
Dense.c:227:9: note: vect_is_simple_use: operand IO_41(D) + _28, type of def: internal
Dense.c:227:9: note: transform store. ncopies = 1
Dense.c:227:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.inputs
Dense.c:227:9: note: created &<retval>
Dense.c:227:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval>] = _92;
Dense.c:227:9: note: vectorizing stmts using SLP.
Dense.c:227:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:227:9: note: Vectorizing SLP tree:
Dense.c:227:9: note: node 0x855e758 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:227:9: note: op template: <retval>.feed = layer$feed_40;
Dense.c:227:9: note: 	stmt 0 <retval>.feed = layer$feed_40;
Dense.c:227:9: note: 	stmt 1 <retval>.back = layer$back_49;
Dense.c:227:9: note: 	children 0x855e958
Dense.c:227:9: note: node (external) 0x855e958 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:227:9: note: 	{ layer$feed_40, layer$back_49 }
Dense.c:227:9: note: ------>vectorizing SLP node starting from: <retval>.feed = layer$feed_40;
Dense.c:227:9: note: vect_is_simple_use: operand layer$back_49 = PHI <layer$back_55(D)(11), _BackDense(9), _SIMD_BackDense(15)>, type of def: internal
Dense.c:227:9: note: transform store. ncopies = 1
Dense.c:227:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.feed
Dense.c:227:9: note: created &<retval>.feed
Dense.c:227:9: note: add new stmt: MEM <vector(2) long unsigned int> [(void (*<T921>) (struct PULSE_Layer *) *)&<retval> + 32B] = _96;
Dense.c:227:9: note: vectorizing stmts using SLP.
Dense.c:227:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:227:9: note: Vectorizing SLP tree:
Dense.c:227:9: note: node 0x855e858 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:227:9: note: op template: <retval>.parent = 0B;
Dense.c:227:9: note: 	stmt 0 <retval>.parent = 0B;
Dense.c:227:9: note: 	stmt 1 <retval>.child = 0B;
Dense.c:227:9: note: 	children 0x855e9d8
Dense.c:227:9: note: node (constant) 0x855e9d8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:227:9: note: 	{ 0B, 0B }
Dense.c:227:9: note: ------>vectorizing SLP node starting from: <retval>.parent = 0B;
Dense.c:227:9: note: vect_is_simple_use: operand 0B, type of def: constant
Dense.c:227:9: note: transform store. ncopies = 1
Dense.c:227:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.parent
Dense.c:227:9: note: created &<retval>.parent
Dense.c:227:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer * *)&<retval> + 72B] = { 0, 0 };
Dense.c:227:9: note: vectorizing stmts using SLP.
Dense.c:227:9: optimized: basic block part vectorized using 32 byte vectors
Dense.c:227:9: note: Vectorizing SLP tree:
Dense.c:227:9: note: node 0x855ead8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:227:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_36(D);
Dense.c:227:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_36(D);
Dense.c:227:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&<retval> + 96B] = _4;
Dense.c:227:9: note: 	stmt 2 MEM <float *> [(struct PULSE_Layer *)&<retval> + 104B] = _11;
Dense.c:227:9: note: 	stmt 3 MEM <float *> [(struct PULSE_Layer *)&<retval> + 112B] = _12;
Dense.c:227:9: note: 	children 0x855eb58
Dense.c:227:9: note: node (external) 0x855eb58 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:227:9: note: 	{ MODEL_36(D), _4, _11, _12 }
Dense.c:227:9: note: ------>vectorizing SLP node starting from: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_36(D);
Dense.c:227:9: note: vect_is_simple_use: operand MODEL_36(D) + _3, type of def: internal
Dense.c:227:9: note: vect_is_simple_use: operand aligned_alloc (64, _10), type of def: internal
Dense.c:227:9: note: vect_is_simple_use: operand aligned_alloc (64, _10), type of def: internal
Dense.c:227:9: note: transform store. ncopies = 1
Dense.c:227:9: note: create vector_type-pointer variable to type: vector(4) long unsigned int  vectorizing a pointer ref: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B]
Dense.c:227:9: note: created &MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B]
Dense.c:227:9: note: add new stmt: MEM <vector(4) long unsigned int> [(struct PULSE_Layer *)&<retval> + 88B] = _103;
Dense.c:227:9: note: vectorizing stmts using SLP.
Dense.c:227:9: note: ***** The result for vector mode V32QI would be the same
Activations.c:17:19: optimized: loop vectorized using 32 byte vectors
Activations.c:17:19: optimized: loop vectorized using 16 byte vectors
Activations.c:17:19: optimized: loop vectorized using 32 byte vectors
Activations.c:17:19: optimized: loop vectorized using 16 byte vectors
Activations.c:14:13: note: vectorized 2 loops in function.
Activations.c:14:13: note: ***** Analysis failed with vector mode V8SF
Activations.c:14:13: note: ***** The result for vector mode V32QI would be the same
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V16QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V16QI
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V8QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V8QI
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V4QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V4QI
Activations.c:28:19: missed: couldn't vectorize loop
Activations.c:28:19: missed: not vectorized: unsupported control flow in loop.
Activations.c:28:19: optimized: loop vectorized using 32 byte vectors
Activations.c:28:19: optimized: loop vectorized using 16 byte vectors
Activations.c:25:13: note: vectorized 1 loops in function.
Activations.c:25:13: note: ***** Analysis failed with vector mode V8SF
Activations.c:25:13: note: ***** The result for vector mode V32QI would be the same
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V16QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V16QI
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V8QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V8QI
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V4QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V4QI
Activations.c:38:1: note: ***** Analysis failed with vector mode VOID
Activations.c:5:19: missed: couldn't vectorize loop
Activations.c:9:21: missed: statement clobbers memory: _36 = expf (_18);
Activations.c:5:19: missed: couldn't vectorize loop
Activations.c:9:21: missed: statement clobbers memory: _12 = expf (_11);
Activations.c:3:13: note: vectorized 0 loops in function.
Activations.c:9:21: missed: statement clobbers memory: _12 = expf (_11);
Activations.c:9:21: missed: statement clobbers memory: _36 = expf (_18);
Activations.c:11:1: note: ***** Analysis failed with vector mode V8SF
Activations.c:11:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SF
Activations.c:44:2: note: ***** Analysis failed with vector mode VOID
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:4:18: note: vectorized 0 loops in function.
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_4, inputs_15, _3);
PULSE.c:8:2: missed: statement clobbers memory: _5 (layer_17);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_12, pretmp_26, _9);
PULSE.c:13:28: note: ***** Analysis failed with vector mode V8SI
PULSE.c:13:28: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:22:2: missed: statement clobbers memory: _1 (layer_10(D));
PULSE.c:22:2: missed: statement clobbers memory: _14 (_2);
PULSE.c:22:2: missed: statement clobbers memory: _21 (_15);
PULSE.c:22:2: missed: statement clobbers memory: _28 (_22);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_29);
PULSE.c:26:3: missed: statement clobbers memory: memset (_34, 0, _32);
PULSE.c:26:3: missed: statement clobbers memory: memset (_27, 0, _25);
PULSE.c:26:3: missed: statement clobbers memory: memset (_20, 0, _18);
PULSE.c:26:3: missed: statement clobbers memory: memset (_7, 0, _5);
PULSE.c:28:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:28:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:32:6: note: vectorized 1 loops in function.
PULSE.c:34:8: missed: statement clobbers memory: _1 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_2);
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _9 = rand ();
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8SI
PULSE.c:32:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:61:16: missed: couldn't vectorize loop
PULSE.c:61:16: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:64:17: missed: couldn't vectorize loop
PULSE.c:64:17: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:73:19: missed: couldn't vectorize loop
PULSE.c:73:19: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _69 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:53:22: missed: couldn't vectorize loop
PULSE.c:53:22: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:47:6: note: vectorized 1 loops in function.
PULSE.c:48:1: missed: statement clobbers memory: saved_stack.7_39 = __builtin_stack_save ();
PULSE.c:49:46: missed: statement clobbers memory: _1 = PULSE_GetLossFunctionPtr (loss_function_40(D));
PULSE.c:60:10: missed: statement clobbers memory: random.4_46 = __builtin_alloca_with_align (_4, 32);
PULSE.c:34:8: missed: statement clobbers memory: _61 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_62);
PULSE.c:39:29: missed: statement clobbers memory: _69 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _72 = rand ();
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_98, inputs_93, _97);
PULSE.c:8:2: missed: statement clobbers memory: _99 (layer_94);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_105, _104, _103);
PULSE.c:67:11: missed: statement clobbers memory: loss_55 = PULSE_GetLoss_42 (_17, _16, _12, _11);
PULSE.c:22:2: missed: statement clobbers memory: _86 (output_34);
PULSE.c:22:2: missed: statement clobbers memory: _108 (_87);
PULSE.c:22:2: missed: statement clobbers memory: _115 (_109);
PULSE.c:22:2: missed: statement clobbers memory: _122 (_116);
PULSE.c:22:2: missed: statement clobbers memory: _129 (_123);
PULSE.c:22:2: missed: statement clobbers memory: _136 (_130);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_137);
PULSE.c:26:3: missed: statement clobbers memory: memset (_142, 0, _140);
PULSE.c:26:3: missed: statement clobbers memory: memset (_135, 0, _133);
PULSE.c:26:3: missed: statement clobbers memory: memset (_128, 0, _126);
PULSE.c:26:3: missed: statement clobbers memory: memset (_121, 0, _119);
PULSE.c:26:3: missed: statement clobbers memory: memset (_114, 0, _112);
PULSE.c:26:3: missed: statement clobbers memory: memset (_92, 0, _90);
PULSE.c:75:6: missed: statement clobbers memory: _21 (current_191, args);
PULSE.c:81:4: missed: statement clobbers memory: printf ("Epoch: %d | Item: %d | Loss: %.10f | Batch Loss: %.10f\r", i_188, j_189, _25, _24);
PULSE.c:47:6: missed: statement clobbers memory: __builtin_stack_restore (saved_stack.7_39);
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4DI
PULSE.c:47:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:91:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:91:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:123:20: missed: couldn't vectorize loop
PULSE.c:123:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:123:20: missed: couldn't vectorize loop
PULSE.c:123:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:103:20: missed: couldn't vectorize loop
PULSE.c:103:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:103:20: missed: couldn't vectorize loop
PULSE.c:103:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:93:13: note: vectorized 0 loops in function.
PULSE.c:96:39: missed: statement clobbers memory: layers_35 = malloc (_3);
PULSE.c:99:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:116:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:117:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:118:46: missed: statement clobbers memory: WEIGHTS_128 = aligned_alloc (64, 0);
PULSE.c:109:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.104_79];
PULSE.c:110:21: missed: statement clobbers memory: _64 = PULSE_GetDenseWeightsSize (args);
PULSE.c:111:16: missed: statement clobbers memory: _67 = PULSE_GetDenseIOSize (args);
PULSE.c:109:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})_195];
PULSE.c:110:21: missed: statement clobbers memory: _184 = PULSE_GetDenseWeightsSize (args);
PULSE.c:111:16: missed: statement clobbers memory: _187 = PULSE_GetDenseIOSize (args);
PULSE.c:116:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:117:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:118:46: missed: statement clobbers memory: WEIGHTS_40 = aligned_alloc (64, _5);
PULSE.c:119:41: missed: statement clobbers memory: IO_42 = aligned_alloc (64, _7);
PULSE.c:129:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.106_81];
PULSE.c:130:17: missed: statement clobbers memory: *_10 = PULSE_CreateDenseLayer (args, WEIGHTS_PTR_134, IO_42); [return slot optimization]
PULSE.c:131:20: missed: statement clobbers memory: _55 = PULSE_GetDenseWeightsSize (args);
PULSE.c:132:15: missed: statement clobbers memory: PULSE_GetDenseIOSize (args);
PULSE.c:129:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.106_132];
PULSE.c:130:17: missed: statement clobbers memory: *_112 = PULSE_CreateDenseLayer (args, WEIGHTS_PTR_143, IO_42); [return slot optimization]
PULSE.c:131:20: missed: statement clobbers memory: _96 = PULSE_GetDenseWeightsSize (args);
PULSE.c:132:15: missed: statement clobbers memory: PULSE_GetDenseIOSize (args);
PULSE.c:138:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:139:9: note: ***** Analysis succeeded with vector mode V8SI
PULSE.c:139:9: note: SLPing BB part
PULSE.c:139:9: note: Costing subgraph: 
PULSE.c:139:9: note: node 0x410ab08 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:139:9: note: op template: <retval>.layers = layers_35;
PULSE.c:139:9: note: 	stmt 0 <retval>.layers = layers_35;
PULSE.c:139:9: note: 	stmt 1 <retval>.weights = WEIGHTS_147;
PULSE.c:139:9: note: 	children 0x410ac08
PULSE.c:139:9: note: node (external) 0x410ac08 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:139:9: note: 	{ layers_35, WEIGHTS_147 }
PULSE.c:139:9: note: Cost model analysis: 
PULSE.c:139:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:139:9: note: Costing subgraph: 
PULSE.c:139:9: note: node 0x410ac88 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:139:9: note: op template: <retval>.io = 0B;
PULSE.c:139:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:139:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:139:9: note: 	children 0x410ad08
PULSE.c:139:9: note: node (constant) 0x410ad08 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:139:9: note: 	{ 0B, 0B }
PULSE.c:139:9: note: Cost model analysis: 
PULSE.c:139:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:139:9: note: Basic block will be vectorized using SLP
PULSE.c:139:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:139:9: note: Vectorizing SLP tree:
PULSE.c:139:9: note: node 0x410ab08 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:139:9: note: op template: <retval>.layers = layers_35;
PULSE.c:139:9: note: 	stmt 0 <retval>.layers = layers_35;
PULSE.c:139:9: note: 	stmt 1 <retval>.weights = WEIGHTS_147;
PULSE.c:139:9: note: 	children 0x410ac08
PULSE.c:139:9: note: node (external) 0x410ac08 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:139:9: note: 	{ layers_35, WEIGHTS_147 }
PULSE.c:139:9: note: ------>vectorizing SLP node starting from: <retval>.layers = layers_35;
PULSE.c:139:9: note: vect_is_simple_use: operand WEIGHTS_147 = PHI <WEIGHTS_40(36), WEIGHTS_128(25)>, type of def: internal
PULSE.c:139:9: note: conflicting alias set types.
PULSE.c:139:9: note: transform store. ncopies = 1
PULSE.c:139:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.layers
PULSE.c:139:9: note: created &<retval>
PULSE.c:139:9: note: add new stmt: MEM <vector(2) long unsigned int> [(void *)&<retval>] = _15;
PULSE.c:139:9: note: vectorizing stmts using SLP.
PULSE.c:139:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:139:9: note: Vectorizing SLP tree:
PULSE.c:139:9: note: node 0x410ac88 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:139:9: note: op template: <retval>.io = 0B;
PULSE.c:139:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:139:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:139:9: note: 	children 0x410ad08
PULSE.c:139:9: note: node (constant) 0x410ad08 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:139:9: note: 	{ 0B, 0B }
PULSE.c:139:9: note: ------>vectorizing SLP node starting from: <retval>.io = 0B;
PULSE.c:139:9: note: vect_is_simple_use: operand 0B, type of def: constant
PULSE.c:139:9: note: transform store. ncopies = 1
PULSE.c:139:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.io
PULSE.c:139:9: note: created &<retval>.io
PULSE.c:139:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval> + 16B] = { 0, 0 };
PULSE.c:139:9: note: vectorizing stmts using SLP.
PULSE.c:139:9: note: ***** The result for vector mode V32QI would be the same
PULSE.c:151:1: note: ***** Analysis failed with vector mode VOID
Loss.c:15:19: optimized: loop vectorized using 32 byte vectors
Loss.c:15:19: optimized:  loop versioned for vectorization because of possible aliasing
Loss.c:15:19: optimized: loop vectorized using 16 byte vectors
Loss.c:12:23: note: vectorized 1 loops in function.
Loss.c:12:23: note: ***** Analysis failed with vector mode V8SF
Loss.c:12:23: note: ***** The result for vector mode V32QI would be the same
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V16QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V16QI
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V8QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V8QI
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V4QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V4QI
Loss.c:7:19: missed: couldn't vectorize loop
Loss.c:8:38: missed: not vectorized: unsupported use in stmt.
Loss.c:4:23: note: vectorized 0 loops in function.
Loss.c:9:13: note: ***** Analysis failed with vector mode V8SF
Loss.c:9:13: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SF
Loss.c:23:2: note: ***** Analysis failed with vector mode VOID
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _28 (pretmp_91, _51, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:19: missed: not vectorized: no vectype for stmt: _22 = *_21;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:170:10: optimized: loop vectorized using 32 byte vectors
Dense.c:170:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:170:10: optimized: loop vectorized using 16 byte vectors
Dense.c:161:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
Dense.c:154:10: optimized: loop vectorized using 32 byte vectors
Dense.c:154:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:154:10: optimized: loop vectorized using 16 byte vectors
Dense.c:145:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
Dense.c:136:13: note: vectorized 2 loops in function.
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_28, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
Dense.c:136:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:136:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:105:24: missed: couldn't vectorize loop
Dense.c:105:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:108:17: missed: couldn't vectorize loop
Dense.c:108:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:122:24: missed: couldn't vectorize loop
Dense.c:122:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:125:17: missed: couldn't vectorize loop
Dense.c:125:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:90:15: missed: couldn't vectorize loop
Dense.c:90:15: missed: not vectorized: number of iterations cannot be computed.
Dense.c:83:13: note: vectorized 0 loops in function.
Dense.c:86:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:110:17: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:110:17: note: SLPing BB part
Dense.c:127:17: note: Costing subgraph: 
Dense.c:127:17: note: node 0x84a3c88 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:127:17: note: op template: _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 0 _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 1 _198 = (sizetype) wi_137;
Dense.c:127:17: note: 	children 0x84a3d88
Dense.c:127:17: note: node (external) 0x84a3d88 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:127:17: note: 	{ j_135, wi_137 }
Dense.c:127:17: note: Cost model analysis: 
Dense.c:127:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:127:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:127:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: Costing subgraph: 
Dense.c:110:17: note: node 0x84a3e88 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:110:17: note: op template: _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 0 _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 1 _217 = (sizetype) wi_136;
Dense.c:110:17: note: 	children 0x84a3f88
Dense.c:110:17: note: node (external) 0x84a3f88 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:110:17: note: 	{ j_134, wi_136 }
Dense.c:110:17: note: Cost model analysis: 
Dense.c:110:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:110:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:110:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: ***** The result for vector mode V32QI would be the same
Dense.c:110:17: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:110:17: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:110:17: note: SLPing BB part
Dense.c:127:17: note: Costing subgraph: 
Dense.c:127:17: note: node 0x84a3e88 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:127:17: note: op template: _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 0 _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 1 _198 = (sizetype) wi_137;
Dense.c:127:17: note: 	children 0x84a3c88
Dense.c:127:17: note: node (external) 0x84a3c88 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:127:17: note: 	{ j_135, wi_137 }
Dense.c:127:17: note: Cost model analysis: 
Dense.c:127:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:127:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:127:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: Costing subgraph: 
Dense.c:110:17: note: node 0x84a3e08 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:110:17: note: op template: _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 0 _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 1 _217 = (sizetype) wi_136;
Dense.c:110:17: note: 	children 0x84a3f08
Dense.c:110:17: note: node (external) 0x84a3f08 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:110:17: note: 	{ j_134, wi_136 }
Dense.c:110:17: note: Cost model analysis: 
Dense.c:110:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:110:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:110:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:105:51: note: ***** Analysis failed with vector mode V8QI
Dense.c:105:51: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:105:51: note: ***** Analysis failed with vector mode V4QI
Dense.c:62:15: missed: couldn't vectorize loop
Dense.c:62:15: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:75:11: optimized: loop vectorized using 32 byte vectors
Dense.c:75:11: optimized: loop vectorized using 16 byte vectors
Dense.c:66:11: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:881:10: missed: not vectorized: no vectype for stmt: _53 = MEM[(__m256 * {ref-all})w_ptr_82];
 scalar_type: __m256
Dense.c:54:13: note: vectorized 1 loops in function.
Dense.c:57:2: missed: statement clobbers memory: memcpy (_4, dense$baiases_63, _3);
Dense.c:79:2: missed: statement clobbers memory: _27 (pretmp_163, _68, 0);
Dense.c:54:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:54:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:179:107: note: ***** Analysis failed with vector mode V8SI
Dense.c:179:107: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
Dense.c:180:85: note: ***** Analysis failed with vector mode V8SI
Dense.c:180:85: note: ***** The result for vector mode V32QI would be the same
Dense.c:180:85: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:180:85: note: ***** Analysis failed with vector mode V16QI
Dense.c:180:85: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:180:85: note: ***** Analysis failed with vector mode V8QI
Dense.c:180:85: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:180:85: note: ***** Analysis failed with vector mode V4QI
Dense.c:192:19: missed: couldn't vectorize loop
Dense.c:192:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:182:13: note: vectorized 0 loops in function.
Dense.c:188:37: missed: statement clobbers memory: _9 = aligned_alloc (64, _8);
Dense.c:189:34: missed: statement clobbers memory: _11 = aligned_alloc (64, _10);
Dense.c:190:35: missed: statement clobbers memory: _12 = aligned_alloc (64, _10);
Dense.c:193:38: missed: statement clobbers memory: _13 = rand ();
Dense.c:193:72: missed: statement clobbers memory: _79 = sqrt (_20);
Dense.c:206:19: missed: statement clobbers memory: _31 = PULSE_GetActivationFunctionPtr (_30);
Dense.c:208:34: missed: statement clobbers memory: _33 = aligned_alloc (64, _10);
Dense.c:224:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:225:4: missed: statement clobbers memory: exit (1);
Dense.c:229:9: note: ***** Analysis succeeded with vector mode V8SI
Dense.c:229:9: note: SLPing BB part
Dense.c:229:9: note: Costing subgraph: 
Dense.c:229:9: note: node 0x82bb2e8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:229:9: note: op template: <retval>.n_inputs = args$n_inputs_53;
Dense.c:229:9: note: 	stmt 0 <retval>.n_inputs = args$n_inputs_53;
Dense.c:229:9: note: 	stmt 1 <retval>.n_outputs = args$n_outputs_54;
Dense.c:229:9: note: 	children 0x82bb368
Dense.c:229:9: note: node 0x82bb368 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:229:9: note: op template: args$n_inputs_53 = args.n_inputs;
Dense.c:229:9: note: 	stmt 0 args$n_inputs_53 = args.n_inputs;
Dense.c:229:9: note: 	stmt 1 args$n_outputs_54 = args.n_outputs;
Dense.c:229:9: note: Cost model analysis: 
Dense.c:229:9: note: Cost model analysis for part in loop 0:
  Vector cost: 40
  Scalar cost: 56
Dense.c:229:9: note: Costing subgraph: 
Dense.c:229:9: note: node 0x82bb468 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:229:9: note: op template: <retval>.inputs = IO_42(D);
Dense.c:229:9: note: 	stmt 0 <retval>.inputs = IO_42(D);
Dense.c:229:9: note: 	stmt 1 <retval>.outputs = _29;
Dense.c:229:9: note: 	children 0x82bb4e8
Dense.c:229:9: note: node (external) 0x82bb4e8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:229:9: note: 	{ IO_42(D), _29 }
Dense.c:229:9: note: Cost model analysis: 
Dense.c:229:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:229:9: note: Costing subgraph: 
Dense.c:229:9: note: node 0x82bb5e8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:229:9: note: op template: <retval>.feed = layer$feed_41;
Dense.c:229:9: note: 	stmt 0 <retval>.feed = layer$feed_41;
Dense.c:229:9: note: 	stmt 1 <retval>.back = layer$back_51;
Dense.c:229:9: note: 	children 0x82bb7e8
Dense.c:229:9: note: node (external) 0x82bb7e8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:229:9: note: 	{ layer$feed_41, layer$back_51 }
Dense.c:229:9: note: Cost model analysis: 
Dense.c:229:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:229:9: note: Costing subgraph: 
Dense.c:229:9: note: node 0x82bb6e8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:229:9: note: op template: <retval>.parent = 0B;
Dense.c:229:9: note: 	stmt 0 <retval>.parent = 0B;
Dense.c:229:9: note: 	stmt 1 <retval>.child = 0B;
Dense.c:229:9: note: 	children 0x82bb868
Dense.c:229:9: note: node (constant) 0x82bb868 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:229:9: note: 	{ 0B, 0B }
Dense.c:229:9: note: Cost model analysis: 
Dense.c:229:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:229:9: note: Costing subgraph: 
Dense.c:229:9: note: node 0x82bb968 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:229:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_37(D);
Dense.c:229:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_37(D);
Dense.c:229:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&<retval> + 96B] = _4;
Dense.c:229:9: note: 	stmt 2 MEM <float *> [(struct PULSE_Layer *)&<retval> + 104B] = _11;
Dense.c:229:9: note: 	stmt 3 MEM <float *> [(struct PULSE_Layer *)&<retval> + 112B] = _12;
Dense.c:229:9: note: 	children 0x82bb9e8
Dense.c:229:9: note: node (external) 0x82bb9e8 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:229:9: note: 	{ MODEL_37(D), _4, _11, _12 }
Dense.c:229:9: note: Cost model analysis: 
Dense.c:229:9: note: Cost model analysis for part in loop 0:
  Vector cost: 64
  Scalar cost: 64
Dense.c:229:9: note: Basic block will be vectorized using SLP
Dense.c:229:9: optimized: basic block part vectorized using 8 byte vectors
Dense.c:229:9: note: Vectorizing SLP tree:
Dense.c:229:9: note: node 0x82bb2e8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:229:9: note: op template: <retval>.n_inputs = args$n_inputs_53;
Dense.c:229:9: note: 	stmt 0 <retval>.n_inputs = args$n_inputs_53;
Dense.c:229:9: note: 	stmt 1 <retval>.n_outputs = args$n_outputs_54;
Dense.c:229:9: note: 	children 0x82bb368
Dense.c:229:9: note: node 0x82bb368 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:229:9: note: op template: args$n_inputs_53 = args.n_inputs;
Dense.c:229:9: note: 	stmt 0 args$n_inputs_53 = args.n_inputs;
Dense.c:229:9: note: 	stmt 1 args$n_outputs_54 = args.n_outputs;
Dense.c:229:9: note: ------>vectorizing SLP node starting from: args$n_inputs_53 = args.n_inputs;
Dense.c:229:9: note: transform load. ncopies = 1
Dense.c:229:9: note: conflicting alias set types.
Dense.c:229:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: args.n_inputs
Dense.c:229:9: note: created &args
Dense.c:229:9: note: add new stmt: vect_args_n_inputs_53.504_88 = MEM <vector(2) unsigned int> [(void *)&args];
Dense.c:229:9: note: extracting lane for live stmt args$n_inputs_53 = args.n_inputs;
Dense.c:229:9: note: extracting lane for live stmt args$n_outputs_54 = args.n_outputs;
Dense.c:229:9: note: ------>vectorizing SLP node starting from: <retval>.n_inputs = args$n_inputs_53;
Dense.c:229:9: note: vect_is_simple_use: operand args.n_inputs, type of def: internal
Dense.c:229:9: note: vect_is_simple_use: operand args.n_outputs, type of def: internal
Dense.c:229:9: note: transform store. ncopies = 1
Dense.c:229:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: <retval>.n_inputs
Dense.c:229:9: note: created &<retval>.n_inputs
Dense.c:229:9: note: add new stmt: MEM <vector(2) unsigned int> [(unsigned int *)&<retval> + 128B] = vect_args_n_inputs_53.504_88;
Dense.c:229:9: note: vectorizing stmts using SLP.
Dense.c:229:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:229:9: note: Vectorizing SLP tree:
Dense.c:229:9: note: node 0x82bb468 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:229:9: note: op template: <retval>.inputs = IO_42(D);
Dense.c:229:9: note: 	stmt 0 <retval>.inputs = IO_42(D);
Dense.c:229:9: note: 	stmt 1 <retval>.outputs = _29;
Dense.c:229:9: note: 	children 0x82bb4e8
Dense.c:229:9: note: node (external) 0x82bb4e8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:229:9: note: 	{ IO_42(D), _29 }
Dense.c:229:9: note: ------>vectorizing SLP node starting from: <retval>.inputs = IO_42(D);
Dense.c:229:9: note: vect_is_simple_use: operand IO_42(D) + _28, type of def: internal
Dense.c:229:9: note: transform store. ncopies = 1
Dense.c:229:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.inputs
Dense.c:229:9: note: created &<retval>
Dense.c:229:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval>] = _94;
Dense.c:229:9: note: vectorizing stmts using SLP.
Dense.c:229:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:229:9: note: Vectorizing SLP tree:
Dense.c:229:9: note: node 0x82bb5e8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:229:9: note: op template: <retval>.feed = layer$feed_41;
Dense.c:229:9: note: 	stmt 0 <retval>.feed = layer$feed_41;
Dense.c:229:9: note: 	stmt 1 <retval>.back = layer$back_51;
Dense.c:229:9: note: 	children 0x82bb7e8
Dense.c:229:9: note: node (external) 0x82bb7e8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:229:9: note: 	{ layer$feed_41, layer$back_51 }
Dense.c:229:9: note: ------>vectorizing SLP node starting from: <retval>.feed = layer$feed_41;
Dense.c:229:9: note: vect_is_simple_use: operand layer$back_51 = PHI <layer$back_57(D)(11), _BackDense(9), _SIMD_BackDense(15)>, type of def: internal
Dense.c:229:9: note: transform store. ncopies = 1
Dense.c:229:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.feed
Dense.c:229:9: note: created &<retval>.feed
Dense.c:229:9: note: add new stmt: MEM <vector(2) long unsigned int> [(void (*<T921>) (struct PULSE_Layer *) *)&<retval> + 32B] = _98;
Dense.c:229:9: note: vectorizing stmts using SLP.
Dense.c:229:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:229:9: note: Vectorizing SLP tree:
Dense.c:229:9: note: node 0x82bb6e8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:229:9: note: op template: <retval>.parent = 0B;
Dense.c:229:9: note: 	stmt 0 <retval>.parent = 0B;
Dense.c:229:9: note: 	stmt 1 <retval>.child = 0B;
Dense.c:229:9: note: 	children 0x82bb868
Dense.c:229:9: note: node (constant) 0x82bb868 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:229:9: note: 	{ 0B, 0B }
Dense.c:229:9: note: ------>vectorizing SLP node starting from: <retval>.parent = 0B;
Dense.c:229:9: note: vect_is_simple_use: operand 0B, type of def: constant
Dense.c:229:9: note: transform store. ncopies = 1
Dense.c:229:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.parent
Dense.c:229:9: note: created &<retval>.parent
Dense.c:229:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer * *)&<retval> + 72B] = { 0, 0 };
Dense.c:229:9: note: vectorizing stmts using SLP.
Dense.c:229:9: optimized: basic block part vectorized using 32 byte vectors
Dense.c:229:9: note: Vectorizing SLP tree:
Dense.c:229:9: note: node 0x82bb968 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:229:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_37(D);
Dense.c:229:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_37(D);
Dense.c:229:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&<retval> + 96B] = _4;
Dense.c:229:9: note: 	stmt 2 MEM <float *> [(struct PULSE_Layer *)&<retval> + 104B] = _11;
Dense.c:229:9: note: 	stmt 3 MEM <float *> [(struct PULSE_Layer *)&<retval> + 112B] = _12;
Dense.c:229:9: note: 	children 0x82bb9e8
Dense.c:229:9: note: node (external) 0x82bb9e8 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:229:9: note: 	{ MODEL_37(D), _4, _11, _12 }
Dense.c:229:9: note: ------>vectorizing SLP node starting from: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_37(D);
Dense.c:229:9: note: vect_is_simple_use: operand MODEL_37(D) + _3, type of def: internal
Dense.c:229:9: note: vect_is_simple_use: operand aligned_alloc (64, _10), type of def: internal
Dense.c:229:9: note: vect_is_simple_use: operand aligned_alloc (64, _10), type of def: internal
Dense.c:229:9: note: transform store. ncopies = 1
Dense.c:229:9: note: create vector_type-pointer variable to type: vector(4) long unsigned int  vectorizing a pointer ref: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B]
Dense.c:229:9: note: created &MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B]
Dense.c:229:9: note: add new stmt: MEM <vector(4) long unsigned int> [(struct PULSE_Layer *)&<retval> + 88B] = _105;
Dense.c:229:9: note: vectorizing stmts using SLP.
Dense.c:229:9: note: ***** The result for vector mode V32QI would be the same
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:4:18: note: vectorized 0 loops in function.
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_4, inputs_15, _3);
PULSE.c:8:2: missed: statement clobbers memory: _5 (layer_17);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_12, pretmp_26, _9);
PULSE.c:13:28: note: ***** Analysis failed with vector mode V8SI
PULSE.c:13:28: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:22:2: missed: statement clobbers memory: _1 (layer_10(D));
PULSE.c:22:2: missed: statement clobbers memory: _14 (_2);
PULSE.c:22:2: missed: statement clobbers memory: _21 (_15);
PULSE.c:22:2: missed: statement clobbers memory: _28 (_22);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_29);
PULSE.c:26:3: missed: statement clobbers memory: memset (_34, 0, _32);
PULSE.c:26:3: missed: statement clobbers memory: memset (_27, 0, _25);
PULSE.c:26:3: missed: statement clobbers memory: memset (_20, 0, _18);
PULSE.c:26:3: missed: statement clobbers memory: memset (_7, 0, _5);
PULSE.c:28:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:28:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:32:6: note: vectorized 1 loops in function.
PULSE.c:34:8: missed: statement clobbers memory: _1 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_2);
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _9 = rand ();
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8SI
PULSE.c:32:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:61:16: missed: couldn't vectorize loop
PULSE.c:61:16: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:64:17: missed: couldn't vectorize loop
PULSE.c:64:17: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:73:19: missed: couldn't vectorize loop
PULSE.c:73:19: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _69 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:53:22: missed: couldn't vectorize loop
PULSE.c:53:22: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:47:6: note: vectorized 1 loops in function.
PULSE.c:48:1: missed: statement clobbers memory: saved_stack.7_39 = __builtin_stack_save ();
PULSE.c:49:46: missed: statement clobbers memory: _1 = PULSE_GetLossFunctionPtr (loss_function_40(D));
PULSE.c:60:10: missed: statement clobbers memory: random.4_46 = __builtin_alloca_with_align (_4, 32);
PULSE.c:34:8: missed: statement clobbers memory: _61 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_62);
PULSE.c:39:29: missed: statement clobbers memory: _69 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _72 = rand ();
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_98, inputs_93, _97);
PULSE.c:8:2: missed: statement clobbers memory: _99 (layer_94);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_105, _104, _103);
PULSE.c:67:11: missed: statement clobbers memory: loss_55 = PULSE_GetLoss_42 (_17, _16, _12, _11);
PULSE.c:22:2: missed: statement clobbers memory: _86 (output_34);
PULSE.c:22:2: missed: statement clobbers memory: _108 (_87);
PULSE.c:22:2: missed: statement clobbers memory: _115 (_109);
PULSE.c:22:2: missed: statement clobbers memory: _122 (_116);
PULSE.c:22:2: missed: statement clobbers memory: _129 (_123);
PULSE.c:22:2: missed: statement clobbers memory: _136 (_130);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_137);
PULSE.c:26:3: missed: statement clobbers memory: memset (_142, 0, _140);
PULSE.c:26:3: missed: statement clobbers memory: memset (_135, 0, _133);
PULSE.c:26:3: missed: statement clobbers memory: memset (_128, 0, _126);
PULSE.c:26:3: missed: statement clobbers memory: memset (_121, 0, _119);
PULSE.c:26:3: missed: statement clobbers memory: memset (_114, 0, _112);
PULSE.c:26:3: missed: statement clobbers memory: memset (_92, 0, _90);
PULSE.c:75:6: missed: statement clobbers memory: _21 (current_191, args);
PULSE.c:81:4: missed: statement clobbers memory: printf ("Epoch: %d | Item: %d | Loss: %.10f | Batch Loss: %.10f\r", i_188, j_189, _25, _24);
PULSE.c:47:6: missed: statement clobbers memory: __builtin_stack_restore (saved_stack.7_39);
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4DI
PULSE.c:47:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:91:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:91:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:123:20: missed: couldn't vectorize loop
PULSE.c:123:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:123:20: missed: couldn't vectorize loop
PULSE.c:123:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:103:20: missed: couldn't vectorize loop
PULSE.c:103:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:103:20: missed: couldn't vectorize loop
PULSE.c:103:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:93:13: note: vectorized 0 loops in function.
PULSE.c:96:39: missed: statement clobbers memory: layers_39 = malloc (_3);
PULSE.c:99:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:116:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:117:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:118:46: missed: statement clobbers memory: WEIGHTS_56 = aligned_alloc (64, 0);
PULSE.c:109:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.104_85];
PULSE.c:110:21: missed: statement clobbers memory: _70 = PULSE_GetDenseWeightsSize (args);
PULSE.c:111:16: missed: statement clobbers memory: _73 = PULSE_GetDenseIOSize (args);
PULSE.c:109:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})_208];
PULSE.c:110:21: missed: statement clobbers memory: _197 = PULSE_GetDenseWeightsSize (args);
PULSE.c:111:16: missed: statement clobbers memory: _200 = PULSE_GetDenseIOSize (args);
PULSE.c:116:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:117:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:118:46: missed: statement clobbers memory: WEIGHTS_44 = aligned_alloc (64, _5);
PULSE.c:119:41: missed: statement clobbers memory: IO_46 = aligned_alloc (64, _7);
PULSE.c:129:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.106_87];
PULSE.c:130:17: missed: statement clobbers memory: *_10 = PULSE_CreateDenseLayer (args, WEIGHTS_PTR_141, IO_PTR_142); [return slot optimization]
PULSE.c:131:20: missed: statement clobbers memory: _59 = PULSE_GetDenseWeightsSize (args);
PULSE.c:132:15: missed: statement clobbers memory: _62 = PULSE_GetDenseIOSize (args);
PULSE.c:129:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.106_127];
PULSE.c:130:17: missed: statement clobbers memory: *_109 = PULSE_CreateDenseLayer (args, WEIGHTS_PTR_149, IO_PTR_148); [return slot optimization]
PULSE.c:131:20: missed: statement clobbers memory: _97 = PULSE_GetDenseWeightsSize (args);
PULSE.c:132:15: missed: statement clobbers memory: _82 = PULSE_GetDenseIOSize (args);
PULSE.c:138:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:139:9: note: ***** Analysis succeeded with vector mode V8SI
PULSE.c:139:9: note: SLPing BB part
PULSE.c:139:9: note: Costing subgraph: 
PULSE.c:139:9: note: node 0x39ae368 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:139:9: note: op template: <retval>.layers = layers_39;
PULSE.c:139:9: note: 	stmt 0 <retval>.layers = layers_39;
PULSE.c:139:9: note: 	stmt 1 <retval>.weights = WEIGHTS_153;
PULSE.c:139:9: note: 	children 0x39ae468
PULSE.c:139:9: note: node (external) 0x39ae468 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:139:9: note: 	{ layers_39, WEIGHTS_153 }
PULSE.c:139:9: note: Cost model analysis: 
PULSE.c:139:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:139:9: note: Costing subgraph: 
PULSE.c:139:9: note: node 0x39ae4e8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:139:9: note: op template: <retval>.io = 0B;
PULSE.c:139:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:139:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:139:9: note: 	children 0x39ae568
PULSE.c:139:9: note: node (constant) 0x39ae568 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:139:9: note: 	{ 0B, 0B }
PULSE.c:139:9: note: Cost model analysis: 
PULSE.c:139:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:139:9: note: Basic block will be vectorized using SLP
PULSE.c:139:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:139:9: note: Vectorizing SLP tree:
PULSE.c:139:9: note: node 0x39ae368 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:139:9: note: op template: <retval>.layers = layers_39;
PULSE.c:139:9: note: 	stmt 0 <retval>.layers = layers_39;
PULSE.c:139:9: note: 	stmt 1 <retval>.weights = WEIGHTS_153;
PULSE.c:139:9: note: 	children 0x39ae468
PULSE.c:139:9: note: node (external) 0x39ae468 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:139:9: note: 	{ layers_39, WEIGHTS_153 }
PULSE.c:139:9: note: ------>vectorizing SLP node starting from: <retval>.layers = layers_39;
PULSE.c:139:9: note: vect_is_simple_use: operand WEIGHTS_153 = PHI <WEIGHTS_44(36), WEIGHTS_56(25)>, type of def: internal
PULSE.c:139:9: note: conflicting alias set types.
PULSE.c:139:9: note: transform store. ncopies = 1
PULSE.c:139:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.layers
PULSE.c:139:9: note: created &<retval>
PULSE.c:139:9: note: add new stmt: MEM <vector(2) long unsigned int> [(void *)&<retval>] = _18;
PULSE.c:139:9: note: vectorizing stmts using SLP.
PULSE.c:139:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:139:9: note: Vectorizing SLP tree:
PULSE.c:139:9: note: node 0x39ae4e8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:139:9: note: op template: <retval>.io = 0B;
PULSE.c:139:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:139:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:139:9: note: 	children 0x39ae568
PULSE.c:139:9: note: node (constant) 0x39ae568 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:139:9: note: 	{ 0B, 0B }
PULSE.c:139:9: note: ------>vectorizing SLP node starting from: <retval>.io = 0B;
PULSE.c:139:9: note: vect_is_simple_use: operand 0B, type of def: constant
PULSE.c:139:9: note: transform store. ncopies = 1
PULSE.c:139:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.io
PULSE.c:139:9: note: created &<retval>.io
PULSE.c:139:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval> + 16B] = { 0, 0 };
PULSE.c:139:9: note: vectorizing stmts using SLP.
PULSE.c:139:9: note: ***** The result for vector mode V32QI would be the same
PULSE.c:151:1: note: ***** Analysis failed with vector mode VOID
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:4:18: note: vectorized 0 loops in function.
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_4, inputs_15, _3);
PULSE.c:8:2: missed: statement clobbers memory: _5 (layer_17);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_12, pretmp_26, _9);
PULSE.c:13:28: note: ***** Analysis failed with vector mode V8SI
PULSE.c:13:28: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:22:2: missed: statement clobbers memory: _1 (layer_10(D));
PULSE.c:22:2: missed: statement clobbers memory: _14 (_2);
PULSE.c:22:2: missed: statement clobbers memory: _21 (_15);
PULSE.c:22:2: missed: statement clobbers memory: _28 (_22);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_29);
PULSE.c:26:3: missed: statement clobbers memory: memset (_34, 0, _32);
PULSE.c:26:3: missed: statement clobbers memory: memset (_27, 0, _25);
PULSE.c:26:3: missed: statement clobbers memory: memset (_20, 0, _18);
PULSE.c:26:3: missed: statement clobbers memory: memset (_7, 0, _5);
PULSE.c:28:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:28:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:32:6: note: vectorized 1 loops in function.
PULSE.c:34:8: missed: statement clobbers memory: _1 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_2);
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _9 = rand ();
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8SI
PULSE.c:32:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:61:16: missed: couldn't vectorize loop
PULSE.c:61:16: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:64:17: missed: couldn't vectorize loop
PULSE.c:64:17: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:73:19: missed: couldn't vectorize loop
PULSE.c:73:19: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _69 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:53:22: missed: couldn't vectorize loop
PULSE.c:53:22: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:47:6: note: vectorized 1 loops in function.
PULSE.c:48:1: missed: statement clobbers memory: saved_stack.7_39 = __builtin_stack_save ();
PULSE.c:49:46: missed: statement clobbers memory: _1 = PULSE_GetLossFunctionPtr (loss_function_40(D));
PULSE.c:60:10: missed: statement clobbers memory: random.4_46 = __builtin_alloca_with_align (_4, 32);
PULSE.c:34:8: missed: statement clobbers memory: _61 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_62);
PULSE.c:39:29: missed: statement clobbers memory: _69 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _72 = rand ();
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_98, inputs_93, _97);
PULSE.c:8:2: missed: statement clobbers memory: _99 (layer_94);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_105, _104, _103);
PULSE.c:67:11: missed: statement clobbers memory: loss_55 = PULSE_GetLoss_42 (_17, _16, _12, _11);
PULSE.c:22:2: missed: statement clobbers memory: _86 (output_34);
PULSE.c:22:2: missed: statement clobbers memory: _108 (_87);
PULSE.c:22:2: missed: statement clobbers memory: _115 (_109);
PULSE.c:22:2: missed: statement clobbers memory: _122 (_116);
PULSE.c:22:2: missed: statement clobbers memory: _129 (_123);
PULSE.c:22:2: missed: statement clobbers memory: _136 (_130);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_137);
PULSE.c:26:3: missed: statement clobbers memory: memset (_142, 0, _140);
PULSE.c:26:3: missed: statement clobbers memory: memset (_135, 0, _133);
PULSE.c:26:3: missed: statement clobbers memory: memset (_128, 0, _126);
PULSE.c:26:3: missed: statement clobbers memory: memset (_121, 0, _119);
PULSE.c:26:3: missed: statement clobbers memory: memset (_114, 0, _112);
PULSE.c:26:3: missed: statement clobbers memory: memset (_92, 0, _90);
PULSE.c:75:6: missed: statement clobbers memory: _21 (current_191, args);
PULSE.c:81:4: missed: statement clobbers memory: printf ("Epoch: %d | Item: %d | Loss: %.10f | Batch Loss: %.10f\r", i_188, j_189, _25, _24);
PULSE.c:47:6: missed: statement clobbers memory: __builtin_stack_restore (saved_stack.7_39);
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4DI
PULSE.c:47:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:91:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:91:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:123:20: missed: couldn't vectorize loop
PULSE.c:123:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:123:20: missed: couldn't vectorize loop
PULSE.c:123:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:103:20: missed: couldn't vectorize loop
PULSE.c:103:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:103:20: missed: couldn't vectorize loop
PULSE.c:103:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:93:13: note: vectorized 0 loops in function.
PULSE.c:96:39: missed: statement clobbers memory: layers_39 = malloc (_3);
PULSE.c:99:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:116:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:117:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:118:46: missed: statement clobbers memory: WEIGHTS_56 = aligned_alloc (64, 0);
PULSE.c:109:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.104_85];
PULSE.c:110:21: missed: statement clobbers memory: _70 = PULSE_GetDenseWeightsSize (args);
PULSE.c:111:16: missed: statement clobbers memory: _73 = PULSE_GetDenseIOSize (args);
PULSE.c:109:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})_208];
PULSE.c:110:21: missed: statement clobbers memory: _197 = PULSE_GetDenseWeightsSize (args);
PULSE.c:111:16: missed: statement clobbers memory: _200 = PULSE_GetDenseIOSize (args);
PULSE.c:116:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:117:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:118:46: missed: statement clobbers memory: WEIGHTS_44 = aligned_alloc (64, _5);
PULSE.c:119:41: missed: statement clobbers memory: IO_46 = aligned_alloc (64, _7);
PULSE.c:129:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.106_87];
PULSE.c:130:17: missed: statement clobbers memory: *_10 = PULSE_CreateDenseLayer (args, WEIGHTS_PTR_141, IO_PTR_142); [return slot optimization]
PULSE.c:131:20: missed: statement clobbers memory: _59 = PULSE_GetDenseWeightsSize (args);
PULSE.c:132:15: missed: statement clobbers memory: _62 = PULSE_GetDenseIOSize (args);
PULSE.c:129:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.106_127];
PULSE.c:130:17: missed: statement clobbers memory: *_109 = PULSE_CreateDenseLayer (args, WEIGHTS_PTR_149, IO_PTR_148); [return slot optimization]
PULSE.c:131:20: missed: statement clobbers memory: _97 = PULSE_GetDenseWeightsSize (args);
PULSE.c:132:15: missed: statement clobbers memory: _82 = PULSE_GetDenseIOSize (args);
PULSE.c:138:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:139:9: note: ***** Analysis succeeded with vector mode V8SI
PULSE.c:139:9: note: SLPing BB part
PULSE.c:139:9: note: Costing subgraph: 
PULSE.c:139:9: note: node 0x3deb488 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:139:9: note: op template: <retval>.layers = layers_39;
PULSE.c:139:9: note: 	stmt 0 <retval>.layers = layers_39;
PULSE.c:139:9: note: 	stmt 1 <retval>.weights = WEIGHTS_153;
PULSE.c:139:9: note: 	children 0x3deb588
PULSE.c:139:9: note: node (external) 0x3deb588 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:139:9: note: 	{ layers_39, WEIGHTS_153 }
PULSE.c:139:9: note: Cost model analysis: 
PULSE.c:139:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:139:9: note: Costing subgraph: 
PULSE.c:139:9: note: node 0x3deb608 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:139:9: note: op template: <retval>.io = 0B;
PULSE.c:139:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:139:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:139:9: note: 	children 0x3deb688
PULSE.c:139:9: note: node (constant) 0x3deb688 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:139:9: note: 	{ 0B, 0B }
PULSE.c:139:9: note: Cost model analysis: 
PULSE.c:139:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:139:9: note: Basic block will be vectorized using SLP
PULSE.c:139:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:139:9: note: Vectorizing SLP tree:
PULSE.c:139:9: note: node 0x3deb488 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:139:9: note: op template: <retval>.layers = layers_39;
PULSE.c:139:9: note: 	stmt 0 <retval>.layers = layers_39;
PULSE.c:139:9: note: 	stmt 1 <retval>.weights = WEIGHTS_153;
PULSE.c:139:9: note: 	children 0x3deb588
PULSE.c:139:9: note: node (external) 0x3deb588 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:139:9: note: 	{ layers_39, WEIGHTS_153 }
PULSE.c:139:9: note: ------>vectorizing SLP node starting from: <retval>.layers = layers_39;
PULSE.c:139:9: note: vect_is_simple_use: operand WEIGHTS_153 = PHI <WEIGHTS_44(36), WEIGHTS_56(25)>, type of def: internal
PULSE.c:139:9: note: conflicting alias set types.
PULSE.c:139:9: note: transform store. ncopies = 1
PULSE.c:139:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.layers
PULSE.c:139:9: note: created &<retval>
PULSE.c:139:9: note: add new stmt: MEM <vector(2) long unsigned int> [(void *)&<retval>] = _18;
PULSE.c:139:9: note: vectorizing stmts using SLP.
PULSE.c:139:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:139:9: note: Vectorizing SLP tree:
PULSE.c:139:9: note: node 0x3deb608 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:139:9: note: op template: <retval>.io = 0B;
PULSE.c:139:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:139:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:139:9: note: 	children 0x3deb688
PULSE.c:139:9: note: node (constant) 0x3deb688 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:139:9: note: 	{ 0B, 0B }
PULSE.c:139:9: note: ------>vectorizing SLP node starting from: <retval>.io = 0B;
PULSE.c:139:9: note: vect_is_simple_use: operand 0B, type of def: constant
PULSE.c:139:9: note: transform store. ncopies = 1
PULSE.c:139:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.io
PULSE.c:139:9: note: created &<retval>.io
PULSE.c:139:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval> + 16B] = { 0, 0 };
PULSE.c:139:9: note: vectorizing stmts using SLP.
PULSE.c:139:9: note: ***** The result for vector mode V32QI would be the same
PULSE.c:147:3: missed: statement clobbers memory: free (_1);
PULSE.c:153:3: missed: statement clobbers memory: free (prephitmp_16);
PULSE.c:159:3: missed: statement clobbers memory: free (_4);
PULSE.c:162:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:162:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _28 (pretmp_91, _51, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:19: missed: not vectorized: no vectype for stmt: _22 = *_21;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:170:10: optimized: loop vectorized using 32 byte vectors
Dense.c:170:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:170:10: optimized: loop vectorized using 16 byte vectors
Dense.c:161:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
Dense.c:154:10: optimized: loop vectorized using 32 byte vectors
Dense.c:154:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:154:10: optimized: loop vectorized using 16 byte vectors
Dense.c:145:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
Dense.c:136:13: note: vectorized 2 loops in function.
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_28, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
Dense.c:136:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:136:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:105:24: missed: couldn't vectorize loop
Dense.c:105:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:108:17: missed: couldn't vectorize loop
Dense.c:108:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:122:24: missed: couldn't vectorize loop
Dense.c:122:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:125:17: missed: couldn't vectorize loop
Dense.c:125:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:90:15: missed: couldn't vectorize loop
Dense.c:90:15: missed: not vectorized: number of iterations cannot be computed.
Dense.c:83:13: note: vectorized 0 loops in function.
Dense.c:86:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:110:17: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:110:17: note: SLPing BB part
Dense.c:127:17: note: Costing subgraph: 
Dense.c:127:17: note: node 0x8dccab8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:127:17: note: op template: _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 0 _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 1 _198 = (sizetype) wi_137;
Dense.c:127:17: note: 	children 0x8dccbb8
Dense.c:127:17: note: node (external) 0x8dccbb8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:127:17: note: 	{ j_135, wi_137 }
Dense.c:127:17: note: Cost model analysis: 
Dense.c:127:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:127:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:127:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: Costing subgraph: 
Dense.c:110:17: note: node 0x8dcccb8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:110:17: note: op template: _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 0 _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 1 _217 = (sizetype) wi_136;
Dense.c:110:17: note: 	children 0x8dccdb8
Dense.c:110:17: note: node (external) 0x8dccdb8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:110:17: note: 	{ j_134, wi_136 }
Dense.c:110:17: note: Cost model analysis: 
Dense.c:110:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:110:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:110:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: ***** The result for vector mode V32QI would be the same
Dense.c:110:17: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:110:17: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:110:17: note: SLPing BB part
Dense.c:127:17: note: Costing subgraph: 
Dense.c:127:17: note: node 0x8dcccb8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:127:17: note: op template: _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 0 _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 1 _198 = (sizetype) wi_137;
Dense.c:127:17: note: 	children 0x8dccab8
Dense.c:127:17: note: node (external) 0x8dccab8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:127:17: note: 	{ j_135, wi_137 }
Dense.c:127:17: note: Cost model analysis: 
Dense.c:127:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:127:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:127:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: Costing subgraph: 
Dense.c:110:17: note: node 0x8dccc38 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:110:17: note: op template: _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 0 _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 1 _217 = (sizetype) wi_136;
Dense.c:110:17: note: 	children 0x8dccd38
Dense.c:110:17: note: node (external) 0x8dccd38 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:110:17: note: 	{ j_134, wi_136 }
Dense.c:110:17: note: Cost model analysis: 
Dense.c:110:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:110:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:110:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:105:51: note: ***** Analysis failed with vector mode V8QI
Dense.c:105:51: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:105:51: note: ***** Analysis failed with vector mode V4QI
Dense.c:62:15: missed: couldn't vectorize loop
Dense.c:62:15: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:75:11: optimized: loop vectorized using 32 byte vectors
Dense.c:75:11: optimized: loop vectorized using 16 byte vectors
Dense.c:66:11: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:881:10: missed: not vectorized: no vectype for stmt: _53 = MEM[(__m256 * {ref-all})w_ptr_82];
 scalar_type: __m256
Dense.c:54:13: note: vectorized 1 loops in function.
Dense.c:57:2: missed: statement clobbers memory: memcpy (_4, dense$baiases_63, _3);
Dense.c:79:2: missed: statement clobbers memory: _27 (pretmp_163, _68, 0);
Dense.c:54:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:54:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:179:107: note: ***** Analysis failed with vector mode V8SI
Dense.c:179:107: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
Dense.c:180:85: note: ***** Analysis failed with vector mode V8SI
Dense.c:180:85: note: ***** The result for vector mode V32QI would be the same
Dense.c:180:85: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:180:85: note: ***** Analysis failed with vector mode V16QI
Dense.c:180:85: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:180:85: note: ***** Analysis failed with vector mode V8QI
Dense.c:180:85: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:180:85: note: ***** Analysis failed with vector mode V4QI
Dense.c:192:19: missed: couldn't vectorize loop
Dense.c:192:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:182:13: note: vectorized 0 loops in function.
Dense.c:188:37: missed: statement clobbers memory: _9 = aligned_alloc (64, _8);
Dense.c:189:34: missed: statement clobbers memory: _11 = aligned_alloc (64, _10);
Dense.c:190:35: missed: statement clobbers memory: _12 = aligned_alloc (64, _10);
Dense.c:193:38: missed: statement clobbers memory: _13 = rand ();
Dense.c:193:72: missed: statement clobbers memory: _79 = sqrt (_20);
Dense.c:206:19: missed: statement clobbers memory: _31 = PULSE_GetActivationFunctionPtr (_30);
Dense.c:208:34: missed: statement clobbers memory: _33 = aligned_alloc (64, _10);
Dense.c:224:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:225:4: missed: statement clobbers memory: exit (1);
Dense.c:229:9: note: ***** Analysis succeeded with vector mode V8SI
Dense.c:229:9: note: SLPing BB part
Dense.c:229:9: note: Costing subgraph: 
Dense.c:229:9: note: node 0x8cce518 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:229:9: note: op template: <retval>.n_inputs = args$n_inputs_53;
Dense.c:229:9: note: 	stmt 0 <retval>.n_inputs = args$n_inputs_53;
Dense.c:229:9: note: 	stmt 1 <retval>.n_outputs = args$n_outputs_54;
Dense.c:229:9: note: 	children 0x8cce598
Dense.c:229:9: note: node 0x8cce598 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:229:9: note: op template: args$n_inputs_53 = args.n_inputs;
Dense.c:229:9: note: 	stmt 0 args$n_inputs_53 = args.n_inputs;
Dense.c:229:9: note: 	stmt 1 args$n_outputs_54 = args.n_outputs;
Dense.c:229:9: note: Cost model analysis: 
Dense.c:229:9: note: Cost model analysis for part in loop 0:
  Vector cost: 40
  Scalar cost: 56
Dense.c:229:9: note: Costing subgraph: 
Dense.c:229:9: note: node 0x8cce698 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:229:9: note: op template: <retval>.inputs = IO_42(D);
Dense.c:229:9: note: 	stmt 0 <retval>.inputs = IO_42(D);
Dense.c:229:9: note: 	stmt 1 <retval>.outputs = _29;
Dense.c:229:9: note: 	children 0x8cce718
Dense.c:229:9: note: node (external) 0x8cce718 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:229:9: note: 	{ IO_42(D), _29 }
Dense.c:229:9: note: Cost model analysis: 
Dense.c:229:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:229:9: note: Costing subgraph: 
Dense.c:229:9: note: node 0x8cce818 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:229:9: note: op template: <retval>.feed = layer$feed_41;
Dense.c:229:9: note: 	stmt 0 <retval>.feed = layer$feed_41;
Dense.c:229:9: note: 	stmt 1 <retval>.back = layer$back_51;
Dense.c:229:9: note: 	children 0x8ccea18
Dense.c:229:9: note: node (external) 0x8ccea18 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:229:9: note: 	{ layer$feed_41, layer$back_51 }
Dense.c:229:9: note: Cost model analysis: 
Dense.c:229:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:229:9: note: Costing subgraph: 
Dense.c:229:9: note: node 0x8cce918 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:229:9: note: op template: <retval>.parent = 0B;
Dense.c:229:9: note: 	stmt 0 <retval>.parent = 0B;
Dense.c:229:9: note: 	stmt 1 <retval>.child = 0B;
Dense.c:229:9: note: 	children 0x8ccea98
Dense.c:229:9: note: node (constant) 0x8ccea98 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:229:9: note: 	{ 0B, 0B }
Dense.c:229:9: note: Cost model analysis: 
Dense.c:229:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:229:9: note: Costing subgraph: 
Dense.c:229:9: note: node 0x8cceb98 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:229:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_37(D);
Dense.c:229:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_37(D);
Dense.c:229:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&<retval> + 96B] = _4;
Dense.c:229:9: note: 	stmt 2 MEM <float *> [(struct PULSE_Layer *)&<retval> + 104B] = _11;
Dense.c:229:9: note: 	stmt 3 MEM <float *> [(struct PULSE_Layer *)&<retval> + 112B] = _12;
Dense.c:229:9: note: 	children 0x8ccec18
Dense.c:229:9: note: node (external) 0x8ccec18 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:229:9: note: 	{ MODEL_37(D), _4, _11, _12 }
Dense.c:229:9: note: Cost model analysis: 
Dense.c:229:9: note: Cost model analysis for part in loop 0:
  Vector cost: 64
  Scalar cost: 64
Dense.c:229:9: note: Basic block will be vectorized using SLP
Dense.c:229:9: optimized: basic block part vectorized using 8 byte vectors
Dense.c:229:9: note: Vectorizing SLP tree:
Dense.c:229:9: note: node 0x8cce518 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:229:9: note: op template: <retval>.n_inputs = args$n_inputs_53;
Dense.c:229:9: note: 	stmt 0 <retval>.n_inputs = args$n_inputs_53;
Dense.c:229:9: note: 	stmt 1 <retval>.n_outputs = args$n_outputs_54;
Dense.c:229:9: note: 	children 0x8cce598
Dense.c:229:9: note: node 0x8cce598 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:229:9: note: op template: args$n_inputs_53 = args.n_inputs;
Dense.c:229:9: note: 	stmt 0 args$n_inputs_53 = args.n_inputs;
Dense.c:229:9: note: 	stmt 1 args$n_outputs_54 = args.n_outputs;
Dense.c:229:9: note: ------>vectorizing SLP node starting from: args$n_inputs_53 = args.n_inputs;
Dense.c:229:9: note: transform load. ncopies = 1
Dense.c:229:9: note: conflicting alias set types.
Dense.c:229:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: args.n_inputs
Dense.c:229:9: note: created &args
Dense.c:229:9: note: add new stmt: vect_args_n_inputs_53.504_88 = MEM <vector(2) unsigned int> [(void *)&args];
Dense.c:229:9: note: extracting lane for live stmt args$n_inputs_53 = args.n_inputs;
Dense.c:229:9: note: extracting lane for live stmt args$n_outputs_54 = args.n_outputs;
Dense.c:229:9: note: ------>vectorizing SLP node starting from: <retval>.n_inputs = args$n_inputs_53;
Dense.c:229:9: note: vect_is_simple_use: operand args.n_inputs, type of def: internal
Dense.c:229:9: note: vect_is_simple_use: operand args.n_outputs, type of def: internal
Dense.c:229:9: note: transform store. ncopies = 1
Dense.c:229:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: <retval>.n_inputs
Dense.c:229:9: note: created &<retval>.n_inputs
Dense.c:229:9: note: add new stmt: MEM <vector(2) unsigned int> [(unsigned int *)&<retval> + 128B] = vect_args_n_inputs_53.504_88;
Dense.c:229:9: note: vectorizing stmts using SLP.
Dense.c:229:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:229:9: note: Vectorizing SLP tree:
Dense.c:229:9: note: node 0x8cce698 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:229:9: note: op template: <retval>.inputs = IO_42(D);
Dense.c:229:9: note: 	stmt 0 <retval>.inputs = IO_42(D);
Dense.c:229:9: note: 	stmt 1 <retval>.outputs = _29;
Dense.c:229:9: note: 	children 0x8cce718
Dense.c:229:9: note: node (external) 0x8cce718 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:229:9: note: 	{ IO_42(D), _29 }
Dense.c:229:9: note: ------>vectorizing SLP node starting from: <retval>.inputs = IO_42(D);
Dense.c:229:9: note: vect_is_simple_use: operand IO_42(D) + _28, type of def: internal
Dense.c:229:9: note: transform store. ncopies = 1
Dense.c:229:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.inputs
Dense.c:229:9: note: created &<retval>
Dense.c:229:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval>] = _94;
Dense.c:229:9: note: vectorizing stmts using SLP.
Dense.c:229:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:229:9: note: Vectorizing SLP tree:
Dense.c:229:9: note: node 0x8cce818 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:229:9: note: op template: <retval>.feed = layer$feed_41;
Dense.c:229:9: note: 	stmt 0 <retval>.feed = layer$feed_41;
Dense.c:229:9: note: 	stmt 1 <retval>.back = layer$back_51;
Dense.c:229:9: note: 	children 0x8ccea18
Dense.c:229:9: note: node (external) 0x8ccea18 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:229:9: note: 	{ layer$feed_41, layer$back_51 }
Dense.c:229:9: note: ------>vectorizing SLP node starting from: <retval>.feed = layer$feed_41;
Dense.c:229:9: note: vect_is_simple_use: operand layer$back_51 = PHI <layer$back_57(D)(11), _BackDense(9), _SIMD_BackDense(15)>, type of def: internal
Dense.c:229:9: note: transform store. ncopies = 1
Dense.c:229:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.feed
Dense.c:229:9: note: created &<retval>.feed
Dense.c:229:9: note: add new stmt: MEM <vector(2) long unsigned int> [(void (*<T921>) (struct PULSE_Layer *) *)&<retval> + 32B] = _98;
Dense.c:229:9: note: vectorizing stmts using SLP.
Dense.c:229:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:229:9: note: Vectorizing SLP tree:
Dense.c:229:9: note: node 0x8cce918 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:229:9: note: op template: <retval>.parent = 0B;
Dense.c:229:9: note: 	stmt 0 <retval>.parent = 0B;
Dense.c:229:9: note: 	stmt 1 <retval>.child = 0B;
Dense.c:229:9: note: 	children 0x8ccea98
Dense.c:229:9: note: node (constant) 0x8ccea98 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:229:9: note: 	{ 0B, 0B }
Dense.c:229:9: note: ------>vectorizing SLP node starting from: <retval>.parent = 0B;
Dense.c:229:9: note: vect_is_simple_use: operand 0B, type of def: constant
Dense.c:229:9: note: transform store. ncopies = 1
Dense.c:229:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.parent
Dense.c:229:9: note: created &<retval>.parent
Dense.c:229:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer * *)&<retval> + 72B] = { 0, 0 };
Dense.c:229:9: note: vectorizing stmts using SLP.
Dense.c:229:9: optimized: basic block part vectorized using 32 byte vectors
Dense.c:229:9: note: Vectorizing SLP tree:
Dense.c:229:9: note: node 0x8cceb98 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:229:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_37(D);
Dense.c:229:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_37(D);
Dense.c:229:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&<retval> + 96B] = _4;
Dense.c:229:9: note: 	stmt 2 MEM <float *> [(struct PULSE_Layer *)&<retval> + 104B] = _11;
Dense.c:229:9: note: 	stmt 3 MEM <float *> [(struct PULSE_Layer *)&<retval> + 112B] = _12;
Dense.c:229:9: note: 	children 0x8ccec18
Dense.c:229:9: note: node (external) 0x8ccec18 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:229:9: note: 	{ MODEL_37(D), _4, _11, _12 }
Dense.c:229:9: note: ------>vectorizing SLP node starting from: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_37(D);
Dense.c:229:9: note: vect_is_simple_use: operand MODEL_37(D) + _3, type of def: internal
Dense.c:229:9: note: vect_is_simple_use: operand aligned_alloc (64, _10), type of def: internal
Dense.c:229:9: note: vect_is_simple_use: operand aligned_alloc (64, _10), type of def: internal
Dense.c:229:9: note: transform store. ncopies = 1
Dense.c:229:9: note: create vector_type-pointer variable to type: vector(4) long unsigned int  vectorizing a pointer ref: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B]
Dense.c:229:9: note: created &MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B]
Dense.c:229:9: note: add new stmt: MEM <vector(4) long unsigned int> [(struct PULSE_Layer *)&<retval> + 88B] = _105;
Dense.c:229:9: note: vectorizing stmts using SLP.
Dense.c:229:9: note: ***** The result for vector mode V32QI would be the same
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:4:18: note: vectorized 0 loops in function.
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_4, inputs_15, _3);
PULSE.c:8:2: missed: statement clobbers memory: _5 (layer_17);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_12, pretmp_26, _9);
PULSE.c:13:28: note: ***** Analysis failed with vector mode V8SI
PULSE.c:13:28: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:22:2: missed: statement clobbers memory: _1 (layer_10(D));
PULSE.c:22:2: missed: statement clobbers memory: _14 (_2);
PULSE.c:22:2: missed: statement clobbers memory: _21 (_15);
PULSE.c:22:2: missed: statement clobbers memory: _28 (_22);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_29);
PULSE.c:26:3: missed: statement clobbers memory: memset (_34, 0, _32);
PULSE.c:26:3: missed: statement clobbers memory: memset (_27, 0, _25);
PULSE.c:26:3: missed: statement clobbers memory: memset (_20, 0, _18);
PULSE.c:26:3: missed: statement clobbers memory: memset (_7, 0, _5);
PULSE.c:28:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:28:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:32:6: note: vectorized 1 loops in function.
PULSE.c:34:8: missed: statement clobbers memory: _1 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_2);
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _9 = rand ();
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8SI
PULSE.c:32:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:61:16: missed: couldn't vectorize loop
PULSE.c:61:16: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:64:17: missed: couldn't vectorize loop
PULSE.c:64:17: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:73:19: missed: couldn't vectorize loop
PULSE.c:73:19: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _69 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:53:22: missed: couldn't vectorize loop
PULSE.c:53:22: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:47:6: note: vectorized 1 loops in function.
PULSE.c:48:1: missed: statement clobbers memory: saved_stack.7_39 = __builtin_stack_save ();
PULSE.c:49:46: missed: statement clobbers memory: _1 = PULSE_GetLossFunctionPtr (loss_function_40(D));
PULSE.c:60:10: missed: statement clobbers memory: random.4_46 = __builtin_alloca_with_align (_4, 32);
PULSE.c:34:8: missed: statement clobbers memory: _61 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_62);
PULSE.c:39:29: missed: statement clobbers memory: _69 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _72 = rand ();
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_98, inputs_93, _97);
PULSE.c:8:2: missed: statement clobbers memory: _99 (layer_94);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_105, _104, _103);
PULSE.c:67:11: missed: statement clobbers memory: loss_55 = PULSE_GetLoss_42 (_17, _16, _12, _11);
PULSE.c:22:2: missed: statement clobbers memory: _86 (output_34);
PULSE.c:22:2: missed: statement clobbers memory: _108 (_87);
PULSE.c:22:2: missed: statement clobbers memory: _115 (_109);
PULSE.c:22:2: missed: statement clobbers memory: _122 (_116);
PULSE.c:22:2: missed: statement clobbers memory: _129 (_123);
PULSE.c:22:2: missed: statement clobbers memory: _136 (_130);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_137);
PULSE.c:26:3: missed: statement clobbers memory: memset (_142, 0, _140);
PULSE.c:26:3: missed: statement clobbers memory: memset (_135, 0, _133);
PULSE.c:26:3: missed: statement clobbers memory: memset (_128, 0, _126);
PULSE.c:26:3: missed: statement clobbers memory: memset (_121, 0, _119);
PULSE.c:26:3: missed: statement clobbers memory: memset (_114, 0, _112);
PULSE.c:26:3: missed: statement clobbers memory: memset (_92, 0, _90);
PULSE.c:75:6: missed: statement clobbers memory: _21 (current_191, args);
PULSE.c:81:4: missed: statement clobbers memory: printf ("Epoch: %d | Item: %d | Loss: %.10f | Batch Loss: %.10f\r", i_188, j_189, _25, _24);
PULSE.c:47:6: missed: statement clobbers memory: __builtin_stack_restore (saved_stack.7_39);
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4DI
PULSE.c:47:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:91:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:91:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:123:20: missed: couldn't vectorize loop
PULSE.c:123:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:123:20: missed: couldn't vectorize loop
PULSE.c:123:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:103:20: missed: couldn't vectorize loop
PULSE.c:103:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:103:20: missed: couldn't vectorize loop
PULSE.c:103:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:93:13: note: vectorized 0 loops in function.
PULSE.c:96:39: missed: statement clobbers memory: layers_39 = malloc (_3);
PULSE.c:99:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:116:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:117:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:118:46: missed: statement clobbers memory: WEIGHTS_56 = aligned_alloc (64, 0);
PULSE.c:109:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.105_85];
PULSE.c:110:21: missed: statement clobbers memory: _70 = PULSE_GetDenseWeightsSize (args);
PULSE.c:111:16: missed: statement clobbers memory: _73 = PULSE_GetDenseIOSize (args);
PULSE.c:109:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})_208];
PULSE.c:110:21: missed: statement clobbers memory: _197 = PULSE_GetDenseWeightsSize (args);
PULSE.c:111:16: missed: statement clobbers memory: _200 = PULSE_GetDenseIOSize (args);
PULSE.c:116:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:117:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:118:46: missed: statement clobbers memory: WEIGHTS_44 = aligned_alloc (64, _5);
PULSE.c:119:41: missed: statement clobbers memory: IO_46 = aligned_alloc (64, _7);
PULSE.c:129:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.107_87];
PULSE.c:130:17: missed: statement clobbers memory: *_10 = PULSE_CreateDenseLayer (args, WEIGHTS_PTR_141, IO_PTR_142); [return slot optimization]
PULSE.c:131:20: missed: statement clobbers memory: _59 = PULSE_GetDenseWeightsSize (args);
PULSE.c:132:15: missed: statement clobbers memory: _62 = PULSE_GetDenseIOSize (args);
PULSE.c:129:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.107_127];
PULSE.c:130:17: missed: statement clobbers memory: *_109 = PULSE_CreateDenseLayer (args, WEIGHTS_PTR_149, IO_PTR_148); [return slot optimization]
PULSE.c:131:20: missed: statement clobbers memory: _97 = PULSE_GetDenseWeightsSize (args);
PULSE.c:132:15: missed: statement clobbers memory: _82 = PULSE_GetDenseIOSize (args);
PULSE.c:138:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:139:9: note: ***** Analysis succeeded with vector mode V8SI
PULSE.c:139:9: note: SLPing BB part
PULSE.c:139:9: note: Costing subgraph: 
PULSE.c:139:9: note: node 0x3253468 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:139:9: note: op template: <retval>.layers = layers_39;
PULSE.c:139:9: note: 	stmt 0 <retval>.layers = layers_39;
PULSE.c:139:9: note: 	stmt 1 <retval>.weights = WEIGHTS_153;
PULSE.c:139:9: note: 	children 0x3253568
PULSE.c:139:9: note: node (external) 0x3253568 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:139:9: note: 	{ layers_39, WEIGHTS_153 }
PULSE.c:139:9: note: Cost model analysis: 
PULSE.c:139:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:139:9: note: Costing subgraph: 
PULSE.c:139:9: note: node 0x32535e8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:139:9: note: op template: <retval>.io = 0B;
PULSE.c:139:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:139:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:139:9: note: 	children 0x3253668
PULSE.c:139:9: note: node (constant) 0x3253668 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:139:9: note: 	{ 0B, 0B }
PULSE.c:139:9: note: Cost model analysis: 
PULSE.c:139:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:139:9: note: Basic block will be vectorized using SLP
PULSE.c:139:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:139:9: note: Vectorizing SLP tree:
PULSE.c:139:9: note: node 0x3253468 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:139:9: note: op template: <retval>.layers = layers_39;
PULSE.c:139:9: note: 	stmt 0 <retval>.layers = layers_39;
PULSE.c:139:9: note: 	stmt 1 <retval>.weights = WEIGHTS_153;
PULSE.c:139:9: note: 	children 0x3253568
PULSE.c:139:9: note: node (external) 0x3253568 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:139:9: note: 	{ layers_39, WEIGHTS_153 }
PULSE.c:139:9: note: ------>vectorizing SLP node starting from: <retval>.layers = layers_39;
PULSE.c:139:9: note: vect_is_simple_use: operand WEIGHTS_153 = PHI <WEIGHTS_44(36), WEIGHTS_56(25)>, type of def: internal
PULSE.c:139:9: note: conflicting alias set types.
PULSE.c:139:9: note: transform store. ncopies = 1
PULSE.c:139:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.layers
PULSE.c:139:9: note: created &<retval>
PULSE.c:139:9: note: add new stmt: MEM <vector(2) long unsigned int> [(void *)&<retval>] = _18;
PULSE.c:139:9: note: vectorizing stmts using SLP.
PULSE.c:139:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:139:9: note: Vectorizing SLP tree:
PULSE.c:139:9: note: node 0x32535e8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:139:9: note: op template: <retval>.io = 0B;
PULSE.c:139:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:139:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:139:9: note: 	children 0x3253668
PULSE.c:139:9: note: node (constant) 0x3253668 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:139:9: note: 	{ 0B, 0B }
PULSE.c:139:9: note: ------>vectorizing SLP node starting from: <retval>.io = 0B;
PULSE.c:139:9: note: vect_is_simple_use: operand 0B, type of def: constant
PULSE.c:139:9: note: transform store. ncopies = 1
PULSE.c:139:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.io
PULSE.c:139:9: note: created &<retval>.io
PULSE.c:139:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval> + 16B] = { 0, 0 };
PULSE.c:139:9: note: vectorizing stmts using SLP.
PULSE.c:139:9: note: ***** The result for vector mode V32QI would be the same
PULSE.c:147:3: missed: statement clobbers memory: free (_1);
PULSE.c:153:3: missed: statement clobbers memory: free (prephitmp_16);
PULSE.c:159:3: missed: statement clobbers memory: free (_4);
PULSE.c:162:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:162:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _28 (pretmp_91, _51, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:19: missed: not vectorized: no vectype for stmt: _22 = *_21;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:170:10: optimized: loop vectorized using 32 byte vectors
Dense.c:170:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:170:10: optimized: loop vectorized using 16 byte vectors
Dense.c:161:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
Dense.c:154:10: optimized: loop vectorized using 32 byte vectors
Dense.c:154:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:154:10: optimized: loop vectorized using 16 byte vectors
Dense.c:145:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
Dense.c:136:13: note: vectorized 2 loops in function.
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_28, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
Dense.c:136:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:136:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:105:24: missed: couldn't vectorize loop
Dense.c:105:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:108:17: missed: couldn't vectorize loop
Dense.c:108:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:122:24: missed: couldn't vectorize loop
Dense.c:122:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:125:17: missed: couldn't vectorize loop
Dense.c:125:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:90:15: missed: couldn't vectorize loop
Dense.c:90:15: missed: not vectorized: number of iterations cannot be computed.
Dense.c:83:13: note: vectorized 0 loops in function.
Dense.c:86:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:110:17: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:110:17: note: SLPing BB part
Dense.c:127:17: note: Costing subgraph: 
Dense.c:127:17: note: node 0x8dfd9a8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:127:17: note: op template: _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 0 _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 1 _198 = (sizetype) wi_137;
Dense.c:127:17: note: 	children 0x8dfdaa8
Dense.c:127:17: note: node (external) 0x8dfdaa8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:127:17: note: 	{ j_135, wi_137 }
Dense.c:127:17: note: Cost model analysis: 
Dense.c:127:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:127:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:127:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: Costing subgraph: 
Dense.c:110:17: note: node 0x8dfdba8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:110:17: note: op template: _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 0 _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 1 _217 = (sizetype) wi_136;
Dense.c:110:17: note: 	children 0x8dfdca8
Dense.c:110:17: note: node (external) 0x8dfdca8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:110:17: note: 	{ j_134, wi_136 }
Dense.c:110:17: note: Cost model analysis: 
Dense.c:110:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:110:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:110:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: ***** The result for vector mode V32QI would be the same
Dense.c:110:17: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:110:17: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:110:17: note: SLPing BB part
Dense.c:127:17: note: Costing subgraph: 
Dense.c:127:17: note: node 0x8dfdba8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:127:17: note: op template: _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 0 _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 1 _198 = (sizetype) wi_137;
Dense.c:127:17: note: 	children 0x8dfd9a8
Dense.c:127:17: note: node (external) 0x8dfd9a8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:127:17: note: 	{ j_135, wi_137 }
Dense.c:127:17: note: Cost model analysis: 
Dense.c:127:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:127:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:127:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: Costing subgraph: 
Dense.c:110:17: note: node 0x8dfdb28 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:110:17: note: op template: _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 0 _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 1 _217 = (sizetype) wi_136;
Dense.c:110:17: note: 	children 0x8dfdc28
Dense.c:110:17: note: node (external) 0x8dfdc28 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:110:17: note: 	{ j_134, wi_136 }
Dense.c:110:17: note: Cost model analysis: 
Dense.c:110:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:110:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:110:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:105:51: note: ***** Analysis failed with vector mode V8QI
Dense.c:105:51: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:105:51: note: ***** Analysis failed with vector mode V4QI
Dense.c:62:15: missed: couldn't vectorize loop
Dense.c:62:15: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:75:11: optimized: loop vectorized using 32 byte vectors
Dense.c:75:11: optimized: loop vectorized using 16 byte vectors
Dense.c:66:11: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:881:10: missed: not vectorized: no vectype for stmt: _53 = MEM[(__m256 * {ref-all})w_ptr_82];
 scalar_type: __m256
Dense.c:54:13: note: vectorized 1 loops in function.
Dense.c:57:2: missed: statement clobbers memory: memcpy (_4, dense$baiases_63, _3);
Dense.c:79:2: missed: statement clobbers memory: _27 (pretmp_163, _68, 0);
Dense.c:54:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:54:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:179:107: note: ***** Analysis failed with vector mode V8SI
Dense.c:179:107: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
Dense.c:180:85: note: ***** Analysis failed with vector mode V8SI
Dense.c:180:85: note: ***** The result for vector mode V32QI would be the same
Dense.c:180:85: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:180:85: note: ***** Analysis failed with vector mode V16QI
Dense.c:180:85: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:180:85: note: ***** Analysis failed with vector mode V8QI
Dense.c:180:85: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:180:85: note: ***** Analysis failed with vector mode V4QI
Dense.c:192:19: missed: couldn't vectorize loop
Dense.c:192:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:182:13: note: vectorized 0 loops in function.
Dense.c:188:37: missed: statement clobbers memory: _9 = aligned_alloc (64, _8);
Dense.c:189:34: missed: statement clobbers memory: _11 = aligned_alloc (64, _10);
Dense.c:190:35: missed: statement clobbers memory: _12 = aligned_alloc (64, _10);
Dense.c:193:38: missed: statement clobbers memory: _13 = rand ();
Dense.c:193:72: missed: statement clobbers memory: _79 = sqrt (_20);
Dense.c:206:19: missed: statement clobbers memory: _31 = PULSE_GetActivationFunctionPtr (_30);
Dense.c:208:34: missed: statement clobbers memory: _33 = aligned_alloc (64, _10);
Dense.c:224:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:225:4: missed: statement clobbers memory: exit (1);
Dense.c:229:9: note: ***** Analysis succeeded with vector mode V8SI
Dense.c:229:9: note: SLPing BB part
Dense.c:229:9: note: Costing subgraph: 
Dense.c:229:9: note: node 0x8c82178 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:229:9: note: op template: <retval>.n_inputs = args$n_inputs_53;
Dense.c:229:9: note: 	stmt 0 <retval>.n_inputs = args$n_inputs_53;
Dense.c:229:9: note: 	stmt 1 <retval>.n_outputs = args$n_outputs_54;
Dense.c:229:9: note: 	children 0x8c821f8
Dense.c:229:9: note: node 0x8c821f8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:229:9: note: op template: args$n_inputs_53 = args.n_inputs;
Dense.c:229:9: note: 	stmt 0 args$n_inputs_53 = args.n_inputs;
Dense.c:229:9: note: 	stmt 1 args$n_outputs_54 = args.n_outputs;
Dense.c:229:9: note: Cost model analysis: 
Dense.c:229:9: note: Cost model analysis for part in loop 0:
  Vector cost: 40
  Scalar cost: 56
Dense.c:229:9: note: Costing subgraph: 
Dense.c:229:9: note: node 0x8c822f8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:229:9: note: op template: <retval>.inputs = IO_42(D);
Dense.c:229:9: note: 	stmt 0 <retval>.inputs = IO_42(D);
Dense.c:229:9: note: 	stmt 1 <retval>.outputs = _29;
Dense.c:229:9: note: 	children 0x8c82378
Dense.c:229:9: note: node (external) 0x8c82378 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:229:9: note: 	{ IO_42(D), _29 }
Dense.c:229:9: note: Cost model analysis: 
Dense.c:229:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:229:9: note: Costing subgraph: 
Dense.c:229:9: note: node 0x8c82478 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:229:9: note: op template: <retval>.feed = layer$feed_41;
Dense.c:229:9: note: 	stmt 0 <retval>.feed = layer$feed_41;
Dense.c:229:9: note: 	stmt 1 <retval>.back = layer$back_51;
Dense.c:229:9: note: 	children 0x8c82678
Dense.c:229:9: note: node (external) 0x8c82678 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:229:9: note: 	{ layer$feed_41, layer$back_51 }
Dense.c:229:9: note: Cost model analysis: 
Dense.c:229:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:229:9: note: Costing subgraph: 
Dense.c:229:9: note: node 0x8c82578 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:229:9: note: op template: <retval>.parent = 0B;
Dense.c:229:9: note: 	stmt 0 <retval>.parent = 0B;
Dense.c:229:9: note: 	stmt 1 <retval>.child = 0B;
Dense.c:229:9: note: 	children 0x8c826f8
Dense.c:229:9: note: node (constant) 0x8c826f8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:229:9: note: 	{ 0B, 0B }
Dense.c:229:9: note: Cost model analysis: 
Dense.c:229:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:229:9: note: Costing subgraph: 
Dense.c:229:9: note: node 0x8c827f8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:229:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_37(D);
Dense.c:229:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_37(D);
Dense.c:229:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&<retval> + 96B] = _4;
Dense.c:229:9: note: 	stmt 2 MEM <float *> [(struct PULSE_Layer *)&<retval> + 104B] = _11;
Dense.c:229:9: note: 	stmt 3 MEM <float *> [(struct PULSE_Layer *)&<retval> + 112B] = _12;
Dense.c:229:9: note: 	children 0x8c82878
Dense.c:229:9: note: node (external) 0x8c82878 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:229:9: note: 	{ MODEL_37(D), _4, _11, _12 }
Dense.c:229:9: note: Cost model analysis: 
Dense.c:229:9: note: Cost model analysis for part in loop 0:
  Vector cost: 64
  Scalar cost: 64
Dense.c:229:9: note: Basic block will be vectorized using SLP
Dense.c:229:9: optimized: basic block part vectorized using 8 byte vectors
Dense.c:229:9: note: Vectorizing SLP tree:
Dense.c:229:9: note: node 0x8c82178 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:229:9: note: op template: <retval>.n_inputs = args$n_inputs_53;
Dense.c:229:9: note: 	stmt 0 <retval>.n_inputs = args$n_inputs_53;
Dense.c:229:9: note: 	stmt 1 <retval>.n_outputs = args$n_outputs_54;
Dense.c:229:9: note: 	children 0x8c821f8
Dense.c:229:9: note: node 0x8c821f8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:229:9: note: op template: args$n_inputs_53 = args.n_inputs;
Dense.c:229:9: note: 	stmt 0 args$n_inputs_53 = args.n_inputs;
Dense.c:229:9: note: 	stmt 1 args$n_outputs_54 = args.n_outputs;
Dense.c:229:9: note: ------>vectorizing SLP node starting from: args$n_inputs_53 = args.n_inputs;
Dense.c:229:9: note: transform load. ncopies = 1
Dense.c:229:9: note: conflicting alias set types.
Dense.c:229:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: args.n_inputs
Dense.c:229:9: note: created &args
Dense.c:229:9: note: add new stmt: vect_args_n_inputs_53.504_88 = MEM <vector(2) unsigned int> [(void *)&args];
Dense.c:229:9: note: extracting lane for live stmt args$n_inputs_53 = args.n_inputs;
Dense.c:229:9: note: extracting lane for live stmt args$n_outputs_54 = args.n_outputs;
Dense.c:229:9: note: ------>vectorizing SLP node starting from: <retval>.n_inputs = args$n_inputs_53;
Dense.c:229:9: note: vect_is_simple_use: operand args.n_inputs, type of def: internal
Dense.c:229:9: note: vect_is_simple_use: operand args.n_outputs, type of def: internal
Dense.c:229:9: note: transform store. ncopies = 1
Dense.c:229:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: <retval>.n_inputs
Dense.c:229:9: note: created &<retval>.n_inputs
Dense.c:229:9: note: add new stmt: MEM <vector(2) unsigned int> [(unsigned int *)&<retval> + 128B] = vect_args_n_inputs_53.504_88;
Dense.c:229:9: note: vectorizing stmts using SLP.
Dense.c:229:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:229:9: note: Vectorizing SLP tree:
Dense.c:229:9: note: node 0x8c822f8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:229:9: note: op template: <retval>.inputs = IO_42(D);
Dense.c:229:9: note: 	stmt 0 <retval>.inputs = IO_42(D);
Dense.c:229:9: note: 	stmt 1 <retval>.outputs = _29;
Dense.c:229:9: note: 	children 0x8c82378
Dense.c:229:9: note: node (external) 0x8c82378 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:229:9: note: 	{ IO_42(D), _29 }
Dense.c:229:9: note: ------>vectorizing SLP node starting from: <retval>.inputs = IO_42(D);
Dense.c:229:9: note: vect_is_simple_use: operand IO_42(D) + _28, type of def: internal
Dense.c:229:9: note: transform store. ncopies = 1
Dense.c:229:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.inputs
Dense.c:229:9: note: created &<retval>
Dense.c:229:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval>] = _94;
Dense.c:229:9: note: vectorizing stmts using SLP.
Dense.c:229:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:229:9: note: Vectorizing SLP tree:
Dense.c:229:9: note: node 0x8c82478 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:229:9: note: op template: <retval>.feed = layer$feed_41;
Dense.c:229:9: note: 	stmt 0 <retval>.feed = layer$feed_41;
Dense.c:229:9: note: 	stmt 1 <retval>.back = layer$back_51;
Dense.c:229:9: note: 	children 0x8c82678
Dense.c:229:9: note: node (external) 0x8c82678 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:229:9: note: 	{ layer$feed_41, layer$back_51 }
Dense.c:229:9: note: ------>vectorizing SLP node starting from: <retval>.feed = layer$feed_41;
Dense.c:229:9: note: vect_is_simple_use: operand layer$back_51 = PHI <layer$back_57(D)(11), _BackDense(9), _SIMD_BackDense(15)>, type of def: internal
Dense.c:229:9: note: transform store. ncopies = 1
Dense.c:229:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.feed
Dense.c:229:9: note: created &<retval>.feed
Dense.c:229:9: note: add new stmt: MEM <vector(2) long unsigned int> [(void (*<T921>) (struct PULSE_Layer *) *)&<retval> + 32B] = _98;
Dense.c:229:9: note: vectorizing stmts using SLP.
Dense.c:229:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:229:9: note: Vectorizing SLP tree:
Dense.c:229:9: note: node 0x8c82578 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:229:9: note: op template: <retval>.parent = 0B;
Dense.c:229:9: note: 	stmt 0 <retval>.parent = 0B;
Dense.c:229:9: note: 	stmt 1 <retval>.child = 0B;
Dense.c:229:9: note: 	children 0x8c826f8
Dense.c:229:9: note: node (constant) 0x8c826f8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:229:9: note: 	{ 0B, 0B }
Dense.c:229:9: note: ------>vectorizing SLP node starting from: <retval>.parent = 0B;
Dense.c:229:9: note: vect_is_simple_use: operand 0B, type of def: constant
Dense.c:229:9: note: transform store. ncopies = 1
Dense.c:229:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.parent
Dense.c:229:9: note: created &<retval>.parent
Dense.c:229:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer * *)&<retval> + 72B] = { 0, 0 };
Dense.c:229:9: note: vectorizing stmts using SLP.
Dense.c:229:9: optimized: basic block part vectorized using 32 byte vectors
Dense.c:229:9: note: Vectorizing SLP tree:
Dense.c:229:9: note: node 0x8c827f8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:229:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_37(D);
Dense.c:229:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_37(D);
Dense.c:229:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&<retval> + 96B] = _4;
Dense.c:229:9: note: 	stmt 2 MEM <float *> [(struct PULSE_Layer *)&<retval> + 104B] = _11;
Dense.c:229:9: note: 	stmt 3 MEM <float *> [(struct PULSE_Layer *)&<retval> + 112B] = _12;
Dense.c:229:9: note: 	children 0x8c82878
Dense.c:229:9: note: node (external) 0x8c82878 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:229:9: note: 	{ MODEL_37(D), _4, _11, _12 }
Dense.c:229:9: note: ------>vectorizing SLP node starting from: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_37(D);
Dense.c:229:9: note: vect_is_simple_use: operand MODEL_37(D) + _3, type of def: internal
Dense.c:229:9: note: vect_is_simple_use: operand aligned_alloc (64, _10), type of def: internal
Dense.c:229:9: note: vect_is_simple_use: operand aligned_alloc (64, _10), type of def: internal
Dense.c:229:9: note: transform store. ncopies = 1
Dense.c:229:9: note: create vector_type-pointer variable to type: vector(4) long unsigned int  vectorizing a pointer ref: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B]
Dense.c:229:9: note: created &MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B]
Dense.c:229:9: note: add new stmt: MEM <vector(4) long unsigned int> [(struct PULSE_Layer *)&<retval> + 88B] = _105;
Dense.c:229:9: note: vectorizing stmts using SLP.
Dense.c:229:9: note: ***** The result for vector mode V32QI would be the same
Activations.c:17:19: optimized: loop vectorized using 32 byte vectors
Activations.c:17:19: optimized: loop vectorized using 16 byte vectors
Activations.c:17:19: optimized: loop vectorized using 32 byte vectors
Activations.c:17:19: optimized: loop vectorized using 16 byte vectors
Activations.c:14:13: note: vectorized 2 loops in function.
Activations.c:14:13: note: ***** Analysis failed with vector mode V8SF
Activations.c:14:13: note: ***** The result for vector mode V32QI would be the same
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V16QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V16QI
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V8QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V8QI
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V4QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V4QI
Activations.c:28:19: missed: couldn't vectorize loop
Activations.c:28:19: missed: not vectorized: unsupported control flow in loop.
Activations.c:28:19: optimized: loop vectorized using 32 byte vectors
Activations.c:28:19: optimized: loop vectorized using 16 byte vectors
Activations.c:25:13: note: vectorized 1 loops in function.
Activations.c:25:13: note: ***** Analysis failed with vector mode V8SF
Activations.c:25:13: note: ***** The result for vector mode V32QI would be the same
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V16QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V16QI
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V8QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V8QI
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V4QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V4QI
Activations.c:38:1: note: ***** Analysis failed with vector mode VOID
Activations.c:5:19: missed: couldn't vectorize loop
Activations.c:9:21: missed: statement clobbers memory: _36 = expf (_18);
Activations.c:5:19: missed: couldn't vectorize loop
Activations.c:9:21: missed: statement clobbers memory: _12 = expf (_11);
Activations.c:3:13: note: vectorized 0 loops in function.
Activations.c:9:21: missed: statement clobbers memory: _12 = expf (_11);
Activations.c:9:21: missed: statement clobbers memory: _36 = expf (_18);
Activations.c:11:1: note: ***** Analysis failed with vector mode V8SF
Activations.c:11:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SF
Activations.c:44:2: note: ***** Analysis failed with vector mode VOID
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:4:18: note: vectorized 0 loops in function.
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_4, inputs_15, _3);
PULSE.c:8:2: missed: statement clobbers memory: _5 (layer_17);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_12, pretmp_26, _9);
PULSE.c:13:28: note: ***** Analysis failed with vector mode V8SI
PULSE.c:13:28: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:22:2: missed: statement clobbers memory: _1 (layer_10(D));
PULSE.c:22:2: missed: statement clobbers memory: _14 (_2);
PULSE.c:22:2: missed: statement clobbers memory: _21 (_15);
PULSE.c:22:2: missed: statement clobbers memory: _28 (_22);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_29);
PULSE.c:26:3: missed: statement clobbers memory: memset (_34, 0, _32);
PULSE.c:26:3: missed: statement clobbers memory: memset (_27, 0, _25);
PULSE.c:26:3: missed: statement clobbers memory: memset (_20, 0, _18);
PULSE.c:26:3: missed: statement clobbers memory: memset (_7, 0, _5);
PULSE.c:28:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:28:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:32:6: note: vectorized 1 loops in function.
PULSE.c:34:8: missed: statement clobbers memory: _1 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_2);
PULSE.c:39:29: missed: statement clobbers memory: _7 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _9 = rand ();
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8SI
PULSE.c:32:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:61:16: missed: couldn't vectorize loop
PULSE.c:61:16: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:64:17: missed: couldn't vectorize loop
PULSE.c:64:17: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:73:19: missed: couldn't vectorize loop
PULSE.c:73:19: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:29: missed: statement clobbers memory: _69 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:53:22: missed: couldn't vectorize loop
PULSE.c:53:22: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:47:6: note: vectorized 1 loops in function.
PULSE.c:48:1: missed: statement clobbers memory: saved_stack.7_39 = __builtin_stack_save ();
PULSE.c:49:46: missed: statement clobbers memory: _1 = PULSE_GetLossFunctionPtr (loss_function_40(D));
PULSE.c:60:10: missed: statement clobbers memory: random.4_46 = __builtin_alloca_with_align (_4, 32);
PULSE.c:34:8: missed: statement clobbers memory: _61 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_62);
PULSE.c:39:29: missed: statement clobbers memory: _69 = rand ();
PULSE.c:40:30: missed: statement clobbers memory: _72 = rand ();
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_98, inputs_93, _97);
PULSE.c:8:2: missed: statement clobbers memory: _99 (layer_94);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_105, _104, _103);
PULSE.c:67:11: missed: statement clobbers memory: loss_55 = PULSE_GetLoss_42 (_17, _16, _12, _11);
PULSE.c:22:2: missed: statement clobbers memory: _86 (output_34);
PULSE.c:22:2: missed: statement clobbers memory: _108 (_87);
PULSE.c:22:2: missed: statement clobbers memory: _115 (_109);
PULSE.c:22:2: missed: statement clobbers memory: _122 (_116);
PULSE.c:22:2: missed: statement clobbers memory: _129 (_123);
PULSE.c:22:2: missed: statement clobbers memory: _136 (_130);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_137);
PULSE.c:26:3: missed: statement clobbers memory: memset (_142, 0, _140);
PULSE.c:26:3: missed: statement clobbers memory: memset (_135, 0, _133);
PULSE.c:26:3: missed: statement clobbers memory: memset (_128, 0, _126);
PULSE.c:26:3: missed: statement clobbers memory: memset (_121, 0, _119);
PULSE.c:26:3: missed: statement clobbers memory: memset (_114, 0, _112);
PULSE.c:26:3: missed: statement clobbers memory: memset (_92, 0, _90);
PULSE.c:75:6: missed: statement clobbers memory: _21 (current_191, args);
PULSE.c:81:4: missed: statement clobbers memory: printf ("Epoch: %d | Item: %d | Loss: %.10f | Batch Loss: %.10f\r", i_188, j_189, _25, _24);
PULSE.c:47:6: missed: statement clobbers memory: __builtin_stack_restore (saved_stack.7_39);
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4DI
PULSE.c:47:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:91:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:91:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:123:20: missed: couldn't vectorize loop
PULSE.c:123:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:123:20: missed: couldn't vectorize loop
PULSE.c:123:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:103:20: missed: couldn't vectorize loop
PULSE.c:103:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:103:20: missed: couldn't vectorize loop
PULSE.c:103:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:93:13: note: vectorized 0 loops in function.
PULSE.c:96:39: missed: statement clobbers memory: layers_39 = malloc (_3);
PULSE.c:99:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:116:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:117:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:118:46: missed: statement clobbers memory: WEIGHTS_56 = aligned_alloc (64, 0);
PULSE.c:109:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.105_85];
PULSE.c:110:21: missed: statement clobbers memory: _70 = PULSE_GetDenseWeightsSize (args);
PULSE.c:111:16: missed: statement clobbers memory: _73 = PULSE_GetDenseIOSize (args);
PULSE.c:109:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})_208];
PULSE.c:110:21: missed: statement clobbers memory: _197 = PULSE_GetDenseWeightsSize (args);
PULSE.c:111:16: missed: statement clobbers memory: _200 = PULSE_GetDenseIOSize (args);
PULSE.c:116:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:117:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:118:46: missed: statement clobbers memory: WEIGHTS_44 = aligned_alloc (64, _5);
PULSE.c:119:41: missed: statement clobbers memory: IO_46 = aligned_alloc (64, _7);
PULSE.c:129:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.107_87];
PULSE.c:130:17: missed: statement clobbers memory: *_10 = PULSE_CreateDenseLayer (args, WEIGHTS_PTR_141, IO_PTR_142); [return slot optimization]
PULSE.c:131:20: missed: statement clobbers memory: _59 = PULSE_GetDenseWeightsSize (args);
PULSE.c:132:15: missed: statement clobbers memory: _62 = PULSE_GetDenseIOSize (args);
PULSE.c:129:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.107_127];
PULSE.c:130:17: missed: statement clobbers memory: *_109 = PULSE_CreateDenseLayer (args, WEIGHTS_PTR_149, IO_PTR_148); [return slot optimization]
PULSE.c:131:20: missed: statement clobbers memory: _97 = PULSE_GetDenseWeightsSize (args);
PULSE.c:132:15: missed: statement clobbers memory: _82 = PULSE_GetDenseIOSize (args);
PULSE.c:138:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:139:9: note: ***** Analysis succeeded with vector mode V8SI
PULSE.c:139:9: note: SLPing BB part
PULSE.c:139:9: note: Costing subgraph: 
PULSE.c:139:9: note: node 0x3b70468 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:139:9: note: op template: <retval>.layers = layers_39;
PULSE.c:139:9: note: 	stmt 0 <retval>.layers = layers_39;
PULSE.c:139:9: note: 	stmt 1 <retval>.weights = WEIGHTS_153;
PULSE.c:139:9: note: 	children 0x3b70568
PULSE.c:139:9: note: node (external) 0x3b70568 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:139:9: note: 	{ layers_39, WEIGHTS_153 }
PULSE.c:139:9: note: Cost model analysis: 
PULSE.c:139:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:139:9: note: Costing subgraph: 
PULSE.c:139:9: note: node 0x3b705e8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:139:9: note: op template: <retval>.io = 0B;
PULSE.c:139:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:139:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:139:9: note: 	children 0x3b70668
PULSE.c:139:9: note: node (constant) 0x3b70668 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:139:9: note: 	{ 0B, 0B }
PULSE.c:139:9: note: Cost model analysis: 
PULSE.c:139:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:139:9: note: Basic block will be vectorized using SLP
PULSE.c:139:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:139:9: note: Vectorizing SLP tree:
PULSE.c:139:9: note: node 0x3b70468 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:139:9: note: op template: <retval>.layers = layers_39;
PULSE.c:139:9: note: 	stmt 0 <retval>.layers = layers_39;
PULSE.c:139:9: note: 	stmt 1 <retval>.weights = WEIGHTS_153;
PULSE.c:139:9: note: 	children 0x3b70568
PULSE.c:139:9: note: node (external) 0x3b70568 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:139:9: note: 	{ layers_39, WEIGHTS_153 }
PULSE.c:139:9: note: ------>vectorizing SLP node starting from: <retval>.layers = layers_39;
PULSE.c:139:9: note: vect_is_simple_use: operand WEIGHTS_153 = PHI <WEIGHTS_44(36), WEIGHTS_56(25)>, type of def: internal
PULSE.c:139:9: note: conflicting alias set types.
PULSE.c:139:9: note: transform store. ncopies = 1
PULSE.c:139:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.layers
PULSE.c:139:9: note: created &<retval>
PULSE.c:139:9: note: add new stmt: MEM <vector(2) long unsigned int> [(void *)&<retval>] = _18;
PULSE.c:139:9: note: vectorizing stmts using SLP.
PULSE.c:139:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:139:9: note: Vectorizing SLP tree:
PULSE.c:139:9: note: node 0x3b705e8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:139:9: note: op template: <retval>.io = 0B;
PULSE.c:139:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:139:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:139:9: note: 	children 0x3b70668
PULSE.c:139:9: note: node (constant) 0x3b70668 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:139:9: note: 	{ 0B, 0B }
PULSE.c:139:9: note: ------>vectorizing SLP node starting from: <retval>.io = 0B;
PULSE.c:139:9: note: vect_is_simple_use: operand 0B, type of def: constant
PULSE.c:139:9: note: transform store. ncopies = 1
PULSE.c:139:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.io
PULSE.c:139:9: note: created &<retval>.io
PULSE.c:139:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval> + 16B] = { 0, 0 };
PULSE.c:139:9: note: vectorizing stmts using SLP.
PULSE.c:139:9: note: ***** The result for vector mode V32QI would be the same
PULSE.c:147:3: missed: statement clobbers memory: free (_1);
PULSE.c:153:3: missed: statement clobbers memory: free (prephitmp_16);
PULSE.c:159:3: missed: statement clobbers memory: free (_4);
PULSE.c:162:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:162:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Loss.c:15:19: optimized: loop vectorized using 32 byte vectors
Loss.c:15:19: optimized:  loop versioned for vectorization because of possible aliasing
Loss.c:15:19: optimized: loop vectorized using 16 byte vectors
Loss.c:12:23: note: vectorized 1 loops in function.
Loss.c:12:23: note: ***** Analysis failed with vector mode V8SF
Loss.c:12:23: note: ***** The result for vector mode V32QI would be the same
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V16QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V16QI
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V8QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V8QI
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V4QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V4QI
Loss.c:7:19: missed: couldn't vectorize loop
Loss.c:8:38: missed: not vectorized: unsupported use in stmt.
Loss.c:4:23: note: vectorized 0 loops in function.
Loss.c:9:13: note: ***** Analysis failed with vector mode V8SF
Loss.c:9:13: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SF
Loss.c:23:2: note: ***** Analysis failed with vector mode VOID
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _28 (pretmp_91, _51, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:19: missed: not vectorized: no vectype for stmt: _22 = *_21;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:170:10: optimized: loop vectorized using 32 byte vectors
Dense.c:170:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:170:10: optimized: loop vectorized using 16 byte vectors
Dense.c:161:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
Dense.c:154:10: optimized: loop vectorized using 32 byte vectors
Dense.c:154:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:154:10: optimized: loop vectorized using 16 byte vectors
Dense.c:145:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
Dense.c:136:13: note: vectorized 2 loops in function.
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_28, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
Dense.c:136:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:136:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:105:24: missed: couldn't vectorize loop
Dense.c:105:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:108:17: missed: couldn't vectorize loop
Dense.c:108:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:122:24: missed: couldn't vectorize loop
Dense.c:122:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:125:17: missed: couldn't vectorize loop
Dense.c:125:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:90:15: missed: couldn't vectorize loop
Dense.c:90:15: missed: not vectorized: number of iterations cannot be computed.
Dense.c:83:13: note: vectorized 0 loops in function.
Dense.c:86:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:110:17: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:110:17: note: SLPing BB part
Dense.c:127:17: note: Costing subgraph: 
Dense.c:127:17: note: node 0x7547c98 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:127:17: note: op template: _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 0 _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 1 _198 = (sizetype) wi_137;
Dense.c:127:17: note: 	children 0x7547d98
Dense.c:127:17: note: node (external) 0x7547d98 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:127:17: note: 	{ j_135, wi_137 }
Dense.c:127:17: note: Cost model analysis: 
Dense.c:127:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:127:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:127:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: Costing subgraph: 
Dense.c:110:17: note: node 0x7547e98 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:110:17: note: op template: _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 0 _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 1 _217 = (sizetype) wi_136;
Dense.c:110:17: note: 	children 0x7547f98
Dense.c:110:17: note: node (external) 0x7547f98 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:110:17: note: 	{ j_134, wi_136 }
Dense.c:110:17: note: Cost model analysis: 
Dense.c:110:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:110:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:110:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: ***** The result for vector mode V32QI would be the same
Dense.c:110:17: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:110:17: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:110:17: note: SLPing BB part
Dense.c:127:17: note: Costing subgraph: 
Dense.c:127:17: note: node 0x7547e98 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:127:17: note: op template: _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 0 _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 1 _198 = (sizetype) wi_137;
Dense.c:127:17: note: 	children 0x7547c98
Dense.c:127:17: note: node (external) 0x7547c98 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:127:17: note: 	{ j_135, wi_137 }
Dense.c:127:17: note: Cost model analysis: 
Dense.c:127:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:127:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:127:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: Costing subgraph: 
Dense.c:110:17: note: node 0x7547e18 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:110:17: note: op template: _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 0 _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 1 _217 = (sizetype) wi_136;
Dense.c:110:17: note: 	children 0x7547f18
Dense.c:110:17: note: node (external) 0x7547f18 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:110:17: note: 	{ j_134, wi_136 }
Dense.c:110:17: note: Cost model analysis: 
Dense.c:110:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:110:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:110:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:105:51: note: ***** Analysis failed with vector mode V8QI
Dense.c:105:51: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:105:51: note: ***** Analysis failed with vector mode V4QI
Dense.c:62:15: missed: couldn't vectorize loop
Dense.c:62:15: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:75:11: optimized: loop vectorized using 32 byte vectors
Dense.c:75:11: optimized: loop vectorized using 16 byte vectors
Dense.c:66:11: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:881:10: missed: not vectorized: no vectype for stmt: _53 = MEM[(__m256 * {ref-all})w_ptr_82];
 scalar_type: __m256
Dense.c:54:13: note: vectorized 1 loops in function.
Dense.c:57:2: missed: statement clobbers memory: memcpy (_4, dense$baiases_63, _3);
Dense.c:79:2: missed: statement clobbers memory: _27 (pretmp_163, _68, 0);
Dense.c:54:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:54:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:179:107: note: ***** Analysis failed with vector mode V8SI
Dense.c:179:107: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
Dense.c:180:85: note: ***** Analysis failed with vector mode V8SI
Dense.c:180:85: note: ***** The result for vector mode V32QI would be the same
Dense.c:180:85: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:180:85: note: ***** Analysis failed with vector mode V16QI
Dense.c:180:85: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:180:85: note: ***** Analysis failed with vector mode V8QI
Dense.c:180:85: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:180:85: note: ***** Analysis failed with vector mode V4QI
Dense.c:192:19: missed: couldn't vectorize loop
Dense.c:192:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:182:13: note: vectorized 0 loops in function.
Dense.c:188:37: missed: statement clobbers memory: _9 = aligned_alloc (64, _8);
Dense.c:189:34: missed: statement clobbers memory: _11 = aligned_alloc (64, _10);
Dense.c:190:35: missed: statement clobbers memory: _12 = aligned_alloc (64, _10);
Dense.c:193:38: missed: statement clobbers memory: _13 = rand ();
Dense.c:193:72: missed: statement clobbers memory: _79 = sqrt (_20);
Dense.c:206:19: missed: statement clobbers memory: _31 = PULSE_GetActivationFunctionPtr (_30);
Dense.c:208:34: missed: statement clobbers memory: _33 = aligned_alloc (64, _10);
Dense.c:224:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:225:4: missed: statement clobbers memory: exit (1);
Dense.c:229:9: note: ***** Analysis succeeded with vector mode V8SI
Dense.c:229:9: note: SLPing BB part
Dense.c:229:9: note: Costing subgraph: 
Dense.c:229:9: note: node 0x74f7cc8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:229:9: note: op template: <retval>.n_inputs = args$n_inputs_53;
Dense.c:229:9: note: 	stmt 0 <retval>.n_inputs = args$n_inputs_53;
Dense.c:229:9: note: 	stmt 1 <retval>.n_outputs = args$n_outputs_54;
Dense.c:229:9: note: 	children 0x74f7d48
Dense.c:229:9: note: node 0x74f7d48 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:229:9: note: op template: args$n_inputs_53 = args.n_inputs;
Dense.c:229:9: note: 	stmt 0 args$n_inputs_53 = args.n_inputs;
Dense.c:229:9: note: 	stmt 1 args$n_outputs_54 = args.n_outputs;
Dense.c:229:9: note: Cost model analysis: 
Dense.c:229:9: note: Cost model analysis for part in loop 0:
  Vector cost: 40
  Scalar cost: 56
Dense.c:229:9: note: Costing subgraph: 
Dense.c:229:9: note: node 0x74f7e48 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:229:9: note: op template: <retval>.inputs = IO_42(D);
Dense.c:229:9: note: 	stmt 0 <retval>.inputs = IO_42(D);
Dense.c:229:9: note: 	stmt 1 <retval>.outputs = _29;
Dense.c:229:9: note: 	children 0x74f7ec8
Dense.c:229:9: note: node (external) 0x74f7ec8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:229:9: note: 	{ IO_42(D), _29 }
Dense.c:229:9: note: Cost model analysis: 
Dense.c:229:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:229:9: note: Costing subgraph: 
Dense.c:229:9: note: node 0x74f7fc8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:229:9: note: op template: <retval>.feed = layer$feed_41;
Dense.c:229:9: note: 	stmt 0 <retval>.feed = layer$feed_41;
Dense.c:229:9: note: 	stmt 1 <retval>.back = layer$back_51;
Dense.c:229:9: note: 	children 0x74f81c8
Dense.c:229:9: note: node (external) 0x74f81c8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:229:9: note: 	{ layer$feed_41, layer$back_51 }
Dense.c:229:9: note: Cost model analysis: 
Dense.c:229:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:229:9: note: Costing subgraph: 
Dense.c:229:9: note: node 0x74f80c8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:229:9: note: op template: <retval>.parent = 0B;
Dense.c:229:9: note: 	stmt 0 <retval>.parent = 0B;
Dense.c:229:9: note: 	stmt 1 <retval>.child = 0B;
Dense.c:229:9: note: 	children 0x74f8248
Dense.c:229:9: note: node (constant) 0x74f8248 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:229:9: note: 	{ 0B, 0B }
Dense.c:229:9: note: Cost model analysis: 
Dense.c:229:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:229:9: note: Costing subgraph: 
Dense.c:229:9: note: node 0x74f8348 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:229:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_37(D);
Dense.c:229:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_37(D);
Dense.c:229:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&<retval> + 96B] = _4;
Dense.c:229:9: note: 	stmt 2 MEM <float *> [(struct PULSE_Layer *)&<retval> + 104B] = _11;
Dense.c:229:9: note: 	stmt 3 MEM <float *> [(struct PULSE_Layer *)&<retval> + 112B] = _12;
Dense.c:229:9: note: 	children 0x74f83c8
Dense.c:229:9: note: node (external) 0x74f83c8 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:229:9: note: 	{ MODEL_37(D), _4, _11, _12 }
Dense.c:229:9: note: Cost model analysis: 
Dense.c:229:9: note: Cost model analysis for part in loop 0:
  Vector cost: 64
  Scalar cost: 64
Dense.c:229:9: note: Basic block will be vectorized using SLP
Dense.c:229:9: optimized: basic block part vectorized using 8 byte vectors
Dense.c:229:9: note: Vectorizing SLP tree:
Dense.c:229:9: note: node 0x74f7cc8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:229:9: note: op template: <retval>.n_inputs = args$n_inputs_53;
Dense.c:229:9: note: 	stmt 0 <retval>.n_inputs = args$n_inputs_53;
Dense.c:229:9: note: 	stmt 1 <retval>.n_outputs = args$n_outputs_54;
Dense.c:229:9: note: 	children 0x74f7d48
Dense.c:229:9: note: node 0x74f7d48 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:229:9: note: op template: args$n_inputs_53 = args.n_inputs;
Dense.c:229:9: note: 	stmt 0 args$n_inputs_53 = args.n_inputs;
Dense.c:229:9: note: 	stmt 1 args$n_outputs_54 = args.n_outputs;
Dense.c:229:9: note: ------>vectorizing SLP node starting from: args$n_inputs_53 = args.n_inputs;
Dense.c:229:9: note: transform load. ncopies = 1
Dense.c:229:9: note: conflicting alias set types.
Dense.c:229:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: args.n_inputs
Dense.c:229:9: note: created &args
Dense.c:229:9: note: add new stmt: vect_args_n_inputs_53.504_88 = MEM <vector(2) unsigned int> [(void *)&args];
Dense.c:229:9: note: extracting lane for live stmt args$n_inputs_53 = args.n_inputs;
Dense.c:229:9: note: extracting lane for live stmt args$n_outputs_54 = args.n_outputs;
Dense.c:229:9: note: ------>vectorizing SLP node starting from: <retval>.n_inputs = args$n_inputs_53;
Dense.c:229:9: note: vect_is_simple_use: operand args.n_inputs, type of def: internal
Dense.c:229:9: note: vect_is_simple_use: operand args.n_outputs, type of def: internal
Dense.c:229:9: note: transform store. ncopies = 1
Dense.c:229:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: <retval>.n_inputs
Dense.c:229:9: note: created &<retval>.n_inputs
Dense.c:229:9: note: add new stmt: MEM <vector(2) unsigned int> [(unsigned int *)&<retval> + 128B] = vect_args_n_inputs_53.504_88;
Dense.c:229:9: note: vectorizing stmts using SLP.
Dense.c:229:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:229:9: note: Vectorizing SLP tree:
Dense.c:229:9: note: node 0x74f7e48 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:229:9: note: op template: <retval>.inputs = IO_42(D);
Dense.c:229:9: note: 	stmt 0 <retval>.inputs = IO_42(D);
Dense.c:229:9: note: 	stmt 1 <retval>.outputs = _29;
Dense.c:229:9: note: 	children 0x74f7ec8
Dense.c:229:9: note: node (external) 0x74f7ec8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:229:9: note: 	{ IO_42(D), _29 }
Dense.c:229:9: note: ------>vectorizing SLP node starting from: <retval>.inputs = IO_42(D);
Dense.c:229:9: note: vect_is_simple_use: operand IO_42(D) + _28, type of def: internal
Dense.c:229:9: note: transform store. ncopies = 1
Dense.c:229:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.inputs
Dense.c:229:9: note: created &<retval>
Dense.c:229:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval>] = _94;
Dense.c:229:9: note: vectorizing stmts using SLP.
Dense.c:229:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:229:9: note: Vectorizing SLP tree:
Dense.c:229:9: note: node 0x74f7fc8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:229:9: note: op template: <retval>.feed = layer$feed_41;
Dense.c:229:9: note: 	stmt 0 <retval>.feed = layer$feed_41;
Dense.c:229:9: note: 	stmt 1 <retval>.back = layer$back_51;
Dense.c:229:9: note: 	children 0x74f81c8
Dense.c:229:9: note: node (external) 0x74f81c8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:229:9: note: 	{ layer$feed_41, layer$back_51 }
Dense.c:229:9: note: ------>vectorizing SLP node starting from: <retval>.feed = layer$feed_41;
Dense.c:229:9: note: vect_is_simple_use: operand layer$back_51 = PHI <layer$back_57(D)(11), _BackDense(9), _SIMD_BackDense(15)>, type of def: internal
Dense.c:229:9: note: transform store. ncopies = 1
Dense.c:229:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.feed
Dense.c:229:9: note: created &<retval>.feed
Dense.c:229:9: note: add new stmt: MEM <vector(2) long unsigned int> [(void (*<T921>) (struct PULSE_Layer *) *)&<retval> + 32B] = _98;
Dense.c:229:9: note: vectorizing stmts using SLP.
Dense.c:229:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:229:9: note: Vectorizing SLP tree:
Dense.c:229:9: note: node 0x74f80c8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:229:9: note: op template: <retval>.parent = 0B;
Dense.c:229:9: note: 	stmt 0 <retval>.parent = 0B;
Dense.c:229:9: note: 	stmt 1 <retval>.child = 0B;
Dense.c:229:9: note: 	children 0x74f8248
Dense.c:229:9: note: node (constant) 0x74f8248 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:229:9: note: 	{ 0B, 0B }
Dense.c:229:9: note: ------>vectorizing SLP node starting from: <retval>.parent = 0B;
Dense.c:229:9: note: vect_is_simple_use: operand 0B, type of def: constant
Dense.c:229:9: note: transform store. ncopies = 1
Dense.c:229:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.parent
Dense.c:229:9: note: created &<retval>.parent
Dense.c:229:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer * *)&<retval> + 72B] = { 0, 0 };
Dense.c:229:9: note: vectorizing stmts using SLP.
Dense.c:229:9: optimized: basic block part vectorized using 32 byte vectors
Dense.c:229:9: note: Vectorizing SLP tree:
Dense.c:229:9: note: node 0x74f8348 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:229:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_37(D);
Dense.c:229:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_37(D);
Dense.c:229:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&<retval> + 96B] = _4;
Dense.c:229:9: note: 	stmt 2 MEM <float *> [(struct PULSE_Layer *)&<retval> + 104B] = _11;
Dense.c:229:9: note: 	stmt 3 MEM <float *> [(struct PULSE_Layer *)&<retval> + 112B] = _12;
Dense.c:229:9: note: 	children 0x74f83c8
Dense.c:229:9: note: node (external) 0x74f83c8 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:229:9: note: 	{ MODEL_37(D), _4, _11, _12 }
Dense.c:229:9: note: ------>vectorizing SLP node starting from: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_37(D);
Dense.c:229:9: note: vect_is_simple_use: operand MODEL_37(D) + _3, type of def: internal
Dense.c:229:9: note: vect_is_simple_use: operand aligned_alloc (64, _10), type of def: internal
Dense.c:229:9: note: vect_is_simple_use: operand aligned_alloc (64, _10), type of def: internal
Dense.c:229:9: note: transform store. ncopies = 1
Dense.c:229:9: note: create vector_type-pointer variable to type: vector(4) long unsigned int  vectorizing a pointer ref: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B]
Dense.c:229:9: note: created &MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B]
Dense.c:229:9: note: add new stmt: MEM <vector(4) long unsigned int> [(struct PULSE_Layer *)&<retval> + 88B] = _105;
Dense.c:229:9: note: vectorizing stmts using SLP.
Dense.c:229:9: note: ***** The result for vector mode V32QI would be the same
Activations.c:17:19: optimized: loop vectorized using 32 byte vectors
Activations.c:17:19: optimized: loop vectorized using 16 byte vectors
Activations.c:17:19: optimized: loop vectorized using 32 byte vectors
Activations.c:17:19: optimized: loop vectorized using 16 byte vectors
Activations.c:14:13: note: vectorized 2 loops in function.
Activations.c:14:13: note: ***** Analysis failed with vector mode V8SF
Activations.c:14:13: note: ***** The result for vector mode V32QI would be the same
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V16QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V16QI
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V8QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V8QI
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V4QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V4QI
Activations.c:28:19: missed: couldn't vectorize loop
Activations.c:28:19: missed: not vectorized: unsupported control flow in loop.
Activations.c:28:19: optimized: loop vectorized using 32 byte vectors
Activations.c:28:19: optimized: loop vectorized using 16 byte vectors
Activations.c:25:13: note: vectorized 1 loops in function.
Activations.c:25:13: note: ***** Analysis failed with vector mode V8SF
Activations.c:25:13: note: ***** The result for vector mode V32QI would be the same
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V16QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V16QI
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V8QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V8QI
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V4QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V4QI
Activations.c:38:1: note: ***** Analysis failed with vector mode VOID
Activations.c:5:19: missed: couldn't vectorize loop
Activations.c:9:21: missed: statement clobbers memory: _36 = expf (_18);
Activations.c:5:19: missed: couldn't vectorize loop
Activations.c:9:21: missed: statement clobbers memory: _12 = expf (_11);
Activations.c:3:13: note: vectorized 0 loops in function.
Activations.c:9:21: missed: statement clobbers memory: _12 = expf (_11);
Activations.c:9:21: missed: statement clobbers memory: _36 = expf (_18);
Activations.c:11:1: note: ***** Analysis failed with vector mode V8SF
Activations.c:11:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SF
Activations.c:44:2: note: ***** Analysis failed with vector mode VOID
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:4:18: note: vectorized 0 loops in function.
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_4, inputs_15, _3);
PULSE.c:8:2: missed: statement clobbers memory: _5 (layer_17);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_12, pretmp_26, _9);
PULSE.c:13:28: note: ***** Analysis failed with vector mode V8SI
PULSE.c:13:28: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:22:2: missed: statement clobbers memory: _1 (layer_10(D));
PULSE.c:22:2: missed: statement clobbers memory: _14 (_2);
PULSE.c:22:2: missed: statement clobbers memory: _21 (_15);
PULSE.c:22:2: missed: statement clobbers memory: _28 (_22);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_29);
PULSE.c:26:3: missed: statement clobbers memory: memset (_34, 0, _32);
PULSE.c:26:3: missed: statement clobbers memory: memset (_27, 0, _25);
PULSE.c:26:3: missed: statement clobbers memory: memset (_20, 0, _18);
PULSE.c:26:3: missed: statement clobbers memory: memset (_7, 0, _5);
PULSE.c:28:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:28:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:39: missed: statement clobbers memory: _7 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:32:6: note: vectorized 1 loops in function.
PULSE.c:34:8: missed: statement clobbers memory: _1 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_2);
PULSE.c:39:39: missed: statement clobbers memory: _7 = rand ();
PULSE.c:40:40: missed: statement clobbers memory: _9 = rand ();
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8SI
PULSE.c:32:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:61:16: missed: couldn't vectorize loop
PULSE.c:61:16: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:64:17: missed: couldn't vectorize loop
PULSE.c:64:17: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:73:19: missed: couldn't vectorize loop
PULSE.c:73:19: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:39: missed: statement clobbers memory: _69 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:53:22: missed: couldn't vectorize loop
PULSE.c:53:22: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:47:6: note: vectorized 1 loops in function.
PULSE.c:48:1: missed: statement clobbers memory: saved_stack.7_39 = __builtin_stack_save ();
PULSE.c:49:46: missed: statement clobbers memory: _1 = PULSE_GetLossFunctionPtr (loss_function_40(D));
PULSE.c:60:15: missed: statement clobbers memory: random.4_46 = __builtin_alloca_with_align (_4, 32);
PULSE.c:34:8: missed: statement clobbers memory: _61 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_62);
PULSE.c:39:39: missed: statement clobbers memory: _69 = rand ();
PULSE.c:40:40: missed: statement clobbers memory: _72 = rand ();
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_98, inputs_93, _97);
PULSE.c:8:2: missed: statement clobbers memory: _99 (layer_94);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_105, _104, _103);
PULSE.c:67:11: missed: statement clobbers memory: loss_55 = PULSE_GetLoss_42 (_17, _16, _12, _11);
PULSE.c:22:2: missed: statement clobbers memory: _86 (output_34);
PULSE.c:22:2: missed: statement clobbers memory: _108 (_87);
PULSE.c:22:2: missed: statement clobbers memory: _115 (_109);
PULSE.c:22:2: missed: statement clobbers memory: _122 (_116);
PULSE.c:22:2: missed: statement clobbers memory: _129 (_123);
PULSE.c:22:2: missed: statement clobbers memory: _136 (_130);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_137);
PULSE.c:26:3: missed: statement clobbers memory: memset (_142, 0, _140);
PULSE.c:26:3: missed: statement clobbers memory: memset (_135, 0, _133);
PULSE.c:26:3: missed: statement clobbers memory: memset (_128, 0, _126);
PULSE.c:26:3: missed: statement clobbers memory: memset (_121, 0, _119);
PULSE.c:26:3: missed: statement clobbers memory: memset (_114, 0, _112);
PULSE.c:26:3: missed: statement clobbers memory: memset (_92, 0, _90);
PULSE.c:75:6: missed: statement clobbers memory: _21 (current_191, args);
PULSE.c:81:4: missed: statement clobbers memory: printf ("Epoch: %d | Item: %d | Loss: %.10f | Batch Loss: %.10f\r", i_188, j_189, _25, _24);
PULSE.c:47:6: missed: statement clobbers memory: __builtin_stack_restore (saved_stack.7_39);
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4DI
PULSE.c:47:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:91:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:91:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:123:20: missed: couldn't vectorize loop
PULSE.c:123:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:123:20: missed: couldn't vectorize loop
PULSE.c:123:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:103:20: missed: couldn't vectorize loop
PULSE.c:103:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:103:20: missed: couldn't vectorize loop
PULSE.c:103:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:93:13: note: vectorized 0 loops in function.
PULSE.c:96:39: missed: statement clobbers memory: layers_39 = malloc (_3);
PULSE.c:99:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:116:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:117:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:118:46: missed: statement clobbers memory: WEIGHTS_55 = aligned_alloc (64, 0);
PULSE.c:109:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.105_84];
PULSE.c:110:21: missed: statement clobbers memory: _69 = PULSE_GetDenseWeightsSize (args);
PULSE.c:111:16: missed: statement clobbers memory: _72 = PULSE_GetDenseIOSize (args);
PULSE.c:109:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})_207];
PULSE.c:110:21: missed: statement clobbers memory: _196 = PULSE_GetDenseWeightsSize (args);
PULSE.c:111:16: missed: statement clobbers memory: _199 = PULSE_GetDenseIOSize (args);
PULSE.c:116:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:117:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:118:46: missed: statement clobbers memory: WEIGHTS_44 = aligned_alloc (64, _5);
PULSE.c:119:41: missed: statement clobbers memory: IO_46 = aligned_alloc (64, _7);
PULSE.c:129:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.107_86];
PULSE.c:130:17: missed: statement clobbers memory: *_10 = PULSE_CreateDenseLayer (args, WEIGHTS_PTR_140, IO_PTR_141); [return slot optimization]
PULSE.c:131:20: missed: statement clobbers memory: _58 = PULSE_GetDenseWeightsSize (args);
PULSE.c:132:15: missed: statement clobbers memory: _61 = PULSE_GetDenseIOSize (args);
PULSE.c:129:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.107_126];
PULSE.c:130:17: missed: statement clobbers memory: *_108 = PULSE_CreateDenseLayer (args, WEIGHTS_PTR_148, IO_PTR_147); [return slot optimization]
PULSE.c:131:20: missed: statement clobbers memory: _96 = PULSE_GetDenseWeightsSize (args);
PULSE.c:132:15: missed: statement clobbers memory: _81 = PULSE_GetDenseIOSize (args);
PULSE.c:138:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:139:9: note: ***** Analysis succeeded with vector mode V8SI
PULSE.c:139:9: note: SLPing BB part
PULSE.c:139:9: note: Costing subgraph: 
PULSE.c:139:9: note: node 0x4616398 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:139:9: note: op template: <retval>.layers = layers_39;
PULSE.c:139:9: note: 	stmt 0 <retval>.layers = layers_39;
PULSE.c:139:9: note: 	stmt 1 <retval>.weights = WEIGHTS_152;
PULSE.c:139:9: note: 	children 0x4616498
PULSE.c:139:9: note: node (external) 0x4616498 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:139:9: note: 	{ layers_39, WEIGHTS_152 }
PULSE.c:139:9: note: Cost model analysis: 
PULSE.c:139:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:139:9: note: Basic block will be vectorized using SLP
PULSE.c:139:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:139:9: note: Vectorizing SLP tree:
PULSE.c:139:9: note: node 0x4616398 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:139:9: note: op template: <retval>.layers = layers_39;
PULSE.c:139:9: note: 	stmt 0 <retval>.layers = layers_39;
PULSE.c:139:9: note: 	stmt 1 <retval>.weights = WEIGHTS_152;
PULSE.c:139:9: note: 	children 0x4616498
PULSE.c:139:9: note: node (external) 0x4616498 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:139:9: note: 	{ layers_39, WEIGHTS_152 }
PULSE.c:139:9: note: ------>vectorizing SLP node starting from: <retval>.layers = layers_39;
PULSE.c:139:9: note: vect_is_simple_use: operand WEIGHTS_152 = PHI <WEIGHTS_44(36), WEIGHTS_55(25)>, type of def: internal
PULSE.c:139:9: note: conflicting alias set types.
PULSE.c:139:9: note: transform store. ncopies = 1
PULSE.c:139:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.layers
PULSE.c:139:9: note: created &<retval>
PULSE.c:139:9: note: add new stmt: MEM <vector(2) long unsigned int> [(void *)&<retval>] = _18;
PULSE.c:139:9: note: vectorizing stmts using SLP.
PULSE.c:139:9: note: ***** The result for vector mode V32QI would be the same
PULSE.c:147:3: missed: statement clobbers memory: free (_1);
PULSE.c:153:3: missed: statement clobbers memory: free (prephitmp_16);
PULSE.c:159:3: missed: statement clobbers memory: free (_4);
PULSE.c:162:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:162:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Loss.c:15:19: optimized: loop vectorized using 32 byte vectors
Loss.c:15:19: optimized:  loop versioned for vectorization because of possible aliasing
Loss.c:15:19: optimized: loop vectorized using 16 byte vectors
Loss.c:12:23: note: vectorized 1 loops in function.
Loss.c:12:23: note: ***** Analysis failed with vector mode V8SF
Loss.c:12:23: note: ***** The result for vector mode V32QI would be the same
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V16QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V16QI
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V8QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V8QI
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V4QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V4QI
Loss.c:7:19: missed: couldn't vectorize loop
Loss.c:8:38: missed: not vectorized: unsupported use in stmt.
Loss.c:4:23: note: vectorized 0 loops in function.
Loss.c:9:13: note: ***** Analysis failed with vector mode V8SF
Loss.c:9:13: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SF
Loss.c:23:2: note: ***** Analysis failed with vector mode VOID
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _28 (pretmp_91, _51, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:19: missed: not vectorized: no vectype for stmt: _22 = *_21;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:170:10: optimized: loop vectorized using 32 byte vectors
Dense.c:170:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:170:10: optimized: loop vectorized using 16 byte vectors
Dense.c:161:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
Dense.c:154:10: optimized: loop vectorized using 32 byte vectors
Dense.c:154:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:154:10: optimized: loop vectorized using 16 byte vectors
Dense.c:145:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
Dense.c:136:13: note: vectorized 2 loops in function.
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_28, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
Dense.c:136:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:136:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:105:24: missed: couldn't vectorize loop
Dense.c:105:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:108:17: missed: couldn't vectorize loop
Dense.c:108:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:122:24: missed: couldn't vectorize loop
Dense.c:122:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:125:17: missed: couldn't vectorize loop
Dense.c:125:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:90:15: missed: couldn't vectorize loop
Dense.c:90:15: missed: not vectorized: number of iterations cannot be computed.
Dense.c:83:13: note: vectorized 0 loops in function.
Dense.c:86:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:110:17: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:110:17: note: SLPing BB part
Dense.c:127:17: note: Costing subgraph: 
Dense.c:127:17: note: node 0x8a89218 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:127:17: note: op template: _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 0 _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 1 _198 = (sizetype) wi_137;
Dense.c:127:17: note: 	children 0x8a89318
Dense.c:127:17: note: node (external) 0x8a89318 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:127:17: note: 	{ j_135, wi_137 }
Dense.c:127:17: note: Cost model analysis: 
Dense.c:127:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:127:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:127:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: Costing subgraph: 
Dense.c:110:17: note: node 0x8a89418 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:110:17: note: op template: _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 0 _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 1 _217 = (sizetype) wi_136;
Dense.c:110:17: note: 	children 0x8a89518
Dense.c:110:17: note: node (external) 0x8a89518 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:110:17: note: 	{ j_134, wi_136 }
Dense.c:110:17: note: Cost model analysis: 
Dense.c:110:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:110:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:110:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: ***** The result for vector mode V32QI would be the same
Dense.c:110:17: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:110:17: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:110:17: note: SLPing BB part
Dense.c:127:17: note: Costing subgraph: 
Dense.c:127:17: note: node 0x8a89418 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:127:17: note: op template: _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 0 _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 1 _198 = (sizetype) wi_137;
Dense.c:127:17: note: 	children 0x8a89218
Dense.c:127:17: note: node (external) 0x8a89218 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:127:17: note: 	{ j_135, wi_137 }
Dense.c:127:17: note: Cost model analysis: 
Dense.c:127:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:127:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:127:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: Costing subgraph: 
Dense.c:110:17: note: node 0x8a89398 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:110:17: note: op template: _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 0 _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 1 _217 = (sizetype) wi_136;
Dense.c:110:17: note: 	children 0x8a89498
Dense.c:110:17: note: node (external) 0x8a89498 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:110:17: note: 	{ j_134, wi_136 }
Dense.c:110:17: note: Cost model analysis: 
Dense.c:110:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:110:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:110:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:105:51: note: ***** Analysis failed with vector mode V8QI
Dense.c:105:51: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:105:51: note: ***** Analysis failed with vector mode V4QI
Dense.c:62:15: missed: couldn't vectorize loop
Dense.c:62:15: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:75:11: optimized: loop vectorized using 32 byte vectors
Dense.c:75:11: optimized: loop vectorized using 16 byte vectors
Dense.c:66:11: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:881:10: missed: not vectorized: no vectype for stmt: _53 = MEM[(__m256 * {ref-all})w_ptr_82];
 scalar_type: __m256
Dense.c:54:13: note: vectorized 1 loops in function.
Dense.c:57:2: missed: statement clobbers memory: memcpy (_4, dense$baiases_63, _3);
Dense.c:79:2: missed: statement clobbers memory: _27 (pretmp_163, _68, 0);
Dense.c:54:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:54:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:179:107: note: ***** Analysis failed with vector mode V8SI
Dense.c:179:107: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
Dense.c:180:85: note: ***** Analysis failed with vector mode V8SI
Dense.c:180:85: note: ***** The result for vector mode V32QI would be the same
Dense.c:180:85: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:180:85: note: ***** Analysis failed with vector mode V16QI
Dense.c:180:85: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:180:85: note: ***** Analysis failed with vector mode V8QI
Dense.c:180:85: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:180:85: note: ***** Analysis failed with vector mode V4QI
Dense.c:192:19: missed: couldn't vectorize loop
Dense.c:192:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:182:13: note: vectorized 0 loops in function.
Dense.c:188:37: missed: statement clobbers memory: _9 = aligned_alloc (64, _8);
Dense.c:189:34: missed: statement clobbers memory: _11 = aligned_alloc (64, _10);
Dense.c:190:35: missed: statement clobbers memory: _12 = aligned_alloc (64, _10);
Dense.c:193:38: missed: statement clobbers memory: _13 = rand ();
Dense.c:193:72: missed: statement clobbers memory: _79 = sqrt (_20);
Dense.c:206:19: missed: statement clobbers memory: _31 = PULSE_GetActivationFunctionPtr (_30);
Dense.c:208:34: missed: statement clobbers memory: _33 = aligned_alloc (64, _10);
Dense.c:224:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:225:4: missed: statement clobbers memory: exit (1);
Dense.c:229:9: note: ***** Analysis succeeded with vector mode V8SI
Dense.c:229:9: note: SLPing BB part
Dense.c:229:9: note: Costing subgraph: 
Dense.c:229:9: note: node 0x890d578 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:229:9: note: op template: <retval>.n_inputs = args$n_inputs_53;
Dense.c:229:9: note: 	stmt 0 <retval>.n_inputs = args$n_inputs_53;
Dense.c:229:9: note: 	stmt 1 <retval>.n_outputs = args$n_outputs_54;
Dense.c:229:9: note: 	children 0x890d5f8
Dense.c:229:9: note: node 0x890d5f8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:229:9: note: op template: args$n_inputs_53 = args.n_inputs;
Dense.c:229:9: note: 	stmt 0 args$n_inputs_53 = args.n_inputs;
Dense.c:229:9: note: 	stmt 1 args$n_outputs_54 = args.n_outputs;
Dense.c:229:9: note: Cost model analysis: 
Dense.c:229:9: note: Cost model analysis for part in loop 0:
  Vector cost: 40
  Scalar cost: 56
Dense.c:229:9: note: Costing subgraph: 
Dense.c:229:9: note: node 0x890d6f8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:229:9: note: op template: <retval>.inputs = IO_42(D);
Dense.c:229:9: note: 	stmt 0 <retval>.inputs = IO_42(D);
Dense.c:229:9: note: 	stmt 1 <retval>.outputs = _29;
Dense.c:229:9: note: 	children 0x890d778
Dense.c:229:9: note: node (external) 0x890d778 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:229:9: note: 	{ IO_42(D), _29 }
Dense.c:229:9: note: Cost model analysis: 
Dense.c:229:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:229:9: note: Costing subgraph: 
Dense.c:229:9: note: node 0x890d878 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:229:9: note: op template: <retval>.feed = layer$feed_41;
Dense.c:229:9: note: 	stmt 0 <retval>.feed = layer$feed_41;
Dense.c:229:9: note: 	stmt 1 <retval>.back = layer$back_51;
Dense.c:229:9: note: 	children 0x890da78
Dense.c:229:9: note: node (external) 0x890da78 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:229:9: note: 	{ layer$feed_41, layer$back_51 }
Dense.c:229:9: note: Cost model analysis: 
Dense.c:229:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:229:9: note: Costing subgraph: 
Dense.c:229:9: note: node 0x890d978 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:229:9: note: op template: <retval>.parent = 0B;
Dense.c:229:9: note: 	stmt 0 <retval>.parent = 0B;
Dense.c:229:9: note: 	stmt 1 <retval>.child = 0B;
Dense.c:229:9: note: 	children 0x890daf8
Dense.c:229:9: note: node (constant) 0x890daf8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:229:9: note: 	{ 0B, 0B }
Dense.c:229:9: note: Cost model analysis: 
Dense.c:229:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:229:9: note: Costing subgraph: 
Dense.c:229:9: note: node 0x890dbf8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:229:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_37(D);
Dense.c:229:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_37(D);
Dense.c:229:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&<retval> + 96B] = _4;
Dense.c:229:9: note: 	stmt 2 MEM <float *> [(struct PULSE_Layer *)&<retval> + 104B] = _11;
Dense.c:229:9: note: 	stmt 3 MEM <float *> [(struct PULSE_Layer *)&<retval> + 112B] = _12;
Dense.c:229:9: note: 	children 0x890dc78
Dense.c:229:9: note: node (external) 0x890dc78 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:229:9: note: 	{ MODEL_37(D), _4, _11, _12 }
Dense.c:229:9: note: Cost model analysis: 
Dense.c:229:9: note: Cost model analysis for part in loop 0:
  Vector cost: 64
  Scalar cost: 64
Dense.c:229:9: note: Basic block will be vectorized using SLP
Dense.c:229:9: optimized: basic block part vectorized using 8 byte vectors
Dense.c:229:9: note: Vectorizing SLP tree:
Dense.c:229:9: note: node 0x890d578 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:229:9: note: op template: <retval>.n_inputs = args$n_inputs_53;
Dense.c:229:9: note: 	stmt 0 <retval>.n_inputs = args$n_inputs_53;
Dense.c:229:9: note: 	stmt 1 <retval>.n_outputs = args$n_outputs_54;
Dense.c:229:9: note: 	children 0x890d5f8
Dense.c:229:9: note: node 0x890d5f8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:229:9: note: op template: args$n_inputs_53 = args.n_inputs;
Dense.c:229:9: note: 	stmt 0 args$n_inputs_53 = args.n_inputs;
Dense.c:229:9: note: 	stmt 1 args$n_outputs_54 = args.n_outputs;
Dense.c:229:9: note: ------>vectorizing SLP node starting from: args$n_inputs_53 = args.n_inputs;
Dense.c:229:9: note: transform load. ncopies = 1
Dense.c:229:9: note: conflicting alias set types.
Dense.c:229:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: args.n_inputs
Dense.c:229:9: note: created &args
Dense.c:229:9: note: add new stmt: vect_args_n_inputs_53.504_88 = MEM <vector(2) unsigned int> [(void *)&args];
Dense.c:229:9: note: extracting lane for live stmt args$n_inputs_53 = args.n_inputs;
Dense.c:229:9: note: extracting lane for live stmt args$n_outputs_54 = args.n_outputs;
Dense.c:229:9: note: ------>vectorizing SLP node starting from: <retval>.n_inputs = args$n_inputs_53;
Dense.c:229:9: note: vect_is_simple_use: operand args.n_inputs, type of def: internal
Dense.c:229:9: note: vect_is_simple_use: operand args.n_outputs, type of def: internal
Dense.c:229:9: note: transform store. ncopies = 1
Dense.c:229:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: <retval>.n_inputs
Dense.c:229:9: note: created &<retval>.n_inputs
Dense.c:229:9: note: add new stmt: MEM <vector(2) unsigned int> [(unsigned int *)&<retval> + 128B] = vect_args_n_inputs_53.504_88;
Dense.c:229:9: note: vectorizing stmts using SLP.
Dense.c:229:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:229:9: note: Vectorizing SLP tree:
Dense.c:229:9: note: node 0x890d6f8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:229:9: note: op template: <retval>.inputs = IO_42(D);
Dense.c:229:9: note: 	stmt 0 <retval>.inputs = IO_42(D);
Dense.c:229:9: note: 	stmt 1 <retval>.outputs = _29;
Dense.c:229:9: note: 	children 0x890d778
Dense.c:229:9: note: node (external) 0x890d778 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:229:9: note: 	{ IO_42(D), _29 }
Dense.c:229:9: note: ------>vectorizing SLP node starting from: <retval>.inputs = IO_42(D);
Dense.c:229:9: note: vect_is_simple_use: operand IO_42(D) + _28, type of def: internal
Dense.c:229:9: note: transform store. ncopies = 1
Dense.c:229:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.inputs
Dense.c:229:9: note: created &<retval>
Dense.c:229:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval>] = _94;
Dense.c:229:9: note: vectorizing stmts using SLP.
Dense.c:229:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:229:9: note: Vectorizing SLP tree:
Dense.c:229:9: note: node 0x890d878 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:229:9: note: op template: <retval>.feed = layer$feed_41;
Dense.c:229:9: note: 	stmt 0 <retval>.feed = layer$feed_41;
Dense.c:229:9: note: 	stmt 1 <retval>.back = layer$back_51;
Dense.c:229:9: note: 	children 0x890da78
Dense.c:229:9: note: node (external) 0x890da78 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:229:9: note: 	{ layer$feed_41, layer$back_51 }
Dense.c:229:9: note: ------>vectorizing SLP node starting from: <retval>.feed = layer$feed_41;
Dense.c:229:9: note: vect_is_simple_use: operand layer$back_51 = PHI <layer$back_57(D)(11), _BackDense(9), _SIMD_BackDense(15)>, type of def: internal
Dense.c:229:9: note: transform store. ncopies = 1
Dense.c:229:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.feed
Dense.c:229:9: note: created &<retval>.feed
Dense.c:229:9: note: add new stmt: MEM <vector(2) long unsigned int> [(void (*<T921>) (struct PULSE_Layer *) *)&<retval> + 32B] = _98;
Dense.c:229:9: note: vectorizing stmts using SLP.
Dense.c:229:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:229:9: note: Vectorizing SLP tree:
Dense.c:229:9: note: node 0x890d978 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:229:9: note: op template: <retval>.parent = 0B;
Dense.c:229:9: note: 	stmt 0 <retval>.parent = 0B;
Dense.c:229:9: note: 	stmt 1 <retval>.child = 0B;
Dense.c:229:9: note: 	children 0x890daf8
Dense.c:229:9: note: node (constant) 0x890daf8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:229:9: note: 	{ 0B, 0B }
Dense.c:229:9: note: ------>vectorizing SLP node starting from: <retval>.parent = 0B;
Dense.c:229:9: note: vect_is_simple_use: operand 0B, type of def: constant
Dense.c:229:9: note: transform store. ncopies = 1
Dense.c:229:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.parent
Dense.c:229:9: note: created &<retval>.parent
Dense.c:229:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer * *)&<retval> + 72B] = { 0, 0 };
Dense.c:229:9: note: vectorizing stmts using SLP.
Dense.c:229:9: optimized: basic block part vectorized using 32 byte vectors
Dense.c:229:9: note: Vectorizing SLP tree:
Dense.c:229:9: note: node 0x890dbf8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:229:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_37(D);
Dense.c:229:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_37(D);
Dense.c:229:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&<retval> + 96B] = _4;
Dense.c:229:9: note: 	stmt 2 MEM <float *> [(struct PULSE_Layer *)&<retval> + 104B] = _11;
Dense.c:229:9: note: 	stmt 3 MEM <float *> [(struct PULSE_Layer *)&<retval> + 112B] = _12;
Dense.c:229:9: note: 	children 0x890dc78
Dense.c:229:9: note: node (external) 0x890dc78 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:229:9: note: 	{ MODEL_37(D), _4, _11, _12 }
Dense.c:229:9: note: ------>vectorizing SLP node starting from: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_37(D);
Dense.c:229:9: note: vect_is_simple_use: operand MODEL_37(D) + _3, type of def: internal
Dense.c:229:9: note: vect_is_simple_use: operand aligned_alloc (64, _10), type of def: internal
Dense.c:229:9: note: vect_is_simple_use: operand aligned_alloc (64, _10), type of def: internal
Dense.c:229:9: note: transform store. ncopies = 1
Dense.c:229:9: note: create vector_type-pointer variable to type: vector(4) long unsigned int  vectorizing a pointer ref: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B]
Dense.c:229:9: note: created &MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B]
Dense.c:229:9: note: add new stmt: MEM <vector(4) long unsigned int> [(struct PULSE_Layer *)&<retval> + 88B] = _105;
Dense.c:229:9: note: vectorizing stmts using SLP.
Dense.c:229:9: note: ***** The result for vector mode V32QI would be the same
Activations.c:17:19: optimized: loop vectorized using 32 byte vectors
Activations.c:17:19: optimized: loop vectorized using 16 byte vectors
Activations.c:17:19: optimized: loop vectorized using 32 byte vectors
Activations.c:17:19: optimized: loop vectorized using 16 byte vectors
Activations.c:14:13: note: vectorized 2 loops in function.
Activations.c:14:13: note: ***** Analysis failed with vector mode V8SF
Activations.c:14:13: note: ***** The result for vector mode V32QI would be the same
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V16QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V16QI
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V8QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V8QI
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V4QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V4QI
Activations.c:28:19: missed: couldn't vectorize loop
Activations.c:28:19: missed: not vectorized: unsupported control flow in loop.
Activations.c:28:19: optimized: loop vectorized using 32 byte vectors
Activations.c:28:19: optimized: loop vectorized using 16 byte vectors
Activations.c:25:13: note: vectorized 1 loops in function.
Activations.c:25:13: note: ***** Analysis failed with vector mode V8SF
Activations.c:25:13: note: ***** The result for vector mode V32QI would be the same
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V16QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V16QI
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V8QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V8QI
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V4QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V4QI
Activations.c:38:1: note: ***** Analysis failed with vector mode VOID
Activations.c:5:19: missed: couldn't vectorize loop
Activations.c:9:21: missed: statement clobbers memory: _36 = expf (_18);
Activations.c:5:19: missed: couldn't vectorize loop
Activations.c:9:21: missed: statement clobbers memory: _12 = expf (_11);
Activations.c:3:13: note: vectorized 0 loops in function.
Activations.c:9:21: missed: statement clobbers memory: _12 = expf (_11);
Activations.c:9:21: missed: statement clobbers memory: _36 = expf (_18);
Activations.c:11:1: note: ***** Analysis failed with vector mode V8SF
Activations.c:11:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SF
Activations.c:44:2: note: ***** Analysis failed with vector mode VOID
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:4:18: note: vectorized 0 loops in function.
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_4, inputs_15, _3);
PULSE.c:8:2: missed: statement clobbers memory: _5 (layer_17);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_12, pretmp_26, _9);
PULSE.c:13:28: note: ***** Analysis failed with vector mode V8SI
PULSE.c:13:28: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:22:2: missed: statement clobbers memory: _1 (layer_10(D));
PULSE.c:22:2: missed: statement clobbers memory: _14 (_2);
PULSE.c:22:2: missed: statement clobbers memory: _21 (_15);
PULSE.c:22:2: missed: statement clobbers memory: _28 (_22);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_29);
PULSE.c:26:3: missed: statement clobbers memory: memset (_34, 0, _32);
PULSE.c:26:3: missed: statement clobbers memory: memset (_27, 0, _25);
PULSE.c:26:3: missed: statement clobbers memory: memset (_20, 0, _18);
PULSE.c:26:3: missed: statement clobbers memory: memset (_7, 0, _5);
PULSE.c:28:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:28:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:39: missed: statement clobbers memory: _7 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:32:6: note: vectorized 1 loops in function.
PULSE.c:34:8: missed: statement clobbers memory: _1 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_2);
PULSE.c:39:39: missed: statement clobbers memory: _7 = rand ();
PULSE.c:40:40: missed: statement clobbers memory: _9 = rand ();
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8SI
PULSE.c:32:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:61:16: missed: couldn't vectorize loop
PULSE.c:61:16: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:64:17: missed: couldn't vectorize loop
PULSE.c:64:17: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:73:19: missed: couldn't vectorize loop
PULSE.c:73:19: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:39: missed: statement clobbers memory: _69 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:53:22: missed: couldn't vectorize loop
PULSE.c:53:22: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:47:6: note: vectorized 1 loops in function.
PULSE.c:48:1: missed: statement clobbers memory: saved_stack.7_39 = __builtin_stack_save ();
PULSE.c:49:46: missed: statement clobbers memory: _1 = PULSE_GetLossFunctionPtr (loss_function_40(D));
PULSE.c:60:15: missed: statement clobbers memory: random.4_46 = __builtin_alloca_with_align (_4, 32);
PULSE.c:34:8: missed: statement clobbers memory: _61 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_62);
PULSE.c:39:39: missed: statement clobbers memory: _69 = rand ();
PULSE.c:40:40: missed: statement clobbers memory: _72 = rand ();
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_98, inputs_93, _97);
PULSE.c:8:2: missed: statement clobbers memory: _99 (layer_94);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_105, _104, _103);
PULSE.c:67:11: missed: statement clobbers memory: loss_55 = PULSE_GetLoss_42 (_17, _16, _12, _11);
PULSE.c:22:2: missed: statement clobbers memory: _86 (output_34);
PULSE.c:22:2: missed: statement clobbers memory: _108 (_87);
PULSE.c:22:2: missed: statement clobbers memory: _115 (_109);
PULSE.c:22:2: missed: statement clobbers memory: _122 (_116);
PULSE.c:22:2: missed: statement clobbers memory: _129 (_123);
PULSE.c:22:2: missed: statement clobbers memory: _136 (_130);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_137);
PULSE.c:26:3: missed: statement clobbers memory: memset (_142, 0, _140);
PULSE.c:26:3: missed: statement clobbers memory: memset (_135, 0, _133);
PULSE.c:26:3: missed: statement clobbers memory: memset (_128, 0, _126);
PULSE.c:26:3: missed: statement clobbers memory: memset (_121, 0, _119);
PULSE.c:26:3: missed: statement clobbers memory: memset (_114, 0, _112);
PULSE.c:26:3: missed: statement clobbers memory: memset (_92, 0, _90);
PULSE.c:75:6: missed: statement clobbers memory: _21 (current_191, args);
PULSE.c:81:4: missed: statement clobbers memory: printf ("Epoch: %d | Item: %d | Loss: %.10f | Batch Loss: %.10f\r", i_188, j_189, _25, _24);
PULSE.c:47:6: missed: statement clobbers memory: __builtin_stack_restore (saved_stack.7_39);
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4DI
PULSE.c:47:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:91:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:91:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:123:20: missed: couldn't vectorize loop
PULSE.c:123:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:123:20: missed: couldn't vectorize loop
PULSE.c:123:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:103:20: missed: couldn't vectorize loop
PULSE.c:103:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:103:20: missed: couldn't vectorize loop
PULSE.c:103:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:93:13: note: vectorized 0 loops in function.
PULSE.c:96:39: missed: statement clobbers memory: layers_39 = malloc (_3);
PULSE.c:99:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:116:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:117:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:118:46: missed: statement clobbers memory: WEIGHTS_55 = aligned_alloc (64, 0);
PULSE.c:109:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.105_84];
PULSE.c:110:21: missed: statement clobbers memory: _69 = PULSE_GetDenseWeightsSize (args);
PULSE.c:111:16: missed: statement clobbers memory: _72 = PULSE_GetDenseIOSize (args);
PULSE.c:109:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})_207];
PULSE.c:110:21: missed: statement clobbers memory: _196 = PULSE_GetDenseWeightsSize (args);
PULSE.c:111:16: missed: statement clobbers memory: _199 = PULSE_GetDenseIOSize (args);
PULSE.c:116:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:117:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:118:46: missed: statement clobbers memory: WEIGHTS_44 = aligned_alloc (64, _5);
PULSE.c:119:41: missed: statement clobbers memory: IO_46 = aligned_alloc (64, _7);
PULSE.c:129:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.107_86];
PULSE.c:130:17: missed: statement clobbers memory: *_10 = PULSE_CreateDenseLayer (args, WEIGHTS_PTR_140, IO_PTR_141); [return slot optimization]
PULSE.c:131:20: missed: statement clobbers memory: _58 = PULSE_GetDenseWeightsSize (args);
PULSE.c:132:15: missed: statement clobbers memory: _61 = PULSE_GetDenseIOSize (args);
PULSE.c:129:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.107_126];
PULSE.c:130:17: missed: statement clobbers memory: *_108 = PULSE_CreateDenseLayer (args, WEIGHTS_PTR_148, IO_PTR_147); [return slot optimization]
PULSE.c:131:20: missed: statement clobbers memory: _96 = PULSE_GetDenseWeightsSize (args);
PULSE.c:132:15: missed: statement clobbers memory: _81 = PULSE_GetDenseIOSize (args);
PULSE.c:138:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:139:9: note: ***** Analysis succeeded with vector mode V8SI
PULSE.c:139:9: note: SLPing BB part
PULSE.c:139:9: note: Costing subgraph: 
PULSE.c:139:9: note: node 0x3cd2398 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:139:9: note: op template: <retval>.layers = layers_39;
PULSE.c:139:9: note: 	stmt 0 <retval>.layers = layers_39;
PULSE.c:139:9: note: 	stmt 1 <retval>.weights = WEIGHTS_152;
PULSE.c:139:9: note: 	children 0x3cd2498
PULSE.c:139:9: note: node (external) 0x3cd2498 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:139:9: note: 	{ layers_39, WEIGHTS_152 }
PULSE.c:139:9: note: Cost model analysis: 
PULSE.c:139:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:139:9: note: Basic block will be vectorized using SLP
PULSE.c:139:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:139:9: note: Vectorizing SLP tree:
PULSE.c:139:9: note: node 0x3cd2398 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:139:9: note: op template: <retval>.layers = layers_39;
PULSE.c:139:9: note: 	stmt 0 <retval>.layers = layers_39;
PULSE.c:139:9: note: 	stmt 1 <retval>.weights = WEIGHTS_152;
PULSE.c:139:9: note: 	children 0x3cd2498
PULSE.c:139:9: note: node (external) 0x3cd2498 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:139:9: note: 	{ layers_39, WEIGHTS_152 }
PULSE.c:139:9: note: ------>vectorizing SLP node starting from: <retval>.layers = layers_39;
PULSE.c:139:9: note: vect_is_simple_use: operand WEIGHTS_152 = PHI <WEIGHTS_44(36), WEIGHTS_55(25)>, type of def: internal
PULSE.c:139:9: note: conflicting alias set types.
PULSE.c:139:9: note: transform store. ncopies = 1
PULSE.c:139:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.layers
PULSE.c:139:9: note: created &<retval>
PULSE.c:139:9: note: add new stmt: MEM <vector(2) long unsigned int> [(void *)&<retval>] = _18;
PULSE.c:139:9: note: vectorizing stmts using SLP.
PULSE.c:139:9: note: ***** The result for vector mode V32QI would be the same
PULSE.c:147:3: missed: statement clobbers memory: free (_1);
PULSE.c:153:3: missed: statement clobbers memory: free (prephitmp_16);
PULSE.c:159:3: missed: statement clobbers memory: free (_4);
PULSE.c:162:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:162:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Loss.c:15:19: optimized: loop vectorized using 32 byte vectors
Loss.c:15:19: optimized:  loop versioned for vectorization because of possible aliasing
Loss.c:15:19: optimized: loop vectorized using 16 byte vectors
Loss.c:12:23: note: vectorized 1 loops in function.
Loss.c:12:23: note: ***** Analysis failed with vector mode V8SF
Loss.c:12:23: note: ***** The result for vector mode V32QI would be the same
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V16QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V16QI
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V8QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V8QI
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V4QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V4QI
Loss.c:7:19: missed: couldn't vectorize loop
Loss.c:8:38: missed: not vectorized: unsupported use in stmt.
Loss.c:4:23: note: vectorized 0 loops in function.
Loss.c:9:13: note: ***** Analysis failed with vector mode V8SF
Loss.c:9:13: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SF
Loss.c:23:2: note: ***** Analysis failed with vector mode VOID
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _28 (pretmp_91, _51, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:19: missed: not vectorized: no vectype for stmt: _22 = *_21;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:170:10: optimized: loop vectorized using 32 byte vectors
Dense.c:170:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:170:10: optimized: loop vectorized using 16 byte vectors
Dense.c:161:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
Dense.c:154:10: optimized: loop vectorized using 32 byte vectors
Dense.c:154:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:154:10: optimized: loop vectorized using 16 byte vectors
Dense.c:145:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
Dense.c:136:13: note: vectorized 2 loops in function.
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_28, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
Dense.c:136:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:136:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:105:24: missed: couldn't vectorize loop
Dense.c:105:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:108:17: missed: couldn't vectorize loop
Dense.c:108:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:122:24: missed: couldn't vectorize loop
Dense.c:122:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:125:17: missed: couldn't vectorize loop
Dense.c:125:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:90:15: missed: couldn't vectorize loop
Dense.c:90:15: missed: not vectorized: number of iterations cannot be computed.
Dense.c:83:13: note: vectorized 0 loops in function.
Dense.c:86:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:110:17: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:110:17: note: SLPing BB part
Dense.c:127:17: note: Costing subgraph: 
Dense.c:127:17: note: node 0x8ced608 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:127:17: note: op template: _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 0 _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 1 _198 = (sizetype) wi_137;
Dense.c:127:17: note: 	children 0x8ced708
Dense.c:127:17: note: node (external) 0x8ced708 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:127:17: note: 	{ j_135, wi_137 }
Dense.c:127:17: note: Cost model analysis: 
Dense.c:127:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:127:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:127:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: Costing subgraph: 
Dense.c:110:17: note: node 0x8ced808 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:110:17: note: op template: _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 0 _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 1 _217 = (sizetype) wi_136;
Dense.c:110:17: note: 	children 0x8ced908
Dense.c:110:17: note: node (external) 0x8ced908 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:110:17: note: 	{ j_134, wi_136 }
Dense.c:110:17: note: Cost model analysis: 
Dense.c:110:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:110:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:110:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: ***** The result for vector mode V32QI would be the same
Dense.c:110:17: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:110:17: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:110:17: note: SLPing BB part
Dense.c:127:17: note: Costing subgraph: 
Dense.c:127:17: note: node 0x8ced808 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:127:17: note: op template: _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 0 _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 1 _198 = (sizetype) wi_137;
Dense.c:127:17: note: 	children 0x8ced608
Dense.c:127:17: note: node (external) 0x8ced608 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:127:17: note: 	{ j_135, wi_137 }
Dense.c:127:17: note: Cost model analysis: 
Dense.c:127:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:127:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:127:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: Costing subgraph: 
Dense.c:110:17: note: node 0x8ced788 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:110:17: note: op template: _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 0 _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 1 _217 = (sizetype) wi_136;
Dense.c:110:17: note: 	children 0x8ced888
Dense.c:110:17: note: node (external) 0x8ced888 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:110:17: note: 	{ j_134, wi_136 }
Dense.c:110:17: note: Cost model analysis: 
Dense.c:110:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:110:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:110:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:105:51: note: ***** Analysis failed with vector mode V8QI
Dense.c:105:51: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:105:51: note: ***** Analysis failed with vector mode V4QI
Dense.c:62:15: missed: couldn't vectorize loop
Dense.c:62:15: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:75:11: optimized: loop vectorized using 32 byte vectors
Dense.c:75:11: optimized: loop vectorized using 16 byte vectors
Dense.c:66:11: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:881:10: missed: not vectorized: no vectype for stmt: _53 = MEM[(__m256 * {ref-all})w_ptr_82];
 scalar_type: __m256
Dense.c:54:13: note: vectorized 1 loops in function.
Dense.c:57:2: missed: statement clobbers memory: memcpy (_4, dense$baiases_63, _3);
Dense.c:79:2: missed: statement clobbers memory: _27 (pretmp_163, _68, 0);
Dense.c:54:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:54:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:179:107: note: ***** Analysis failed with vector mode V8SI
Dense.c:179:107: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
Dense.c:180:85: note: ***** Analysis failed with vector mode V8SI
Dense.c:180:85: note: ***** The result for vector mode V32QI would be the same
Dense.c:180:85: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:180:85: note: ***** Analysis failed with vector mode V16QI
Dense.c:180:85: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:180:85: note: ***** Analysis failed with vector mode V8QI
Dense.c:180:85: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:180:85: note: ***** Analysis failed with vector mode V4QI
Dense.c:181:105: note: ***** Analysis failed with vector mode V8SI
Dense.c:181:105: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
Dense.c:193:19: missed: couldn't vectorize loop
Dense.c:193:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:183:13: note: vectorized 0 loops in function.
Dense.c:189:37: missed: statement clobbers memory: _9 = aligned_alloc (64, _8);
Dense.c:190:34: missed: statement clobbers memory: _11 = aligned_alloc (64, _10);
Dense.c:191:35: missed: statement clobbers memory: _12 = aligned_alloc (64, _10);
Dense.c:194:38: missed: statement clobbers memory: _13 = rand ();
Dense.c:194:72: missed: statement clobbers memory: _79 = sqrt (_20);
Dense.c:207:19: missed: statement clobbers memory: _31 = PULSE_GetActivationFunctionPtr (_30);
Dense.c:209:34: missed: statement clobbers memory: _33 = aligned_alloc (64, _10);
Dense.c:225:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:226:4: missed: statement clobbers memory: exit (1);
Dense.c:230:9: note: ***** Analysis succeeded with vector mode V8SI
Dense.c:230:9: note: SLPing BB part
Dense.c:230:9: note: Costing subgraph: 
Dense.c:230:9: note: node 0x8b1d668 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:230:9: note: op template: <retval>.n_inputs = args$n_inputs_53;
Dense.c:230:9: note: 	stmt 0 <retval>.n_inputs = args$n_inputs_53;
Dense.c:230:9: note: 	stmt 1 <retval>.n_outputs = args$n_outputs_54;
Dense.c:230:9: note: 	children 0x8b1d6e8
Dense.c:230:9: note: node 0x8b1d6e8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:230:9: note: op template: args$n_inputs_53 = args.n_inputs;
Dense.c:230:9: note: 	stmt 0 args$n_inputs_53 = args.n_inputs;
Dense.c:230:9: note: 	stmt 1 args$n_outputs_54 = args.n_outputs;
Dense.c:230:9: note: Cost model analysis: 
Dense.c:230:9: note: Cost model analysis for part in loop 0:
  Vector cost: 40
  Scalar cost: 56
Dense.c:230:9: note: Costing subgraph: 
Dense.c:230:9: note: node 0x8b1d7e8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:230:9: note: op template: <retval>.inputs = IO_42(D);
Dense.c:230:9: note: 	stmt 0 <retval>.inputs = IO_42(D);
Dense.c:230:9: note: 	stmt 1 <retval>.outputs = _29;
Dense.c:230:9: note: 	children 0x8b1d868
Dense.c:230:9: note: node (external) 0x8b1d868 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:230:9: note: 	{ IO_42(D), _29 }
Dense.c:230:9: note: Cost model analysis: 
Dense.c:230:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:230:9: note: Costing subgraph: 
Dense.c:230:9: note: node 0x8b1d968 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:230:9: note: op template: <retval>.feed = layer$feed_41;
Dense.c:230:9: note: 	stmt 0 <retval>.feed = layer$feed_41;
Dense.c:230:9: note: 	stmt 1 <retval>.back = layer$back_51;
Dense.c:230:9: note: 	children 0x8b1db68
Dense.c:230:9: note: node (external) 0x8b1db68 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:230:9: note: 	{ layer$feed_41, layer$back_51 }
Dense.c:230:9: note: Cost model analysis: 
Dense.c:230:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:230:9: note: Costing subgraph: 
Dense.c:230:9: note: node 0x8b1da68 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:230:9: note: op template: <retval>.parent = 0B;
Dense.c:230:9: note: 	stmt 0 <retval>.parent = 0B;
Dense.c:230:9: note: 	stmt 1 <retval>.child = 0B;
Dense.c:230:9: note: 	children 0x8b1dbe8
Dense.c:230:9: note: node (constant) 0x8b1dbe8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:230:9: note: 	{ 0B, 0B }
Dense.c:230:9: note: Cost model analysis: 
Dense.c:230:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:230:9: note: Costing subgraph: 
Dense.c:230:9: note: node 0x8b1dce8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:230:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_37(D);
Dense.c:230:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_37(D);
Dense.c:230:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&<retval> + 96B] = _4;
Dense.c:230:9: note: 	stmt 2 MEM <float *> [(struct PULSE_Layer *)&<retval> + 104B] = _11;
Dense.c:230:9: note: 	stmt 3 MEM <float *> [(struct PULSE_Layer *)&<retval> + 112B] = _12;
Dense.c:230:9: note: 	children 0x8b1dd68
Dense.c:230:9: note: node (external) 0x8b1dd68 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:230:9: note: 	{ MODEL_37(D), _4, _11, _12 }
Dense.c:230:9: note: Cost model analysis: 
Dense.c:230:9: note: Cost model analysis for part in loop 0:
  Vector cost: 64
  Scalar cost: 64
Dense.c:230:9: note: Basic block will be vectorized using SLP
Dense.c:230:9: optimized: basic block part vectorized using 8 byte vectors
Dense.c:230:9: note: Vectorizing SLP tree:
Dense.c:230:9: note: node 0x8b1d668 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:230:9: note: op template: <retval>.n_inputs = args$n_inputs_53;
Dense.c:230:9: note: 	stmt 0 <retval>.n_inputs = args$n_inputs_53;
Dense.c:230:9: note: 	stmt 1 <retval>.n_outputs = args$n_outputs_54;
Dense.c:230:9: note: 	children 0x8b1d6e8
Dense.c:230:9: note: node 0x8b1d6e8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:230:9: note: op template: args$n_inputs_53 = args.n_inputs;
Dense.c:230:9: note: 	stmt 0 args$n_inputs_53 = args.n_inputs;
Dense.c:230:9: note: 	stmt 1 args$n_outputs_54 = args.n_outputs;
Dense.c:230:9: note: ------>vectorizing SLP node starting from: args$n_inputs_53 = args.n_inputs;
Dense.c:230:9: note: transform load. ncopies = 1
Dense.c:230:9: note: conflicting alias set types.
Dense.c:230:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: args.n_inputs
Dense.c:230:9: note: created &args
Dense.c:230:9: note: add new stmt: vect_args_n_inputs_53.508_88 = MEM <vector(2) unsigned int> [(void *)&args];
Dense.c:230:9: note: extracting lane for live stmt args$n_inputs_53 = args.n_inputs;
Dense.c:230:9: note: extracting lane for live stmt args$n_outputs_54 = args.n_outputs;
Dense.c:230:9: note: ------>vectorizing SLP node starting from: <retval>.n_inputs = args$n_inputs_53;
Dense.c:230:9: note: vect_is_simple_use: operand args.n_inputs, type of def: internal
Dense.c:230:9: note: vect_is_simple_use: operand args.n_outputs, type of def: internal
Dense.c:230:9: note: transform store. ncopies = 1
Dense.c:230:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: <retval>.n_inputs
Dense.c:230:9: note: created &<retval>.n_inputs
Dense.c:230:9: note: add new stmt: MEM <vector(2) unsigned int> [(unsigned int *)&<retval> + 128B] = vect_args_n_inputs_53.508_88;
Dense.c:230:9: note: vectorizing stmts using SLP.
Dense.c:230:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:230:9: note: Vectorizing SLP tree:
Dense.c:230:9: note: node 0x8b1d7e8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:230:9: note: op template: <retval>.inputs = IO_42(D);
Dense.c:230:9: note: 	stmt 0 <retval>.inputs = IO_42(D);
Dense.c:230:9: note: 	stmt 1 <retval>.outputs = _29;
Dense.c:230:9: note: 	children 0x8b1d868
Dense.c:230:9: note: node (external) 0x8b1d868 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:230:9: note: 	{ IO_42(D), _29 }
Dense.c:230:9: note: ------>vectorizing SLP node starting from: <retval>.inputs = IO_42(D);
Dense.c:230:9: note: vect_is_simple_use: operand IO_42(D) + _28, type of def: internal
Dense.c:230:9: note: transform store. ncopies = 1
Dense.c:230:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.inputs
Dense.c:230:9: note: created &<retval>
Dense.c:230:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval>] = _94;
Dense.c:230:9: note: vectorizing stmts using SLP.
Dense.c:230:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:230:9: note: Vectorizing SLP tree:
Dense.c:230:9: note: node 0x8b1d968 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:230:9: note: op template: <retval>.feed = layer$feed_41;
Dense.c:230:9: note: 	stmt 0 <retval>.feed = layer$feed_41;
Dense.c:230:9: note: 	stmt 1 <retval>.back = layer$back_51;
Dense.c:230:9: note: 	children 0x8b1db68
Dense.c:230:9: note: node (external) 0x8b1db68 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:230:9: note: 	{ layer$feed_41, layer$back_51 }
Dense.c:230:9: note: ------>vectorizing SLP node starting from: <retval>.feed = layer$feed_41;
Dense.c:230:9: note: vect_is_simple_use: operand layer$back_51 = PHI <layer$back_57(D)(11), _BackDense(9), _SIMD_BackDense(15)>, type of def: internal
Dense.c:230:9: note: transform store. ncopies = 1
Dense.c:230:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.feed
Dense.c:230:9: note: created &<retval>.feed
Dense.c:230:9: note: add new stmt: MEM <vector(2) long unsigned int> [(void (*<T921>) (struct PULSE_Layer *) *)&<retval> + 32B] = _98;
Dense.c:230:9: note: vectorizing stmts using SLP.
Dense.c:230:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:230:9: note: Vectorizing SLP tree:
Dense.c:230:9: note: node 0x8b1da68 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:230:9: note: op template: <retval>.parent = 0B;
Dense.c:230:9: note: 	stmt 0 <retval>.parent = 0B;
Dense.c:230:9: note: 	stmt 1 <retval>.child = 0B;
Dense.c:230:9: note: 	children 0x8b1dbe8
Dense.c:230:9: note: node (constant) 0x8b1dbe8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:230:9: note: 	{ 0B, 0B }
Dense.c:230:9: note: ------>vectorizing SLP node starting from: <retval>.parent = 0B;
Dense.c:230:9: note: vect_is_simple_use: operand 0B, type of def: constant
Dense.c:230:9: note: transform store. ncopies = 1
Dense.c:230:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.parent
Dense.c:230:9: note: created &<retval>.parent
Dense.c:230:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer * *)&<retval> + 72B] = { 0, 0 };
Dense.c:230:9: note: vectorizing stmts using SLP.
Dense.c:230:9: optimized: basic block part vectorized using 32 byte vectors
Dense.c:230:9: note: Vectorizing SLP tree:
Dense.c:230:9: note: node 0x8b1dce8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:230:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_37(D);
Dense.c:230:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_37(D);
Dense.c:230:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&<retval> + 96B] = _4;
Dense.c:230:9: note: 	stmt 2 MEM <float *> [(struct PULSE_Layer *)&<retval> + 104B] = _11;
Dense.c:230:9: note: 	stmt 3 MEM <float *> [(struct PULSE_Layer *)&<retval> + 112B] = _12;
Dense.c:230:9: note: 	children 0x8b1dd68
Dense.c:230:9: note: node (external) 0x8b1dd68 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:230:9: note: 	{ MODEL_37(D), _4, _11, _12 }
Dense.c:230:9: note: ------>vectorizing SLP node starting from: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_37(D);
Dense.c:230:9: note: vect_is_simple_use: operand MODEL_37(D) + _3, type of def: internal
Dense.c:230:9: note: vect_is_simple_use: operand aligned_alloc (64, _10), type of def: internal
Dense.c:230:9: note: vect_is_simple_use: operand aligned_alloc (64, _10), type of def: internal
Dense.c:230:9: note: transform store. ncopies = 1
Dense.c:230:9: note: create vector_type-pointer variable to type: vector(4) long unsigned int  vectorizing a pointer ref: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B]
Dense.c:230:9: note: created &MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B]
Dense.c:230:9: note: add new stmt: MEM <vector(4) long unsigned int> [(struct PULSE_Layer *)&<retval> + 88B] = _105;
Dense.c:230:9: note: vectorizing stmts using SLP.
Dense.c:230:9: note: ***** The result for vector mode V32QI would be the same
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:4:18: note: vectorized 0 loops in function.
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_4, inputs_15, _3);
PULSE.c:8:2: missed: statement clobbers memory: _5 (layer_17);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_12, pretmp_26, _9);
PULSE.c:13:28: note: ***** Analysis failed with vector mode V8SI
PULSE.c:13:28: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:22:2: missed: statement clobbers memory: _1 (layer_10(D));
PULSE.c:22:2: missed: statement clobbers memory: _14 (_2);
PULSE.c:22:2: missed: statement clobbers memory: _21 (_15);
PULSE.c:22:2: missed: statement clobbers memory: _28 (_22);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_29);
PULSE.c:26:3: missed: statement clobbers memory: memset (_34, 0, _32);
PULSE.c:26:3: missed: statement clobbers memory: memset (_27, 0, _25);
PULSE.c:26:3: missed: statement clobbers memory: memset (_20, 0, _18);
PULSE.c:26:3: missed: statement clobbers memory: memset (_7, 0, _5);
PULSE.c:28:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:28:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:39: missed: statement clobbers memory: _7 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:32:6: note: vectorized 1 loops in function.
PULSE.c:34:8: missed: statement clobbers memory: _1 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_2);
PULSE.c:39:39: missed: statement clobbers memory: _7 = rand ();
PULSE.c:40:40: missed: statement clobbers memory: _9 = rand ();
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8SI
PULSE.c:32:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:61:16: missed: couldn't vectorize loop
PULSE.c:61:16: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:64:17: missed: couldn't vectorize loop
PULSE.c:64:17: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:73:19: missed: couldn't vectorize loop
PULSE.c:73:19: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:39: missed: statement clobbers memory: _69 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:53:22: missed: couldn't vectorize loop
PULSE.c:53:22: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:47:6: note: vectorized 1 loops in function.
PULSE.c:48:1: missed: statement clobbers memory: saved_stack.7_39 = __builtin_stack_save ();
PULSE.c:49:46: missed: statement clobbers memory: _1 = PULSE_GetLossFunctionPtr (loss_function_40(D));
PULSE.c:60:15: missed: statement clobbers memory: random.4_46 = __builtin_alloca_with_align (_4, 32);
PULSE.c:34:8: missed: statement clobbers memory: _61 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_62);
PULSE.c:39:39: missed: statement clobbers memory: _69 = rand ();
PULSE.c:40:40: missed: statement clobbers memory: _72 = rand ();
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_98, inputs_93, _97);
PULSE.c:8:2: missed: statement clobbers memory: _99 (layer_94);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_105, _104, _103);
PULSE.c:67:11: missed: statement clobbers memory: loss_55 = PULSE_GetLoss_42 (_17, _16, _12, _11);
PULSE.c:22:2: missed: statement clobbers memory: _86 (output_34);
PULSE.c:22:2: missed: statement clobbers memory: _108 (_87);
PULSE.c:22:2: missed: statement clobbers memory: _115 (_109);
PULSE.c:22:2: missed: statement clobbers memory: _122 (_116);
PULSE.c:22:2: missed: statement clobbers memory: _129 (_123);
PULSE.c:22:2: missed: statement clobbers memory: _136 (_130);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_137);
PULSE.c:26:3: missed: statement clobbers memory: memset (_142, 0, _140);
PULSE.c:26:3: missed: statement clobbers memory: memset (_135, 0, _133);
PULSE.c:26:3: missed: statement clobbers memory: memset (_128, 0, _126);
PULSE.c:26:3: missed: statement clobbers memory: memset (_121, 0, _119);
PULSE.c:26:3: missed: statement clobbers memory: memset (_114, 0, _112);
PULSE.c:26:3: missed: statement clobbers memory: memset (_92, 0, _90);
PULSE.c:75:6: missed: statement clobbers memory: _21 (current_191, args);
PULSE.c:81:4: missed: statement clobbers memory: printf ("Epoch: %d | Item: %d | Loss: %.10f | Batch Loss: %.10f\r", i_188, j_189, _25, _24);
PULSE.c:47:6: missed: statement clobbers memory: __builtin_stack_restore (saved_stack.7_39);
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4DI
PULSE.c:47:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:91:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:91:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:125:20: missed: couldn't vectorize loop
PULSE.c:125:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:125:20: missed: couldn't vectorize loop
PULSE.c:125:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:104:20: missed: couldn't vectorize loop
PULSE.c:104:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:104:20: missed: couldn't vectorize loop
PULSE.c:104:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:93:13: note: vectorized 0 loops in function.
PULSE.c:96:39: missed: statement clobbers memory: layers_41 = malloc (_3);
PULSE.c:99:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:118:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:119:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:120:46: missed: statement clobbers memory: WEIGHTS_86 = aligned_alloc (64, 0);
PULSE.c:110:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.105_94];
PULSE.c:111:21: missed: statement clobbers memory: _75 = PULSE_GetDenseWeightsSize (args);
PULSE.c:112:16: missed: statement clobbers memory: _78 = PULSE_GetDenseIOSize (args);
PULSE.c:113:19: missed: statement clobbers memory: _81 = PULSE_GetDenseFixesSize (args);
PULSE.c:110:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})_224];
PULSE.c:111:21: missed: statement clobbers memory: _210 = PULSE_GetDenseWeightsSize (args);
PULSE.c:112:16: missed: statement clobbers memory: _213 = PULSE_GetDenseIOSize (args);
PULSE.c:113:19: missed: statement clobbers memory: _216 = PULSE_GetDenseFixesSize (args);
PULSE.c:118:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:119:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:120:46: missed: statement clobbers memory: WEIGHTS_46 = aligned_alloc (64, _5);
PULSE.c:121:41: missed: statement clobbers memory: IO_48 = aligned_alloc (64, _7);
PULSE.c:131:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.107_96];
PULSE.c:132:17: missed: statement clobbers memory: *_10 = PULSE_CreateDenseLayer (args, WEIGHTS_PTR_152, IO_PTR_153); [return slot optimization]
PULSE.c:133:20: missed: statement clobbers memory: _64 = PULSE_GetDenseWeightsSize (args);
PULSE.c:134:15: missed: statement clobbers memory: _67 = PULSE_GetDenseIOSize (args);
PULSE.c:131:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.107_124];
PULSE.c:132:17: missed: statement clobbers memory: *_106 = PULSE_CreateDenseLayer (args, WEIGHTS_PTR_148, IO_PTR_143); [return slot optimization]
PULSE.c:133:20: missed: statement clobbers memory: _95 = PULSE_GetDenseWeightsSize (args);
PULSE.c:134:15: missed: statement clobbers memory: _59 = PULSE_GetDenseIOSize (args);
PULSE.c:140:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:141:9: note: ***** Analysis succeeded with vector mode V8SI
PULSE.c:141:9: note: SLPing BB part
PULSE.c:141:9: note: Costing subgraph: 
PULSE.c:141:9: note: node 0x4246488 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:141:9: note: op template: <retval>.weights_size = WEIGHTS_SIZE_164;
PULSE.c:141:9: note: 	stmt 0 <retval>.weights_size = WEIGHTS_SIZE_164;
PULSE.c:141:9: note: 	stmt 1 <retval>.io_size = IO_SIZE_167;
PULSE.c:141:9: note: 	children 0x4246508
PULSE.c:141:9: note: node 0x4246508 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:141:9: note: op template: WEIGHTS_SIZE_164 = PHI <WEIGHTS_SIZE_161(36), 0(25)>
PULSE.c:141:9: note: 	stmt 0 WEIGHTS_SIZE_164 = PHI <WEIGHTS_SIZE_161(36), 0(25)>
PULSE.c:141:9: note: 	stmt 1 IO_SIZE_167 = PHI <IO_SIZE_160(36), 0(25)>
PULSE.c:141:9: note: 	children 0x4246588 0x4246d88
PULSE.c:141:9: note: node 0x4246588 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:141:9: note: op template: WEIGHTS_SIZE_161 = PHI <WEIGHTS_SIZE_20(11), WEIGHTS_SIZE_206(76)>
PULSE.c:141:9: note: 	stmt 0 WEIGHTS_SIZE_161 = PHI <WEIGHTS_SIZE_20(11), WEIGHTS_SIZE_206(76)>
PULSE.c:141:9: note: 	stmt 1 IO_SIZE_160 = PHI <IO_SIZE_22(11), IO_SIZE_192(76)>
PULSE.c:141:9: note: 	children 0x4246608 0x4246988
PULSE.c:141:9: note: node 0x4246608 (max_nunits=2, refcnt=3) vector(2) unsigned int
PULSE.c:141:9: note: op template: WEIGHTS_SIZE_20 = PHI <WEIGHTS_SIZE_73(77), WEIGHTS_SIZE_76(10)>
PULSE.c:141:9: note: 	stmt 0 WEIGHTS_SIZE_20 = PHI <WEIGHTS_SIZE_73(77), WEIGHTS_SIZE_76(10)>
PULSE.c:141:9: note: 	stmt 1 IO_SIZE_22 = PHI <IO_SIZE_147(77), IO_SIZE_79(10)>
PULSE.c:141:9: note: 	children 0x4246688 0x4246788
PULSE.c:141:9: note: node 0x4246688 (max_nunits=2, refcnt=2) vector(2) unsigned int
PULSE.c:141:9: note: op template: WEIGHTS_SIZE_73 = PHI <WEIGHTS_SIZE_20(73), 0(71)>
PULSE.c:141:9: note: 	stmt 0 WEIGHTS_SIZE_73 = PHI <WEIGHTS_SIZE_20(73), 0(71)>
PULSE.c:141:9: note: 	stmt 1 IO_SIZE_147 = PHI <IO_SIZE_22(73), 0(71)>
PULSE.c:141:9: note: 	children 0x4246608 0x4246708
PULSE.c:141:9: note: node (constant) 0x4246708 (max_nunits=1, refcnt=1) vector(2) unsigned int
PULSE.c:141:9: note: 	{ 0, 0 }
PULSE.c:141:9: note: node 0x4246788 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:141:9: note: op template: WEIGHTS_SIZE_76 = WEIGHTS_SIZE_73 + _75;
PULSE.c:141:9: note: 	stmt 0 WEIGHTS_SIZE_76 = WEIGHTS_SIZE_73 + _75;
PULSE.c:141:9: note: 	stmt 1 IO_SIZE_79 = _78 + IO_SIZE_147;
PULSE.c:141:9: note: 	children 0x4246688 0x4246908
PULSE.c:141:9: note: node (external) 0x4246908 (max_nunits=1, refcnt=1) vector(2) unsigned int
PULSE.c:141:9: note: 	{ _75, _78 }
PULSE.c:141:9: note: node 0x4246988 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:141:9: note: op template: WEIGHTS_SIZE_206 = PHI <WEIGHTS_SIZE_238(68)>
PULSE.c:141:9: note: 	stmt 0 WEIGHTS_SIZE_206 = PHI <WEIGHTS_SIZE_238(68)>
PULSE.c:141:9: note: 	stmt 1 IO_SIZE_192 = PHI <IO_SIZE_239(68)>
PULSE.c:141:9: note: 	children 0x4246a08
PULSE.c:141:9: note: node 0x4246a08 (max_nunits=2, refcnt=2) vector(2) unsigned int
PULSE.c:141:9: note: op template: WEIGHTS_SIZE_238 = PHI <WEIGHTS_SIZE_211(59), WEIGHTS_SIZE_200(57)>
PULSE.c:141:9: note: 	stmt 0 WEIGHTS_SIZE_238 = PHI <WEIGHTS_SIZE_211(59), WEIGHTS_SIZE_200(57)>
PULSE.c:141:9: note: 	stmt 1 IO_SIZE_239 = PHI <IO_SIZE_214(59), IO_SIZE_201(57)>
PULSE.c:141:9: note: 	children 0x4246a88 0x4246b88
PULSE.c:141:9: note: node 0x4246a88 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:141:9: note: op template: WEIGHTS_SIZE_211 = WEIGHTS_SIZE_200 + _210;
PULSE.c:141:9: note: 	stmt 0 WEIGHTS_SIZE_211 = WEIGHTS_SIZE_200 + _210;
PULSE.c:141:9: note: 	stmt 1 IO_SIZE_214 = _213 + IO_SIZE_201;
PULSE.c:141:9: note: 	children 0x4246b88 0x4246d08
PULSE.c:141:9: note: node 0x4246b88 (max_nunits=2, refcnt=2) vector(2) unsigned int
PULSE.c:141:9: note: op template: WEIGHTS_SIZE_200 = PHI <WEIGHTS_SIZE_243(72), WEIGHTS_SIZE_238(69)>
PULSE.c:141:9: note: 	stmt 0 WEIGHTS_SIZE_200 = PHI <WEIGHTS_SIZE_243(72), WEIGHTS_SIZE_238(69)>
PULSE.c:141:9: note: 	stmt 1 IO_SIZE_201 = PHI <IO_SIZE_244(72), IO_SIZE_239(69)>
PULSE.c:141:9: note: 	children 0x4246c08 0x4246a08
PULSE.c:141:9: note: node 0x4246c08 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:141:9: note: op template: WEIGHTS_SIZE_243 = PHI <WEIGHTS_SIZE_20(74)>
PULSE.c:141:9: note: 	stmt 0 WEIGHTS_SIZE_243 = PHI <WEIGHTS_SIZE_20(74)>
PULSE.c:141:9: note: 	stmt 1 IO_SIZE_244 = PHI <IO_SIZE_22(74)>
PULSE.c:141:9: note: 	children 0x4246608
PULSE.c:141:9: note: node (external) 0x4246d08 (max_nunits=1, refcnt=1) vector(2) unsigned int
PULSE.c:141:9: note: 	{ _210, _213 }
PULSE.c:141:9: note: node (constant) 0x4246d88 (max_nunits=1, refcnt=1) vector(2) unsigned int
PULSE.c:141:9: note: 	{ 0, 0 }
PULSE.c:141:9: note: Cost model analysis: 
PULSE.c:141:9: note: Cost model analysis for part in loop 0:
  Vector cost: 48
  Scalar cost: 48
PULSE.c:141:9: note: Cost model analysis for part in loop 1:
  Vector cost: 44
  Scalar cost: 24
PULSE.c:141:9: missed: not vectorized: vectorization is not profitable.
PULSE.c:141:9: note: Costing subgraph: 
PULSE.c:141:9: note: node 0x4246e88 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:141:9: note: op template: <retval>.layers = layers_41;
PULSE.c:141:9: note: 	stmt 0 <retval>.layers = layers_41;
PULSE.c:141:9: note: 	stmt 1 <retval>.weights = WEIGHTS_163;
PULSE.c:141:9: note: 	children 0x4246f88
PULSE.c:141:9: note: node (external) 0x4246f88 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:141:9: note: 	{ layers_41, WEIGHTS_163 }
PULSE.c:141:9: note: Cost model analysis: 
PULSE.c:141:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:141:9: note: Costing subgraph: 
PULSE.c:141:9: note: node 0x4247008 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:141:9: note: op template: <retval>.io = 0B;
PULSE.c:141:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:141:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:141:9: note: 	children 0x4247088
PULSE.c:141:9: note: node (constant) 0x4247088 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:141:9: note: 	{ 0B, 0B }
PULSE.c:141:9: note: Cost model analysis: 
PULSE.c:141:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:141:9: note: Basic block will be vectorized using SLP
PULSE.c:141:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:141:9: note: Vectorizing SLP tree:
PULSE.c:141:9: note: node 0x4246e88 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:141:9: note: op template: <retval>.layers = layers_41;
PULSE.c:141:9: note: 	stmt 0 <retval>.layers = layers_41;
PULSE.c:141:9: note: 	stmt 1 <retval>.weights = WEIGHTS_163;
PULSE.c:141:9: note: 	children 0x4246f88
PULSE.c:141:9: note: node (external) 0x4246f88 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:141:9: note: 	{ layers_41, WEIGHTS_163 }
PULSE.c:141:9: note: ------>vectorizing SLP node starting from: <retval>.layers = layers_41;
PULSE.c:141:9: note: vect_is_simple_use: operand WEIGHTS_163 = PHI <WEIGHTS_46(36), WEIGHTS_86(25)>, type of def: internal
PULSE.c:141:9: note: conflicting alias set types.
PULSE.c:141:9: note: transform store. ncopies = 1
PULSE.c:141:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.layers
PULSE.c:141:9: note: created &<retval>
PULSE.c:141:9: note: add new stmt: MEM <vector(2) long unsigned int> [(void *)&<retval>] = _19;
PULSE.c:141:9: note: vectorizing stmts using SLP.
PULSE.c:141:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:141:9: note: Vectorizing SLP tree:
PULSE.c:141:9: note: node 0x4247008 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:141:9: note: op template: <retval>.io = 0B;
PULSE.c:141:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:141:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:141:9: note: 	children 0x4247088
PULSE.c:141:9: note: node (constant) 0x4247088 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:141:9: note: 	{ 0B, 0B }
PULSE.c:141:9: note: ------>vectorizing SLP node starting from: <retval>.io = 0B;
PULSE.c:141:9: note: vect_is_simple_use: operand 0B, type of def: constant
PULSE.c:141:9: note: transform store. ncopies = 1
PULSE.c:141:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.io
PULSE.c:141:9: note: created &<retval>.io
PULSE.c:141:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval> + 16B] = { 0, 0 };
PULSE.c:141:9: note: vectorizing stmts using SLP.
PULSE.c:141:9: note: ***** The result for vector mode V32QI would be the same
PULSE.c:149:3: missed: statement clobbers memory: free (_1);
PULSE.c:155:3: missed: statement clobbers memory: free (prephitmp_16);
PULSE.c:161:3: missed: statement clobbers memory: free (_4);
PULSE.c:164:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:164:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _28 (pretmp_91, _51, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:19: missed: not vectorized: no vectype for stmt: _22 = *_21;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:170:10: optimized: loop vectorized using 32 byte vectors
Dense.c:170:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:170:10: optimized: loop vectorized using 16 byte vectors
Dense.c:161:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
Dense.c:154:10: optimized: loop vectorized using 32 byte vectors
Dense.c:154:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:154:10: optimized: loop vectorized using 16 byte vectors
Dense.c:145:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
Dense.c:136:13: note: vectorized 2 loops in function.
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_28, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
Dense.c:136:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:136:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:105:24: missed: couldn't vectorize loop
Dense.c:105:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:108:17: missed: couldn't vectorize loop
Dense.c:108:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:122:24: missed: couldn't vectorize loop
Dense.c:122:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:125:17: missed: couldn't vectorize loop
Dense.c:125:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:90:15: missed: couldn't vectorize loop
Dense.c:90:15: missed: not vectorized: number of iterations cannot be computed.
Dense.c:83:13: note: vectorized 0 loops in function.
Dense.c:86:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:110:17: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:110:17: note: SLPing BB part
Dense.c:127:17: note: Costing subgraph: 
Dense.c:127:17: note: node 0x7476da8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:127:17: note: op template: _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 0 _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 1 _198 = (sizetype) wi_137;
Dense.c:127:17: note: 	children 0x7476ea8
Dense.c:127:17: note: node (external) 0x7476ea8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:127:17: note: 	{ j_135, wi_137 }
Dense.c:127:17: note: Cost model analysis: 
Dense.c:127:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:127:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:127:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: Costing subgraph: 
Dense.c:110:17: note: node 0x7476fa8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:110:17: note: op template: _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 0 _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 1 _217 = (sizetype) wi_136;
Dense.c:110:17: note: 	children 0x74770a8
Dense.c:110:17: note: node (external) 0x74770a8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:110:17: note: 	{ j_134, wi_136 }
Dense.c:110:17: note: Cost model analysis: 
Dense.c:110:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:110:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:110:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: ***** The result for vector mode V32QI would be the same
Dense.c:110:17: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:110:17: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:110:17: note: SLPing BB part
Dense.c:127:17: note: Costing subgraph: 
Dense.c:127:17: note: node 0x7476fa8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:127:17: note: op template: _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 0 _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 1 _198 = (sizetype) wi_137;
Dense.c:127:17: note: 	children 0x7476da8
Dense.c:127:17: note: node (external) 0x7476da8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:127:17: note: 	{ j_135, wi_137 }
Dense.c:127:17: note: Cost model analysis: 
Dense.c:127:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:127:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:127:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: Costing subgraph: 
Dense.c:110:17: note: node 0x7476f28 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:110:17: note: op template: _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 0 _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 1 _217 = (sizetype) wi_136;
Dense.c:110:17: note: 	children 0x7477028
Dense.c:110:17: note: node (external) 0x7477028 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:110:17: note: 	{ j_134, wi_136 }
Dense.c:110:17: note: Cost model analysis: 
Dense.c:110:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:110:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:110:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:105:51: note: ***** Analysis failed with vector mode V8QI
Dense.c:105:51: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:105:51: note: ***** Analysis failed with vector mode V4QI
Dense.c:62:15: missed: couldn't vectorize loop
Dense.c:62:15: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:75:11: optimized: loop vectorized using 32 byte vectors
Dense.c:75:11: optimized: loop vectorized using 16 byte vectors
Dense.c:66:11: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:881:10: missed: not vectorized: no vectype for stmt: _53 = MEM[(__m256 * {ref-all})w_ptr_82];
 scalar_type: __m256
Dense.c:54:13: note: vectorized 1 loops in function.
Dense.c:57:2: missed: statement clobbers memory: memcpy (_4, dense$baiases_63, _3);
Dense.c:79:2: missed: statement clobbers memory: _27 (pretmp_163, _68, 0);
Dense.c:54:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:54:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:179:107: note: ***** Analysis failed with vector mode V8SI
Dense.c:179:107: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
Dense.c:180:85: note: ***** Analysis failed with vector mode V8SI
Dense.c:180:85: note: ***** The result for vector mode V32QI would be the same
Dense.c:180:85: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:180:85: note: ***** Analysis failed with vector mode V16QI
Dense.c:180:85: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:180:85: note: ***** Analysis failed with vector mode V8QI
Dense.c:180:85: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:180:85: note: ***** Analysis failed with vector mode V4QI
Dense.c:181:105: note: ***** Analysis failed with vector mode V8SI
Dense.c:181:105: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
Dense.c:193:19: missed: couldn't vectorize loop
Dense.c:193:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:183:13: note: vectorized 0 loops in function.
Dense.c:189:37: missed: statement clobbers memory: _9 = aligned_alloc (64, _8);
Dense.c:190:34: missed: statement clobbers memory: _11 = aligned_alloc (64, _10);
Dense.c:191:35: missed: statement clobbers memory: _12 = aligned_alloc (64, _10);
Dense.c:194:38: missed: statement clobbers memory: _13 = rand ();
Dense.c:194:72: missed: statement clobbers memory: _79 = sqrt (_20);
Dense.c:207:19: missed: statement clobbers memory: _31 = PULSE_GetActivationFunctionPtr (_30);
Dense.c:209:34: missed: statement clobbers memory: _33 = aligned_alloc (64, _10);
Dense.c:225:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:226:4: missed: statement clobbers memory: exit (1);
Dense.c:230:9: note: ***** Analysis succeeded with vector mode V8SI
Dense.c:230:9: note: SLPing BB part
Dense.c:230:9: note: Costing subgraph: 
Dense.c:230:9: note: node 0x7380de8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:230:9: note: op template: <retval>.n_inputs = args$n_inputs_53;
Dense.c:230:9: note: 	stmt 0 <retval>.n_inputs = args$n_inputs_53;
Dense.c:230:9: note: 	stmt 1 <retval>.n_outputs = args$n_outputs_54;
Dense.c:230:9: note: 	children 0x7380e68
Dense.c:230:9: note: node 0x7380e68 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:230:9: note: op template: args$n_inputs_53 = args.n_inputs;
Dense.c:230:9: note: 	stmt 0 args$n_inputs_53 = args.n_inputs;
Dense.c:230:9: note: 	stmt 1 args$n_outputs_54 = args.n_outputs;
Dense.c:230:9: note: Cost model analysis: 
Dense.c:230:9: note: Cost model analysis for part in loop 0:
  Vector cost: 40
  Scalar cost: 56
Dense.c:230:9: note: Costing subgraph: 
Dense.c:230:9: note: node 0x7380f68 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:230:9: note: op template: <retval>.inputs = IO_42(D);
Dense.c:230:9: note: 	stmt 0 <retval>.inputs = IO_42(D);
Dense.c:230:9: note: 	stmt 1 <retval>.outputs = _29;
Dense.c:230:9: note: 	children 0x7380fe8
Dense.c:230:9: note: node (external) 0x7380fe8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:230:9: note: 	{ IO_42(D), _29 }
Dense.c:230:9: note: Cost model analysis: 
Dense.c:230:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:230:9: note: Costing subgraph: 
Dense.c:230:9: note: node 0x73810e8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:230:9: note: op template: <retval>.feed = layer$feed_41;
Dense.c:230:9: note: 	stmt 0 <retval>.feed = layer$feed_41;
Dense.c:230:9: note: 	stmt 1 <retval>.back = layer$back_51;
Dense.c:230:9: note: 	children 0x73812e8
Dense.c:230:9: note: node (external) 0x73812e8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:230:9: note: 	{ layer$feed_41, layer$back_51 }
Dense.c:230:9: note: Cost model analysis: 
Dense.c:230:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:230:9: note: Costing subgraph: 
Dense.c:230:9: note: node 0x73811e8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:230:9: note: op template: <retval>.parent = 0B;
Dense.c:230:9: note: 	stmt 0 <retval>.parent = 0B;
Dense.c:230:9: note: 	stmt 1 <retval>.child = 0B;
Dense.c:230:9: note: 	children 0x7381368
Dense.c:230:9: note: node (constant) 0x7381368 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:230:9: note: 	{ 0B, 0B }
Dense.c:230:9: note: Cost model analysis: 
Dense.c:230:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:230:9: note: Costing subgraph: 
Dense.c:230:9: note: node 0x7381468 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:230:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_37(D);
Dense.c:230:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_37(D);
Dense.c:230:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&<retval> + 96B] = _4;
Dense.c:230:9: note: 	stmt 2 MEM <float *> [(struct PULSE_Layer *)&<retval> + 104B] = _11;
Dense.c:230:9: note: 	stmt 3 MEM <float *> [(struct PULSE_Layer *)&<retval> + 112B] = _12;
Dense.c:230:9: note: 	children 0x73814e8
Dense.c:230:9: note: node (external) 0x73814e8 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:230:9: note: 	{ MODEL_37(D), _4, _11, _12 }
Dense.c:230:9: note: Cost model analysis: 
Dense.c:230:9: note: Cost model analysis for part in loop 0:
  Vector cost: 64
  Scalar cost: 64
Dense.c:230:9: note: Basic block will be vectorized using SLP
Dense.c:230:9: optimized: basic block part vectorized using 8 byte vectors
Dense.c:230:9: note: Vectorizing SLP tree:
Dense.c:230:9: note: node 0x7380de8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:230:9: note: op template: <retval>.n_inputs = args$n_inputs_53;
Dense.c:230:9: note: 	stmt 0 <retval>.n_inputs = args$n_inputs_53;
Dense.c:230:9: note: 	stmt 1 <retval>.n_outputs = args$n_outputs_54;
Dense.c:230:9: note: 	children 0x7380e68
Dense.c:230:9: note: node 0x7380e68 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:230:9: note: op template: args$n_inputs_53 = args.n_inputs;
Dense.c:230:9: note: 	stmt 0 args$n_inputs_53 = args.n_inputs;
Dense.c:230:9: note: 	stmt 1 args$n_outputs_54 = args.n_outputs;
Dense.c:230:9: note: ------>vectorizing SLP node starting from: args$n_inputs_53 = args.n_inputs;
Dense.c:230:9: note: transform load. ncopies = 1
Dense.c:230:9: note: conflicting alias set types.
Dense.c:230:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: args.n_inputs
Dense.c:230:9: note: created &args
Dense.c:230:9: note: add new stmt: vect_args_n_inputs_53.508_88 = MEM <vector(2) unsigned int> [(void *)&args];
Dense.c:230:9: note: extracting lane for live stmt args$n_inputs_53 = args.n_inputs;
Dense.c:230:9: note: extracting lane for live stmt args$n_outputs_54 = args.n_outputs;
Dense.c:230:9: note: ------>vectorizing SLP node starting from: <retval>.n_inputs = args$n_inputs_53;
Dense.c:230:9: note: vect_is_simple_use: operand args.n_inputs, type of def: internal
Dense.c:230:9: note: vect_is_simple_use: operand args.n_outputs, type of def: internal
Dense.c:230:9: note: transform store. ncopies = 1
Dense.c:230:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: <retval>.n_inputs
Dense.c:230:9: note: created &<retval>.n_inputs
Dense.c:230:9: note: add new stmt: MEM <vector(2) unsigned int> [(unsigned int *)&<retval> + 128B] = vect_args_n_inputs_53.508_88;
Dense.c:230:9: note: vectorizing stmts using SLP.
Dense.c:230:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:230:9: note: Vectorizing SLP tree:
Dense.c:230:9: note: node 0x7380f68 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:230:9: note: op template: <retval>.inputs = IO_42(D);
Dense.c:230:9: note: 	stmt 0 <retval>.inputs = IO_42(D);
Dense.c:230:9: note: 	stmt 1 <retval>.outputs = _29;
Dense.c:230:9: note: 	children 0x7380fe8
Dense.c:230:9: note: node (external) 0x7380fe8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:230:9: note: 	{ IO_42(D), _29 }
Dense.c:230:9: note: ------>vectorizing SLP node starting from: <retval>.inputs = IO_42(D);
Dense.c:230:9: note: vect_is_simple_use: operand IO_42(D) + _28, type of def: internal
Dense.c:230:9: note: transform store. ncopies = 1
Dense.c:230:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.inputs
Dense.c:230:9: note: created &<retval>
Dense.c:230:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval>] = _94;
Dense.c:230:9: note: vectorizing stmts using SLP.
Dense.c:230:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:230:9: note: Vectorizing SLP tree:
Dense.c:230:9: note: node 0x73810e8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:230:9: note: op template: <retval>.feed = layer$feed_41;
Dense.c:230:9: note: 	stmt 0 <retval>.feed = layer$feed_41;
Dense.c:230:9: note: 	stmt 1 <retval>.back = layer$back_51;
Dense.c:230:9: note: 	children 0x73812e8
Dense.c:230:9: note: node (external) 0x73812e8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:230:9: note: 	{ layer$feed_41, layer$back_51 }
Dense.c:230:9: note: ------>vectorizing SLP node starting from: <retval>.feed = layer$feed_41;
Dense.c:230:9: note: vect_is_simple_use: operand layer$back_51 = PHI <layer$back_57(D)(11), _BackDense(9), _SIMD_BackDense(15)>, type of def: internal
Dense.c:230:9: note: transform store. ncopies = 1
Dense.c:230:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.feed
Dense.c:230:9: note: created &<retval>.feed
Dense.c:230:9: note: add new stmt: MEM <vector(2) long unsigned int> [(void (*<T921>) (struct PULSE_Layer *) *)&<retval> + 32B] = _98;
Dense.c:230:9: note: vectorizing stmts using SLP.
Dense.c:230:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:230:9: note: Vectorizing SLP tree:
Dense.c:230:9: note: node 0x73811e8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:230:9: note: op template: <retval>.parent = 0B;
Dense.c:230:9: note: 	stmt 0 <retval>.parent = 0B;
Dense.c:230:9: note: 	stmt 1 <retval>.child = 0B;
Dense.c:230:9: note: 	children 0x7381368
Dense.c:230:9: note: node (constant) 0x7381368 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:230:9: note: 	{ 0B, 0B }
Dense.c:230:9: note: ------>vectorizing SLP node starting from: <retval>.parent = 0B;
Dense.c:230:9: note: vect_is_simple_use: operand 0B, type of def: constant
Dense.c:230:9: note: transform store. ncopies = 1
Dense.c:230:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.parent
Dense.c:230:9: note: created &<retval>.parent
Dense.c:230:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer * *)&<retval> + 72B] = { 0, 0 };
Dense.c:230:9: note: vectorizing stmts using SLP.
Dense.c:230:9: optimized: basic block part vectorized using 32 byte vectors
Dense.c:230:9: note: Vectorizing SLP tree:
Dense.c:230:9: note: node 0x7381468 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:230:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_37(D);
Dense.c:230:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_37(D);
Dense.c:230:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&<retval> + 96B] = _4;
Dense.c:230:9: note: 	stmt 2 MEM <float *> [(struct PULSE_Layer *)&<retval> + 104B] = _11;
Dense.c:230:9: note: 	stmt 3 MEM <float *> [(struct PULSE_Layer *)&<retval> + 112B] = _12;
Dense.c:230:9: note: 	children 0x73814e8
Dense.c:230:9: note: node (external) 0x73814e8 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:230:9: note: 	{ MODEL_37(D), _4, _11, _12 }
Dense.c:230:9: note: ------>vectorizing SLP node starting from: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_37(D);
Dense.c:230:9: note: vect_is_simple_use: operand MODEL_37(D) + _3, type of def: internal
Dense.c:230:9: note: vect_is_simple_use: operand aligned_alloc (64, _10), type of def: internal
Dense.c:230:9: note: vect_is_simple_use: operand aligned_alloc (64, _10), type of def: internal
Dense.c:230:9: note: transform store. ncopies = 1
Dense.c:230:9: note: create vector_type-pointer variable to type: vector(4) long unsigned int  vectorizing a pointer ref: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B]
Dense.c:230:9: note: created &MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B]
Dense.c:230:9: note: add new stmt: MEM <vector(4) long unsigned int> [(struct PULSE_Layer *)&<retval> + 88B] = _105;
Dense.c:230:9: note: vectorizing stmts using SLP.
Dense.c:230:9: note: ***** The result for vector mode V32QI would be the same
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:4:18: note: vectorized 0 loops in function.
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_4, inputs_15, _3);
PULSE.c:8:2: missed: statement clobbers memory: _5 (layer_17);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_12, pretmp_26, _9);
PULSE.c:13:28: note: ***** Analysis failed with vector mode V8SI
PULSE.c:13:28: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:22:2: missed: statement clobbers memory: _1 (layer_10(D));
PULSE.c:22:2: missed: statement clobbers memory: _14 (_2);
PULSE.c:22:2: missed: statement clobbers memory: _21 (_15);
PULSE.c:22:2: missed: statement clobbers memory: _28 (_22);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_29);
PULSE.c:26:3: missed: statement clobbers memory: memset (_34, 0, _32);
PULSE.c:26:3: missed: statement clobbers memory: memset (_27, 0, _25);
PULSE.c:26:3: missed: statement clobbers memory: memset (_20, 0, _18);
PULSE.c:26:3: missed: statement clobbers memory: memset (_7, 0, _5);
PULSE.c:28:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:28:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:39: missed: statement clobbers memory: _7 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:32:6: note: vectorized 1 loops in function.
PULSE.c:34:8: missed: statement clobbers memory: _1 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_2);
PULSE.c:39:39: missed: statement clobbers memory: _7 = rand ();
PULSE.c:40:40: missed: statement clobbers memory: _9 = rand ();
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8SI
PULSE.c:32:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:62:16: missed: couldn't vectorize loop
PULSE.c:62:16: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:65:17: missed: couldn't vectorize loop
PULSE.c:65:17: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:74:19: missed: couldn't vectorize loop
PULSE.c:74:19: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:39: missed: statement clobbers memory: _69 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:54:22: missed: couldn't vectorize loop
PULSE.c:54:22: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:47:6: note: vectorized 1 loops in function.
PULSE.c:48:1: missed: statement clobbers memory: saved_stack.7_39 = __builtin_stack_save ();
PULSE.c:49:46: missed: statement clobbers memory: _1 = PULSE_GetLossFunctionPtr (loss_function_40(D));
PULSE.c:61:15: missed: statement clobbers memory: random.4_46 = __builtin_alloca_with_align (_4, 32);
PULSE.c:34:8: missed: statement clobbers memory: _61 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_62);
PULSE.c:39:39: missed: statement clobbers memory: _69 = rand ();
PULSE.c:40:40: missed: statement clobbers memory: _72 = rand ();
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_98, inputs_93, _97);
PULSE.c:8:2: missed: statement clobbers memory: _99 (layer_94);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_105, _104, _103);
PULSE.c:68:11: missed: statement clobbers memory: loss_55 = PULSE_GetLoss_42 (_17, _16, _12, _11);
PULSE.c:22:2: missed: statement clobbers memory: _86 (output_34);
PULSE.c:22:2: missed: statement clobbers memory: _108 (_87);
PULSE.c:22:2: missed: statement clobbers memory: _115 (_109);
PULSE.c:22:2: missed: statement clobbers memory: _122 (_116);
PULSE.c:22:2: missed: statement clobbers memory: _129 (_123);
PULSE.c:22:2: missed: statement clobbers memory: _136 (_130);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_137);
PULSE.c:26:3: missed: statement clobbers memory: memset (_142, 0, _140);
PULSE.c:26:3: missed: statement clobbers memory: memset (_135, 0, _133);
PULSE.c:26:3: missed: statement clobbers memory: memset (_128, 0, _126);
PULSE.c:26:3: missed: statement clobbers memory: memset (_121, 0, _119);
PULSE.c:26:3: missed: statement clobbers memory: memset (_114, 0, _112);
PULSE.c:26:3: missed: statement clobbers memory: memset (_92, 0, _90);
PULSE.c:76:6: missed: statement clobbers memory: _21 (current_191, args);
PULSE.c:82:4: missed: statement clobbers memory: printf ("Epoch: %d | Item: %d | Loss: %.10f | Batch Loss: %.10f\r", i_188, j_189, _25, _24);
PULSE.c:47:6: missed: statement clobbers memory: __builtin_stack_restore (saved_stack.7_39);
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4DI
PULSE.c:47:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:92:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:92:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:126:20: missed: couldn't vectorize loop
PULSE.c:126:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:126:20: missed: couldn't vectorize loop
PULSE.c:126:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:105:20: missed: couldn't vectorize loop
PULSE.c:105:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:105:20: missed: couldn't vectorize loop
PULSE.c:105:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:94:13: note: vectorized 0 loops in function.
PULSE.c:97:39: missed: statement clobbers memory: layers_41 = malloc (_3);
PULSE.c:100:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:119:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:120:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:121:46: missed: statement clobbers memory: WEIGHTS_86 = aligned_alloc (64, 0);
PULSE.c:111:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.105_94];
PULSE.c:112:21: missed: statement clobbers memory: _75 = PULSE_GetDenseWeightsSize (args);
PULSE.c:113:16: missed: statement clobbers memory: _78 = PULSE_GetDenseIOSize (args);
PULSE.c:114:19: missed: statement clobbers memory: _81 = PULSE_GetDenseFixesSize (args);
PULSE.c:111:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})_224];
PULSE.c:112:21: missed: statement clobbers memory: _210 = PULSE_GetDenseWeightsSize (args);
PULSE.c:113:16: missed: statement clobbers memory: _213 = PULSE_GetDenseIOSize (args);
PULSE.c:114:19: missed: statement clobbers memory: _216 = PULSE_GetDenseFixesSize (args);
PULSE.c:119:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:120:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:121:46: missed: statement clobbers memory: WEIGHTS_46 = aligned_alloc (64, _5);
PULSE.c:122:41: missed: statement clobbers memory: IO_48 = aligned_alloc (64, _7);
PULSE.c:132:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.107_96];
PULSE.c:133:17: missed: statement clobbers memory: *_10 = PULSE_CreateDenseLayer (args, WEIGHTS_PTR_152, IO_PTR_153, 0B); [return slot optimization]
PULSE.c:134:20: missed: statement clobbers memory: _64 = PULSE_GetDenseWeightsSize (args);
PULSE.c:135:15: missed: statement clobbers memory: _67 = PULSE_GetDenseIOSize (args);
PULSE.c:132:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.107_124];
PULSE.c:133:17: missed: statement clobbers memory: *_106 = PULSE_CreateDenseLayer (args, WEIGHTS_PTR_148, IO_PTR_143, 0B); [return slot optimization]
PULSE.c:134:20: missed: statement clobbers memory: _95 = PULSE_GetDenseWeightsSize (args);
PULSE.c:135:15: missed: statement clobbers memory: _59 = PULSE_GetDenseIOSize (args);
PULSE.c:141:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:142:9: note: ***** Analysis succeeded with vector mode V8SI
PULSE.c:142:9: note: SLPing BB part
PULSE.c:142:9: note: Costing subgraph: 
PULSE.c:142:9: note: node 0x4c5e478 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:142:9: note: op template: <retval>.weights_size = WEIGHTS_SIZE_164;
PULSE.c:142:9: note: 	stmt 0 <retval>.weights_size = WEIGHTS_SIZE_164;
PULSE.c:142:9: note: 	stmt 1 <retval>.io_size = IO_SIZE_167;
PULSE.c:142:9: note: 	children 0x4c5e4f8
PULSE.c:142:9: note: node 0x4c5e4f8 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:142:9: note: op template: WEIGHTS_SIZE_164 = PHI <WEIGHTS_SIZE_161(36), 0(25)>
PULSE.c:142:9: note: 	stmt 0 WEIGHTS_SIZE_164 = PHI <WEIGHTS_SIZE_161(36), 0(25)>
PULSE.c:142:9: note: 	stmt 1 IO_SIZE_167 = PHI <IO_SIZE_160(36), 0(25)>
PULSE.c:142:9: note: 	children 0x4c5e578 0x4c5ed78
PULSE.c:142:9: note: node 0x4c5e578 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:142:9: note: op template: WEIGHTS_SIZE_161 = PHI <WEIGHTS_SIZE_20(11), WEIGHTS_SIZE_206(76)>
PULSE.c:142:9: note: 	stmt 0 WEIGHTS_SIZE_161 = PHI <WEIGHTS_SIZE_20(11), WEIGHTS_SIZE_206(76)>
PULSE.c:142:9: note: 	stmt 1 IO_SIZE_160 = PHI <IO_SIZE_22(11), IO_SIZE_192(76)>
PULSE.c:142:9: note: 	children 0x4c5e5f8 0x4c5e978
PULSE.c:142:9: note: node 0x4c5e5f8 (max_nunits=2, refcnt=3) vector(2) unsigned int
PULSE.c:142:9: note: op template: WEIGHTS_SIZE_20 = PHI <WEIGHTS_SIZE_73(77), WEIGHTS_SIZE_76(10)>
PULSE.c:142:9: note: 	stmt 0 WEIGHTS_SIZE_20 = PHI <WEIGHTS_SIZE_73(77), WEIGHTS_SIZE_76(10)>
PULSE.c:142:9: note: 	stmt 1 IO_SIZE_22 = PHI <IO_SIZE_147(77), IO_SIZE_79(10)>
PULSE.c:142:9: note: 	children 0x4c5e678 0x4c5e778
PULSE.c:142:9: note: node 0x4c5e678 (max_nunits=2, refcnt=2) vector(2) unsigned int
PULSE.c:142:9: note: op template: WEIGHTS_SIZE_73 = PHI <WEIGHTS_SIZE_20(73), 0(71)>
PULSE.c:142:9: note: 	stmt 0 WEIGHTS_SIZE_73 = PHI <WEIGHTS_SIZE_20(73), 0(71)>
PULSE.c:142:9: note: 	stmt 1 IO_SIZE_147 = PHI <IO_SIZE_22(73), 0(71)>
PULSE.c:142:9: note: 	children 0x4c5e5f8 0x4c5e6f8
PULSE.c:142:9: note: node (constant) 0x4c5e6f8 (max_nunits=1, refcnt=1) vector(2) unsigned int
PULSE.c:142:9: note: 	{ 0, 0 }
PULSE.c:142:9: note: node 0x4c5e778 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:142:9: note: op template: WEIGHTS_SIZE_76 = WEIGHTS_SIZE_73 + _75;
PULSE.c:142:9: note: 	stmt 0 WEIGHTS_SIZE_76 = WEIGHTS_SIZE_73 + _75;
PULSE.c:142:9: note: 	stmt 1 IO_SIZE_79 = _78 + IO_SIZE_147;
PULSE.c:142:9: note: 	children 0x4c5e678 0x4c5e8f8
PULSE.c:142:9: note: node (external) 0x4c5e8f8 (max_nunits=1, refcnt=1) vector(2) unsigned int
PULSE.c:142:9: note: 	{ _75, _78 }
PULSE.c:142:9: note: node 0x4c5e978 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:142:9: note: op template: WEIGHTS_SIZE_206 = PHI <WEIGHTS_SIZE_238(68)>
PULSE.c:142:9: note: 	stmt 0 WEIGHTS_SIZE_206 = PHI <WEIGHTS_SIZE_238(68)>
PULSE.c:142:9: note: 	stmt 1 IO_SIZE_192 = PHI <IO_SIZE_239(68)>
PULSE.c:142:9: note: 	children 0x4c5e9f8
PULSE.c:142:9: note: node 0x4c5e9f8 (max_nunits=2, refcnt=2) vector(2) unsigned int
PULSE.c:142:9: note: op template: WEIGHTS_SIZE_238 = PHI <WEIGHTS_SIZE_211(59), WEIGHTS_SIZE_200(57)>
PULSE.c:142:9: note: 	stmt 0 WEIGHTS_SIZE_238 = PHI <WEIGHTS_SIZE_211(59), WEIGHTS_SIZE_200(57)>
PULSE.c:142:9: note: 	stmt 1 IO_SIZE_239 = PHI <IO_SIZE_214(59), IO_SIZE_201(57)>
PULSE.c:142:9: note: 	children 0x4c5ea78 0x4c5eb78
PULSE.c:142:9: note: node 0x4c5ea78 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:142:9: note: op template: WEIGHTS_SIZE_211 = WEIGHTS_SIZE_200 + _210;
PULSE.c:142:9: note: 	stmt 0 WEIGHTS_SIZE_211 = WEIGHTS_SIZE_200 + _210;
PULSE.c:142:9: note: 	stmt 1 IO_SIZE_214 = _213 + IO_SIZE_201;
PULSE.c:142:9: note: 	children 0x4c5eb78 0x4c5ecf8
PULSE.c:142:9: note: node 0x4c5eb78 (max_nunits=2, refcnt=2) vector(2) unsigned int
PULSE.c:142:9: note: op template: WEIGHTS_SIZE_200 = PHI <WEIGHTS_SIZE_243(72), WEIGHTS_SIZE_238(69)>
PULSE.c:142:9: note: 	stmt 0 WEIGHTS_SIZE_200 = PHI <WEIGHTS_SIZE_243(72), WEIGHTS_SIZE_238(69)>
PULSE.c:142:9: note: 	stmt 1 IO_SIZE_201 = PHI <IO_SIZE_244(72), IO_SIZE_239(69)>
PULSE.c:142:9: note: 	children 0x4c5ebf8 0x4c5e9f8
PULSE.c:142:9: note: node 0x4c5ebf8 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:142:9: note: op template: WEIGHTS_SIZE_243 = PHI <WEIGHTS_SIZE_20(74)>
PULSE.c:142:9: note: 	stmt 0 WEIGHTS_SIZE_243 = PHI <WEIGHTS_SIZE_20(74)>
PULSE.c:142:9: note: 	stmt 1 IO_SIZE_244 = PHI <IO_SIZE_22(74)>
PULSE.c:142:9: note: 	children 0x4c5e5f8
PULSE.c:142:9: note: node (external) 0x4c5ecf8 (max_nunits=1, refcnt=1) vector(2) unsigned int
PULSE.c:142:9: note: 	{ _210, _213 }
PULSE.c:142:9: note: node (constant) 0x4c5ed78 (max_nunits=1, refcnt=1) vector(2) unsigned int
PULSE.c:142:9: note: 	{ 0, 0 }
PULSE.c:142:9: note: Cost model analysis: 
PULSE.c:142:9: note: Cost model analysis for part in loop 0:
  Vector cost: 48
  Scalar cost: 48
PULSE.c:142:9: note: Cost model analysis for part in loop 1:
  Vector cost: 44
  Scalar cost: 24
PULSE.c:142:9: missed: not vectorized: vectorization is not profitable.
PULSE.c:142:9: note: Costing subgraph: 
PULSE.c:142:9: note: node 0x4c5ee78 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:142:9: note: op template: <retval>.layers = layers_41;
PULSE.c:142:9: note: 	stmt 0 <retval>.layers = layers_41;
PULSE.c:142:9: note: 	stmt 1 <retval>.weights = WEIGHTS_163;
PULSE.c:142:9: note: 	children 0x4c5ef78
PULSE.c:142:9: note: node (external) 0x4c5ef78 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:142:9: note: 	{ layers_41, WEIGHTS_163 }
PULSE.c:142:9: note: Cost model analysis: 
PULSE.c:142:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:142:9: note: Costing subgraph: 
PULSE.c:142:9: note: node 0x4c5eff8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:142:9: note: op template: <retval>.io = 0B;
PULSE.c:142:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:142:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:142:9: note: 	children 0x4c5f078
PULSE.c:142:9: note: node (constant) 0x4c5f078 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:142:9: note: 	{ 0B, 0B }
PULSE.c:142:9: note: Cost model analysis: 
PULSE.c:142:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:142:9: note: Basic block will be vectorized using SLP
PULSE.c:142:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:142:9: note: Vectorizing SLP tree:
PULSE.c:142:9: note: node 0x4c5ee78 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:142:9: note: op template: <retval>.layers = layers_41;
PULSE.c:142:9: note: 	stmt 0 <retval>.layers = layers_41;
PULSE.c:142:9: note: 	stmt 1 <retval>.weights = WEIGHTS_163;
PULSE.c:142:9: note: 	children 0x4c5ef78
PULSE.c:142:9: note: node (external) 0x4c5ef78 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:142:9: note: 	{ layers_41, WEIGHTS_163 }
PULSE.c:142:9: note: ------>vectorizing SLP node starting from: <retval>.layers = layers_41;
PULSE.c:142:9: note: vect_is_simple_use: operand WEIGHTS_163 = PHI <WEIGHTS_46(36), WEIGHTS_86(25)>, type of def: internal
PULSE.c:142:9: note: conflicting alias set types.
PULSE.c:142:9: note: transform store. ncopies = 1
PULSE.c:142:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.layers
PULSE.c:142:9: note: created &<retval>
PULSE.c:142:9: note: add new stmt: MEM <vector(2) long unsigned int> [(void *)&<retval>] = _19;
PULSE.c:142:9: note: vectorizing stmts using SLP.
PULSE.c:142:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:142:9: note: Vectorizing SLP tree:
PULSE.c:142:9: note: node 0x4c5eff8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:142:9: note: op template: <retval>.io = 0B;
PULSE.c:142:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:142:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:142:9: note: 	children 0x4c5f078
PULSE.c:142:9: note: node (constant) 0x4c5f078 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:142:9: note: 	{ 0B, 0B }
PULSE.c:142:9: note: ------>vectorizing SLP node starting from: <retval>.io = 0B;
PULSE.c:142:9: note: vect_is_simple_use: operand 0B, type of def: constant
PULSE.c:142:9: note: transform store. ncopies = 1
PULSE.c:142:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.io
PULSE.c:142:9: note: created &<retval>.io
PULSE.c:142:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval> + 16B] = { 0, 0 };
PULSE.c:142:9: note: vectorizing stmts using SLP.
PULSE.c:142:9: note: ***** The result for vector mode V32QI would be the same
PULSE.c:148:2: missed: statement clobbers memory: free (_1);
PULSE.c:149:2: missed: statement clobbers memory: free (_2);
PULSE.c:150:2: missed: statement clobbers memory: free (_3);
PULSE.c:151:2: missed: statement clobbers memory: free (_4);
PULSE.c:152:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:152:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _28 (pretmp_91, _51, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:19: missed: not vectorized: no vectype for stmt: _22 = *_21;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:170:10: optimized: loop vectorized using 32 byte vectors
Dense.c:170:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:170:10: optimized: loop vectorized using 16 byte vectors
Dense.c:161:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
Dense.c:154:10: optimized: loop vectorized using 32 byte vectors
Dense.c:154:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:154:10: optimized: loop vectorized using 16 byte vectors
Dense.c:145:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
Dense.c:136:13: note: vectorized 2 loops in function.
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_28, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
Dense.c:136:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:136:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:105:24: missed: couldn't vectorize loop
Dense.c:105:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:108:17: missed: couldn't vectorize loop
Dense.c:108:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:122:24: missed: couldn't vectorize loop
Dense.c:122:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:125:17: missed: couldn't vectorize loop
Dense.c:125:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:90:15: missed: couldn't vectorize loop
Dense.c:90:15: missed: not vectorized: number of iterations cannot be computed.
Dense.c:83:13: note: vectorized 0 loops in function.
Dense.c:86:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:110:17: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:110:17: note: SLPing BB part
Dense.c:127:17: note: Costing subgraph: 
Dense.c:127:17: note: node 0x81d5348 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:127:17: note: op template: _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 0 _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 1 _198 = (sizetype) wi_137;
Dense.c:127:17: note: 	children 0x81d5448
Dense.c:127:17: note: node (external) 0x81d5448 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:127:17: note: 	{ j_135, wi_137 }
Dense.c:127:17: note: Cost model analysis: 
Dense.c:127:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:127:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:127:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: Costing subgraph: 
Dense.c:110:17: note: node 0x81d5548 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:110:17: note: op template: _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 0 _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 1 _217 = (sizetype) wi_136;
Dense.c:110:17: note: 	children 0x81d5648
Dense.c:110:17: note: node (external) 0x81d5648 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:110:17: note: 	{ j_134, wi_136 }
Dense.c:110:17: note: Cost model analysis: 
Dense.c:110:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:110:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:110:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: ***** The result for vector mode V32QI would be the same
Dense.c:110:17: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:110:17: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:110:17: note: SLPing BB part
Dense.c:127:17: note: Costing subgraph: 
Dense.c:127:17: note: node 0x81d5548 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:127:17: note: op template: _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 0 _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 1 _198 = (sizetype) wi_137;
Dense.c:127:17: note: 	children 0x81d5348
Dense.c:127:17: note: node (external) 0x81d5348 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:127:17: note: 	{ j_135, wi_137 }
Dense.c:127:17: note: Cost model analysis: 
Dense.c:127:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:127:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:127:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: Costing subgraph: 
Dense.c:110:17: note: node 0x81d54c8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:110:17: note: op template: _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 0 _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 1 _217 = (sizetype) wi_136;
Dense.c:110:17: note: 	children 0x81d55c8
Dense.c:110:17: note: node (external) 0x81d55c8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:110:17: note: 	{ j_134, wi_136 }
Dense.c:110:17: note: Cost model analysis: 
Dense.c:110:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:110:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:110:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:105:51: note: ***** Analysis failed with vector mode V8QI
Dense.c:105:51: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:105:51: note: ***** Analysis failed with vector mode V4QI
Dense.c:62:15: missed: couldn't vectorize loop
Dense.c:62:15: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:75:11: optimized: loop vectorized using 32 byte vectors
Dense.c:75:11: optimized: loop vectorized using 16 byte vectors
Dense.c:66:11: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:881:10: missed: not vectorized: no vectype for stmt: _53 = MEM[(__m256 * {ref-all})w_ptr_82];
 scalar_type: __m256
Dense.c:54:13: note: vectorized 1 loops in function.
Dense.c:57:2: missed: statement clobbers memory: memcpy (_4, dense$baiases_63, _3);
Dense.c:79:2: missed: statement clobbers memory: _27 (pretmp_163, _68, 0);
Dense.c:54:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:54:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:179:107: note: ***** Analysis failed with vector mode V8SI
Dense.c:179:107: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
Dense.c:180:85: note: ***** Analysis failed with vector mode V8SI
Dense.c:180:85: note: ***** The result for vector mode V32QI would be the same
Dense.c:180:85: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:180:85: note: ***** Analysis failed with vector mode V16QI
Dense.c:180:85: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:180:85: note: ***** Analysis failed with vector mode V8QI
Dense.c:180:85: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:180:85: note: ***** Analysis failed with vector mode V4QI
Dense.c:181:105: note: ***** Analysis failed with vector mode V8SI
Dense.c:181:105: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
Dense.c:193:19: missed: couldn't vectorize loop
Dense.c:193:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:183:13: note: vectorized 0 loops in function.
Dense.c:189:37: missed: statement clobbers memory: _9 = aligned_alloc (64, _8);
Dense.c:190:34: missed: statement clobbers memory: _11 = aligned_alloc (64, _10);
Dense.c:191:35: missed: statement clobbers memory: _12 = aligned_alloc (64, _10);
Dense.c:194:38: missed: statement clobbers memory: _13 = rand ();
Dense.c:194:72: missed: statement clobbers memory: _79 = sqrt (_20);
Dense.c:207:19: missed: statement clobbers memory: _31 = PULSE_GetActivationFunctionPtr (_30);
Dense.c:209:34: missed: statement clobbers memory: _33 = aligned_alloc (64, _10);
Dense.c:225:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:226:4: missed: statement clobbers memory: exit (1);
Dense.c:230:9: note: ***** Analysis succeeded with vector mode V8SI
Dense.c:230:9: note: SLPing BB part
Dense.c:230:9: note: Costing subgraph: 
Dense.c:230:9: note: node 0x80d5a08 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:230:9: note: op template: <retval>.n_inputs = args$n_inputs_53;
Dense.c:230:9: note: 	stmt 0 <retval>.n_inputs = args$n_inputs_53;
Dense.c:230:9: note: 	stmt 1 <retval>.n_outputs = args$n_outputs_54;
Dense.c:230:9: note: 	children 0x80d5a88
Dense.c:230:9: note: node 0x80d5a88 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:230:9: note: op template: args$n_inputs_53 = args.n_inputs;
Dense.c:230:9: note: 	stmt 0 args$n_inputs_53 = args.n_inputs;
Dense.c:230:9: note: 	stmt 1 args$n_outputs_54 = args.n_outputs;
Dense.c:230:9: note: Cost model analysis: 
Dense.c:230:9: note: Cost model analysis for part in loop 0:
  Vector cost: 40
  Scalar cost: 56
Dense.c:230:9: note: Costing subgraph: 
Dense.c:230:9: note: node 0x80d5b88 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:230:9: note: op template: <retval>.inputs = IO_42(D);
Dense.c:230:9: note: 	stmt 0 <retval>.inputs = IO_42(D);
Dense.c:230:9: note: 	stmt 1 <retval>.outputs = _29;
Dense.c:230:9: note: 	children 0x80d5c08
Dense.c:230:9: note: node (external) 0x80d5c08 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:230:9: note: 	{ IO_42(D), _29 }
Dense.c:230:9: note: Cost model analysis: 
Dense.c:230:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:230:9: note: Costing subgraph: 
Dense.c:230:9: note: node 0x80d5c88 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:230:9: note: op template: <retval>.feed = layer$feed_41;
Dense.c:230:9: note: 	stmt 0 <retval>.feed = layer$feed_41;
Dense.c:230:9: note: 	stmt 1 <retval>.back = layer$back_51;
Dense.c:230:9: note: 	stmt 2 <retval>.fix = layer$fix_52;
Dense.c:230:9: note: 	stmt 3 <retval>.activate = _32;
Dense.c:230:9: note: 	children 0x80d5d88
Dense.c:230:9: note: node (external) 0x80d5d88 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:230:9: note: 	{ layer$feed_41, layer$back_51, layer$fix_52, _32 }
Dense.c:230:9: note: Cost model analysis: 
Dense.c:230:9: note: Cost model analysis for part in loop 0:
  Vector cost: 64
  Scalar cost: 64
Dense.c:230:9: note: Costing subgraph: 
Dense.c:230:9: note: node 0x80d5e88 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:230:9: note: op template: <retval>.parent = 0B;
Dense.c:230:9: note: 	stmt 0 <retval>.parent = 0B;
Dense.c:230:9: note: 	stmt 1 <retval>.child = 0B;
Dense.c:230:9: note: 	children 0x80d5f08
Dense.c:230:9: note: node (constant) 0x80d5f08 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:230:9: note: 	{ 0B, 0B }
Dense.c:230:9: note: Cost model analysis: 
Dense.c:230:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:230:9: note: Costing subgraph: 
Dense.c:230:9: note: node 0x80d6008 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:230:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&<retval> + 80B] = MODEL_37(D);
Dense.c:230:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&<retval> + 80B] = MODEL_37(D);
Dense.c:230:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = _4;
Dense.c:230:9: note: 	stmt 2 MEM <float *> [(struct PULSE_Layer *)&<retval> + 96B] = _11;
Dense.c:230:9: note: 	stmt 3 MEM <float *> [(struct PULSE_Layer *)&<retval> + 104B] = _12;
Dense.c:230:9: note: 	children 0x80d6088
Dense.c:230:9: note: node (external) 0x80d6088 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:230:9: note: 	{ MODEL_37(D), _4, _11, _12 }
Dense.c:230:9: note: Cost model analysis: 
Dense.c:230:9: note: Cost model analysis for part in loop 0:
  Vector cost: 64
  Scalar cost: 64
Dense.c:230:9: note: Basic block will be vectorized using SLP
Dense.c:230:9: optimized: basic block part vectorized using 8 byte vectors
Dense.c:230:9: note: Vectorizing SLP tree:
Dense.c:230:9: note: node 0x80d5a08 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:230:9: note: op template: <retval>.n_inputs = args$n_inputs_53;
Dense.c:230:9: note: 	stmt 0 <retval>.n_inputs = args$n_inputs_53;
Dense.c:230:9: note: 	stmt 1 <retval>.n_outputs = args$n_outputs_54;
Dense.c:230:9: note: 	children 0x80d5a88
Dense.c:230:9: note: node 0x80d5a88 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:230:9: note: op template: args$n_inputs_53 = args.n_inputs;
Dense.c:230:9: note: 	stmt 0 args$n_inputs_53 = args.n_inputs;
Dense.c:230:9: note: 	stmt 1 args$n_outputs_54 = args.n_outputs;
Dense.c:230:9: note: ------>vectorizing SLP node starting from: args$n_inputs_53 = args.n_inputs;
Dense.c:230:9: note: transform load. ncopies = 1
Dense.c:230:9: note: conflicting alias set types.
Dense.c:230:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: args.n_inputs
Dense.c:230:9: note: created &args
Dense.c:230:9: note: add new stmt: vect_args_n_inputs_53.508_88 = MEM <vector(2) unsigned int> [(void *)&args];
Dense.c:230:9: note: extracting lane for live stmt args$n_inputs_53 = args.n_inputs;
Dense.c:230:9: note: extracting lane for live stmt args$n_outputs_54 = args.n_outputs;
Dense.c:230:9: note: ------>vectorizing SLP node starting from: <retval>.n_inputs = args$n_inputs_53;
Dense.c:230:9: note: vect_is_simple_use: operand args.n_inputs, type of def: internal
Dense.c:230:9: note: vect_is_simple_use: operand args.n_outputs, type of def: internal
Dense.c:230:9: note: transform store. ncopies = 1
Dense.c:230:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: <retval>.n_inputs
Dense.c:230:9: note: created &<retval>.n_inputs
Dense.c:230:9: note: add new stmt: MEM <vector(2) unsigned int> [(unsigned int *)&<retval> + 120B] = vect_args_n_inputs_53.508_88;
Dense.c:230:9: note: vectorizing stmts using SLP.
Dense.c:230:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:230:9: note: Vectorizing SLP tree:
Dense.c:230:9: note: node 0x80d5b88 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:230:9: note: op template: <retval>.inputs = IO_42(D);
Dense.c:230:9: note: 	stmt 0 <retval>.inputs = IO_42(D);
Dense.c:230:9: note: 	stmt 1 <retval>.outputs = _29;
Dense.c:230:9: note: 	children 0x80d5c08
Dense.c:230:9: note: node (external) 0x80d5c08 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:230:9: note: 	{ IO_42(D), _29 }
Dense.c:230:9: note: ------>vectorizing SLP node starting from: <retval>.inputs = IO_42(D);
Dense.c:230:9: note: vect_is_simple_use: operand IO_42(D) + _28, type of def: internal
Dense.c:230:9: note: transform store. ncopies = 1
Dense.c:230:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.inputs
Dense.c:230:9: note: created &<retval>
Dense.c:230:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval>] = _94;
Dense.c:230:9: note: vectorizing stmts using SLP.
Dense.c:230:9: optimized: basic block part vectorized using 32 byte vectors
Dense.c:230:9: note: Vectorizing SLP tree:
Dense.c:230:9: note: node 0x80d5c88 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:230:9: note: op template: <retval>.feed = layer$feed_41;
Dense.c:230:9: note: 	stmt 0 <retval>.feed = layer$feed_41;
Dense.c:230:9: note: 	stmt 1 <retval>.back = layer$back_51;
Dense.c:230:9: note: 	stmt 2 <retval>.fix = layer$fix_52;
Dense.c:230:9: note: 	stmt 3 <retval>.activate = _32;
Dense.c:230:9: note: 	children 0x80d5d88
Dense.c:230:9: note: node (external) 0x80d5d88 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:230:9: note: 	{ layer$feed_41, layer$back_51, layer$fix_52, _32 }
Dense.c:230:9: note: ------>vectorizing SLP node starting from: <retval>.feed = layer$feed_41;
Dense.c:230:9: note: vect_is_simple_use: operand layer$back_51 = PHI <layer$back_57(D)(11), _BackDense(9), _SIMD_BackDense(15)>, type of def: internal
Dense.c:230:9: note: vect_is_simple_use: operand layer$fix_52 = PHI <layer$fix_58(D)(11), _FixDense(9), _SIMD_FixDense(15)>, type of def: internal
Dense.c:230:9: note: vect_is_simple_use: operand (void (*<T91a>) (float *, unsigned int, char)) _31, type of def: internal
Dense.c:230:9: note: conflicting alias set types.
Dense.c:230:9: note: transform store. ncopies = 1
Dense.c:230:9: note: create vector_type-pointer variable to type: vector(4) long unsigned int  vectorizing a pointer ref: <retval>.feed
Dense.c:230:9: note: created &<retval>.feed
Dense.c:230:9: note: add new stmt: MEM <vector(4) long unsigned int> [(void *)&<retval> + 32B] = _100;
Dense.c:230:9: note: vectorizing stmts using SLP.
Dense.c:230:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:230:9: note: Vectorizing SLP tree:
Dense.c:230:9: note: node 0x80d5e88 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:230:9: note: op template: <retval>.parent = 0B;
Dense.c:230:9: note: 	stmt 0 <retval>.parent = 0B;
Dense.c:230:9: note: 	stmt 1 <retval>.child = 0B;
Dense.c:230:9: note: 	children 0x80d5f08
Dense.c:230:9: note: node (constant) 0x80d5f08 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:230:9: note: 	{ 0B, 0B }
Dense.c:230:9: note: ------>vectorizing SLP node starting from: <retval>.parent = 0B;
Dense.c:230:9: note: vect_is_simple_use: operand 0B, type of def: constant
Dense.c:230:9: note: transform store. ncopies = 1
Dense.c:230:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.parent
Dense.c:230:9: note: created &<retval>.parent
Dense.c:230:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer * *)&<retval> + 64B] = { 0, 0 };
Dense.c:230:9: note: vectorizing stmts using SLP.
Dense.c:230:9: optimized: basic block part vectorized using 32 byte vectors
Dense.c:230:9: note: Vectorizing SLP tree:
Dense.c:230:9: note: node 0x80d6008 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:230:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&<retval> + 80B] = MODEL_37(D);
Dense.c:230:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&<retval> + 80B] = MODEL_37(D);
Dense.c:230:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = _4;
Dense.c:230:9: note: 	stmt 2 MEM <float *> [(struct PULSE_Layer *)&<retval> + 96B] = _11;
Dense.c:230:9: note: 	stmt 3 MEM <float *> [(struct PULSE_Layer *)&<retval> + 104B] = _12;
Dense.c:230:9: note: 	children 0x80d6088
Dense.c:230:9: note: node (external) 0x80d6088 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:230:9: note: 	{ MODEL_37(D), _4, _11, _12 }
Dense.c:230:9: note: ------>vectorizing SLP node starting from: MEM <float *> [(struct PULSE_Layer *)&<retval> + 80B] = MODEL_37(D);
Dense.c:230:9: note: vect_is_simple_use: operand MODEL_37(D) + _3, type of def: internal
Dense.c:230:9: note: vect_is_simple_use: operand aligned_alloc (64, _10), type of def: internal
Dense.c:230:9: note: vect_is_simple_use: operand aligned_alloc (64, _10), type of def: internal
Dense.c:230:9: note: transform store. ncopies = 1
Dense.c:230:9: note: create vector_type-pointer variable to type: vector(4) long unsigned int  vectorizing a pointer ref: MEM <float *> [(struct PULSE_Layer *)&<retval> + 80B]
Dense.c:230:9: note: created &MEM <float *> [(struct PULSE_Layer *)&<retval> + 80B]
Dense.c:230:9: note: add new stmt: MEM <vector(4) long unsigned int> [(struct PULSE_Layer *)&<retval> + 80B] = _107;
Dense.c:230:9: note: vectorizing stmts using SLP.
Dense.c:230:9: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _28 (pretmp_91, _51, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:19: missed: not vectorized: no vectype for stmt: _22 = *_21;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:170:10: optimized: loop vectorized using 32 byte vectors
Dense.c:170:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:170:10: optimized: loop vectorized using 16 byte vectors
Dense.c:161:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
Dense.c:154:10: optimized: loop vectorized using 32 byte vectors
Dense.c:154:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:154:10: optimized: loop vectorized using 16 byte vectors
Dense.c:145:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
Dense.c:136:13: note: vectorized 2 loops in function.
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_28, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
Dense.c:136:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:136:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:136:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:136:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:105:24: missed: couldn't vectorize loop
Dense.c:105:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:108:17: missed: couldn't vectorize loop
Dense.c:108:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:122:24: missed: couldn't vectorize loop
Dense.c:122:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:125:17: missed: couldn't vectorize loop
Dense.c:125:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:90:15: missed: couldn't vectorize loop
Dense.c:90:15: missed: not vectorized: number of iterations cannot be computed.
Dense.c:83:13: note: vectorized 0 loops in function.
Dense.c:86:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:110:17: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:110:17: note: SLPing BB part
Dense.c:127:17: note: Costing subgraph: 
Dense.c:127:17: note: node 0x8797e58 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:127:17: note: op template: _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 0 _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 1 _198 = (sizetype) wi_137;
Dense.c:127:17: note: 	children 0x8797f58
Dense.c:127:17: note: node (external) 0x8797f58 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:127:17: note: 	{ j_135, wi_137 }
Dense.c:127:17: note: Cost model analysis: 
Dense.c:127:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:127:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:127:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: Costing subgraph: 
Dense.c:110:17: note: node 0x8798058 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:110:17: note: op template: _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 0 _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 1 _217 = (sizetype) wi_136;
Dense.c:110:17: note: 	children 0x8798158
Dense.c:110:17: note: node (external) 0x8798158 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:110:17: note: 	{ j_134, wi_136 }
Dense.c:110:17: note: Cost model analysis: 
Dense.c:110:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:110:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:110:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: ***** The result for vector mode V32QI would be the same
Dense.c:110:17: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:110:17: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:110:17: note: SLPing BB part
Dense.c:127:17: note: Costing subgraph: 
Dense.c:127:17: note: node 0x8798058 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:127:17: note: op template: _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 0 _42 = (sizetype) j_135;
Dense.c:127:17: note: 	stmt 1 _198 = (sizetype) wi_137;
Dense.c:127:17: note: 	children 0x8797e58
Dense.c:127:17: note: node (external) 0x8797e58 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:127:17: note: 	{ j_135, wi_137 }
Dense.c:127:17: note: Cost model analysis: 
Dense.c:127:17: note: Scalar 2 and vector 3 loop part do not match up, skipping scalar part
Dense.c:127:17: note: Cost model analysis for part in loop 3:
  Vector cost: 36
  Scalar cost: 8
Dense.c:127:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: Costing subgraph: 
Dense.c:110:17: note: node 0x8797fd8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:110:17: note: op template: _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 0 _20 = (sizetype) j_134;
Dense.c:110:17: note: 	stmt 1 _217 = (sizetype) wi_136;
Dense.c:110:17: note: 	children 0x87980d8
Dense.c:110:17: note: node (external) 0x87980d8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:110:17: note: 	{ j_134, wi_136 }
Dense.c:110:17: note: Cost model analysis: 
Dense.c:110:17: note: Scalar 4 and vector 5 loop part do not match up, skipping scalar part
Dense.c:110:17: note: Cost model analysis for part in loop 5:
  Vector cost: 36
  Scalar cost: 8
Dense.c:110:17: missed: not vectorized: vectorization is not profitable.
Dense.c:110:17: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:105:51: note: ***** Analysis failed with vector mode V8QI
Dense.c:105:51: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:105:51: note: ***** Analysis failed with vector mode V4QI
Dense.c:62:15: missed: couldn't vectorize loop
Dense.c:62:15: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:75:11: optimized: loop vectorized using 32 byte vectors
Dense.c:75:11: optimized: loop vectorized using 16 byte vectors
Dense.c:66:11: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:881:10: missed: not vectorized: no vectype for stmt: _53 = MEM[(__m256 * {ref-all})w_ptr_82];
 scalar_type: __m256
Dense.c:54:13: note: vectorized 1 loops in function.
Dense.c:57:2: missed: statement clobbers memory: memcpy (_4, dense$baiases_63, _3);
Dense.c:79:2: missed: statement clobbers memory: _27 (pretmp_163, _68, 0);
Dense.c:54:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:54:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:179:107: note: ***** Analysis failed with vector mode V8SI
Dense.c:179:107: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
Dense.c:180:85: note: ***** Analysis failed with vector mode V8SI
Dense.c:180:85: note: ***** The result for vector mode V32QI would be the same
Dense.c:180:85: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:180:85: note: ***** Analysis failed with vector mode V16QI
Dense.c:180:85: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:180:85: note: ***** Analysis failed with vector mode V8QI
Dense.c:180:85: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:180:85: note: ***** Analysis failed with vector mode V4QI
Dense.c:181:105: note: ***** Analysis failed with vector mode V8SI
Dense.c:181:105: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
Dense.c:193:19: missed: couldn't vectorize loop
Dense.c:193:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:183:13: note: vectorized 0 loops in function.
Dense.c:189:37: missed: statement clobbers memory: _9 = aligned_alloc (64, _8);
Dense.c:190:34: missed: statement clobbers memory: _11 = aligned_alloc (64, _10);
Dense.c:191:35: missed: statement clobbers memory: _12 = aligned_alloc (64, _10);
Dense.c:194:38: missed: statement clobbers memory: _13 = rand ();
Dense.c:194:72: missed: statement clobbers memory: _79 = sqrt (_20);
Dense.c:207:19: missed: statement clobbers memory: _31 = PULSE_GetActivationFunctionPtr (_30);
Dense.c:209:34: missed: statement clobbers memory: _33 = aligned_alloc (64, _10);
Dense.c:225:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:226:4: missed: statement clobbers memory: exit (1);
Dense.c:230:9: note: ***** Analysis succeeded with vector mode V8SI
Dense.c:230:9: note: SLPing BB part
Dense.c:230:9: note: Costing subgraph: 
Dense.c:230:9: note: node 0x86a10e8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:230:9: note: op template: <retval>.n_inputs = args$n_inputs_53;
Dense.c:230:9: note: 	stmt 0 <retval>.n_inputs = args$n_inputs_53;
Dense.c:230:9: note: 	stmt 1 <retval>.n_outputs = args$n_outputs_54;
Dense.c:230:9: note: 	children 0x86a1168
Dense.c:230:9: note: node 0x86a1168 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:230:9: note: op template: args$n_inputs_53 = args.n_inputs;
Dense.c:230:9: note: 	stmt 0 args$n_inputs_53 = args.n_inputs;
Dense.c:230:9: note: 	stmt 1 args$n_outputs_54 = args.n_outputs;
Dense.c:230:9: note: Cost model analysis: 
Dense.c:230:9: note: Cost model analysis for part in loop 0:
  Vector cost: 40
  Scalar cost: 56
Dense.c:230:9: note: Costing subgraph: 
Dense.c:230:9: note: node 0x86a1268 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:230:9: note: op template: <retval>.inputs = IO_42(D);
Dense.c:230:9: note: 	stmt 0 <retval>.inputs = IO_42(D);
Dense.c:230:9: note: 	stmt 1 <retval>.outputs = _29;
Dense.c:230:9: note: 	children 0x86a12e8
Dense.c:230:9: note: node (external) 0x86a12e8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:230:9: note: 	{ IO_42(D), _29 }
Dense.c:230:9: note: Cost model analysis: 
Dense.c:230:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:230:9: note: Costing subgraph: 
Dense.c:230:9: note: node 0x86a1368 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:230:9: note: op template: <retval>.feed = layer$feed_41;
Dense.c:230:9: note: 	stmt 0 <retval>.feed = layer$feed_41;
Dense.c:230:9: note: 	stmt 1 <retval>.back = layer$back_51;
Dense.c:230:9: note: 	stmt 2 <retval>.fix = layer$fix_52;
Dense.c:230:9: note: 	stmt 3 <retval>.activate = _32;
Dense.c:230:9: note: 	children 0x86a1468
Dense.c:230:9: note: node (external) 0x86a1468 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:230:9: note: 	{ layer$feed_41, layer$back_51, layer$fix_52, _32 }
Dense.c:230:9: note: Cost model analysis: 
Dense.c:230:9: note: Cost model analysis for part in loop 0:
  Vector cost: 64
  Scalar cost: 64
Dense.c:230:9: note: Costing subgraph: 
Dense.c:230:9: note: node 0x86a1568 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:230:9: note: op template: <retval>.parent = 0B;
Dense.c:230:9: note: 	stmt 0 <retval>.parent = 0B;
Dense.c:230:9: note: 	stmt 1 <retval>.child = 0B;
Dense.c:230:9: note: 	children 0x86a15e8
Dense.c:230:9: note: node (constant) 0x86a15e8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:230:9: note: 	{ 0B, 0B }
Dense.c:230:9: note: Cost model analysis: 
Dense.c:230:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:230:9: note: Costing subgraph: 
Dense.c:230:9: note: node 0x86a16e8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:230:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&<retval> + 80B] = MODEL_37(D);
Dense.c:230:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&<retval> + 80B] = MODEL_37(D);
Dense.c:230:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = _4;
Dense.c:230:9: note: 	stmt 2 MEM <float *> [(struct PULSE_Layer *)&<retval> + 96B] = _11;
Dense.c:230:9: note: 	stmt 3 MEM <float *> [(struct PULSE_Layer *)&<retval> + 104B] = _12;
Dense.c:230:9: note: 	children 0x86a1768
Dense.c:230:9: note: node (external) 0x86a1768 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:230:9: note: 	{ MODEL_37(D), _4, _11, _12 }
Dense.c:230:9: note: Cost model analysis: 
Dense.c:230:9: note: Cost model analysis for part in loop 0:
  Vector cost: 64
  Scalar cost: 64
Dense.c:230:9: note: Basic block will be vectorized using SLP
Dense.c:230:9: optimized: basic block part vectorized using 8 byte vectors
Dense.c:230:9: note: Vectorizing SLP tree:
Dense.c:230:9: note: node 0x86a10e8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:230:9: note: op template: <retval>.n_inputs = args$n_inputs_53;
Dense.c:230:9: note: 	stmt 0 <retval>.n_inputs = args$n_inputs_53;
Dense.c:230:9: note: 	stmt 1 <retval>.n_outputs = args$n_outputs_54;
Dense.c:230:9: note: 	children 0x86a1168
Dense.c:230:9: note: node 0x86a1168 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:230:9: note: op template: args$n_inputs_53 = args.n_inputs;
Dense.c:230:9: note: 	stmt 0 args$n_inputs_53 = args.n_inputs;
Dense.c:230:9: note: 	stmt 1 args$n_outputs_54 = args.n_outputs;
Dense.c:230:9: note: ------>vectorizing SLP node starting from: args$n_inputs_53 = args.n_inputs;
Dense.c:230:9: note: transform load. ncopies = 1
Dense.c:230:9: note: conflicting alias set types.
Dense.c:230:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: args.n_inputs
Dense.c:230:9: note: created &args
Dense.c:230:9: note: add new stmt: vect_args_n_inputs_53.508_88 = MEM <vector(2) unsigned int> [(void *)&args];
Dense.c:230:9: note: extracting lane for live stmt args$n_inputs_53 = args.n_inputs;
Dense.c:230:9: note: extracting lane for live stmt args$n_outputs_54 = args.n_outputs;
Dense.c:230:9: note: ------>vectorizing SLP node starting from: <retval>.n_inputs = args$n_inputs_53;
Dense.c:230:9: note: vect_is_simple_use: operand args.n_inputs, type of def: internal
Dense.c:230:9: note: vect_is_simple_use: operand args.n_outputs, type of def: internal
Dense.c:230:9: note: transform store. ncopies = 1
Dense.c:230:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: <retval>.n_inputs
Dense.c:230:9: note: created &<retval>.n_inputs
Dense.c:230:9: note: add new stmt: MEM <vector(2) unsigned int> [(unsigned int *)&<retval> + 120B] = vect_args_n_inputs_53.508_88;
Dense.c:230:9: note: vectorizing stmts using SLP.
Dense.c:230:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:230:9: note: Vectorizing SLP tree:
Dense.c:230:9: note: node 0x86a1268 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:230:9: note: op template: <retval>.inputs = IO_42(D);
Dense.c:230:9: note: 	stmt 0 <retval>.inputs = IO_42(D);
Dense.c:230:9: note: 	stmt 1 <retval>.outputs = _29;
Dense.c:230:9: note: 	children 0x86a12e8
Dense.c:230:9: note: node (external) 0x86a12e8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:230:9: note: 	{ IO_42(D), _29 }
Dense.c:230:9: note: ------>vectorizing SLP node starting from: <retval>.inputs = IO_42(D);
Dense.c:230:9: note: vect_is_simple_use: operand IO_42(D) + _28, type of def: internal
Dense.c:230:9: note: transform store. ncopies = 1
Dense.c:230:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.inputs
Dense.c:230:9: note: created &<retval>
Dense.c:230:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval>] = _94;
Dense.c:230:9: note: vectorizing stmts using SLP.
Dense.c:230:9: optimized: basic block part vectorized using 32 byte vectors
Dense.c:230:9: note: Vectorizing SLP tree:
Dense.c:230:9: note: node 0x86a1368 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:230:9: note: op template: <retval>.feed = layer$feed_41;
Dense.c:230:9: note: 	stmt 0 <retval>.feed = layer$feed_41;
Dense.c:230:9: note: 	stmt 1 <retval>.back = layer$back_51;
Dense.c:230:9: note: 	stmt 2 <retval>.fix = layer$fix_52;
Dense.c:230:9: note: 	stmt 3 <retval>.activate = _32;
Dense.c:230:9: note: 	children 0x86a1468
Dense.c:230:9: note: node (external) 0x86a1468 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:230:9: note: 	{ layer$feed_41, layer$back_51, layer$fix_52, _32 }
Dense.c:230:9: note: ------>vectorizing SLP node starting from: <retval>.feed = layer$feed_41;
Dense.c:230:9: note: vect_is_simple_use: operand layer$back_51 = PHI <layer$back_57(D)(11), _BackDense(9), _SIMD_BackDense(15)>, type of def: internal
Dense.c:230:9: note: vect_is_simple_use: operand layer$fix_52 = PHI <layer$fix_58(D)(11), _FixDense(9), _SIMD_FixDense(15)>, type of def: internal
Dense.c:230:9: note: vect_is_simple_use: operand (void (*<T91a>) (float *, unsigned int, char)) _31, type of def: internal
Dense.c:230:9: note: conflicting alias set types.
Dense.c:230:9: note: transform store. ncopies = 1
Dense.c:230:9: note: create vector_type-pointer variable to type: vector(4) long unsigned int  vectorizing a pointer ref: <retval>.feed
Dense.c:230:9: note: created &<retval>.feed
Dense.c:230:9: note: add new stmt: MEM <vector(4) long unsigned int> [(void *)&<retval> + 32B] = _100;
Dense.c:230:9: note: vectorizing stmts using SLP.
Dense.c:230:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:230:9: note: Vectorizing SLP tree:
Dense.c:230:9: note: node 0x86a1568 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:230:9: note: op template: <retval>.parent = 0B;
Dense.c:230:9: note: 	stmt 0 <retval>.parent = 0B;
Dense.c:230:9: note: 	stmt 1 <retval>.child = 0B;
Dense.c:230:9: note: 	children 0x86a15e8
Dense.c:230:9: note: node (constant) 0x86a15e8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:230:9: note: 	{ 0B, 0B }
Dense.c:230:9: note: ------>vectorizing SLP node starting from: <retval>.parent = 0B;
Dense.c:230:9: note: vect_is_simple_use: operand 0B, type of def: constant
Dense.c:230:9: note: transform store. ncopies = 1
Dense.c:230:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.parent
Dense.c:230:9: note: created &<retval>.parent
Dense.c:230:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer * *)&<retval> + 64B] = { 0, 0 };
Dense.c:230:9: note: vectorizing stmts using SLP.
Dense.c:230:9: optimized: basic block part vectorized using 32 byte vectors
Dense.c:230:9: note: Vectorizing SLP tree:
Dense.c:230:9: note: node 0x86a16e8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:230:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&<retval> + 80B] = MODEL_37(D);
Dense.c:230:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&<retval> + 80B] = MODEL_37(D);
Dense.c:230:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = _4;
Dense.c:230:9: note: 	stmt 2 MEM <float *> [(struct PULSE_Layer *)&<retval> + 96B] = _11;
Dense.c:230:9: note: 	stmt 3 MEM <float *> [(struct PULSE_Layer *)&<retval> + 104B] = _12;
Dense.c:230:9: note: 	children 0x86a1768
Dense.c:230:9: note: node (external) 0x86a1768 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:230:9: note: 	{ MODEL_37(D), _4, _11, _12 }
Dense.c:230:9: note: ------>vectorizing SLP node starting from: MEM <float *> [(struct PULSE_Layer *)&<retval> + 80B] = MODEL_37(D);
Dense.c:230:9: note: vect_is_simple_use: operand MODEL_37(D) + _3, type of def: internal
Dense.c:230:9: note: vect_is_simple_use: operand aligned_alloc (64, _10), type of def: internal
Dense.c:230:9: note: vect_is_simple_use: operand aligned_alloc (64, _10), type of def: internal
Dense.c:230:9: note: transform store. ncopies = 1
Dense.c:230:9: note: create vector_type-pointer variable to type: vector(4) long unsigned int  vectorizing a pointer ref: MEM <float *> [(struct PULSE_Layer *)&<retval> + 80B]
Dense.c:230:9: note: created &MEM <float *> [(struct PULSE_Layer *)&<retval> + 80B]
Dense.c:230:9: note: add new stmt: MEM <vector(4) long unsigned int> [(struct PULSE_Layer *)&<retval> + 80B] = _107;
Dense.c:230:9: note: vectorizing stmts using SLP.
Dense.c:230:9: note: ***** The result for vector mode V32QI would be the same
Activations.c:17:19: optimized: loop vectorized using 32 byte vectors
Activations.c:17:19: optimized: loop vectorized using 16 byte vectors
Activations.c:17:19: optimized: loop vectorized using 32 byte vectors
Activations.c:17:19: optimized: loop vectorized using 16 byte vectors
Activations.c:14:13: note: vectorized 2 loops in function.
Activations.c:14:13: note: ***** Analysis failed with vector mode V8SF
Activations.c:14:13: note: ***** The result for vector mode V32QI would be the same
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V16QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V16QI
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V8QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V8QI
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V4QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V4QI
Activations.c:28:19: missed: couldn't vectorize loop
Activations.c:28:19: missed: not vectorized: unsupported control flow in loop.
Activations.c:28:19: optimized: loop vectorized using 32 byte vectors
Activations.c:28:19: optimized: loop vectorized using 16 byte vectors
Activations.c:25:13: note: vectorized 1 loops in function.
Activations.c:25:13: note: ***** Analysis failed with vector mode V8SF
Activations.c:25:13: note: ***** The result for vector mode V32QI would be the same
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V16QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V16QI
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V8QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V8QI
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V4QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V4QI
Activations.c:38:1: note: ***** Analysis failed with vector mode VOID
Activations.c:5:19: missed: couldn't vectorize loop
Activations.c:9:21: missed: statement clobbers memory: _36 = expf (_18);
Activations.c:5:19: missed: couldn't vectorize loop
Activations.c:9:21: missed: statement clobbers memory: _12 = expf (_11);
Activations.c:3:13: note: vectorized 0 loops in function.
Activations.c:9:21: missed: statement clobbers memory: _12 = expf (_11);
Activations.c:9:21: missed: statement clobbers memory: _36 = expf (_18);
Activations.c:11:1: note: ***** Analysis failed with vector mode V8SF
Activations.c:11:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SF
Activations.c:44:2: note: ***** Analysis failed with vector mode VOID
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:4:18: note: vectorized 0 loops in function.
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_4, inputs_15, _3);
PULSE.c:8:2: missed: statement clobbers memory: _5 (layer_17);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_12, pretmp_26, _9);
PULSE.c:13:28: note: ***** Analysis failed with vector mode V8SI
PULSE.c:13:28: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:22:2: missed: statement clobbers memory: _1 (layer_10(D));
PULSE.c:22:2: missed: statement clobbers memory: _14 (_2);
PULSE.c:22:2: missed: statement clobbers memory: _21 (_15);
PULSE.c:22:2: missed: statement clobbers memory: _28 (_22);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_29);
PULSE.c:26:3: missed: statement clobbers memory: memset (_34, 0, _32);
PULSE.c:26:3: missed: statement clobbers memory: memset (_27, 0, _25);
PULSE.c:26:3: missed: statement clobbers memory: memset (_20, 0, _18);
PULSE.c:26:3: missed: statement clobbers memory: memset (_7, 0, _5);
PULSE.c:28:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:28:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:39: missed: statement clobbers memory: _7 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:32:6: note: vectorized 1 loops in function.
PULSE.c:34:8: missed: statement clobbers memory: _1 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_2);
PULSE.c:39:39: missed: statement clobbers memory: _7 = rand ();
PULSE.c:40:40: missed: statement clobbers memory: _9 = rand ();
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8SI
PULSE.c:32:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:62:16: missed: couldn't vectorize loop
PULSE.c:62:16: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:65:17: missed: couldn't vectorize loop
PULSE.c:65:17: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:74:19: missed: couldn't vectorize loop
PULSE.c:74:19: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:39: missed: statement clobbers memory: _69 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:54:22: missed: couldn't vectorize loop
PULSE.c:54:22: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:47:6: note: vectorized 1 loops in function.
PULSE.c:48:1: missed: statement clobbers memory: saved_stack.7_39 = __builtin_stack_save ();
PULSE.c:49:46: missed: statement clobbers memory: _1 = PULSE_GetLossFunctionPtr (loss_function_40(D));
PULSE.c:61:15: missed: statement clobbers memory: random.4_46 = __builtin_alloca_with_align (_4, 32);
PULSE.c:34:8: missed: statement clobbers memory: _61 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_62);
PULSE.c:39:39: missed: statement clobbers memory: _69 = rand ();
PULSE.c:40:40: missed: statement clobbers memory: _72 = rand ();
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_98, inputs_93, _97);
PULSE.c:8:2: missed: statement clobbers memory: _99 (layer_94);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_105, _104, _103);
PULSE.c:68:11: missed: statement clobbers memory: loss_55 = PULSE_GetLoss_42 (_17, _16, _12, _11);
PULSE.c:22:2: missed: statement clobbers memory: _86 (output_34);
PULSE.c:22:2: missed: statement clobbers memory: _108 (_87);
PULSE.c:22:2: missed: statement clobbers memory: _115 (_109);
PULSE.c:22:2: missed: statement clobbers memory: _122 (_116);
PULSE.c:22:2: missed: statement clobbers memory: _129 (_123);
PULSE.c:22:2: missed: statement clobbers memory: _136 (_130);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_137);
PULSE.c:26:3: missed: statement clobbers memory: memset (_142, 0, _140);
PULSE.c:26:3: missed: statement clobbers memory: memset (_135, 0, _133);
PULSE.c:26:3: missed: statement clobbers memory: memset (_128, 0, _126);
PULSE.c:26:3: missed: statement clobbers memory: memset (_121, 0, _119);
PULSE.c:26:3: missed: statement clobbers memory: memset (_114, 0, _112);
PULSE.c:26:3: missed: statement clobbers memory: memset (_92, 0, _90);
PULSE.c:76:6: missed: statement clobbers memory: _21 (current_191, args);
PULSE.c:82:4: missed: statement clobbers memory: printf ("Epoch: %d | Item: %d | Loss: %.10f | Batch Loss: %.10f\r", i_188, j_189, _25, _24);
PULSE.c:47:6: missed: statement clobbers memory: __builtin_stack_restore (saved_stack.7_39);
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4DI
PULSE.c:47:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:92:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:92:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:126:20: missed: couldn't vectorize loop
PULSE.c:126:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:126:20: missed: couldn't vectorize loop
PULSE.c:126:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:105:20: missed: couldn't vectorize loop
PULSE.c:105:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:105:20: missed: couldn't vectorize loop
PULSE.c:105:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:94:13: note: vectorized 0 loops in function.
PULSE.c:97:39: missed: statement clobbers memory: layers_41 = malloc (_3);
PULSE.c:100:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:119:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:120:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:121:46: missed: statement clobbers memory: WEIGHTS_86 = aligned_alloc (64, 0);
PULSE.c:111:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.105_94];
PULSE.c:112:21: missed: statement clobbers memory: _75 = PULSE_GetDenseWeightsSize (args);
PULSE.c:113:16: missed: statement clobbers memory: _78 = PULSE_GetDenseIOSize (args);
PULSE.c:114:19: missed: statement clobbers memory: _81 = PULSE_GetDenseFixesSize (args);
PULSE.c:111:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})_224];
PULSE.c:112:21: missed: statement clobbers memory: _210 = PULSE_GetDenseWeightsSize (args);
PULSE.c:113:16: missed: statement clobbers memory: _213 = PULSE_GetDenseIOSize (args);
PULSE.c:114:19: missed: statement clobbers memory: _216 = PULSE_GetDenseFixesSize (args);
PULSE.c:119:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:120:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:121:46: missed: statement clobbers memory: WEIGHTS_46 = aligned_alloc (64, _5);
PULSE.c:122:41: missed: statement clobbers memory: IO_48 = aligned_alloc (64, _7);
PULSE.c:132:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.107_96];
PULSE.c:133:17: missed: statement clobbers memory: *_10 = PULSE_CreateDenseLayer (args, WEIGHTS_PTR_152, IO_PTR_153, 0B); [return slot optimization]
PULSE.c:134:20: missed: statement clobbers memory: _64 = PULSE_GetDenseWeightsSize (args);
PULSE.c:135:15: missed: statement clobbers memory: _67 = PULSE_GetDenseIOSize (args);
PULSE.c:132:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.107_124];
PULSE.c:133:17: missed: statement clobbers memory: *_106 = PULSE_CreateDenseLayer (args, WEIGHTS_PTR_148, IO_PTR_143, 0B); [return slot optimization]
PULSE.c:134:20: missed: statement clobbers memory: _95 = PULSE_GetDenseWeightsSize (args);
PULSE.c:135:15: missed: statement clobbers memory: _59 = PULSE_GetDenseIOSize (args);
PULSE.c:141:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:142:9: note: ***** Analysis succeeded with vector mode V8SI
PULSE.c:142:9: note: SLPing BB part
PULSE.c:142:9: note: Costing subgraph: 
PULSE.c:142:9: note: node 0x4aa4478 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:142:9: note: op template: <retval>.weights_size = WEIGHTS_SIZE_164;
PULSE.c:142:9: note: 	stmt 0 <retval>.weights_size = WEIGHTS_SIZE_164;
PULSE.c:142:9: note: 	stmt 1 <retval>.io_size = IO_SIZE_167;
PULSE.c:142:9: note: 	children 0x4aa44f8
PULSE.c:142:9: note: node 0x4aa44f8 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:142:9: note: op template: WEIGHTS_SIZE_164 = PHI <WEIGHTS_SIZE_161(36), 0(25)>
PULSE.c:142:9: note: 	stmt 0 WEIGHTS_SIZE_164 = PHI <WEIGHTS_SIZE_161(36), 0(25)>
PULSE.c:142:9: note: 	stmt 1 IO_SIZE_167 = PHI <IO_SIZE_160(36), 0(25)>
PULSE.c:142:9: note: 	children 0x4aa4578 0x4aa4d78
PULSE.c:142:9: note: node 0x4aa4578 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:142:9: note: op template: WEIGHTS_SIZE_161 = PHI <WEIGHTS_SIZE_20(11), WEIGHTS_SIZE_206(76)>
PULSE.c:142:9: note: 	stmt 0 WEIGHTS_SIZE_161 = PHI <WEIGHTS_SIZE_20(11), WEIGHTS_SIZE_206(76)>
PULSE.c:142:9: note: 	stmt 1 IO_SIZE_160 = PHI <IO_SIZE_22(11), IO_SIZE_192(76)>
PULSE.c:142:9: note: 	children 0x4aa45f8 0x4aa4978
PULSE.c:142:9: note: node 0x4aa45f8 (max_nunits=2, refcnt=3) vector(2) unsigned int
PULSE.c:142:9: note: op template: WEIGHTS_SIZE_20 = PHI <WEIGHTS_SIZE_73(77), WEIGHTS_SIZE_76(10)>
PULSE.c:142:9: note: 	stmt 0 WEIGHTS_SIZE_20 = PHI <WEIGHTS_SIZE_73(77), WEIGHTS_SIZE_76(10)>
PULSE.c:142:9: note: 	stmt 1 IO_SIZE_22 = PHI <IO_SIZE_147(77), IO_SIZE_79(10)>
PULSE.c:142:9: note: 	children 0x4aa4678 0x4aa4778
PULSE.c:142:9: note: node 0x4aa4678 (max_nunits=2, refcnt=2) vector(2) unsigned int
PULSE.c:142:9: note: op template: WEIGHTS_SIZE_73 = PHI <WEIGHTS_SIZE_20(73), 0(71)>
PULSE.c:142:9: note: 	stmt 0 WEIGHTS_SIZE_73 = PHI <WEIGHTS_SIZE_20(73), 0(71)>
PULSE.c:142:9: note: 	stmt 1 IO_SIZE_147 = PHI <IO_SIZE_22(73), 0(71)>
PULSE.c:142:9: note: 	children 0x4aa45f8 0x4aa46f8
PULSE.c:142:9: note: node (constant) 0x4aa46f8 (max_nunits=1, refcnt=1) vector(2) unsigned int
PULSE.c:142:9: note: 	{ 0, 0 }
PULSE.c:142:9: note: node 0x4aa4778 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:142:9: note: op template: WEIGHTS_SIZE_76 = WEIGHTS_SIZE_73 + _75;
PULSE.c:142:9: note: 	stmt 0 WEIGHTS_SIZE_76 = WEIGHTS_SIZE_73 + _75;
PULSE.c:142:9: note: 	stmt 1 IO_SIZE_79 = _78 + IO_SIZE_147;
PULSE.c:142:9: note: 	children 0x4aa4678 0x4aa48f8
PULSE.c:142:9: note: node (external) 0x4aa48f8 (max_nunits=1, refcnt=1) vector(2) unsigned int
PULSE.c:142:9: note: 	{ _75, _78 }
PULSE.c:142:9: note: node 0x4aa4978 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:142:9: note: op template: WEIGHTS_SIZE_206 = PHI <WEIGHTS_SIZE_238(68)>
PULSE.c:142:9: note: 	stmt 0 WEIGHTS_SIZE_206 = PHI <WEIGHTS_SIZE_238(68)>
PULSE.c:142:9: note: 	stmt 1 IO_SIZE_192 = PHI <IO_SIZE_239(68)>
PULSE.c:142:9: note: 	children 0x4aa49f8
PULSE.c:142:9: note: node 0x4aa49f8 (max_nunits=2, refcnt=2) vector(2) unsigned int
PULSE.c:142:9: note: op template: WEIGHTS_SIZE_238 = PHI <WEIGHTS_SIZE_211(59), WEIGHTS_SIZE_200(57)>
PULSE.c:142:9: note: 	stmt 0 WEIGHTS_SIZE_238 = PHI <WEIGHTS_SIZE_211(59), WEIGHTS_SIZE_200(57)>
PULSE.c:142:9: note: 	stmt 1 IO_SIZE_239 = PHI <IO_SIZE_214(59), IO_SIZE_201(57)>
PULSE.c:142:9: note: 	children 0x4aa4a78 0x4aa4b78
PULSE.c:142:9: note: node 0x4aa4a78 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:142:9: note: op template: WEIGHTS_SIZE_211 = WEIGHTS_SIZE_200 + _210;
PULSE.c:142:9: note: 	stmt 0 WEIGHTS_SIZE_211 = WEIGHTS_SIZE_200 + _210;
PULSE.c:142:9: note: 	stmt 1 IO_SIZE_214 = _213 + IO_SIZE_201;
PULSE.c:142:9: note: 	children 0x4aa4b78 0x4aa4cf8
PULSE.c:142:9: note: node 0x4aa4b78 (max_nunits=2, refcnt=2) vector(2) unsigned int
PULSE.c:142:9: note: op template: WEIGHTS_SIZE_200 = PHI <WEIGHTS_SIZE_243(72), WEIGHTS_SIZE_238(69)>
PULSE.c:142:9: note: 	stmt 0 WEIGHTS_SIZE_200 = PHI <WEIGHTS_SIZE_243(72), WEIGHTS_SIZE_238(69)>
PULSE.c:142:9: note: 	stmt 1 IO_SIZE_201 = PHI <IO_SIZE_244(72), IO_SIZE_239(69)>
PULSE.c:142:9: note: 	children 0x4aa4bf8 0x4aa49f8
PULSE.c:142:9: note: node 0x4aa4bf8 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:142:9: note: op template: WEIGHTS_SIZE_243 = PHI <WEIGHTS_SIZE_20(74)>
PULSE.c:142:9: note: 	stmt 0 WEIGHTS_SIZE_243 = PHI <WEIGHTS_SIZE_20(74)>
PULSE.c:142:9: note: 	stmt 1 IO_SIZE_244 = PHI <IO_SIZE_22(74)>
PULSE.c:142:9: note: 	children 0x4aa45f8
PULSE.c:142:9: note: node (external) 0x4aa4cf8 (max_nunits=1, refcnt=1) vector(2) unsigned int
PULSE.c:142:9: note: 	{ _210, _213 }
PULSE.c:142:9: note: node (constant) 0x4aa4d78 (max_nunits=1, refcnt=1) vector(2) unsigned int
PULSE.c:142:9: note: 	{ 0, 0 }
PULSE.c:142:9: note: Cost model analysis: 
PULSE.c:142:9: note: Cost model analysis for part in loop 0:
  Vector cost: 48
  Scalar cost: 48
PULSE.c:142:9: note: Cost model analysis for part in loop 1:
  Vector cost: 44
  Scalar cost: 24
PULSE.c:142:9: missed: not vectorized: vectorization is not profitable.
PULSE.c:142:9: note: Costing subgraph: 
PULSE.c:142:9: note: node 0x4aa4e78 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:142:9: note: op template: <retval>.layers = layers_41;
PULSE.c:142:9: note: 	stmt 0 <retval>.layers = layers_41;
PULSE.c:142:9: note: 	stmt 1 <retval>.weights = WEIGHTS_163;
PULSE.c:142:9: note: 	children 0x4aa4f78
PULSE.c:142:9: note: node (external) 0x4aa4f78 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:142:9: note: 	{ layers_41, WEIGHTS_163 }
PULSE.c:142:9: note: Cost model analysis: 
PULSE.c:142:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:142:9: note: Costing subgraph: 
PULSE.c:142:9: note: node 0x4aa4ff8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:142:9: note: op template: <retval>.io = 0B;
PULSE.c:142:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:142:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:142:9: note: 	children 0x4aa5078
PULSE.c:142:9: note: node (constant) 0x4aa5078 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:142:9: note: 	{ 0B, 0B }
PULSE.c:142:9: note: Cost model analysis: 
PULSE.c:142:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:142:9: note: Basic block will be vectorized using SLP
PULSE.c:142:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:142:9: note: Vectorizing SLP tree:
PULSE.c:142:9: note: node 0x4aa4e78 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:142:9: note: op template: <retval>.layers = layers_41;
PULSE.c:142:9: note: 	stmt 0 <retval>.layers = layers_41;
PULSE.c:142:9: note: 	stmt 1 <retval>.weights = WEIGHTS_163;
PULSE.c:142:9: note: 	children 0x4aa4f78
PULSE.c:142:9: note: node (external) 0x4aa4f78 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:142:9: note: 	{ layers_41, WEIGHTS_163 }
PULSE.c:142:9: note: ------>vectorizing SLP node starting from: <retval>.layers = layers_41;
PULSE.c:142:9: note: vect_is_simple_use: operand WEIGHTS_163 = PHI <WEIGHTS_46(36), WEIGHTS_86(25)>, type of def: internal
PULSE.c:142:9: note: conflicting alias set types.
PULSE.c:142:9: note: transform store. ncopies = 1
PULSE.c:142:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.layers
PULSE.c:142:9: note: created &<retval>
PULSE.c:142:9: note: add new stmt: MEM <vector(2) long unsigned int> [(void *)&<retval>] = _19;
PULSE.c:142:9: note: vectorizing stmts using SLP.
PULSE.c:142:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:142:9: note: Vectorizing SLP tree:
PULSE.c:142:9: note: node 0x4aa4ff8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:142:9: note: op template: <retval>.io = 0B;
PULSE.c:142:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:142:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:142:9: note: 	children 0x4aa5078
PULSE.c:142:9: note: node (constant) 0x4aa5078 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:142:9: note: 	{ 0B, 0B }
PULSE.c:142:9: note: ------>vectorizing SLP node starting from: <retval>.io = 0B;
PULSE.c:142:9: note: vect_is_simple_use: operand 0B, type of def: constant
PULSE.c:142:9: note: transform store. ncopies = 1
PULSE.c:142:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.io
PULSE.c:142:9: note: created &<retval>.io
PULSE.c:142:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval> + 16B] = { 0, 0 };
PULSE.c:142:9: note: vectorizing stmts using SLP.
PULSE.c:142:9: note: ***** The result for vector mode V32QI would be the same
PULSE.c:148:2: missed: statement clobbers memory: free (_1);
PULSE.c:149:2: missed: statement clobbers memory: free (_2);
PULSE.c:150:2: missed: statement clobbers memory: free (_3);
PULSE.c:151:2: missed: statement clobbers memory: free (_4);
PULSE.c:152:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:152:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Loss.c:15:19: optimized: loop vectorized using 32 byte vectors
Loss.c:15:19: optimized:  loop versioned for vectorization because of possible aliasing
Loss.c:15:19: optimized: loop vectorized using 16 byte vectors
Loss.c:12:23: note: vectorized 1 loops in function.
Loss.c:12:23: note: ***** Analysis failed with vector mode V8SF
Loss.c:12:23: note: ***** The result for vector mode V32QI would be the same
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V16QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V16QI
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V8QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V8QI
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V4QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V4QI
Loss.c:7:19: missed: couldn't vectorize loop
Loss.c:8:38: missed: not vectorized: unsupported use in stmt.
Loss.c:4:23: note: vectorized 0 loops in function.
Loss.c:9:13: note: ***** Analysis failed with vector mode V8SF
Loss.c:9:13: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SF
Loss.c:23:2: note: ***** Analysis failed with vector mode VOID
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _28 (pretmp_91, _51, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:19: missed: not vectorized: no vectype for stmt: _22 = *_21;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:163:10: note: ***** Analysis failed with vector mode V4DI
Dense.c:163:10: note: ***** The result for vector mode V32QI would be the same
Dense.c:163:10: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:163:10: note: ***** Analysis failed with vector mode V16QI
Dense.c:163:10: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:163:10: note: ***** Analysis failed with vector mode V8QI
Dense.c:163:10: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:163:10: note: ***** Analysis failed with vector mode V4QI
Dense.c:145:10: optimized: loop vectorized using 32 byte vectors
Dense.c:145:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:145:10: optimized: loop vectorized using 16 byte vectors
Dense.c:136:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
Dense.c:129:10: optimized: loop vectorized using 32 byte vectors
Dense.c:129:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:129:10: optimized: loop vectorized using 16 byte vectors
Dense.c:120:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
Dense.c:111:13: note: vectorized 2 loops in function.
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_28, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
Dense.c:111:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:111:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:111:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:111:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:111:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:111:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:111:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:111:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:90:23: missed: couldn't vectorize loop
Dense.c:92:31: missed: not vectorized: no vectype for stmt: _119 = this_40(D)->outputs;
 scalar_type: float *
Dense.c:90:23: missed: couldn't vectorize loop
Dense.c:90:23: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:95:16: missed: couldn't vectorize loop
Dense.c:95:16: missed: not vectorized: unsupported control flow in loop.
Dense.c:83:13: note: vectorized 0 loops in function.
Dense.c:86:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:97:16: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:97:16: note: SLPing BB part
Dense.c:97:16: note: Costing subgraph: 
Dense.c:97:16: note: node 0x7d543f8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:97:16: note: op template: _16 = (sizetype) j_73;
Dense.c:97:16: note: 	stmt 0 _16 = (sizetype) j_73;
Dense.c:97:16: note: 	stmt 1 _111 = (sizetype) wi_74;
Dense.c:97:16: note: 	children 0x7d544f8
Dense.c:97:16: note: node (external) 0x7d544f8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:97:16: note: 	{ j_73, wi_74 }
Dense.c:97:16: note: Cost model analysis: 
Dense.c:97:16: note: Scalar 1 and vector 2 loop part do not match up, skipping scalar part
Dense.c:97:16: note: Cost model analysis for part in loop 2:
  Vector cost: 36
  Scalar cost: 8
Dense.c:97:16: missed: not vectorized: vectorization is not profitable.
Dense.c:97:16: note: ***** The result for vector mode V32QI would be the same
Dense.c:97:16: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:97:16: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:97:16: note: SLPing BB part
Dense.c:97:16: note: Costing subgraph: 
Dense.c:97:16: note: node 0x7d543f8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:97:16: note: op template: _16 = (sizetype) j_73;
Dense.c:97:16: note: 	stmt 0 _16 = (sizetype) j_73;
Dense.c:97:16: note: 	stmt 1 _111 = (sizetype) wi_74;
Dense.c:97:16: note: 	children 0x7d54478
Dense.c:97:16: note: node (external) 0x7d54478 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:97:16: note: 	{ j_73, wi_74 }
Dense.c:97:16: note: Cost model analysis: 
Dense.c:97:16: note: Scalar 1 and vector 2 loop part do not match up, skipping scalar part
Dense.c:97:16: note: Cost model analysis for part in loop 2:
  Vector cost: 36
  Scalar cost: 8
Dense.c:97:16: missed: not vectorized: vectorization is not profitable.
Dense.c:97:16: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:90:50: note: ***** Analysis failed with vector mode V8QI
Dense.c:90:50: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:90:50: note: ***** Analysis failed with vector mode V4QI
Dense.c:62:15: missed: couldn't vectorize loop
Dense.c:62:15: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:75:11: optimized: loop vectorized using 32 byte vectors
Dense.c:75:11: optimized: loop vectorized using 16 byte vectors
Dense.c:66:11: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:881:10: missed: not vectorized: no vectype for stmt: _53 = MEM[(__m256 * {ref-all})w_ptr_82];
 scalar_type: __m256
Dense.c:54:13: note: vectorized 1 loops in function.
Dense.c:57:2: missed: statement clobbers memory: memcpy (_4, dense$baiases_63, _3);
Dense.c:79:2: missed: statement clobbers memory: _27 (pretmp_163, _68, 0);
Dense.c:54:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:54:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:154:107: note: ***** Analysis failed with vector mode V8SI
Dense.c:154:107: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
Dense.c:155:85: note: ***** Analysis failed with vector mode V8SI
Dense.c:155:85: note: ***** The result for vector mode V32QI would be the same
Dense.c:155:85: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:155:85: note: ***** Analysis failed with vector mode V16QI
Dense.c:155:85: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:155:85: note: ***** Analysis failed with vector mode V8QI
Dense.c:155:85: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:155:85: note: ***** Analysis failed with vector mode V4QI
Dense.c:156:105: note: ***** Analysis failed with vector mode V8SI
Dense.c:156:105: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
Dense.c:178:19: missed: couldn't vectorize loop
Dense.c:178:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:168:13: note: vectorized 0 loops in function.
Dense.c:179:38: missed: statement clobbers memory: _5 = rand ();
Dense.c:179:72: missed: statement clobbers memory: _65 = sqrt (_12);
Dense.c:192:19: missed: statement clobbers memory: _24 = PULSE_GetActivationFunctionPtr (_23);
Dense.c:211:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:212:4: missed: statement clobbers memory: exit (1);
Dense.c:216:9: note: ***** Analysis succeeded with vector mode V8SI
Dense.c:216:9: note: SLPing BB part
Dense.c:216:9: note: Costing subgraph: 
Dense.c:216:9: note: node 0x7b9ace8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:216:9: note: op template: <retval>.n_inputs = args$n_inputs_41;
Dense.c:216:9: note: 	stmt 0 <retval>.n_inputs = args$n_inputs_41;
Dense.c:216:9: note: 	stmt 1 <retval>.n_outputs = args$n_outputs_42;
Dense.c:216:9: note: 	children 0x7b9ad68
Dense.c:216:9: note: node 0x7b9ad68 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:216:9: note: op template: args$n_inputs_41 = args.n_inputs;
Dense.c:216:9: note: 	stmt 0 args$n_inputs_41 = args.n_inputs;
Dense.c:216:9: note: 	stmt 1 args$n_outputs_42 = args.n_outputs;
Dense.c:216:9: note: Cost model analysis: 
Dense.c:216:9: note: Cost model analysis for part in loop 0:
  Vector cost: 40
  Scalar cost: 56
Dense.c:216:9: note: Costing subgraph: 
Dense.c:216:9: note: node 0x7b9ae68 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:216:9: note: op template: <retval>.inputs = IO_31(D);
Dense.c:216:9: note: 	stmt 0 <retval>.inputs = IO_31(D);
Dense.c:216:9: note: 	stmt 1 <retval>.outputs = _22;
Dense.c:216:9: note: 	children 0x7b9aee8
Dense.c:216:9: note: node (external) 0x7b9aee8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:216:9: note: 	{ IO_31(D), _22 }
Dense.c:216:9: note: Cost model analysis: 
Dense.c:216:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:216:9: note: Costing subgraph: 
Dense.c:216:9: note: node 0x7b9afe8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:216:9: note: op template: <retval>.feed = layer$feed_30;
Dense.c:216:9: note: 	stmt 0 <retval>.feed = layer$feed_30;
Dense.c:216:9: note: 	stmt 1 <retval>.back = layer$back_39;
Dense.c:216:9: note: 	stmt 2 <retval>.fix = layer$fix_40;
Dense.c:216:9: note: 	stmt 3 <retval>.activate = _25;
Dense.c:216:9: note: 	children 0x7b9b0e8
Dense.c:216:9: note: node (external) 0x7b9b0e8 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:216:9: note: 	{ layer$feed_30, layer$back_39, layer$fix_40, _25 }
Dense.c:216:9: note: Cost model analysis: 
Dense.c:216:9: note: Cost model analysis for part in loop 0:
  Vector cost: 64
  Scalar cost: 64
Dense.c:216:9: note: Costing subgraph: 
Dense.c:216:9: note: node 0x7b9b1e8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:216:9: note: op template: <retval>.parent = 0B;
Dense.c:216:9: note: 	stmt 0 <retval>.parent = 0B;
Dense.c:216:9: note: 	stmt 1 <retval>.child = 0B;
Dense.c:216:9: note: 	children 0x7b9b268
Dense.c:216:9: note: node (constant) 0x7b9b268 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:216:9: note: 	{ 0B, 0B }
Dense.c:216:9: note: Cost model analysis: 
Dense.c:216:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:216:9: note: Costing subgraph: 
Dense.c:216:9: note: node 0x7b9b2e8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:216:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_29(D);
Dense.c:216:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_29(D);
Dense.c:216:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&<retval> + 96B] = _4;
Dense.c:216:9: note: 	children 0x7b9b368
Dense.c:216:9: note: node (external) 0x7b9b368 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:216:9: note: 	{ MODEL_29(D), _4 }
Dense.c:216:9: note: Cost model analysis: 
Dense.c:216:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:216:9: note: Basic block will be vectorized using SLP
Dense.c:216:9: optimized: basic block part vectorized using 8 byte vectors
Dense.c:216:9: note: Vectorizing SLP tree:
Dense.c:216:9: note: node 0x7b9ace8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:216:9: note: op template: <retval>.n_inputs = args$n_inputs_41;
Dense.c:216:9: note: 	stmt 0 <retval>.n_inputs = args$n_inputs_41;
Dense.c:216:9: note: 	stmt 1 <retval>.n_outputs = args$n_outputs_42;
Dense.c:216:9: note: 	children 0x7b9ad68
Dense.c:216:9: note: node 0x7b9ad68 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:216:9: note: op template: args$n_inputs_41 = args.n_inputs;
Dense.c:216:9: note: 	stmt 0 args$n_inputs_41 = args.n_inputs;
Dense.c:216:9: note: 	stmt 1 args$n_outputs_42 = args.n_outputs;
Dense.c:216:9: note: ------>vectorizing SLP node starting from: args$n_inputs_41 = args.n_inputs;
Dense.c:216:9: note: transform load. ncopies = 1
Dense.c:216:9: note: conflicting alias set types.
Dense.c:216:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: args.n_inputs
Dense.c:216:9: note: created &args
Dense.c:216:9: note: add new stmt: vect_args_n_inputs_41.490_26 = MEM <vector(2) unsigned int> [(void *)&args];
Dense.c:216:9: note: extracting lane for live stmt args$n_inputs_41 = args.n_inputs;
Dense.c:216:9: note: extracting lane for live stmt args$n_outputs_42 = args.n_outputs;
Dense.c:216:9: note: ------>vectorizing SLP node starting from: <retval>.n_inputs = args$n_inputs_41;
Dense.c:216:9: note: vect_is_simple_use: operand args.n_inputs, type of def: internal
Dense.c:216:9: note: vect_is_simple_use: operand args.n_outputs, type of def: internal
Dense.c:216:9: note: transform store. ncopies = 1
Dense.c:216:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: <retval>.n_inputs
Dense.c:216:9: note: created &<retval>.n_inputs
Dense.c:216:9: note: add new stmt: MEM <vector(2) unsigned int> [(unsigned int *)&<retval> + 128B] = vect_args_n_inputs_41.490_26;
Dense.c:216:9: note: vectorizing stmts using SLP.
Dense.c:216:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:216:9: note: Vectorizing SLP tree:
Dense.c:216:9: note: node 0x7b9ae68 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:216:9: note: op template: <retval>.inputs = IO_31(D);
Dense.c:216:9: note: 	stmt 0 <retval>.inputs = IO_31(D);
Dense.c:216:9: note: 	stmt 1 <retval>.outputs = _22;
Dense.c:216:9: note: 	children 0x7b9aee8
Dense.c:216:9: note: node (external) 0x7b9aee8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:216:9: note: 	{ IO_31(D), _22 }
Dense.c:216:9: note: ------>vectorizing SLP node starting from: <retval>.inputs = IO_31(D);
Dense.c:216:9: note: vect_is_simple_use: operand IO_31(D) + _21, type of def: internal
Dense.c:216:9: note: transform store. ncopies = 1
Dense.c:216:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.inputs
Dense.c:216:9: note: created &<retval>
Dense.c:216:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval>] = _78;
Dense.c:216:9: note: vectorizing stmts using SLP.
Dense.c:216:9: optimized: basic block part vectorized using 32 byte vectors
Dense.c:216:9: note: Vectorizing SLP tree:
Dense.c:216:9: note: node 0x7b9afe8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:216:9: note: op template: <retval>.feed = layer$feed_30;
Dense.c:216:9: note: 	stmt 0 <retval>.feed = layer$feed_30;
Dense.c:216:9: note: 	stmt 1 <retval>.back = layer$back_39;
Dense.c:216:9: note: 	stmt 2 <retval>.fix = layer$fix_40;
Dense.c:216:9: note: 	stmt 3 <retval>.activate = _25;
Dense.c:216:9: note: 	children 0x7b9b0e8
Dense.c:216:9: note: node (external) 0x7b9b0e8 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:216:9: note: 	{ layer$feed_30, layer$back_39, layer$fix_40, _25 }
Dense.c:216:9: note: ------>vectorizing SLP node starting from: <retval>.feed = layer$feed_30;
Dense.c:216:9: note: vect_is_simple_use: operand layer$back_39 = PHI <layer$back_45(D)(11), _BackDense(9), _SIMD_BackDense(15)>, type of def: internal
Dense.c:216:9: note: vect_is_simple_use: operand layer$fix_40 = PHI <layer$fix_46(D)(11), _FixDense(9), _SIMD_FixDense(15)>, type of def: internal
Dense.c:216:9: note: vect_is_simple_use: operand (void (*<T91a>) (float *, unsigned int, char)) _24, type of def: internal
Dense.c:216:9: note: conflicting alias set types.
Dense.c:216:9: note: transform store. ncopies = 1
Dense.c:216:9: note: create vector_type-pointer variable to type: vector(4) long unsigned int  vectorizing a pointer ref: <retval>.feed
Dense.c:216:9: note: created &<retval>.feed
Dense.c:216:9: note: add new stmt: MEM <vector(4) long unsigned int> [(void *)&<retval> + 32B] = _84;
Dense.c:216:9: note: vectorizing stmts using SLP.
Dense.c:216:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:216:9: note: Vectorizing SLP tree:
Dense.c:216:9: note: node 0x7b9b1e8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:216:9: note: op template: <retval>.parent = 0B;
Dense.c:216:9: note: 	stmt 0 <retval>.parent = 0B;
Dense.c:216:9: note: 	stmt 1 <retval>.child = 0B;
Dense.c:216:9: note: 	children 0x7b9b268
Dense.c:216:9: note: node (constant) 0x7b9b268 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:216:9: note: 	{ 0B, 0B }
Dense.c:216:9: note: ------>vectorizing SLP node starting from: <retval>.parent = 0B;
Dense.c:216:9: note: vect_is_simple_use: operand 0B, type of def: constant
Dense.c:216:9: note: transform store. ncopies = 1
Dense.c:216:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.parent
Dense.c:216:9: note: created &<retval>.parent
Dense.c:216:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer * *)&<retval> + 72B] = { 0, 0 };
Dense.c:216:9: note: vectorizing stmts using SLP.
Dense.c:216:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:216:9: note: Vectorizing SLP tree:
Dense.c:216:9: note: node 0x7b9b2e8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:216:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_29(D);
Dense.c:216:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_29(D);
Dense.c:216:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&<retval> + 96B] = _4;
Dense.c:216:9: note: 	children 0x7b9b368
Dense.c:216:9: note: node (external) 0x7b9b368 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:216:9: note: 	{ MODEL_29(D), _4 }
Dense.c:216:9: note: ------>vectorizing SLP node starting from: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_29(D);
Dense.c:216:9: note: vect_is_simple_use: operand MODEL_29(D) + _3, type of def: internal
Dense.c:216:9: note: transform store. ncopies = 1
Dense.c:216:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B]
Dense.c:216:9: note: created &MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B]
Dense.c:216:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer *)&<retval> + 88B] = _89;
Dense.c:216:9: note: vectorizing stmts using SLP.
Dense.c:216:9: note: ***** The result for vector mode V32QI would be the same
Activations.c:17:19: optimized: loop vectorized using 32 byte vectors
Activations.c:17:19: optimized: loop vectorized using 16 byte vectors
Activations.c:17:19: optimized: loop vectorized using 32 byte vectors
Activations.c:17:19: optimized: loop vectorized using 16 byte vectors
Activations.c:14:13: note: vectorized 2 loops in function.
Activations.c:14:13: note: ***** Analysis failed with vector mode V8SF
Activations.c:14:13: note: ***** The result for vector mode V32QI would be the same
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V16QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V16QI
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V8QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V8QI
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V4QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V4QI
Activations.c:28:19: missed: couldn't vectorize loop
Activations.c:28:19: missed: not vectorized: unsupported control flow in loop.
Activations.c:28:19: optimized: loop vectorized using 32 byte vectors
Activations.c:28:19: optimized: loop vectorized using 16 byte vectors
Activations.c:25:13: note: vectorized 1 loops in function.
Activations.c:25:13: note: ***** Analysis failed with vector mode V8SF
Activations.c:25:13: note: ***** The result for vector mode V32QI would be the same
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V16QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V16QI
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V8QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V8QI
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V4QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V4QI
Activations.c:38:1: note: ***** Analysis failed with vector mode VOID
Activations.c:5:19: missed: couldn't vectorize loop
Activations.c:9:21: missed: statement clobbers memory: _36 = expf (_18);
Activations.c:5:19: missed: couldn't vectorize loop
Activations.c:9:21: missed: statement clobbers memory: _12 = expf (_11);
Activations.c:3:13: note: vectorized 0 loops in function.
Activations.c:9:21: missed: statement clobbers memory: _12 = expf (_11);
Activations.c:9:21: missed: statement clobbers memory: _36 = expf (_18);
Activations.c:11:1: note: ***** Analysis failed with vector mode V8SF
Activations.c:11:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SF
Activations.c:44:2: note: ***** Analysis failed with vector mode VOID
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:4:18: note: vectorized 0 loops in function.
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_4, inputs_15, _3);
PULSE.c:8:2: missed: statement clobbers memory: _5 (layer_17);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_12, pretmp_26, _9);
PULSE.c:13:28: note: ***** Analysis failed with vector mode V8SI
PULSE.c:13:28: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:22:2: missed: statement clobbers memory: _1 (layer_10(D));
PULSE.c:22:2: missed: statement clobbers memory: _14 (_2);
PULSE.c:22:2: missed: statement clobbers memory: _21 (_15);
PULSE.c:22:2: missed: statement clobbers memory: _28 (_22);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_29);
PULSE.c:26:3: missed: statement clobbers memory: memset (_34, 0, _32);
PULSE.c:26:3: missed: statement clobbers memory: memset (_27, 0, _25);
PULSE.c:26:3: missed: statement clobbers memory: memset (_20, 0, _18);
PULSE.c:26:3: missed: statement clobbers memory: memset (_7, 0, _5);
PULSE.c:28:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:28:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:39: missed: statement clobbers memory: _7 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:32:6: note: vectorized 1 loops in function.
PULSE.c:34:8: missed: statement clobbers memory: _1 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_2);
PULSE.c:39:39: missed: statement clobbers memory: _7 = rand ();
PULSE.c:40:40: missed: statement clobbers memory: _9 = rand ();
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8SI
PULSE.c:32:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:66:16: missed: couldn't vectorize loop
PULSE.c:66:16: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:69:17: missed: couldn't vectorize loop
PULSE.c:69:17: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:78:19: missed: couldn't vectorize loop
PULSE.c:78:19: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:39: missed: statement clobbers memory: _83 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:55:22: missed: couldn't vectorize loop
PULSE.c:55:22: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:47:6: note: vectorized 1 loops in function.
PULSE.c:48:1: missed: statement clobbers memory: saved_stack.7_47 = __builtin_stack_save ();
PULSE.c:49:46: missed: statement clobbers memory: _1 = PULSE_GetLossFunctionPtr (loss_function_48(D));
PULSE.c:50:48: missed: statement clobbers memory: TMP_52 = aligned_alloc (64, _4);
PULSE.c:57:14: missed: statement clobbers memory: _72 = _5 (output_207, TMP_PTR_206);
PULSE.c:65:15: missed: statement clobbers memory: random.4_56 = __builtin_alloca_with_align (_10, 32);
PULSE.c:34:8: missed: statement clobbers memory: _75 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_76);
PULSE.c:39:39: missed: statement clobbers memory: _83 = rand ();
PULSE.c:40:40: missed: statement clobbers memory: _86 = rand ();
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_112, inputs_107, _111);
PULSE.c:8:2: missed: statement clobbers memory: _113 (layer_108);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_119, _118, _117);
PULSE.c:72:11: missed: statement clobbers memory: loss_65 = PULSE_GetLoss_50 (_23, _22, _18, _17);
PULSE.c:22:2: missed: statement clobbers memory: _100 (output_208);
PULSE.c:22:2: missed: statement clobbers memory: _122 (_101);
PULSE.c:22:2: missed: statement clobbers memory: _129 (_123);
PULSE.c:22:2: missed: statement clobbers memory: _136 (_130);
PULSE.c:22:2: missed: statement clobbers memory: _143 (_137);
PULSE.c:22:2: missed: statement clobbers memory: _150 (_144);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_151);
PULSE.c:26:3: missed: statement clobbers memory: memset (_156, 0, _154);
PULSE.c:26:3: missed: statement clobbers memory: memset (_149, 0, _147);
PULSE.c:26:3: missed: statement clobbers memory: memset (_142, 0, _140);
PULSE.c:26:3: missed: statement clobbers memory: memset (_135, 0, _133);
PULSE.c:26:3: missed: statement clobbers memory: memset (_128, 0, _126);
PULSE.c:26:3: missed: statement clobbers memory: memset (_106, 0, _104);
PULSE.c:80:6: missed: statement clobbers memory: _27 (current_212, args);
PULSE.c:86:4: missed: statement clobbers memory: printf ("Epoch: %d | Item: %d | Loss: %.10f | Batch Loss: %.10f\r", i_209, j_210, _31, _30);
PULSE.c:47:6: missed: statement clobbers memory: __builtin_stack_restore (saved_stack.7_47);
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4DI
PULSE.c:47:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:96:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:96:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:130:20: missed: couldn't vectorize loop
PULSE.c:130:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:130:20: missed: couldn't vectorize loop
PULSE.c:130:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:109:20: missed: couldn't vectorize loop
PULSE.c:109:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:109:20: missed: couldn't vectorize loop
PULSE.c:109:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:98:13: note: vectorized 0 loops in function.
PULSE.c:101:39: missed: statement clobbers memory: layers_41 = malloc (_3);
PULSE.c:104:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:123:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:124:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:125:46: missed: statement clobbers memory: WEIGHTS_86 = aligned_alloc (64, 0);
PULSE.c:115:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.106_94];
PULSE.c:116:21: missed: statement clobbers memory: _75 = PULSE_GetDenseWeightsSize (args);
PULSE.c:117:16: missed: statement clobbers memory: _78 = PULSE_GetDenseIOSize (args);
PULSE.c:118:19: missed: statement clobbers memory: _81 = PULSE_GetDenseFixesSize (args);
PULSE.c:115:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})_224];
PULSE.c:116:21: missed: statement clobbers memory: _210 = PULSE_GetDenseWeightsSize (args);
PULSE.c:117:16: missed: statement clobbers memory: _213 = PULSE_GetDenseIOSize (args);
PULSE.c:118:19: missed: statement clobbers memory: _216 = PULSE_GetDenseFixesSize (args);
PULSE.c:123:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:124:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:125:46: missed: statement clobbers memory: WEIGHTS_46 = aligned_alloc (64, _5);
PULSE.c:126:41: missed: statement clobbers memory: IO_48 = aligned_alloc (64, _7);
PULSE.c:136:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.108_96];
PULSE.c:137:17: missed: statement clobbers memory: *_10 = PULSE_CreateDenseLayer (args, WEIGHTS_PTR_152, IO_PTR_153, 0B); [return slot optimization]
PULSE.c:138:20: missed: statement clobbers memory: _64 = PULSE_GetDenseWeightsSize (args);
PULSE.c:139:15: missed: statement clobbers memory: _67 = PULSE_GetDenseIOSize (args);
PULSE.c:136:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.108_124];
PULSE.c:137:17: missed: statement clobbers memory: *_106 = PULSE_CreateDenseLayer (args, WEIGHTS_PTR_148, IO_PTR_143, 0B); [return slot optimization]
PULSE.c:138:20: missed: statement clobbers memory: _95 = PULSE_GetDenseWeightsSize (args);
PULSE.c:139:15: missed: statement clobbers memory: _59 = PULSE_GetDenseIOSize (args);
PULSE.c:145:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:146:9: note: ***** Analysis succeeded with vector mode V8SI
PULSE.c:146:9: note: SLPing BB part
PULSE.c:146:9: note: Costing subgraph: 
PULSE.c:146:9: note: node 0x50604d8 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:146:9: note: op template: <retval>.weights_size = WEIGHTS_SIZE_164;
PULSE.c:146:9: note: 	stmt 0 <retval>.weights_size = WEIGHTS_SIZE_164;
PULSE.c:146:9: note: 	stmt 1 <retval>.io_size = IO_SIZE_167;
PULSE.c:146:9: note: 	children 0x5060558
PULSE.c:146:9: note: node 0x5060558 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:146:9: note: op template: WEIGHTS_SIZE_164 = PHI <WEIGHTS_SIZE_161(36), 0(25)>
PULSE.c:146:9: note: 	stmt 0 WEIGHTS_SIZE_164 = PHI <WEIGHTS_SIZE_161(36), 0(25)>
PULSE.c:146:9: note: 	stmt 1 IO_SIZE_167 = PHI <IO_SIZE_160(36), 0(25)>
PULSE.c:146:9: note: 	children 0x50605d8 0x5060dd8
PULSE.c:146:9: note: node 0x50605d8 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:146:9: note: op template: WEIGHTS_SIZE_161 = PHI <WEIGHTS_SIZE_20(11), WEIGHTS_SIZE_206(76)>
PULSE.c:146:9: note: 	stmt 0 WEIGHTS_SIZE_161 = PHI <WEIGHTS_SIZE_20(11), WEIGHTS_SIZE_206(76)>
PULSE.c:146:9: note: 	stmt 1 IO_SIZE_160 = PHI <IO_SIZE_22(11), IO_SIZE_192(76)>
PULSE.c:146:9: note: 	children 0x5060658 0x50609d8
PULSE.c:146:9: note: node 0x5060658 (max_nunits=2, refcnt=3) vector(2) unsigned int
PULSE.c:146:9: note: op template: WEIGHTS_SIZE_20 = PHI <WEIGHTS_SIZE_73(77), WEIGHTS_SIZE_76(10)>
PULSE.c:146:9: note: 	stmt 0 WEIGHTS_SIZE_20 = PHI <WEIGHTS_SIZE_73(77), WEIGHTS_SIZE_76(10)>
PULSE.c:146:9: note: 	stmt 1 IO_SIZE_22 = PHI <IO_SIZE_147(77), IO_SIZE_79(10)>
PULSE.c:146:9: note: 	children 0x50606d8 0x50607d8
PULSE.c:146:9: note: node 0x50606d8 (max_nunits=2, refcnt=2) vector(2) unsigned int
PULSE.c:146:9: note: op template: WEIGHTS_SIZE_73 = PHI <WEIGHTS_SIZE_20(73), 0(71)>
PULSE.c:146:9: note: 	stmt 0 WEIGHTS_SIZE_73 = PHI <WEIGHTS_SIZE_20(73), 0(71)>
PULSE.c:146:9: note: 	stmt 1 IO_SIZE_147 = PHI <IO_SIZE_22(73), 0(71)>
PULSE.c:146:9: note: 	children 0x5060658 0x5060758
PULSE.c:146:9: note: node (constant) 0x5060758 (max_nunits=1, refcnt=1) vector(2) unsigned int
PULSE.c:146:9: note: 	{ 0, 0 }
PULSE.c:146:9: note: node 0x50607d8 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:146:9: note: op template: WEIGHTS_SIZE_76 = WEIGHTS_SIZE_73 + _75;
PULSE.c:146:9: note: 	stmt 0 WEIGHTS_SIZE_76 = WEIGHTS_SIZE_73 + _75;
PULSE.c:146:9: note: 	stmt 1 IO_SIZE_79 = _78 + IO_SIZE_147;
PULSE.c:146:9: note: 	children 0x50606d8 0x5060958
PULSE.c:146:9: note: node (external) 0x5060958 (max_nunits=1, refcnt=1) vector(2) unsigned int
PULSE.c:146:9: note: 	{ _75, _78 }
PULSE.c:146:9: note: node 0x50609d8 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:146:9: note: op template: WEIGHTS_SIZE_206 = PHI <WEIGHTS_SIZE_238(68)>
PULSE.c:146:9: note: 	stmt 0 WEIGHTS_SIZE_206 = PHI <WEIGHTS_SIZE_238(68)>
PULSE.c:146:9: note: 	stmt 1 IO_SIZE_192 = PHI <IO_SIZE_239(68)>
PULSE.c:146:9: note: 	children 0x5060a58
PULSE.c:146:9: note: node 0x5060a58 (max_nunits=2, refcnt=2) vector(2) unsigned int
PULSE.c:146:9: note: op template: WEIGHTS_SIZE_238 = PHI <WEIGHTS_SIZE_211(59), WEIGHTS_SIZE_200(57)>
PULSE.c:146:9: note: 	stmt 0 WEIGHTS_SIZE_238 = PHI <WEIGHTS_SIZE_211(59), WEIGHTS_SIZE_200(57)>
PULSE.c:146:9: note: 	stmt 1 IO_SIZE_239 = PHI <IO_SIZE_214(59), IO_SIZE_201(57)>
PULSE.c:146:9: note: 	children 0x5060ad8 0x5060bd8
PULSE.c:146:9: note: node 0x5060ad8 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:146:9: note: op template: WEIGHTS_SIZE_211 = WEIGHTS_SIZE_200 + _210;
PULSE.c:146:9: note: 	stmt 0 WEIGHTS_SIZE_211 = WEIGHTS_SIZE_200 + _210;
PULSE.c:146:9: note: 	stmt 1 IO_SIZE_214 = _213 + IO_SIZE_201;
PULSE.c:146:9: note: 	children 0x5060bd8 0x5060d58
PULSE.c:146:9: note: node 0x5060bd8 (max_nunits=2, refcnt=2) vector(2) unsigned int
PULSE.c:146:9: note: op template: WEIGHTS_SIZE_200 = PHI <WEIGHTS_SIZE_243(72), WEIGHTS_SIZE_238(69)>
PULSE.c:146:9: note: 	stmt 0 WEIGHTS_SIZE_200 = PHI <WEIGHTS_SIZE_243(72), WEIGHTS_SIZE_238(69)>
PULSE.c:146:9: note: 	stmt 1 IO_SIZE_201 = PHI <IO_SIZE_244(72), IO_SIZE_239(69)>
PULSE.c:146:9: note: 	children 0x5060c58 0x5060a58
PULSE.c:146:9: note: node 0x5060c58 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:146:9: note: op template: WEIGHTS_SIZE_243 = PHI <WEIGHTS_SIZE_20(74)>
PULSE.c:146:9: note: 	stmt 0 WEIGHTS_SIZE_243 = PHI <WEIGHTS_SIZE_20(74)>
PULSE.c:146:9: note: 	stmt 1 IO_SIZE_244 = PHI <IO_SIZE_22(74)>
PULSE.c:146:9: note: 	children 0x5060658
PULSE.c:146:9: note: node (external) 0x5060d58 (max_nunits=1, refcnt=1) vector(2) unsigned int
PULSE.c:146:9: note: 	{ _210, _213 }
PULSE.c:146:9: note: node (constant) 0x5060dd8 (max_nunits=1, refcnt=1) vector(2) unsigned int
PULSE.c:146:9: note: 	{ 0, 0 }
PULSE.c:146:9: note: Cost model analysis: 
PULSE.c:146:9: note: Cost model analysis for part in loop 0:
  Vector cost: 48
  Scalar cost: 48
PULSE.c:146:9: note: Cost model analysis for part in loop 1:
  Vector cost: 44
  Scalar cost: 24
PULSE.c:146:9: missed: not vectorized: vectorization is not profitable.
PULSE.c:146:9: note: Costing subgraph: 
PULSE.c:146:9: note: node 0x5060ed8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:146:9: note: op template: <retval>.layers = layers_41;
PULSE.c:146:9: note: 	stmt 0 <retval>.layers = layers_41;
PULSE.c:146:9: note: 	stmt 1 <retval>.weights = WEIGHTS_163;
PULSE.c:146:9: note: 	children 0x5060fd8
PULSE.c:146:9: note: node (external) 0x5060fd8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:146:9: note: 	{ layers_41, WEIGHTS_163 }
PULSE.c:146:9: note: Cost model analysis: 
PULSE.c:146:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:146:9: note: Costing subgraph: 
PULSE.c:146:9: note: node 0x5061058 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:146:9: note: op template: <retval>.io = 0B;
PULSE.c:146:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:146:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:146:9: note: 	children 0x50610d8
PULSE.c:146:9: note: node (constant) 0x50610d8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:146:9: note: 	{ 0B, 0B }
PULSE.c:146:9: note: Cost model analysis: 
PULSE.c:146:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:146:9: note: Basic block will be vectorized using SLP
PULSE.c:146:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:146:9: note: Vectorizing SLP tree:
PULSE.c:146:9: note: node 0x5060ed8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:146:9: note: op template: <retval>.layers = layers_41;
PULSE.c:146:9: note: 	stmt 0 <retval>.layers = layers_41;
PULSE.c:146:9: note: 	stmt 1 <retval>.weights = WEIGHTS_163;
PULSE.c:146:9: note: 	children 0x5060fd8
PULSE.c:146:9: note: node (external) 0x5060fd8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:146:9: note: 	{ layers_41, WEIGHTS_163 }
PULSE.c:146:9: note: ------>vectorizing SLP node starting from: <retval>.layers = layers_41;
PULSE.c:146:9: note: vect_is_simple_use: operand WEIGHTS_163 = PHI <WEIGHTS_46(36), WEIGHTS_86(25)>, type of def: internal
PULSE.c:146:9: note: conflicting alias set types.
PULSE.c:146:9: note: transform store. ncopies = 1
PULSE.c:146:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.layers
PULSE.c:146:9: note: created &<retval>
PULSE.c:146:9: note: add new stmt: MEM <vector(2) long unsigned int> [(void *)&<retval>] = _19;
PULSE.c:146:9: note: vectorizing stmts using SLP.
PULSE.c:146:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:146:9: note: Vectorizing SLP tree:
PULSE.c:146:9: note: node 0x5061058 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:146:9: note: op template: <retval>.io = 0B;
PULSE.c:146:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:146:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:146:9: note: 	children 0x50610d8
PULSE.c:146:9: note: node (constant) 0x50610d8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:146:9: note: 	{ 0B, 0B }
PULSE.c:146:9: note: ------>vectorizing SLP node starting from: <retval>.io = 0B;
PULSE.c:146:9: note: vect_is_simple_use: operand 0B, type of def: constant
PULSE.c:146:9: note: transform store. ncopies = 1
PULSE.c:146:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.io
PULSE.c:146:9: note: created &<retval>.io
PULSE.c:146:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval> + 16B] = { 0, 0 };
PULSE.c:146:9: note: vectorizing stmts using SLP.
PULSE.c:146:9: note: ***** The result for vector mode V32QI would be the same
PULSE.c:152:2: missed: statement clobbers memory: free (_1);
PULSE.c:153:2: missed: statement clobbers memory: free (_2);
PULSE.c:154:2: missed: statement clobbers memory: free (_3);
PULSE.c:155:2: missed: statement clobbers memory: free (_4);
PULSE.c:156:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:156:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Loss.c:15:19: optimized: loop vectorized using 32 byte vectors
Loss.c:15:19: optimized:  loop versioned for vectorization because of possible aliasing
Loss.c:15:19: optimized: loop vectorized using 16 byte vectors
Loss.c:12:23: note: vectorized 1 loops in function.
Loss.c:12:23: note: ***** Analysis failed with vector mode V8SF
Loss.c:12:23: note: ***** The result for vector mode V32QI would be the same
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V16QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V16QI
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V8QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V8QI
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V4QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V4QI
Loss.c:7:19: missed: couldn't vectorize loop
Loss.c:8:38: missed: not vectorized: unsupported use in stmt.
Loss.c:4:23: note: vectorized 0 loops in function.
Loss.c:9:13: note: ***** Analysis failed with vector mode V8SF
Loss.c:9:13: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SF
Loss.c:23:2: note: ***** Analysis failed with vector mode VOID
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:4:18: note: vectorized 0 loops in function.
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_4, inputs_15, _3);
PULSE.c:8:2: missed: statement clobbers memory: _5 (layer_17);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_12, pretmp_26, _9);
PULSE.c:13:28: note: ***** Analysis failed with vector mode V8SI
PULSE.c:13:28: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:22:2: missed: statement clobbers memory: _1 (layer_10(D));
PULSE.c:22:2: missed: statement clobbers memory: _14 (_2);
PULSE.c:22:2: missed: statement clobbers memory: _21 (_15);
PULSE.c:22:2: missed: statement clobbers memory: _28 (_22);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_29);
PULSE.c:26:3: missed: statement clobbers memory: memset (_34, 0, _32);
PULSE.c:26:3: missed: statement clobbers memory: memset (_27, 0, _25);
PULSE.c:26:3: missed: statement clobbers memory: memset (_20, 0, _18);
PULSE.c:26:3: missed: statement clobbers memory: memset (_7, 0, _5);
PULSE.c:28:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:28:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:39: missed: statement clobbers memory: _7 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:32:6: note: vectorized 1 loops in function.
PULSE.c:34:8: missed: statement clobbers memory: _1 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_2);
PULSE.c:39:39: missed: statement clobbers memory: _7 = rand ();
PULSE.c:40:40: missed: statement clobbers memory: _9 = rand ();
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8SI
PULSE.c:32:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:66:16: missed: couldn't vectorize loop
PULSE.c:66:16: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:69:17: missed: couldn't vectorize loop
PULSE.c:69:17: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:78:19: missed: couldn't vectorize loop
PULSE.c:78:19: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:39: missed: statement clobbers memory: _83 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:55:22: missed: couldn't vectorize loop
PULSE.c:55:22: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:47:6: note: vectorized 1 loops in function.
PULSE.c:48:1: missed: statement clobbers memory: saved_stack.7_47 = __builtin_stack_save ();
PULSE.c:49:46: missed: statement clobbers memory: _1 = PULSE_GetLossFunctionPtr (loss_function_48(D));
PULSE.c:50:48: missed: statement clobbers memory: TMP_52 = aligned_alloc (64, _4);
PULSE.c:57:14: missed: statement clobbers memory: _72 = _5 (output_207, TMP_PTR_206);
PULSE.c:65:15: missed: statement clobbers memory: random.4_56 = __builtin_alloca_with_align (_10, 32);
PULSE.c:34:8: missed: statement clobbers memory: _75 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_76);
PULSE.c:39:39: missed: statement clobbers memory: _83 = rand ();
PULSE.c:40:40: missed: statement clobbers memory: _86 = rand ();
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_112, inputs_107, _111);
PULSE.c:8:2: missed: statement clobbers memory: _113 (layer_108);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_119, _118, _117);
PULSE.c:72:11: missed: statement clobbers memory: loss_65 = PULSE_GetLoss_50 (_23, _22, _18, _17);
PULSE.c:22:2: missed: statement clobbers memory: _100 (output_208);
PULSE.c:22:2: missed: statement clobbers memory: _122 (_101);
PULSE.c:22:2: missed: statement clobbers memory: _129 (_123);
PULSE.c:22:2: missed: statement clobbers memory: _136 (_130);
PULSE.c:22:2: missed: statement clobbers memory: _143 (_137);
PULSE.c:22:2: missed: statement clobbers memory: _150 (_144);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_151);
PULSE.c:26:3: missed: statement clobbers memory: memset (_156, 0, _154);
PULSE.c:26:3: missed: statement clobbers memory: memset (_149, 0, _147);
PULSE.c:26:3: missed: statement clobbers memory: memset (_142, 0, _140);
PULSE.c:26:3: missed: statement clobbers memory: memset (_135, 0, _133);
PULSE.c:26:3: missed: statement clobbers memory: memset (_128, 0, _126);
PULSE.c:26:3: missed: statement clobbers memory: memset (_106, 0, _104);
PULSE.c:80:6: missed: statement clobbers memory: _27 (current_212, args);
PULSE.c:86:4: missed: statement clobbers memory: printf ("Epoch: %d | Item: %d | Loss: %.10f | Batch Loss: %.10f\r", i_209, j_210, _31, _30);
PULSE.c:47:6: missed: statement clobbers memory: __builtin_stack_restore (saved_stack.7_47);
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4DI
PULSE.c:47:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:96:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:96:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:130:20: missed: couldn't vectorize loop
PULSE.c:130:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:130:20: missed: couldn't vectorize loop
PULSE.c:130:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:109:20: missed: couldn't vectorize loop
PULSE.c:109:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:109:20: missed: couldn't vectorize loop
PULSE.c:109:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:98:13: note: vectorized 0 loops in function.
PULSE.c:101:39: missed: statement clobbers memory: layers_41 = malloc (_3);
PULSE.c:104:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:123:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:124:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:125:46: missed: statement clobbers memory: WEIGHTS_86 = aligned_alloc (64, 0);
PULSE.c:115:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.106_94];
PULSE.c:116:21: missed: statement clobbers memory: _75 = PULSE_GetDenseWeightsSize (args);
PULSE.c:117:16: missed: statement clobbers memory: _78 = PULSE_GetDenseIOSize (args);
PULSE.c:118:19: missed: statement clobbers memory: _81 = PULSE_GetDenseFixesSize (args);
PULSE.c:115:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})_224];
PULSE.c:116:21: missed: statement clobbers memory: _210 = PULSE_GetDenseWeightsSize (args);
PULSE.c:117:16: missed: statement clobbers memory: _213 = PULSE_GetDenseIOSize (args);
PULSE.c:118:19: missed: statement clobbers memory: _216 = PULSE_GetDenseFixesSize (args);
PULSE.c:123:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:124:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:125:46: missed: statement clobbers memory: WEIGHTS_46 = aligned_alloc (64, _5);
PULSE.c:126:41: missed: statement clobbers memory: IO_48 = aligned_alloc (64, _7);
PULSE.c:136:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.108_96];
PULSE.c:137:17: missed: statement clobbers memory: *_10 = PULSE_CreateDenseLayer (args, WEIGHTS_PTR_152, IO_PTR_153, 0B); [return slot optimization]
PULSE.c:138:20: missed: statement clobbers memory: _64 = PULSE_GetDenseWeightsSize (args);
PULSE.c:139:15: missed: statement clobbers memory: _67 = PULSE_GetDenseIOSize (args);
PULSE.c:136:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.108_124];
PULSE.c:137:17: missed: statement clobbers memory: *_106 = PULSE_CreateDenseLayer (args, WEIGHTS_PTR_148, IO_PTR_143, 0B); [return slot optimization]
PULSE.c:138:20: missed: statement clobbers memory: _95 = PULSE_GetDenseWeightsSize (args);
PULSE.c:139:15: missed: statement clobbers memory: _59 = PULSE_GetDenseIOSize (args);
PULSE.c:145:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:146:9: note: ***** Analysis succeeded with vector mode V8SI
PULSE.c:146:9: note: SLPing BB part
PULSE.c:146:9: note: Costing subgraph: 
PULSE.c:146:9: note: node 0x33c14c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:146:9: note: op template: <retval>.weights_size = WEIGHTS_SIZE_164;
PULSE.c:146:9: note: 	stmt 0 <retval>.weights_size = WEIGHTS_SIZE_164;
PULSE.c:146:9: note: 	stmt 1 <retval>.io_size = IO_SIZE_167;
PULSE.c:146:9: note: 	children 0x33c1548
PULSE.c:146:9: note: node 0x33c1548 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:146:9: note: op template: WEIGHTS_SIZE_164 = PHI <WEIGHTS_SIZE_161(36), 0(25)>
PULSE.c:146:9: note: 	stmt 0 WEIGHTS_SIZE_164 = PHI <WEIGHTS_SIZE_161(36), 0(25)>
PULSE.c:146:9: note: 	stmt 1 IO_SIZE_167 = PHI <IO_SIZE_160(36), 0(25)>
PULSE.c:146:9: note: 	children 0x33c15c8 0x33c1dc8
PULSE.c:146:9: note: node 0x33c15c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:146:9: note: op template: WEIGHTS_SIZE_161 = PHI <WEIGHTS_SIZE_20(11), WEIGHTS_SIZE_206(76)>
PULSE.c:146:9: note: 	stmt 0 WEIGHTS_SIZE_161 = PHI <WEIGHTS_SIZE_20(11), WEIGHTS_SIZE_206(76)>
PULSE.c:146:9: note: 	stmt 1 IO_SIZE_160 = PHI <IO_SIZE_22(11), IO_SIZE_192(76)>
PULSE.c:146:9: note: 	children 0x33c1648 0x33c19c8
PULSE.c:146:9: note: node 0x33c1648 (max_nunits=2, refcnt=3) vector(2) unsigned int
PULSE.c:146:9: note: op template: WEIGHTS_SIZE_20 = PHI <WEIGHTS_SIZE_73(77), WEIGHTS_SIZE_76(10)>
PULSE.c:146:9: note: 	stmt 0 WEIGHTS_SIZE_20 = PHI <WEIGHTS_SIZE_73(77), WEIGHTS_SIZE_76(10)>
PULSE.c:146:9: note: 	stmt 1 IO_SIZE_22 = PHI <IO_SIZE_147(77), IO_SIZE_79(10)>
PULSE.c:146:9: note: 	children 0x33c16c8 0x33c17c8
PULSE.c:146:9: note: node 0x33c16c8 (max_nunits=2, refcnt=2) vector(2) unsigned int
PULSE.c:146:9: note: op template: WEIGHTS_SIZE_73 = PHI <WEIGHTS_SIZE_20(73), 0(71)>
PULSE.c:146:9: note: 	stmt 0 WEIGHTS_SIZE_73 = PHI <WEIGHTS_SIZE_20(73), 0(71)>
PULSE.c:146:9: note: 	stmt 1 IO_SIZE_147 = PHI <IO_SIZE_22(73), 0(71)>
PULSE.c:146:9: note: 	children 0x33c1648 0x33c1748
PULSE.c:146:9: note: node (constant) 0x33c1748 (max_nunits=1, refcnt=1) vector(2) unsigned int
PULSE.c:146:9: note: 	{ 0, 0 }
PULSE.c:146:9: note: node 0x33c17c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:146:9: note: op template: WEIGHTS_SIZE_76 = WEIGHTS_SIZE_73 + _75;
PULSE.c:146:9: note: 	stmt 0 WEIGHTS_SIZE_76 = WEIGHTS_SIZE_73 + _75;
PULSE.c:146:9: note: 	stmt 1 IO_SIZE_79 = _78 + IO_SIZE_147;
PULSE.c:146:9: note: 	children 0x33c16c8 0x33c1948
PULSE.c:146:9: note: node (external) 0x33c1948 (max_nunits=1, refcnt=1) vector(2) unsigned int
PULSE.c:146:9: note: 	{ _75, _78 }
PULSE.c:146:9: note: node 0x33c19c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:146:9: note: op template: WEIGHTS_SIZE_206 = PHI <WEIGHTS_SIZE_238(68)>
PULSE.c:146:9: note: 	stmt 0 WEIGHTS_SIZE_206 = PHI <WEIGHTS_SIZE_238(68)>
PULSE.c:146:9: note: 	stmt 1 IO_SIZE_192 = PHI <IO_SIZE_239(68)>
PULSE.c:146:9: note: 	children 0x33c1a48
PULSE.c:146:9: note: node 0x33c1a48 (max_nunits=2, refcnt=2) vector(2) unsigned int
PULSE.c:146:9: note: op template: WEIGHTS_SIZE_238 = PHI <WEIGHTS_SIZE_211(59), WEIGHTS_SIZE_200(57)>
PULSE.c:146:9: note: 	stmt 0 WEIGHTS_SIZE_238 = PHI <WEIGHTS_SIZE_211(59), WEIGHTS_SIZE_200(57)>
PULSE.c:146:9: note: 	stmt 1 IO_SIZE_239 = PHI <IO_SIZE_214(59), IO_SIZE_201(57)>
PULSE.c:146:9: note: 	children 0x33c1ac8 0x33c1bc8
PULSE.c:146:9: note: node 0x33c1ac8 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:146:9: note: op template: WEIGHTS_SIZE_211 = WEIGHTS_SIZE_200 + _210;
PULSE.c:146:9: note: 	stmt 0 WEIGHTS_SIZE_211 = WEIGHTS_SIZE_200 + _210;
PULSE.c:146:9: note: 	stmt 1 IO_SIZE_214 = _213 + IO_SIZE_201;
PULSE.c:146:9: note: 	children 0x33c1bc8 0x33c1d48
PULSE.c:146:9: note: node 0x33c1bc8 (max_nunits=2, refcnt=2) vector(2) unsigned int
PULSE.c:146:9: note: op template: WEIGHTS_SIZE_200 = PHI <WEIGHTS_SIZE_243(72), WEIGHTS_SIZE_238(69)>
PULSE.c:146:9: note: 	stmt 0 WEIGHTS_SIZE_200 = PHI <WEIGHTS_SIZE_243(72), WEIGHTS_SIZE_238(69)>
PULSE.c:146:9: note: 	stmt 1 IO_SIZE_201 = PHI <IO_SIZE_244(72), IO_SIZE_239(69)>
PULSE.c:146:9: note: 	children 0x33c1c48 0x33c1a48
PULSE.c:146:9: note: node 0x33c1c48 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:146:9: note: op template: WEIGHTS_SIZE_243 = PHI <WEIGHTS_SIZE_20(74)>
PULSE.c:146:9: note: 	stmt 0 WEIGHTS_SIZE_243 = PHI <WEIGHTS_SIZE_20(74)>
PULSE.c:146:9: note: 	stmt 1 IO_SIZE_244 = PHI <IO_SIZE_22(74)>
PULSE.c:146:9: note: 	children 0x33c1648
PULSE.c:146:9: note: node (external) 0x33c1d48 (max_nunits=1, refcnt=1) vector(2) unsigned int
PULSE.c:146:9: note: 	{ _210, _213 }
PULSE.c:146:9: note: node (constant) 0x33c1dc8 (max_nunits=1, refcnt=1) vector(2) unsigned int
PULSE.c:146:9: note: 	{ 0, 0 }
PULSE.c:146:9: note: Cost model analysis: 
PULSE.c:146:9: note: Cost model analysis for part in loop 0:
  Vector cost: 48
  Scalar cost: 48
PULSE.c:146:9: note: Cost model analysis for part in loop 1:
  Vector cost: 44
  Scalar cost: 24
PULSE.c:146:9: missed: not vectorized: vectorization is not profitable.
PULSE.c:146:9: note: Costing subgraph: 
PULSE.c:146:9: note: node 0x33c1ec8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:146:9: note: op template: <retval>.layers = layers_41;
PULSE.c:146:9: note: 	stmt 0 <retval>.layers = layers_41;
PULSE.c:146:9: note: 	stmt 1 <retval>.weights = WEIGHTS_163;
PULSE.c:146:9: note: 	children 0x33c1fc8
PULSE.c:146:9: note: node (external) 0x33c1fc8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:146:9: note: 	{ layers_41, WEIGHTS_163 }
PULSE.c:146:9: note: Cost model analysis: 
PULSE.c:146:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:146:9: note: Costing subgraph: 
PULSE.c:146:9: note: node 0x33c2048 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:146:9: note: op template: <retval>.io = 0B;
PULSE.c:146:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:146:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:146:9: note: 	children 0x33c20c8
PULSE.c:146:9: note: node (constant) 0x33c20c8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:146:9: note: 	{ 0B, 0B }
PULSE.c:146:9: note: Cost model analysis: 
PULSE.c:146:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:146:9: note: Basic block will be vectorized using SLP
PULSE.c:146:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:146:9: note: Vectorizing SLP tree:
PULSE.c:146:9: note: node 0x33c1ec8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:146:9: note: op template: <retval>.layers = layers_41;
PULSE.c:146:9: note: 	stmt 0 <retval>.layers = layers_41;
PULSE.c:146:9: note: 	stmt 1 <retval>.weights = WEIGHTS_163;
PULSE.c:146:9: note: 	children 0x33c1fc8
PULSE.c:146:9: note: node (external) 0x33c1fc8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:146:9: note: 	{ layers_41, WEIGHTS_163 }
PULSE.c:146:9: note: ------>vectorizing SLP node starting from: <retval>.layers = layers_41;
PULSE.c:146:9: note: vect_is_simple_use: operand WEIGHTS_163 = PHI <WEIGHTS_46(36), WEIGHTS_86(25)>, type of def: internal
PULSE.c:146:9: note: conflicting alias set types.
PULSE.c:146:9: note: transform store. ncopies = 1
PULSE.c:146:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.layers
PULSE.c:146:9: note: created &<retval>
PULSE.c:146:9: note: add new stmt: MEM <vector(2) long unsigned int> [(void *)&<retval>] = _19;
PULSE.c:146:9: note: vectorizing stmts using SLP.
PULSE.c:146:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:146:9: note: Vectorizing SLP tree:
PULSE.c:146:9: note: node 0x33c2048 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:146:9: note: op template: <retval>.io = 0B;
PULSE.c:146:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:146:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:146:9: note: 	children 0x33c20c8
PULSE.c:146:9: note: node (constant) 0x33c20c8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:146:9: note: 	{ 0B, 0B }
PULSE.c:146:9: note: ------>vectorizing SLP node starting from: <retval>.io = 0B;
PULSE.c:146:9: note: vect_is_simple_use: operand 0B, type of def: constant
PULSE.c:146:9: note: transform store. ncopies = 1
PULSE.c:146:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.io
PULSE.c:146:9: note: created &<retval>.io
PULSE.c:146:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval> + 16B] = { 0, 0 };
PULSE.c:146:9: note: vectorizing stmts using SLP.
PULSE.c:146:9: note: ***** The result for vector mode V32QI would be the same
PULSE.c:152:2: missed: statement clobbers memory: free (_1);
PULSE.c:153:2: missed: statement clobbers memory: free (_2);
PULSE.c:154:2: missed: statement clobbers memory: free (_3);
PULSE.c:155:2: missed: statement clobbers memory: free (_4);
PULSE.c:156:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:156:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:4:18: note: vectorized 0 loops in function.
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_4, inputs_15, _3);
PULSE.c:8:2: missed: statement clobbers memory: _5 (layer_17);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_12, pretmp_26, _9);
PULSE.c:13:28: note: ***** Analysis failed with vector mode V8SI
PULSE.c:13:28: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:22:2: missed: statement clobbers memory: _1 (layer_10(D));
PULSE.c:22:2: missed: statement clobbers memory: _14 (_2);
PULSE.c:22:2: missed: statement clobbers memory: _21 (_15);
PULSE.c:22:2: missed: statement clobbers memory: _28 (_22);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_29);
PULSE.c:26:3: missed: statement clobbers memory: memset (_34, 0, _32);
PULSE.c:26:3: missed: statement clobbers memory: memset (_27, 0, _25);
PULSE.c:26:3: missed: statement clobbers memory: memset (_20, 0, _18);
PULSE.c:26:3: missed: statement clobbers memory: memset (_7, 0, _5);
PULSE.c:28:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:28:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:39: missed: statement clobbers memory: _7 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:32:6: note: vectorized 1 loops in function.
PULSE.c:34:8: missed: statement clobbers memory: _1 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_2);
PULSE.c:39:39: missed: statement clobbers memory: _7 = rand ();
PULSE.c:40:40: missed: statement clobbers memory: _9 = rand ();
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8SI
PULSE.c:32:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:66:16: missed: couldn't vectorize loop
PULSE.c:66:16: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:69:17: missed: couldn't vectorize loop
PULSE.c:69:17: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:78:19: missed: couldn't vectorize loop
PULSE.c:78:19: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:39: missed: statement clobbers memory: _83 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:55:22: missed: couldn't vectorize loop
PULSE.c:55:22: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:47:6: note: vectorized 1 loops in function.
PULSE.c:48:1: missed: statement clobbers memory: saved_stack.7_47 = __builtin_stack_save ();
PULSE.c:49:46: missed: statement clobbers memory: _1 = PULSE_GetLossFunctionPtr (loss_function_48(D));
PULSE.c:50:42: missed: statement clobbers memory: TMP_52 = aligned_alloc (64, _4);
PULSE.c:57:14: missed: statement clobbers memory: _72 = _5 (output_207, TMP_PTR_206);
PULSE.c:65:15: missed: statement clobbers memory: random.4_56 = __builtin_alloca_with_align (_10, 32);
PULSE.c:34:8: missed: statement clobbers memory: _75 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_76);
PULSE.c:39:39: missed: statement clobbers memory: _83 = rand ();
PULSE.c:40:40: missed: statement clobbers memory: _86 = rand ();
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_112, inputs_107, _111);
PULSE.c:8:2: missed: statement clobbers memory: _113 (layer_108);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_119, _118, _117);
PULSE.c:72:11: missed: statement clobbers memory: loss_65 = PULSE_GetLoss_50 (_23, _22, _18, _17);
PULSE.c:22:2: missed: statement clobbers memory: _100 (output_208);
PULSE.c:22:2: missed: statement clobbers memory: _122 (_101);
PULSE.c:22:2: missed: statement clobbers memory: _129 (_123);
PULSE.c:22:2: missed: statement clobbers memory: _136 (_130);
PULSE.c:22:2: missed: statement clobbers memory: _143 (_137);
PULSE.c:22:2: missed: statement clobbers memory: _150 (_144);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_151);
PULSE.c:26:3: missed: statement clobbers memory: memset (_156, 0, _154);
PULSE.c:26:3: missed: statement clobbers memory: memset (_149, 0, _147);
PULSE.c:26:3: missed: statement clobbers memory: memset (_142, 0, _140);
PULSE.c:26:3: missed: statement clobbers memory: memset (_135, 0, _133);
PULSE.c:26:3: missed: statement clobbers memory: memset (_128, 0, _126);
PULSE.c:26:3: missed: statement clobbers memory: memset (_106, 0, _104);
PULSE.c:80:6: missed: statement clobbers memory: _27 (current_212, args);
PULSE.c:86:4: missed: statement clobbers memory: printf ("Epoch: %d | Item: %d | Loss: %.10f | Batch Loss: %.10f\r", i_209, j_210, _31, _30);
PULSE.c:47:6: missed: statement clobbers memory: __builtin_stack_restore (saved_stack.7_47);
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4DI
PULSE.c:47:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:96:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:96:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:130:20: missed: couldn't vectorize loop
PULSE.c:130:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:130:20: missed: couldn't vectorize loop
PULSE.c:130:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:109:20: missed: couldn't vectorize loop
PULSE.c:109:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:109:20: missed: couldn't vectorize loop
PULSE.c:109:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:98:13: note: vectorized 0 loops in function.
PULSE.c:101:39: missed: statement clobbers memory: layers_41 = malloc (_3);
PULSE.c:104:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:123:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:124:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:125:46: missed: statement clobbers memory: WEIGHTS_86 = aligned_alloc (64, 0);
PULSE.c:115:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.106_94];
PULSE.c:116:21: missed: statement clobbers memory: _75 = PULSE_GetDenseWeightsSize (args);
PULSE.c:117:16: missed: statement clobbers memory: _78 = PULSE_GetDenseIOSize (args);
PULSE.c:118:19: missed: statement clobbers memory: _81 = PULSE_GetDenseFixesSize (args);
PULSE.c:115:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})_224];
PULSE.c:116:21: missed: statement clobbers memory: _210 = PULSE_GetDenseWeightsSize (args);
PULSE.c:117:16: missed: statement clobbers memory: _213 = PULSE_GetDenseIOSize (args);
PULSE.c:118:19: missed: statement clobbers memory: _216 = PULSE_GetDenseFixesSize (args);
PULSE.c:123:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:124:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:125:46: missed: statement clobbers memory: WEIGHTS_46 = aligned_alloc (64, _5);
PULSE.c:126:41: missed: statement clobbers memory: IO_48 = aligned_alloc (64, _7);
PULSE.c:136:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.108_96];
PULSE.c:137:17: missed: statement clobbers memory: *_10 = PULSE_CreateDenseLayer (args, WEIGHTS_PTR_152, IO_PTR_153, 0B); [return slot optimization]
PULSE.c:138:20: missed: statement clobbers memory: _64 = PULSE_GetDenseWeightsSize (args);
PULSE.c:139:15: missed: statement clobbers memory: _67 = PULSE_GetDenseIOSize (args);
PULSE.c:136:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.108_124];
PULSE.c:137:17: missed: statement clobbers memory: *_106 = PULSE_CreateDenseLayer (args, WEIGHTS_PTR_148, IO_PTR_143, 0B); [return slot optimization]
PULSE.c:138:20: missed: statement clobbers memory: _95 = PULSE_GetDenseWeightsSize (args);
PULSE.c:139:15: missed: statement clobbers memory: _59 = PULSE_GetDenseIOSize (args);
PULSE.c:145:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:146:9: note: ***** Analysis succeeded with vector mode V8SI
PULSE.c:146:9: note: SLPing BB part
PULSE.c:146:9: note: Costing subgraph: 
PULSE.c:146:9: note: node 0x48c04c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:146:9: note: op template: <retval>.weights_size = WEIGHTS_SIZE_164;
PULSE.c:146:9: note: 	stmt 0 <retval>.weights_size = WEIGHTS_SIZE_164;
PULSE.c:146:9: note: 	stmt 1 <retval>.io_size = IO_SIZE_167;
PULSE.c:146:9: note: 	children 0x48c0548
PULSE.c:146:9: note: node 0x48c0548 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:146:9: note: op template: WEIGHTS_SIZE_164 = PHI <WEIGHTS_SIZE_161(36), 0(25)>
PULSE.c:146:9: note: 	stmt 0 WEIGHTS_SIZE_164 = PHI <WEIGHTS_SIZE_161(36), 0(25)>
PULSE.c:146:9: note: 	stmt 1 IO_SIZE_167 = PHI <IO_SIZE_160(36), 0(25)>
PULSE.c:146:9: note: 	children 0x48c05c8 0x48c0dc8
PULSE.c:146:9: note: node 0x48c05c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:146:9: note: op template: WEIGHTS_SIZE_161 = PHI <WEIGHTS_SIZE_20(11), WEIGHTS_SIZE_206(76)>
PULSE.c:146:9: note: 	stmt 0 WEIGHTS_SIZE_161 = PHI <WEIGHTS_SIZE_20(11), WEIGHTS_SIZE_206(76)>
PULSE.c:146:9: note: 	stmt 1 IO_SIZE_160 = PHI <IO_SIZE_22(11), IO_SIZE_192(76)>
PULSE.c:146:9: note: 	children 0x48c0648 0x48c09c8
PULSE.c:146:9: note: node 0x48c0648 (max_nunits=2, refcnt=3) vector(2) unsigned int
PULSE.c:146:9: note: op template: WEIGHTS_SIZE_20 = PHI <WEIGHTS_SIZE_73(77), WEIGHTS_SIZE_76(10)>
PULSE.c:146:9: note: 	stmt 0 WEIGHTS_SIZE_20 = PHI <WEIGHTS_SIZE_73(77), WEIGHTS_SIZE_76(10)>
PULSE.c:146:9: note: 	stmt 1 IO_SIZE_22 = PHI <IO_SIZE_147(77), IO_SIZE_79(10)>
PULSE.c:146:9: note: 	children 0x48c06c8 0x48c07c8
PULSE.c:146:9: note: node 0x48c06c8 (max_nunits=2, refcnt=2) vector(2) unsigned int
PULSE.c:146:9: note: op template: WEIGHTS_SIZE_73 = PHI <WEIGHTS_SIZE_20(73), 0(71)>
PULSE.c:146:9: note: 	stmt 0 WEIGHTS_SIZE_73 = PHI <WEIGHTS_SIZE_20(73), 0(71)>
PULSE.c:146:9: note: 	stmt 1 IO_SIZE_147 = PHI <IO_SIZE_22(73), 0(71)>
PULSE.c:146:9: note: 	children 0x48c0648 0x48c0748
PULSE.c:146:9: note: node (constant) 0x48c0748 (max_nunits=1, refcnt=1) vector(2) unsigned int
PULSE.c:146:9: note: 	{ 0, 0 }
PULSE.c:146:9: note: node 0x48c07c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:146:9: note: op template: WEIGHTS_SIZE_76 = WEIGHTS_SIZE_73 + _75;
PULSE.c:146:9: note: 	stmt 0 WEIGHTS_SIZE_76 = WEIGHTS_SIZE_73 + _75;
PULSE.c:146:9: note: 	stmt 1 IO_SIZE_79 = _78 + IO_SIZE_147;
PULSE.c:146:9: note: 	children 0x48c06c8 0x48c0948
PULSE.c:146:9: note: node (external) 0x48c0948 (max_nunits=1, refcnt=1) vector(2) unsigned int
PULSE.c:146:9: note: 	{ _75, _78 }
PULSE.c:146:9: note: node 0x48c09c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:146:9: note: op template: WEIGHTS_SIZE_206 = PHI <WEIGHTS_SIZE_238(68)>
PULSE.c:146:9: note: 	stmt 0 WEIGHTS_SIZE_206 = PHI <WEIGHTS_SIZE_238(68)>
PULSE.c:146:9: note: 	stmt 1 IO_SIZE_192 = PHI <IO_SIZE_239(68)>
PULSE.c:146:9: note: 	children 0x48c0a48
PULSE.c:146:9: note: node 0x48c0a48 (max_nunits=2, refcnt=2) vector(2) unsigned int
PULSE.c:146:9: note: op template: WEIGHTS_SIZE_238 = PHI <WEIGHTS_SIZE_211(59), WEIGHTS_SIZE_200(57)>
PULSE.c:146:9: note: 	stmt 0 WEIGHTS_SIZE_238 = PHI <WEIGHTS_SIZE_211(59), WEIGHTS_SIZE_200(57)>
PULSE.c:146:9: note: 	stmt 1 IO_SIZE_239 = PHI <IO_SIZE_214(59), IO_SIZE_201(57)>
PULSE.c:146:9: note: 	children 0x48c0ac8 0x48c0bc8
PULSE.c:146:9: note: node 0x48c0ac8 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:146:9: note: op template: WEIGHTS_SIZE_211 = WEIGHTS_SIZE_200 + _210;
PULSE.c:146:9: note: 	stmt 0 WEIGHTS_SIZE_211 = WEIGHTS_SIZE_200 + _210;
PULSE.c:146:9: note: 	stmt 1 IO_SIZE_214 = _213 + IO_SIZE_201;
PULSE.c:146:9: note: 	children 0x48c0bc8 0x48c0d48
PULSE.c:146:9: note: node 0x48c0bc8 (max_nunits=2, refcnt=2) vector(2) unsigned int
PULSE.c:146:9: note: op template: WEIGHTS_SIZE_200 = PHI <WEIGHTS_SIZE_243(72), WEIGHTS_SIZE_238(69)>
PULSE.c:146:9: note: 	stmt 0 WEIGHTS_SIZE_200 = PHI <WEIGHTS_SIZE_243(72), WEIGHTS_SIZE_238(69)>
PULSE.c:146:9: note: 	stmt 1 IO_SIZE_201 = PHI <IO_SIZE_244(72), IO_SIZE_239(69)>
PULSE.c:146:9: note: 	children 0x48c0c48 0x48c0a48
PULSE.c:146:9: note: node 0x48c0c48 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:146:9: note: op template: WEIGHTS_SIZE_243 = PHI <WEIGHTS_SIZE_20(74)>
PULSE.c:146:9: note: 	stmt 0 WEIGHTS_SIZE_243 = PHI <WEIGHTS_SIZE_20(74)>
PULSE.c:146:9: note: 	stmt 1 IO_SIZE_244 = PHI <IO_SIZE_22(74)>
PULSE.c:146:9: note: 	children 0x48c0648
PULSE.c:146:9: note: node (external) 0x48c0d48 (max_nunits=1, refcnt=1) vector(2) unsigned int
PULSE.c:146:9: note: 	{ _210, _213 }
PULSE.c:146:9: note: node (constant) 0x48c0dc8 (max_nunits=1, refcnt=1) vector(2) unsigned int
PULSE.c:146:9: note: 	{ 0, 0 }
PULSE.c:146:9: note: Cost model analysis: 
PULSE.c:146:9: note: Cost model analysis for part in loop 0:
  Vector cost: 48
  Scalar cost: 48
PULSE.c:146:9: note: Cost model analysis for part in loop 1:
  Vector cost: 44
  Scalar cost: 24
PULSE.c:146:9: missed: not vectorized: vectorization is not profitable.
PULSE.c:146:9: note: Costing subgraph: 
PULSE.c:146:9: note: node 0x48c0ec8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:146:9: note: op template: <retval>.layers = layers_41;
PULSE.c:146:9: note: 	stmt 0 <retval>.layers = layers_41;
PULSE.c:146:9: note: 	stmt 1 <retval>.weights = WEIGHTS_163;
PULSE.c:146:9: note: 	children 0x48c0fc8
PULSE.c:146:9: note: node (external) 0x48c0fc8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:146:9: note: 	{ layers_41, WEIGHTS_163 }
PULSE.c:146:9: note: Cost model analysis: 
PULSE.c:146:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:146:9: note: Costing subgraph: 
PULSE.c:146:9: note: node 0x48c1048 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:146:9: note: op template: <retval>.io = 0B;
PULSE.c:146:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:146:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:146:9: note: 	children 0x48c10c8
PULSE.c:146:9: note: node (constant) 0x48c10c8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:146:9: note: 	{ 0B, 0B }
PULSE.c:146:9: note: Cost model analysis: 
PULSE.c:146:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:146:9: note: Basic block will be vectorized using SLP
PULSE.c:146:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:146:9: note: Vectorizing SLP tree:
PULSE.c:146:9: note: node 0x48c0ec8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:146:9: note: op template: <retval>.layers = layers_41;
PULSE.c:146:9: note: 	stmt 0 <retval>.layers = layers_41;
PULSE.c:146:9: note: 	stmt 1 <retval>.weights = WEIGHTS_163;
PULSE.c:146:9: note: 	children 0x48c0fc8
PULSE.c:146:9: note: node (external) 0x48c0fc8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:146:9: note: 	{ layers_41, WEIGHTS_163 }
PULSE.c:146:9: note: ------>vectorizing SLP node starting from: <retval>.layers = layers_41;
PULSE.c:146:9: note: vect_is_simple_use: operand WEIGHTS_163 = PHI <WEIGHTS_46(36), WEIGHTS_86(25)>, type of def: internal
PULSE.c:146:9: note: conflicting alias set types.
PULSE.c:146:9: note: transform store. ncopies = 1
PULSE.c:146:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.layers
PULSE.c:146:9: note: created &<retval>
PULSE.c:146:9: note: add new stmt: MEM <vector(2) long unsigned int> [(void *)&<retval>] = _19;
PULSE.c:146:9: note: vectorizing stmts using SLP.
PULSE.c:146:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:146:9: note: Vectorizing SLP tree:
PULSE.c:146:9: note: node 0x48c1048 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:146:9: note: op template: <retval>.io = 0B;
PULSE.c:146:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:146:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:146:9: note: 	children 0x48c10c8
PULSE.c:146:9: note: node (constant) 0x48c10c8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:146:9: note: 	{ 0B, 0B }
PULSE.c:146:9: note: ------>vectorizing SLP node starting from: <retval>.io = 0B;
PULSE.c:146:9: note: vect_is_simple_use: operand 0B, type of def: constant
PULSE.c:146:9: note: transform store. ncopies = 1
PULSE.c:146:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.io
PULSE.c:146:9: note: created &<retval>.io
PULSE.c:146:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval> + 16B] = { 0, 0 };
PULSE.c:146:9: note: vectorizing stmts using SLP.
PULSE.c:146:9: note: ***** The result for vector mode V32QI would be the same
PULSE.c:152:2: missed: statement clobbers memory: free (_1);
PULSE.c:153:2: missed: statement clobbers memory: free (_2);
PULSE.c:154:2: missed: statement clobbers memory: free (_3);
PULSE.c:155:2: missed: statement clobbers memory: free (_4);
PULSE.c:156:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:156:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _28 (pretmp_91, _51, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:19: missed: not vectorized: no vectype for stmt: _22 = *_21;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:163:10: note: ***** Analysis failed with vector mode V4DI
Dense.c:163:10: note: ***** The result for vector mode V32QI would be the same
Dense.c:163:10: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:163:10: note: ***** Analysis failed with vector mode V16QI
Dense.c:163:10: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:163:10: note: ***** Analysis failed with vector mode V8QI
Dense.c:163:10: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:163:10: note: ***** Analysis failed with vector mode V4QI
Dense.c:145:10: optimized: loop vectorized using 32 byte vectors
Dense.c:145:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:145:10: optimized: loop vectorized using 16 byte vectors
Dense.c:136:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
Dense.c:129:10: optimized: loop vectorized using 32 byte vectors
Dense.c:129:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:129:10: optimized: loop vectorized using 16 byte vectors
Dense.c:120:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
Dense.c:111:13: note: vectorized 2 loops in function.
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_28, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
Dense.c:111:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:111:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:111:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:111:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:111:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:111:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:111:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:111:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:90:23: missed: couldn't vectorize loop
Dense.c:92:31: missed: not vectorized: no vectype for stmt: _119 = this_40(D)->outputs;
 scalar_type: float *
Dense.c:90:23: missed: couldn't vectorize loop
Dense.c:90:23: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:95:16: missed: couldn't vectorize loop
Dense.c:95:16: missed: not vectorized: unsupported control flow in loop.
Dense.c:83:13: note: vectorized 0 loops in function.
Dense.c:86:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:97:16: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:97:16: note: SLPing BB part
Dense.c:97:16: note: Costing subgraph: 
Dense.c:97:16: note: node 0x76a2c18 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:97:16: note: op template: _16 = (sizetype) j_73;
Dense.c:97:16: note: 	stmt 0 _16 = (sizetype) j_73;
Dense.c:97:16: note: 	stmt 1 _111 = (sizetype) wi_74;
Dense.c:97:16: note: 	children 0x76a2d18
Dense.c:97:16: note: node (external) 0x76a2d18 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:97:16: note: 	{ j_73, wi_74 }
Dense.c:97:16: note: Cost model analysis: 
Dense.c:97:16: note: Scalar 1 and vector 2 loop part do not match up, skipping scalar part
Dense.c:97:16: note: Cost model analysis for part in loop 2:
  Vector cost: 36
  Scalar cost: 8
Dense.c:97:16: missed: not vectorized: vectorization is not profitable.
Dense.c:97:16: note: ***** The result for vector mode V32QI would be the same
Dense.c:97:16: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:97:16: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:97:16: note: SLPing BB part
Dense.c:97:16: note: Costing subgraph: 
Dense.c:97:16: note: node 0x76a2c18 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:97:16: note: op template: _16 = (sizetype) j_73;
Dense.c:97:16: note: 	stmt 0 _16 = (sizetype) j_73;
Dense.c:97:16: note: 	stmt 1 _111 = (sizetype) wi_74;
Dense.c:97:16: note: 	children 0x76a2c98
Dense.c:97:16: note: node (external) 0x76a2c98 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:97:16: note: 	{ j_73, wi_74 }
Dense.c:97:16: note: Cost model analysis: 
Dense.c:97:16: note: Scalar 1 and vector 2 loop part do not match up, skipping scalar part
Dense.c:97:16: note: Cost model analysis for part in loop 2:
  Vector cost: 36
  Scalar cost: 8
Dense.c:97:16: missed: not vectorized: vectorization is not profitable.
Dense.c:97:16: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:90:50: note: ***** Analysis failed with vector mode V8QI
Dense.c:90:50: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:90:50: note: ***** Analysis failed with vector mode V4QI
Dense.c:62:15: missed: couldn't vectorize loop
Dense.c:62:15: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:75:11: optimized: loop vectorized using 32 byte vectors
Dense.c:75:11: optimized: loop vectorized using 16 byte vectors
Dense.c:66:11: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:881:10: missed: not vectorized: no vectype for stmt: _53 = MEM[(__m256 * {ref-all})w_ptr_82];
 scalar_type: __m256
Dense.c:54:13: note: vectorized 1 loops in function.
Dense.c:57:2: missed: statement clobbers memory: memcpy (_4, dense$baiases_63, _3);
Dense.c:79:2: missed: statement clobbers memory: _27 (pretmp_163, _68, 0);
Dense.c:54:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:54:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:154:107: note: ***** Analysis failed with vector mode V8SI
Dense.c:154:107: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
Dense.c:155:85: note: ***** Analysis failed with vector mode V8SI
Dense.c:155:85: note: ***** The result for vector mode V32QI would be the same
Dense.c:155:85: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:155:85: note: ***** Analysis failed with vector mode V16QI
Dense.c:155:85: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:155:85: note: ***** Analysis failed with vector mode V8QI
Dense.c:155:85: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:155:85: note: ***** Analysis failed with vector mode V4QI
Dense.c:156:105: note: ***** Analysis failed with vector mode V8SI
Dense.c:156:105: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
Dense.c:178:19: missed: couldn't vectorize loop
Dense.c:178:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:168:13: note: vectorized 0 loops in function.
Dense.c:179:38: missed: statement clobbers memory: _5 = rand ();
Dense.c:179:72: missed: statement clobbers memory: _65 = sqrt (_12);
Dense.c:192:19: missed: statement clobbers memory: _24 = PULSE_GetActivationFunctionPtr (_23);
Dense.c:211:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:212:4: missed: statement clobbers memory: exit (1);
Dense.c:216:9: note: ***** Analysis succeeded with vector mode V8SI
Dense.c:216:9: note: SLPing BB part
Dense.c:216:9: note: Costing subgraph: 
Dense.c:216:9: note: node 0x75a36a8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:216:9: note: op template: <retval>.n_inputs = args$n_inputs_41;
Dense.c:216:9: note: 	stmt 0 <retval>.n_inputs = args$n_inputs_41;
Dense.c:216:9: note: 	stmt 1 <retval>.n_outputs = args$n_outputs_42;
Dense.c:216:9: note: 	children 0x75a3728
Dense.c:216:9: note: node 0x75a3728 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:216:9: note: op template: args$n_inputs_41 = args.n_inputs;
Dense.c:216:9: note: 	stmt 0 args$n_inputs_41 = args.n_inputs;
Dense.c:216:9: note: 	stmt 1 args$n_outputs_42 = args.n_outputs;
Dense.c:216:9: note: Cost model analysis: 
Dense.c:216:9: note: Cost model analysis for part in loop 0:
  Vector cost: 40
  Scalar cost: 56
Dense.c:216:9: note: Costing subgraph: 
Dense.c:216:9: note: node 0x75a3828 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:216:9: note: op template: <retval>.inputs = IO_31(D);
Dense.c:216:9: note: 	stmt 0 <retval>.inputs = IO_31(D);
Dense.c:216:9: note: 	stmt 1 <retval>.outputs = _22;
Dense.c:216:9: note: 	children 0x75a38a8
Dense.c:216:9: note: node (external) 0x75a38a8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:216:9: note: 	{ IO_31(D), _22 }
Dense.c:216:9: note: Cost model analysis: 
Dense.c:216:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:216:9: note: Costing subgraph: 
Dense.c:216:9: note: node 0x75a39a8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:216:9: note: op template: <retval>.feed = layer$feed_30;
Dense.c:216:9: note: 	stmt 0 <retval>.feed = layer$feed_30;
Dense.c:216:9: note: 	stmt 1 <retval>.back = layer$back_39;
Dense.c:216:9: note: 	stmt 2 <retval>.fix = layer$fix_40;
Dense.c:216:9: note: 	stmt 3 <retval>.activate = _25;
Dense.c:216:9: note: 	children 0x75a3aa8
Dense.c:216:9: note: node (external) 0x75a3aa8 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:216:9: note: 	{ layer$feed_30, layer$back_39, layer$fix_40, _25 }
Dense.c:216:9: note: Cost model analysis: 
Dense.c:216:9: note: Cost model analysis for part in loop 0:
  Vector cost: 64
  Scalar cost: 64
Dense.c:216:9: note: Costing subgraph: 
Dense.c:216:9: note: node 0x75a3ba8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:216:9: note: op template: <retval>.parent = 0B;
Dense.c:216:9: note: 	stmt 0 <retval>.parent = 0B;
Dense.c:216:9: note: 	stmt 1 <retval>.child = 0B;
Dense.c:216:9: note: 	children 0x75a3c28
Dense.c:216:9: note: node (constant) 0x75a3c28 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:216:9: note: 	{ 0B, 0B }
Dense.c:216:9: note: Cost model analysis: 
Dense.c:216:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:216:9: note: Costing subgraph: 
Dense.c:216:9: note: node 0x75a3ca8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:216:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_29(D);
Dense.c:216:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_29(D);
Dense.c:216:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&<retval> + 96B] = _4;
Dense.c:216:9: note: 	children 0x75a3d28
Dense.c:216:9: note: node (external) 0x75a3d28 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:216:9: note: 	{ MODEL_29(D), _4 }
Dense.c:216:9: note: Cost model analysis: 
Dense.c:216:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:216:9: note: Basic block will be vectorized using SLP
Dense.c:216:9: optimized: basic block part vectorized using 8 byte vectors
Dense.c:216:9: note: Vectorizing SLP tree:
Dense.c:216:9: note: node 0x75a36a8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:216:9: note: op template: <retval>.n_inputs = args$n_inputs_41;
Dense.c:216:9: note: 	stmt 0 <retval>.n_inputs = args$n_inputs_41;
Dense.c:216:9: note: 	stmt 1 <retval>.n_outputs = args$n_outputs_42;
Dense.c:216:9: note: 	children 0x75a3728
Dense.c:216:9: note: node 0x75a3728 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:216:9: note: op template: args$n_inputs_41 = args.n_inputs;
Dense.c:216:9: note: 	stmt 0 args$n_inputs_41 = args.n_inputs;
Dense.c:216:9: note: 	stmt 1 args$n_outputs_42 = args.n_outputs;
Dense.c:216:9: note: ------>vectorizing SLP node starting from: args$n_inputs_41 = args.n_inputs;
Dense.c:216:9: note: transform load. ncopies = 1
Dense.c:216:9: note: conflicting alias set types.
Dense.c:216:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: args.n_inputs
Dense.c:216:9: note: created &args
Dense.c:216:9: note: add new stmt: vect_args_n_inputs_41.490_26 = MEM <vector(2) unsigned int> [(void *)&args];
Dense.c:216:9: note: extracting lane for live stmt args$n_inputs_41 = args.n_inputs;
Dense.c:216:9: note: extracting lane for live stmt args$n_outputs_42 = args.n_outputs;
Dense.c:216:9: note: ------>vectorizing SLP node starting from: <retval>.n_inputs = args$n_inputs_41;
Dense.c:216:9: note: vect_is_simple_use: operand args.n_inputs, type of def: internal
Dense.c:216:9: note: vect_is_simple_use: operand args.n_outputs, type of def: internal
Dense.c:216:9: note: transform store. ncopies = 1
Dense.c:216:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: <retval>.n_inputs
Dense.c:216:9: note: created &<retval>.n_inputs
Dense.c:216:9: note: add new stmt: MEM <vector(2) unsigned int> [(unsigned int *)&<retval> + 128B] = vect_args_n_inputs_41.490_26;
Dense.c:216:9: note: vectorizing stmts using SLP.
Dense.c:216:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:216:9: note: Vectorizing SLP tree:
Dense.c:216:9: note: node 0x75a3828 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:216:9: note: op template: <retval>.inputs = IO_31(D);
Dense.c:216:9: note: 	stmt 0 <retval>.inputs = IO_31(D);
Dense.c:216:9: note: 	stmt 1 <retval>.outputs = _22;
Dense.c:216:9: note: 	children 0x75a38a8
Dense.c:216:9: note: node (external) 0x75a38a8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:216:9: note: 	{ IO_31(D), _22 }
Dense.c:216:9: note: ------>vectorizing SLP node starting from: <retval>.inputs = IO_31(D);
Dense.c:216:9: note: vect_is_simple_use: operand IO_31(D) + _21, type of def: internal
Dense.c:216:9: note: transform store. ncopies = 1
Dense.c:216:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.inputs
Dense.c:216:9: note: created &<retval>
Dense.c:216:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval>] = _78;
Dense.c:216:9: note: vectorizing stmts using SLP.
Dense.c:216:9: optimized: basic block part vectorized using 32 byte vectors
Dense.c:216:9: note: Vectorizing SLP tree:
Dense.c:216:9: note: node 0x75a39a8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:216:9: note: op template: <retval>.feed = layer$feed_30;
Dense.c:216:9: note: 	stmt 0 <retval>.feed = layer$feed_30;
Dense.c:216:9: note: 	stmt 1 <retval>.back = layer$back_39;
Dense.c:216:9: note: 	stmt 2 <retval>.fix = layer$fix_40;
Dense.c:216:9: note: 	stmt 3 <retval>.activate = _25;
Dense.c:216:9: note: 	children 0x75a3aa8
Dense.c:216:9: note: node (external) 0x75a3aa8 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:216:9: note: 	{ layer$feed_30, layer$back_39, layer$fix_40, _25 }
Dense.c:216:9: note: ------>vectorizing SLP node starting from: <retval>.feed = layer$feed_30;
Dense.c:216:9: note: vect_is_simple_use: operand layer$back_39 = PHI <layer$back_45(D)(11), _BackDense(9), _SIMD_BackDense(15)>, type of def: internal
Dense.c:216:9: note: vect_is_simple_use: operand layer$fix_40 = PHI <layer$fix_46(D)(11), _FixDense(9), _SIMD_FixDense(15)>, type of def: internal
Dense.c:216:9: note: vect_is_simple_use: operand (void (*<T91a>) (float *, unsigned int, char)) _24, type of def: internal
Dense.c:216:9: note: conflicting alias set types.
Dense.c:216:9: note: transform store. ncopies = 1
Dense.c:216:9: note: create vector_type-pointer variable to type: vector(4) long unsigned int  vectorizing a pointer ref: <retval>.feed
Dense.c:216:9: note: created &<retval>.feed
Dense.c:216:9: note: add new stmt: MEM <vector(4) long unsigned int> [(void *)&<retval> + 32B] = _84;
Dense.c:216:9: note: vectorizing stmts using SLP.
Dense.c:216:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:216:9: note: Vectorizing SLP tree:
Dense.c:216:9: note: node 0x75a3ba8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:216:9: note: op template: <retval>.parent = 0B;
Dense.c:216:9: note: 	stmt 0 <retval>.parent = 0B;
Dense.c:216:9: note: 	stmt 1 <retval>.child = 0B;
Dense.c:216:9: note: 	children 0x75a3c28
Dense.c:216:9: note: node (constant) 0x75a3c28 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:216:9: note: 	{ 0B, 0B }
Dense.c:216:9: note: ------>vectorizing SLP node starting from: <retval>.parent = 0B;
Dense.c:216:9: note: vect_is_simple_use: operand 0B, type of def: constant
Dense.c:216:9: note: transform store. ncopies = 1
Dense.c:216:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.parent
Dense.c:216:9: note: created &<retval>.parent
Dense.c:216:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer * *)&<retval> + 72B] = { 0, 0 };
Dense.c:216:9: note: vectorizing stmts using SLP.
Dense.c:216:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:216:9: note: Vectorizing SLP tree:
Dense.c:216:9: note: node 0x75a3ca8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:216:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_29(D);
Dense.c:216:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_29(D);
Dense.c:216:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&<retval> + 96B] = _4;
Dense.c:216:9: note: 	children 0x75a3d28
Dense.c:216:9: note: node (external) 0x75a3d28 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:216:9: note: 	{ MODEL_29(D), _4 }
Dense.c:216:9: note: ------>vectorizing SLP node starting from: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_29(D);
Dense.c:216:9: note: vect_is_simple_use: operand MODEL_29(D) + _3, type of def: internal
Dense.c:216:9: note: transform store. ncopies = 1
Dense.c:216:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B]
Dense.c:216:9: note: created &MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B]
Dense.c:216:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer *)&<retval> + 88B] = _89;
Dense.c:216:9: note: vectorizing stmts using SLP.
Dense.c:216:9: note: ***** The result for vector mode V32QI would be the same
Activations.c:17:19: optimized: loop vectorized using 32 byte vectors
Activations.c:17:19: optimized: loop vectorized using 16 byte vectors
Activations.c:17:19: optimized: loop vectorized using 32 byte vectors
Activations.c:17:19: optimized: loop vectorized using 16 byte vectors
Activations.c:14:13: note: vectorized 2 loops in function.
Activations.c:14:13: note: ***** Analysis failed with vector mode V8SF
Activations.c:14:13: note: ***** The result for vector mode V32QI would be the same
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V16QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V16QI
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V8QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V8QI
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V4QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V4QI
Activations.c:28:19: missed: couldn't vectorize loop
Activations.c:28:19: missed: not vectorized: unsupported control flow in loop.
Activations.c:28:19: optimized: loop vectorized using 32 byte vectors
Activations.c:28:19: optimized: loop vectorized using 16 byte vectors
Activations.c:25:13: note: vectorized 1 loops in function.
Activations.c:25:13: note: ***** Analysis failed with vector mode V8SF
Activations.c:25:13: note: ***** The result for vector mode V32QI would be the same
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V16QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V16QI
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V8QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V8QI
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V4QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V4QI
Activations.c:38:1: note: ***** Analysis failed with vector mode VOID
Activations.c:5:19: missed: couldn't vectorize loop
Activations.c:9:21: missed: statement clobbers memory: _36 = expf (_18);
Activations.c:5:19: missed: couldn't vectorize loop
Activations.c:9:21: missed: statement clobbers memory: _12 = expf (_11);
Activations.c:3:13: note: vectorized 0 loops in function.
Activations.c:9:21: missed: statement clobbers memory: _12 = expf (_11);
Activations.c:9:21: missed: statement clobbers memory: _36 = expf (_18);
Activations.c:11:1: note: ***** Analysis failed with vector mode V8SF
Activations.c:11:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SF
Activations.c:44:2: note: ***** Analysis failed with vector mode VOID
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:4:18: note: vectorized 0 loops in function.
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_4, inputs_15, _3);
PULSE.c:8:2: missed: statement clobbers memory: _5 (layer_17);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_12, pretmp_26, _9);
PULSE.c:13:28: note: ***** Analysis failed with vector mode V8SI
PULSE.c:13:28: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:22:2: missed: statement clobbers memory: _1 (layer_10(D));
PULSE.c:22:2: missed: statement clobbers memory: _14 (_2);
PULSE.c:22:2: missed: statement clobbers memory: _21 (_15);
PULSE.c:22:2: missed: statement clobbers memory: _28 (_22);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_29);
PULSE.c:26:3: missed: statement clobbers memory: memset (_34, 0, _32);
PULSE.c:26:3: missed: statement clobbers memory: memset (_27, 0, _25);
PULSE.c:26:3: missed: statement clobbers memory: memset (_20, 0, _18);
PULSE.c:26:3: missed: statement clobbers memory: memset (_7, 0, _5);
PULSE.c:28:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:28:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:39: missed: statement clobbers memory: _7 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:32:6: note: vectorized 1 loops in function.
PULSE.c:34:8: missed: statement clobbers memory: _1 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_2);
PULSE.c:39:39: missed: statement clobbers memory: _7 = rand ();
PULSE.c:40:40: missed: statement clobbers memory: _9 = rand ();
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8SI
PULSE.c:32:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:66:16: missed: couldn't vectorize loop
PULSE.c:66:16: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:69:17: missed: couldn't vectorize loop
PULSE.c:69:17: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:78:19: missed: couldn't vectorize loop
PULSE.c:78:19: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:39: missed: statement clobbers memory: _83 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:55:22: missed: couldn't vectorize loop
PULSE.c:55:22: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:47:6: note: vectorized 1 loops in function.
PULSE.c:48:1: missed: statement clobbers memory: saved_stack.7_47 = __builtin_stack_save ();
PULSE.c:49:46: missed: statement clobbers memory: _1 = PULSE_GetLossFunctionPtr (loss_function_48(D));
PULSE.c:50:42: missed: statement clobbers memory: TMP_52 = aligned_alloc (64, _4);
PULSE.c:57:14: missed: statement clobbers memory: _72 = _5 (output_207, TMP_PTR_206);
PULSE.c:65:15: missed: statement clobbers memory: random.4_56 = __builtin_alloca_with_align (_10, 32);
PULSE.c:34:8: missed: statement clobbers memory: _75 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_76);
PULSE.c:39:39: missed: statement clobbers memory: _83 = rand ();
PULSE.c:40:40: missed: statement clobbers memory: _86 = rand ();
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_112, inputs_107, _111);
PULSE.c:8:2: missed: statement clobbers memory: _113 (layer_108);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_119, _118, _117);
PULSE.c:72:11: missed: statement clobbers memory: loss_65 = PULSE_GetLoss_50 (_23, _22, _18, _17);
PULSE.c:22:2: missed: statement clobbers memory: _100 (output_208);
PULSE.c:22:2: missed: statement clobbers memory: _122 (_101);
PULSE.c:22:2: missed: statement clobbers memory: _129 (_123);
PULSE.c:22:2: missed: statement clobbers memory: _136 (_130);
PULSE.c:22:2: missed: statement clobbers memory: _143 (_137);
PULSE.c:22:2: missed: statement clobbers memory: _150 (_144);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_151);
PULSE.c:26:3: missed: statement clobbers memory: memset (_156, 0, _154);
PULSE.c:26:3: missed: statement clobbers memory: memset (_149, 0, _147);
PULSE.c:26:3: missed: statement clobbers memory: memset (_142, 0, _140);
PULSE.c:26:3: missed: statement clobbers memory: memset (_135, 0, _133);
PULSE.c:26:3: missed: statement clobbers memory: memset (_128, 0, _126);
PULSE.c:26:3: missed: statement clobbers memory: memset (_106, 0, _104);
PULSE.c:80:6: missed: statement clobbers memory: _27 (current_212, args);
PULSE.c:86:4: missed: statement clobbers memory: printf ("Epoch: %d | Item: %d | Loss: %.10f | Batch Loss: %.10f\r", i_209, j_210, _31, _30);
PULSE.c:47:6: missed: statement clobbers memory: __builtin_stack_restore (saved_stack.7_47);
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4DI
PULSE.c:47:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:96:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:96:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:130:20: missed: couldn't vectorize loop
PULSE.c:130:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:130:20: missed: couldn't vectorize loop
PULSE.c:130:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:109:20: missed: couldn't vectorize loop
PULSE.c:109:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:109:20: missed: couldn't vectorize loop
PULSE.c:109:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:98:13: note: vectorized 0 loops in function.
PULSE.c:101:39: missed: statement clobbers memory: layers_41 = malloc (_3);
PULSE.c:104:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:123:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:124:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:125:46: missed: statement clobbers memory: WEIGHTS_86 = aligned_alloc (64, 0);
PULSE.c:115:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.106_94];
PULSE.c:116:21: missed: statement clobbers memory: _75 = PULSE_GetDenseWeightsSize (args);
PULSE.c:117:16: missed: statement clobbers memory: _78 = PULSE_GetDenseIOSize (args);
PULSE.c:118:19: missed: statement clobbers memory: _81 = PULSE_GetDenseFixesSize (args);
PULSE.c:115:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})_224];
PULSE.c:116:21: missed: statement clobbers memory: _210 = PULSE_GetDenseWeightsSize (args);
PULSE.c:117:16: missed: statement clobbers memory: _213 = PULSE_GetDenseIOSize (args);
PULSE.c:118:19: missed: statement clobbers memory: _216 = PULSE_GetDenseFixesSize (args);
PULSE.c:123:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:124:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:125:46: missed: statement clobbers memory: WEIGHTS_46 = aligned_alloc (64, _5);
PULSE.c:126:41: missed: statement clobbers memory: IO_48 = aligned_alloc (64, _7);
PULSE.c:136:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.108_96];
PULSE.c:137:17: missed: statement clobbers memory: *_10 = PULSE_CreateDenseLayer (args, WEIGHTS_PTR_152, IO_PTR_153, 0B); [return slot optimization]
PULSE.c:138:20: missed: statement clobbers memory: _64 = PULSE_GetDenseWeightsSize (args);
PULSE.c:139:15: missed: statement clobbers memory: _67 = PULSE_GetDenseIOSize (args);
PULSE.c:136:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.108_124];
PULSE.c:137:17: missed: statement clobbers memory: *_106 = PULSE_CreateDenseLayer (args, WEIGHTS_PTR_148, IO_PTR_143, 0B); [return slot optimization]
PULSE.c:138:20: missed: statement clobbers memory: _95 = PULSE_GetDenseWeightsSize (args);
PULSE.c:139:15: missed: statement clobbers memory: _59 = PULSE_GetDenseIOSize (args);
PULSE.c:145:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:146:9: note: ***** Analysis succeeded with vector mode V8SI
PULSE.c:146:9: note: SLPing BB part
PULSE.c:146:9: note: Costing subgraph: 
PULSE.c:146:9: note: node 0x34a74c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:146:9: note: op template: <retval>.weights_size = WEIGHTS_SIZE_164;
PULSE.c:146:9: note: 	stmt 0 <retval>.weights_size = WEIGHTS_SIZE_164;
PULSE.c:146:9: note: 	stmt 1 <retval>.io_size = IO_SIZE_167;
PULSE.c:146:9: note: 	children 0x34a7548
PULSE.c:146:9: note: node 0x34a7548 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:146:9: note: op template: WEIGHTS_SIZE_164 = PHI <WEIGHTS_SIZE_161(36), 0(25)>
PULSE.c:146:9: note: 	stmt 0 WEIGHTS_SIZE_164 = PHI <WEIGHTS_SIZE_161(36), 0(25)>
PULSE.c:146:9: note: 	stmt 1 IO_SIZE_167 = PHI <IO_SIZE_160(36), 0(25)>
PULSE.c:146:9: note: 	children 0x34a75c8 0x34a7dc8
PULSE.c:146:9: note: node 0x34a75c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:146:9: note: op template: WEIGHTS_SIZE_161 = PHI <WEIGHTS_SIZE_20(11), WEIGHTS_SIZE_206(76)>
PULSE.c:146:9: note: 	stmt 0 WEIGHTS_SIZE_161 = PHI <WEIGHTS_SIZE_20(11), WEIGHTS_SIZE_206(76)>
PULSE.c:146:9: note: 	stmt 1 IO_SIZE_160 = PHI <IO_SIZE_22(11), IO_SIZE_192(76)>
PULSE.c:146:9: note: 	children 0x34a7648 0x34a79c8
PULSE.c:146:9: note: node 0x34a7648 (max_nunits=2, refcnt=3) vector(2) unsigned int
PULSE.c:146:9: note: op template: WEIGHTS_SIZE_20 = PHI <WEIGHTS_SIZE_73(77), WEIGHTS_SIZE_76(10)>
PULSE.c:146:9: note: 	stmt 0 WEIGHTS_SIZE_20 = PHI <WEIGHTS_SIZE_73(77), WEIGHTS_SIZE_76(10)>
PULSE.c:146:9: note: 	stmt 1 IO_SIZE_22 = PHI <IO_SIZE_147(77), IO_SIZE_79(10)>
PULSE.c:146:9: note: 	children 0x34a76c8 0x34a77c8
PULSE.c:146:9: note: node 0x34a76c8 (max_nunits=2, refcnt=2) vector(2) unsigned int
PULSE.c:146:9: note: op template: WEIGHTS_SIZE_73 = PHI <WEIGHTS_SIZE_20(73), 0(71)>
PULSE.c:146:9: note: 	stmt 0 WEIGHTS_SIZE_73 = PHI <WEIGHTS_SIZE_20(73), 0(71)>
PULSE.c:146:9: note: 	stmt 1 IO_SIZE_147 = PHI <IO_SIZE_22(73), 0(71)>
PULSE.c:146:9: note: 	children 0x34a7648 0x34a7748
PULSE.c:146:9: note: node (constant) 0x34a7748 (max_nunits=1, refcnt=1) vector(2) unsigned int
PULSE.c:146:9: note: 	{ 0, 0 }
PULSE.c:146:9: note: node 0x34a77c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:146:9: note: op template: WEIGHTS_SIZE_76 = WEIGHTS_SIZE_73 + _75;
PULSE.c:146:9: note: 	stmt 0 WEIGHTS_SIZE_76 = WEIGHTS_SIZE_73 + _75;
PULSE.c:146:9: note: 	stmt 1 IO_SIZE_79 = _78 + IO_SIZE_147;
PULSE.c:146:9: note: 	children 0x34a76c8 0x34a7948
PULSE.c:146:9: note: node (external) 0x34a7948 (max_nunits=1, refcnt=1) vector(2) unsigned int
PULSE.c:146:9: note: 	{ _75, _78 }
PULSE.c:146:9: note: node 0x34a79c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:146:9: note: op template: WEIGHTS_SIZE_206 = PHI <WEIGHTS_SIZE_238(68)>
PULSE.c:146:9: note: 	stmt 0 WEIGHTS_SIZE_206 = PHI <WEIGHTS_SIZE_238(68)>
PULSE.c:146:9: note: 	stmt 1 IO_SIZE_192 = PHI <IO_SIZE_239(68)>
PULSE.c:146:9: note: 	children 0x34a7a48
PULSE.c:146:9: note: node 0x34a7a48 (max_nunits=2, refcnt=2) vector(2) unsigned int
PULSE.c:146:9: note: op template: WEIGHTS_SIZE_238 = PHI <WEIGHTS_SIZE_211(59), WEIGHTS_SIZE_200(57)>
PULSE.c:146:9: note: 	stmt 0 WEIGHTS_SIZE_238 = PHI <WEIGHTS_SIZE_211(59), WEIGHTS_SIZE_200(57)>
PULSE.c:146:9: note: 	stmt 1 IO_SIZE_239 = PHI <IO_SIZE_214(59), IO_SIZE_201(57)>
PULSE.c:146:9: note: 	children 0x34a7ac8 0x34a7bc8
PULSE.c:146:9: note: node 0x34a7ac8 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:146:9: note: op template: WEIGHTS_SIZE_211 = WEIGHTS_SIZE_200 + _210;
PULSE.c:146:9: note: 	stmt 0 WEIGHTS_SIZE_211 = WEIGHTS_SIZE_200 + _210;
PULSE.c:146:9: note: 	stmt 1 IO_SIZE_214 = _213 + IO_SIZE_201;
PULSE.c:146:9: note: 	children 0x34a7bc8 0x34a7d48
PULSE.c:146:9: note: node 0x34a7bc8 (max_nunits=2, refcnt=2) vector(2) unsigned int
PULSE.c:146:9: note: op template: WEIGHTS_SIZE_200 = PHI <WEIGHTS_SIZE_243(72), WEIGHTS_SIZE_238(69)>
PULSE.c:146:9: note: 	stmt 0 WEIGHTS_SIZE_200 = PHI <WEIGHTS_SIZE_243(72), WEIGHTS_SIZE_238(69)>
PULSE.c:146:9: note: 	stmt 1 IO_SIZE_201 = PHI <IO_SIZE_244(72), IO_SIZE_239(69)>
PULSE.c:146:9: note: 	children 0x34a7c48 0x34a7a48
PULSE.c:146:9: note: node 0x34a7c48 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:146:9: note: op template: WEIGHTS_SIZE_243 = PHI <WEIGHTS_SIZE_20(74)>
PULSE.c:146:9: note: 	stmt 0 WEIGHTS_SIZE_243 = PHI <WEIGHTS_SIZE_20(74)>
PULSE.c:146:9: note: 	stmt 1 IO_SIZE_244 = PHI <IO_SIZE_22(74)>
PULSE.c:146:9: note: 	children 0x34a7648
PULSE.c:146:9: note: node (external) 0x34a7d48 (max_nunits=1, refcnt=1) vector(2) unsigned int
PULSE.c:146:9: note: 	{ _210, _213 }
PULSE.c:146:9: note: node (constant) 0x34a7dc8 (max_nunits=1, refcnt=1) vector(2) unsigned int
PULSE.c:146:9: note: 	{ 0, 0 }
PULSE.c:146:9: note: Cost model analysis: 
PULSE.c:146:9: note: Cost model analysis for part in loop 0:
  Vector cost: 48
  Scalar cost: 48
PULSE.c:146:9: note: Cost model analysis for part in loop 1:
  Vector cost: 44
  Scalar cost: 24
PULSE.c:146:9: missed: not vectorized: vectorization is not profitable.
PULSE.c:146:9: note: Costing subgraph: 
PULSE.c:146:9: note: node 0x34a7ec8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:146:9: note: op template: <retval>.layers = layers_41;
PULSE.c:146:9: note: 	stmt 0 <retval>.layers = layers_41;
PULSE.c:146:9: note: 	stmt 1 <retval>.weights = WEIGHTS_163;
PULSE.c:146:9: note: 	children 0x34a7fc8
PULSE.c:146:9: note: node (external) 0x34a7fc8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:146:9: note: 	{ layers_41, WEIGHTS_163 }
PULSE.c:146:9: note: Cost model analysis: 
PULSE.c:146:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:146:9: note: Costing subgraph: 
PULSE.c:146:9: note: node 0x34a8048 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:146:9: note: op template: <retval>.io = 0B;
PULSE.c:146:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:146:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:146:9: note: 	children 0x34a80c8
PULSE.c:146:9: note: node (constant) 0x34a80c8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:146:9: note: 	{ 0B, 0B }
PULSE.c:146:9: note: Cost model analysis: 
PULSE.c:146:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:146:9: note: Basic block will be vectorized using SLP
PULSE.c:146:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:146:9: note: Vectorizing SLP tree:
PULSE.c:146:9: note: node 0x34a7ec8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:146:9: note: op template: <retval>.layers = layers_41;
PULSE.c:146:9: note: 	stmt 0 <retval>.layers = layers_41;
PULSE.c:146:9: note: 	stmt 1 <retval>.weights = WEIGHTS_163;
PULSE.c:146:9: note: 	children 0x34a7fc8
PULSE.c:146:9: note: node (external) 0x34a7fc8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:146:9: note: 	{ layers_41, WEIGHTS_163 }
PULSE.c:146:9: note: ------>vectorizing SLP node starting from: <retval>.layers = layers_41;
PULSE.c:146:9: note: vect_is_simple_use: operand WEIGHTS_163 = PHI <WEIGHTS_46(36), WEIGHTS_86(25)>, type of def: internal
PULSE.c:146:9: note: conflicting alias set types.
PULSE.c:146:9: note: transform store. ncopies = 1
PULSE.c:146:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.layers
PULSE.c:146:9: note: created &<retval>
PULSE.c:146:9: note: add new stmt: MEM <vector(2) long unsigned int> [(void *)&<retval>] = _19;
PULSE.c:146:9: note: vectorizing stmts using SLP.
PULSE.c:146:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:146:9: note: Vectorizing SLP tree:
PULSE.c:146:9: note: node 0x34a8048 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:146:9: note: op template: <retval>.io = 0B;
PULSE.c:146:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:146:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:146:9: note: 	children 0x34a80c8
PULSE.c:146:9: note: node (constant) 0x34a80c8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:146:9: note: 	{ 0B, 0B }
PULSE.c:146:9: note: ------>vectorizing SLP node starting from: <retval>.io = 0B;
PULSE.c:146:9: note: vect_is_simple_use: operand 0B, type of def: constant
PULSE.c:146:9: note: transform store. ncopies = 1
PULSE.c:146:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.io
PULSE.c:146:9: note: created &<retval>.io
PULSE.c:146:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval> + 16B] = { 0, 0 };
PULSE.c:146:9: note: vectorizing stmts using SLP.
PULSE.c:146:9: note: ***** The result for vector mode V32QI would be the same
PULSE.c:152:2: missed: statement clobbers memory: free (_1);
PULSE.c:153:2: missed: statement clobbers memory: free (_2);
PULSE.c:154:2: missed: statement clobbers memory: free (_3);
PULSE.c:155:2: missed: statement clobbers memory: free (_4);
PULSE.c:156:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:156:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Loss.c:15:19: optimized: loop vectorized using 32 byte vectors
Loss.c:15:19: optimized:  loop versioned for vectorization because of possible aliasing
Loss.c:15:19: optimized: loop vectorized using 16 byte vectors
Loss.c:12:23: note: vectorized 1 loops in function.
Loss.c:12:23: note: ***** Analysis failed with vector mode V8SF
Loss.c:12:23: note: ***** The result for vector mode V32QI would be the same
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V16QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V16QI
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V8QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V8QI
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V4QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V4QI
Loss.c:7:19: missed: couldn't vectorize loop
Loss.c:8:38: missed: not vectorized: unsupported use in stmt.
Loss.c:4:23: note: vectorized 0 loops in function.
Loss.c:9:13: note: ***** Analysis failed with vector mode V8SF
Loss.c:9:13: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SF
Loss.c:23:2: note: ***** Analysis failed with vector mode VOID
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _28 (pretmp_91, _51, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:19: missed: not vectorized: no vectype for stmt: _22 = *_21;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:163:10: note: ***** Analysis failed with vector mode V4DI
Dense.c:163:10: note: ***** The result for vector mode V32QI would be the same
Dense.c:163:10: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:163:10: note: ***** Analysis failed with vector mode V16QI
Dense.c:163:10: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:163:10: note: ***** Analysis failed with vector mode V8QI
Dense.c:163:10: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:163:10: note: ***** Analysis failed with vector mode V4QI
Dense.c:145:10: optimized: loop vectorized using 32 byte vectors
Dense.c:145:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:145:10: optimized: loop vectorized using 16 byte vectors
Dense.c:136:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
Dense.c:129:10: optimized: loop vectorized using 32 byte vectors
Dense.c:129:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:129:10: optimized: loop vectorized using 16 byte vectors
Dense.c:120:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
Dense.c:111:13: note: vectorized 2 loops in function.
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_28, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
Dense.c:111:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:111:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:111:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:111:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:111:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:111:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:111:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:111:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:90:23: missed: couldn't vectorize loop
Dense.c:92:31: missed: not vectorized: no vectype for stmt: _119 = this_40(D)->outputs;
 scalar_type: float *
Dense.c:90:23: missed: couldn't vectorize loop
Dense.c:90:23: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:95:16: missed: couldn't vectorize loop
Dense.c:95:16: missed: not vectorized: unsupported control flow in loop.
Dense.c:83:13: note: vectorized 0 loops in function.
Dense.c:86:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:97:16: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:97:16: note: SLPing BB part
Dense.c:97:16: note: Costing subgraph: 
Dense.c:97:16: note: node 0x8774fc8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:97:16: note: op template: _16 = (sizetype) j_73;
Dense.c:97:16: note: 	stmt 0 _16 = (sizetype) j_73;
Dense.c:97:16: note: 	stmt 1 _111 = (sizetype) wi_74;
Dense.c:97:16: note: 	children 0x87750c8
Dense.c:97:16: note: node (external) 0x87750c8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:97:16: note: 	{ j_73, wi_74 }
Dense.c:97:16: note: Cost model analysis: 
Dense.c:97:16: note: Scalar 1 and vector 2 loop part do not match up, skipping scalar part
Dense.c:97:16: note: Cost model analysis for part in loop 2:
  Vector cost: 36
  Scalar cost: 8
Dense.c:97:16: missed: not vectorized: vectorization is not profitable.
Dense.c:97:16: note: ***** The result for vector mode V32QI would be the same
Dense.c:97:16: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:97:16: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:97:16: note: SLPing BB part
Dense.c:97:16: note: Costing subgraph: 
Dense.c:97:16: note: node 0x8774fc8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:97:16: note: op template: _16 = (sizetype) j_73;
Dense.c:97:16: note: 	stmt 0 _16 = (sizetype) j_73;
Dense.c:97:16: note: 	stmt 1 _111 = (sizetype) wi_74;
Dense.c:97:16: note: 	children 0x8775048
Dense.c:97:16: note: node (external) 0x8775048 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:97:16: note: 	{ j_73, wi_74 }
Dense.c:97:16: note: Cost model analysis: 
Dense.c:97:16: note: Scalar 1 and vector 2 loop part do not match up, skipping scalar part
Dense.c:97:16: note: Cost model analysis for part in loop 2:
  Vector cost: 36
  Scalar cost: 8
Dense.c:97:16: missed: not vectorized: vectorization is not profitable.
Dense.c:97:16: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:90:50: note: ***** Analysis failed with vector mode V8QI
Dense.c:90:50: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:90:50: note: ***** Analysis failed with vector mode V4QI
Dense.c:62:15: missed: couldn't vectorize loop
Dense.c:62:15: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:75:11: optimized: loop vectorized using 32 byte vectors
Dense.c:75:11: optimized: loop vectorized using 16 byte vectors
Dense.c:66:11: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:881:10: missed: not vectorized: no vectype for stmt: _53 = MEM[(__m256 * {ref-all})w_ptr_82];
 scalar_type: __m256
Dense.c:54:13: note: vectorized 1 loops in function.
Dense.c:57:2: missed: statement clobbers memory: memcpy (_4, dense$baiases_63, _3);
Dense.c:79:2: missed: statement clobbers memory: _27 (pretmp_163, _68, 0);
Dense.c:54:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:54:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:154:107: note: ***** Analysis failed with vector mode V8SI
Dense.c:154:107: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
Dense.c:155:85: note: ***** Analysis failed with vector mode V8SI
Dense.c:155:85: note: ***** The result for vector mode V32QI would be the same
Dense.c:155:85: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:155:85: note: ***** Analysis failed with vector mode V16QI
Dense.c:155:85: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:155:85: note: ***** Analysis failed with vector mode V8QI
Dense.c:155:85: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:155:85: note: ***** Analysis failed with vector mode V4QI
Dense.c:156:105: note: ***** Analysis failed with vector mode V8SI
Dense.c:156:105: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
Dense.c:178:19: missed: couldn't vectorize loop
Dense.c:178:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:168:13: note: vectorized 0 loops in function.
Dense.c:179:38: missed: statement clobbers memory: _5 = rand ();
Dense.c:179:72: missed: statement clobbers memory: _69 = sqrt (_12);
Dense.c:192:19: missed: statement clobbers memory: _24 = PULSE_GetActivationFunctionPtr (_23);
Dense.c:195:34: missed: statement clobbers memory: _28 = aligned_alloc (64, _27);
Dense.c:211:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:212:4: missed: statement clobbers memory: exit (1);
Dense.c:216:9: note: ***** Analysis succeeded with vector mode V8SI
Dense.c:216:9: note: SLPing BB part
Dense.c:216:9: note: Costing subgraph: 
Dense.c:216:9: note: node 0x85bc898 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:216:9: note: op template: <retval>.n_inputs = args$n_inputs_45;
Dense.c:216:9: note: 	stmt 0 <retval>.n_inputs = args$n_inputs_45;
Dense.c:216:9: note: 	stmt 1 <retval>.n_outputs = args$n_outputs_46;
Dense.c:216:9: note: 	children 0x85bc918
Dense.c:216:9: note: node 0x85bc918 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:216:9: note: op template: args$n_inputs_45 = args.n_inputs;
Dense.c:216:9: note: 	stmt 0 args$n_inputs_45 = args.n_inputs;
Dense.c:216:9: note: 	stmt 1 args$n_outputs_46 = args.n_outputs;
Dense.c:216:9: note: Cost model analysis: 
Dense.c:216:9: note: Cost model analysis for part in loop 0:
  Vector cost: 40
  Scalar cost: 56
Dense.c:216:9: note: Costing subgraph: 
Dense.c:216:9: note: node 0x85bca18 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:216:9: note: op template: <retval>.inputs = IO_34(D);
Dense.c:216:9: note: 	stmt 0 <retval>.inputs = IO_34(D);
Dense.c:216:9: note: 	stmt 1 <retval>.outputs = _22;
Dense.c:216:9: note: 	children 0x85bca98
Dense.c:216:9: note: node (external) 0x85bca98 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:216:9: note: 	{ IO_34(D), _22 }
Dense.c:216:9: note: Cost model analysis: 
Dense.c:216:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:216:9: note: Costing subgraph: 
Dense.c:216:9: note: node 0x85bcb98 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:216:9: note: op template: <retval>.feed = layer$feed_33;
Dense.c:216:9: note: 	stmt 0 <retval>.feed = layer$feed_33;
Dense.c:216:9: note: 	stmt 1 <retval>.back = layer$back_43;
Dense.c:216:9: note: 	stmt 2 <retval>.fix = layer$fix_44;
Dense.c:216:9: note: 	stmt 3 <retval>.activate = _25;
Dense.c:216:9: note: 	children 0x85bcc98
Dense.c:216:9: note: node (external) 0x85bcc98 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:216:9: note: 	{ layer$feed_33, layer$back_43, layer$fix_44, _25 }
Dense.c:216:9: note: Cost model analysis: 
Dense.c:216:9: note: Cost model analysis for part in loop 0:
  Vector cost: 64
  Scalar cost: 64
Dense.c:216:9: note: Costing subgraph: 
Dense.c:216:9: note: node 0x85bcd98 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:216:9: note: op template: <retval>.parent = 0B;
Dense.c:216:9: note: 	stmt 0 <retval>.parent = 0B;
Dense.c:216:9: note: 	stmt 1 <retval>.child = 0B;
Dense.c:216:9: note: 	children 0x85bce18
Dense.c:216:9: note: node (constant) 0x85bce18 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:216:9: note: 	{ 0B, 0B }
Dense.c:216:9: note: Cost model analysis: 
Dense.c:216:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:216:9: note: Costing subgraph: 
Dense.c:216:9: note: node 0x85bce98 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:216:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_32(D);
Dense.c:216:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_32(D);
Dense.c:216:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&<retval> + 96B] = _4;
Dense.c:216:9: note: 	children 0x85bcf18
Dense.c:216:9: note: node (external) 0x85bcf18 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:216:9: note: 	{ MODEL_32(D), _4 }
Dense.c:216:9: note: Cost model analysis: 
Dense.c:216:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:216:9: note: Basic block will be vectorized using SLP
Dense.c:216:9: optimized: basic block part vectorized using 8 byte vectors
Dense.c:216:9: note: Vectorizing SLP tree:
Dense.c:216:9: note: node 0x85bc898 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:216:9: note: op template: <retval>.n_inputs = args$n_inputs_45;
Dense.c:216:9: note: 	stmt 0 <retval>.n_inputs = args$n_inputs_45;
Dense.c:216:9: note: 	stmt 1 <retval>.n_outputs = args$n_outputs_46;
Dense.c:216:9: note: 	children 0x85bc918
Dense.c:216:9: note: node 0x85bc918 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:216:9: note: op template: args$n_inputs_45 = args.n_inputs;
Dense.c:216:9: note: 	stmt 0 args$n_inputs_45 = args.n_inputs;
Dense.c:216:9: note: 	stmt 1 args$n_outputs_46 = args.n_outputs;
Dense.c:216:9: note: ------>vectorizing SLP node starting from: args$n_inputs_45 = args.n_inputs;
Dense.c:216:9: note: transform load. ncopies = 1
Dense.c:216:9: note: conflicting alias set types.
Dense.c:216:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: args.n_inputs
Dense.c:216:9: note: created &args
Dense.c:216:9: note: add new stmt: vect_args_n_inputs_45.490_9 = MEM <vector(2) unsigned int> [(void *)&args];
Dense.c:216:9: note: extracting lane for live stmt args$n_inputs_45 = args.n_inputs;
Dense.c:216:9: note: extracting lane for live stmt args$n_outputs_46 = args.n_outputs;
Dense.c:216:9: note: ------>vectorizing SLP node starting from: <retval>.n_inputs = args$n_inputs_45;
Dense.c:216:9: note: vect_is_simple_use: operand args.n_inputs, type of def: internal
Dense.c:216:9: note: vect_is_simple_use: operand args.n_outputs, type of def: internal
Dense.c:216:9: note: transform store. ncopies = 1
Dense.c:216:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: <retval>.n_inputs
Dense.c:216:9: note: created &<retval>.n_inputs
Dense.c:216:9: note: add new stmt: MEM <vector(2) unsigned int> [(unsigned int *)&<retval> + 128B] = vect_args_n_inputs_45.490_9;
Dense.c:216:9: note: vectorizing stmts using SLP.
Dense.c:216:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:216:9: note: Vectorizing SLP tree:
Dense.c:216:9: note: node 0x85bca18 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:216:9: note: op template: <retval>.inputs = IO_34(D);
Dense.c:216:9: note: 	stmt 0 <retval>.inputs = IO_34(D);
Dense.c:216:9: note: 	stmt 1 <retval>.outputs = _22;
Dense.c:216:9: note: 	children 0x85bca98
Dense.c:216:9: note: node (external) 0x85bca98 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:216:9: note: 	{ IO_34(D), _22 }
Dense.c:216:9: note: ------>vectorizing SLP node starting from: <retval>.inputs = IO_34(D);
Dense.c:216:9: note: vect_is_simple_use: operand IO_34(D) + _21, type of def: internal
Dense.c:216:9: note: transform store. ncopies = 1
Dense.c:216:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.inputs
Dense.c:216:9: note: created &<retval>
Dense.c:216:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval>] = _83;
Dense.c:216:9: note: vectorizing stmts using SLP.
Dense.c:216:9: optimized: basic block part vectorized using 32 byte vectors
Dense.c:216:9: note: Vectorizing SLP tree:
Dense.c:216:9: note: node 0x85bcb98 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:216:9: note: op template: <retval>.feed = layer$feed_33;
Dense.c:216:9: note: 	stmt 0 <retval>.feed = layer$feed_33;
Dense.c:216:9: note: 	stmt 1 <retval>.back = layer$back_43;
Dense.c:216:9: note: 	stmt 2 <retval>.fix = layer$fix_44;
Dense.c:216:9: note: 	stmt 3 <retval>.activate = _25;
Dense.c:216:9: note: 	children 0x85bcc98
Dense.c:216:9: note: node (external) 0x85bcc98 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:216:9: note: 	{ layer$feed_33, layer$back_43, layer$fix_44, _25 }
Dense.c:216:9: note: ------>vectorizing SLP node starting from: <retval>.feed = layer$feed_33;
Dense.c:216:9: note: vect_is_simple_use: operand layer$back_43 = PHI <layer$back_49(D)(11), _BackDense(9), _SIMD_BackDense(15)>, type of def: internal
Dense.c:216:9: note: vect_is_simple_use: operand layer$fix_44 = PHI <layer$fix_50(D)(11), _FixDense(9), _SIMD_FixDense(15)>, type of def: internal
Dense.c:216:9: note: vect_is_simple_use: operand (void (*<T91a>) (float *, unsigned int, char)) _24, type of def: internal
Dense.c:216:9: note: conflicting alias set types.
Dense.c:216:9: note: transform store. ncopies = 1
Dense.c:216:9: note: create vector_type-pointer variable to type: vector(4) long unsigned int  vectorizing a pointer ref: <retval>.feed
Dense.c:216:9: note: created &<retval>.feed
Dense.c:216:9: note: add new stmt: MEM <vector(4) long unsigned int> [(void *)&<retval> + 32B] = _89;
Dense.c:216:9: note: vectorizing stmts using SLP.
Dense.c:216:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:216:9: note: Vectorizing SLP tree:
Dense.c:216:9: note: node 0x85bcd98 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:216:9: note: op template: <retval>.parent = 0B;
Dense.c:216:9: note: 	stmt 0 <retval>.parent = 0B;
Dense.c:216:9: note: 	stmt 1 <retval>.child = 0B;
Dense.c:216:9: note: 	children 0x85bce18
Dense.c:216:9: note: node (constant) 0x85bce18 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:216:9: note: 	{ 0B, 0B }
Dense.c:216:9: note: ------>vectorizing SLP node starting from: <retval>.parent = 0B;
Dense.c:216:9: note: vect_is_simple_use: operand 0B, type of def: constant
Dense.c:216:9: note: transform store. ncopies = 1
Dense.c:216:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.parent
Dense.c:216:9: note: created &<retval>.parent
Dense.c:216:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer * *)&<retval> + 72B] = { 0, 0 };
Dense.c:216:9: note: vectorizing stmts using SLP.
Dense.c:216:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:216:9: note: Vectorizing SLP tree:
Dense.c:216:9: note: node 0x85bce98 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:216:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_32(D);
Dense.c:216:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_32(D);
Dense.c:216:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&<retval> + 96B] = _4;
Dense.c:216:9: note: 	children 0x85bcf18
Dense.c:216:9: note: node (external) 0x85bcf18 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:216:9: note: 	{ MODEL_32(D), _4 }
Dense.c:216:9: note: ------>vectorizing SLP node starting from: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_32(D);
Dense.c:216:9: note: vect_is_simple_use: operand MODEL_32(D) + _3, type of def: internal
Dense.c:216:9: note: transform store. ncopies = 1
Dense.c:216:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B]
Dense.c:216:9: note: created &MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B]
Dense.c:216:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer *)&<retval> + 88B] = _94;
Dense.c:216:9: note: vectorizing stmts using SLP.
Dense.c:216:9: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _28 (pretmp_91, _51, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:19: missed: not vectorized: no vectype for stmt: _22 = *_21;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:163:10: note: ***** Analysis failed with vector mode V4DI
Dense.c:163:10: note: ***** The result for vector mode V32QI would be the same
Dense.c:163:10: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:163:10: note: ***** Analysis failed with vector mode V16QI
Dense.c:163:10: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:163:10: note: ***** Analysis failed with vector mode V8QI
Dense.c:163:10: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:163:10: note: ***** Analysis failed with vector mode V4QI
Dense.c:145:10: optimized: loop vectorized using 32 byte vectors
Dense.c:145:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:145:10: optimized: loop vectorized using 16 byte vectors
Dense.c:136:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
Dense.c:129:10: optimized: loop vectorized using 32 byte vectors
Dense.c:129:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:129:10: optimized: loop vectorized using 16 byte vectors
Dense.c:120:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
Dense.c:111:13: note: vectorized 2 loops in function.
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_28, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
Dense.c:111:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:111:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:111:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:111:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:111:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:111:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:111:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:111:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:90:23: missed: couldn't vectorize loop
Dense.c:92:31: missed: not vectorized: no vectype for stmt: _119 = this_40(D)->outputs;
 scalar_type: float *
Dense.c:90:23: missed: couldn't vectorize loop
Dense.c:90:23: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:95:16: missed: couldn't vectorize loop
Dense.c:95:16: missed: not vectorized: unsupported control flow in loop.
Dense.c:83:13: note: vectorized 0 loops in function.
Dense.c:86:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:97:16: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:97:16: note: SLPing BB part
Dense.c:97:16: note: Costing subgraph: 
Dense.c:97:16: note: node 0x7c09d18 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:97:16: note: op template: _16 = (sizetype) j_73;
Dense.c:97:16: note: 	stmt 0 _16 = (sizetype) j_73;
Dense.c:97:16: note: 	stmt 1 _111 = (sizetype) wi_74;
Dense.c:97:16: note: 	children 0x7c09e18
Dense.c:97:16: note: node (external) 0x7c09e18 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:97:16: note: 	{ j_73, wi_74 }
Dense.c:97:16: note: Cost model analysis: 
Dense.c:97:16: note: Scalar 1 and vector 2 loop part do not match up, skipping scalar part
Dense.c:97:16: note: Cost model analysis for part in loop 2:
  Vector cost: 36
  Scalar cost: 8
Dense.c:97:16: missed: not vectorized: vectorization is not profitable.
Dense.c:97:16: note: ***** The result for vector mode V32QI would be the same
Dense.c:97:16: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:97:16: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:97:16: note: SLPing BB part
Dense.c:97:16: note: Costing subgraph: 
Dense.c:97:16: note: node 0x7c09d18 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:97:16: note: op template: _16 = (sizetype) j_73;
Dense.c:97:16: note: 	stmt 0 _16 = (sizetype) j_73;
Dense.c:97:16: note: 	stmt 1 _111 = (sizetype) wi_74;
Dense.c:97:16: note: 	children 0x7c09d98
Dense.c:97:16: note: node (external) 0x7c09d98 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:97:16: note: 	{ j_73, wi_74 }
Dense.c:97:16: note: Cost model analysis: 
Dense.c:97:16: note: Scalar 1 and vector 2 loop part do not match up, skipping scalar part
Dense.c:97:16: note: Cost model analysis for part in loop 2:
  Vector cost: 36
  Scalar cost: 8
Dense.c:97:16: missed: not vectorized: vectorization is not profitable.
Dense.c:97:16: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:90:50: note: ***** Analysis failed with vector mode V8QI
Dense.c:90:50: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:90:50: note: ***** Analysis failed with vector mode V4QI
Dense.c:62:15: missed: couldn't vectorize loop
Dense.c:62:15: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:75:11: optimized: loop vectorized using 32 byte vectors
Dense.c:75:11: optimized: loop vectorized using 16 byte vectors
Dense.c:66:11: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:881:10: missed: not vectorized: no vectype for stmt: _53 = MEM[(__m256 * {ref-all})w_ptr_82];
 scalar_type: __m256
Dense.c:54:13: note: vectorized 1 loops in function.
Dense.c:57:2: missed: statement clobbers memory: memcpy (_4, dense$baiases_63, _3);
Dense.c:79:2: missed: statement clobbers memory: _27 (pretmp_163, _68, 0);
Dense.c:54:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:54:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:154:107: note: ***** Analysis failed with vector mode V8SI
Dense.c:154:107: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
Dense.c:155:85: note: ***** Analysis failed with vector mode V8SI
Dense.c:155:85: note: ***** The result for vector mode V32QI would be the same
Dense.c:155:85: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:155:85: note: ***** Analysis failed with vector mode V16QI
Dense.c:155:85: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:155:85: note: ***** Analysis failed with vector mode V8QI
Dense.c:155:85: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:155:85: note: ***** Analysis failed with vector mode V4QI
Dense.c:156:105: note: ***** Analysis failed with vector mode V8SI
Dense.c:156:105: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
Dense.c:178:19: missed: couldn't vectorize loop
Dense.c:178:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:168:13: note: vectorized 0 loops in function.
Dense.c:179:38: missed: statement clobbers memory: _5 = rand ();
Dense.c:179:72: missed: statement clobbers memory: _69 = sqrt (_12);
Dense.c:192:19: missed: statement clobbers memory: _24 = PULSE_GetActivationFunctionPtr (_23);
Dense.c:195:34: missed: statement clobbers memory: _28 = aligned_alloc (64, _27);
Dense.c:211:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:212:4: missed: statement clobbers memory: exit (1);
Dense.c:216:9: note: ***** Analysis succeeded with vector mode V8SI
Dense.c:216:9: note: SLPing BB part
Dense.c:216:9: note: Costing subgraph: 
Dense.c:216:9: note: node 0x7a87668 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:216:9: note: op template: <retval>.n_inputs = args$n_inputs_45;
Dense.c:216:9: note: 	stmt 0 <retval>.n_inputs = args$n_inputs_45;
Dense.c:216:9: note: 	stmt 1 <retval>.n_outputs = args$n_outputs_46;
Dense.c:216:9: note: 	children 0x7a876e8
Dense.c:216:9: note: node 0x7a876e8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:216:9: note: op template: args$n_inputs_45 = args.n_inputs;
Dense.c:216:9: note: 	stmt 0 args$n_inputs_45 = args.n_inputs;
Dense.c:216:9: note: 	stmt 1 args$n_outputs_46 = args.n_outputs;
Dense.c:216:9: note: Cost model analysis: 
Dense.c:216:9: note: Cost model analysis for part in loop 0:
  Vector cost: 40
  Scalar cost: 56
Dense.c:216:9: note: Costing subgraph: 
Dense.c:216:9: note: node 0x7a877e8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:216:9: note: op template: <retval>.inputs = IO_34(D);
Dense.c:216:9: note: 	stmt 0 <retval>.inputs = IO_34(D);
Dense.c:216:9: note: 	stmt 1 <retval>.outputs = _22;
Dense.c:216:9: note: 	children 0x7a87868
Dense.c:216:9: note: node (external) 0x7a87868 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:216:9: note: 	{ IO_34(D), _22 }
Dense.c:216:9: note: Cost model analysis: 
Dense.c:216:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:216:9: note: Costing subgraph: 
Dense.c:216:9: note: node 0x7a87968 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:216:9: note: op template: <retval>.feed = layer$feed_33;
Dense.c:216:9: note: 	stmt 0 <retval>.feed = layer$feed_33;
Dense.c:216:9: note: 	stmt 1 <retval>.back = layer$back_43;
Dense.c:216:9: note: 	stmt 2 <retval>.fix = layer$fix_44;
Dense.c:216:9: note: 	stmt 3 <retval>.activate = _25;
Dense.c:216:9: note: 	children 0x7a87a68
Dense.c:216:9: note: node (external) 0x7a87a68 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:216:9: note: 	{ layer$feed_33, layer$back_43, layer$fix_44, _25 }
Dense.c:216:9: note: Cost model analysis: 
Dense.c:216:9: note: Cost model analysis for part in loop 0:
  Vector cost: 64
  Scalar cost: 64
Dense.c:216:9: note: Costing subgraph: 
Dense.c:216:9: note: node 0x7a87b68 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:216:9: note: op template: <retval>.parent = 0B;
Dense.c:216:9: note: 	stmt 0 <retval>.parent = 0B;
Dense.c:216:9: note: 	stmt 1 <retval>.child = 0B;
Dense.c:216:9: note: 	children 0x7a87be8
Dense.c:216:9: note: node (constant) 0x7a87be8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:216:9: note: 	{ 0B, 0B }
Dense.c:216:9: note: Cost model analysis: 
Dense.c:216:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:216:9: note: Costing subgraph: 
Dense.c:216:9: note: node 0x7a87c68 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:216:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_32(D);
Dense.c:216:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_32(D);
Dense.c:216:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&<retval> + 96B] = _4;
Dense.c:216:9: note: 	children 0x7a87ce8
Dense.c:216:9: note: node (external) 0x7a87ce8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:216:9: note: 	{ MODEL_32(D), _4 }
Dense.c:216:9: note: Cost model analysis: 
Dense.c:216:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:216:9: note: Basic block will be vectorized using SLP
Dense.c:216:9: optimized: basic block part vectorized using 8 byte vectors
Dense.c:216:9: note: Vectorizing SLP tree:
Dense.c:216:9: note: node 0x7a87668 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:216:9: note: op template: <retval>.n_inputs = args$n_inputs_45;
Dense.c:216:9: note: 	stmt 0 <retval>.n_inputs = args$n_inputs_45;
Dense.c:216:9: note: 	stmt 1 <retval>.n_outputs = args$n_outputs_46;
Dense.c:216:9: note: 	children 0x7a876e8
Dense.c:216:9: note: node 0x7a876e8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:216:9: note: op template: args$n_inputs_45 = args.n_inputs;
Dense.c:216:9: note: 	stmt 0 args$n_inputs_45 = args.n_inputs;
Dense.c:216:9: note: 	stmt 1 args$n_outputs_46 = args.n_outputs;
Dense.c:216:9: note: ------>vectorizing SLP node starting from: args$n_inputs_45 = args.n_inputs;
Dense.c:216:9: note: transform load. ncopies = 1
Dense.c:216:9: note: conflicting alias set types.
Dense.c:216:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: args.n_inputs
Dense.c:216:9: note: created &args
Dense.c:216:9: note: add new stmt: vect_args_n_inputs_45.490_9 = MEM <vector(2) unsigned int> [(void *)&args];
Dense.c:216:9: note: extracting lane for live stmt args$n_inputs_45 = args.n_inputs;
Dense.c:216:9: note: extracting lane for live stmt args$n_outputs_46 = args.n_outputs;
Dense.c:216:9: note: ------>vectorizing SLP node starting from: <retval>.n_inputs = args$n_inputs_45;
Dense.c:216:9: note: vect_is_simple_use: operand args.n_inputs, type of def: internal
Dense.c:216:9: note: vect_is_simple_use: operand args.n_outputs, type of def: internal
Dense.c:216:9: note: transform store. ncopies = 1
Dense.c:216:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: <retval>.n_inputs
Dense.c:216:9: note: created &<retval>.n_inputs
Dense.c:216:9: note: add new stmt: MEM <vector(2) unsigned int> [(unsigned int *)&<retval> + 128B] = vect_args_n_inputs_45.490_9;
Dense.c:216:9: note: vectorizing stmts using SLP.
Dense.c:216:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:216:9: note: Vectorizing SLP tree:
Dense.c:216:9: note: node 0x7a877e8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:216:9: note: op template: <retval>.inputs = IO_34(D);
Dense.c:216:9: note: 	stmt 0 <retval>.inputs = IO_34(D);
Dense.c:216:9: note: 	stmt 1 <retval>.outputs = _22;
Dense.c:216:9: note: 	children 0x7a87868
Dense.c:216:9: note: node (external) 0x7a87868 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:216:9: note: 	{ IO_34(D), _22 }
Dense.c:216:9: note: ------>vectorizing SLP node starting from: <retval>.inputs = IO_34(D);
Dense.c:216:9: note: vect_is_simple_use: operand IO_34(D) + _21, type of def: internal
Dense.c:216:9: note: transform store. ncopies = 1
Dense.c:216:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.inputs
Dense.c:216:9: note: created &<retval>
Dense.c:216:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval>] = _83;
Dense.c:216:9: note: vectorizing stmts using SLP.
Dense.c:216:9: optimized: basic block part vectorized using 32 byte vectors
Dense.c:216:9: note: Vectorizing SLP tree:
Dense.c:216:9: note: node 0x7a87968 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:216:9: note: op template: <retval>.feed = layer$feed_33;
Dense.c:216:9: note: 	stmt 0 <retval>.feed = layer$feed_33;
Dense.c:216:9: note: 	stmt 1 <retval>.back = layer$back_43;
Dense.c:216:9: note: 	stmt 2 <retval>.fix = layer$fix_44;
Dense.c:216:9: note: 	stmt 3 <retval>.activate = _25;
Dense.c:216:9: note: 	children 0x7a87a68
Dense.c:216:9: note: node (external) 0x7a87a68 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:216:9: note: 	{ layer$feed_33, layer$back_43, layer$fix_44, _25 }
Dense.c:216:9: note: ------>vectorizing SLP node starting from: <retval>.feed = layer$feed_33;
Dense.c:216:9: note: vect_is_simple_use: operand layer$back_43 = PHI <layer$back_49(D)(11), _BackDense(9), _SIMD_BackDense(15)>, type of def: internal
Dense.c:216:9: note: vect_is_simple_use: operand layer$fix_44 = PHI <layer$fix_50(D)(11), _FixDense(9), _SIMD_FixDense(15)>, type of def: internal
Dense.c:216:9: note: vect_is_simple_use: operand (void (*<T91a>) (float *, unsigned int, char)) _24, type of def: internal
Dense.c:216:9: note: conflicting alias set types.
Dense.c:216:9: note: transform store. ncopies = 1
Dense.c:216:9: note: create vector_type-pointer variable to type: vector(4) long unsigned int  vectorizing a pointer ref: <retval>.feed
Dense.c:216:9: note: created &<retval>.feed
Dense.c:216:9: note: add new stmt: MEM <vector(4) long unsigned int> [(void *)&<retval> + 32B] = _89;
Dense.c:216:9: note: vectorizing stmts using SLP.
Dense.c:216:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:216:9: note: Vectorizing SLP tree:
Dense.c:216:9: note: node 0x7a87b68 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:216:9: note: op template: <retval>.parent = 0B;
Dense.c:216:9: note: 	stmt 0 <retval>.parent = 0B;
Dense.c:216:9: note: 	stmt 1 <retval>.child = 0B;
Dense.c:216:9: note: 	children 0x7a87be8
Dense.c:216:9: note: node (constant) 0x7a87be8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:216:9: note: 	{ 0B, 0B }
Dense.c:216:9: note: ------>vectorizing SLP node starting from: <retval>.parent = 0B;
Dense.c:216:9: note: vect_is_simple_use: operand 0B, type of def: constant
Dense.c:216:9: note: transform store. ncopies = 1
Dense.c:216:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.parent
Dense.c:216:9: note: created &<retval>.parent
Dense.c:216:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer * *)&<retval> + 72B] = { 0, 0 };
Dense.c:216:9: note: vectorizing stmts using SLP.
Dense.c:216:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:216:9: note: Vectorizing SLP tree:
Dense.c:216:9: note: node 0x7a87c68 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:216:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_32(D);
Dense.c:216:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_32(D);
Dense.c:216:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&<retval> + 96B] = _4;
Dense.c:216:9: note: 	children 0x7a87ce8
Dense.c:216:9: note: node (external) 0x7a87ce8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:216:9: note: 	{ MODEL_32(D), _4 }
Dense.c:216:9: note: ------>vectorizing SLP node starting from: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_32(D);
Dense.c:216:9: note: vect_is_simple_use: operand MODEL_32(D) + _3, type of def: internal
Dense.c:216:9: note: transform store. ncopies = 1
Dense.c:216:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B]
Dense.c:216:9: note: created &MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B]
Dense.c:216:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer *)&<retval> + 88B] = _94;
Dense.c:216:9: note: vectorizing stmts using SLP.
Dense.c:216:9: note: ***** The result for vector mode V32QI would be the same
Activations.c:17:19: optimized: loop vectorized using 32 byte vectors
Activations.c:17:19: optimized: loop vectorized using 16 byte vectors
Activations.c:17:19: optimized: loop vectorized using 32 byte vectors
Activations.c:17:19: optimized: loop vectorized using 16 byte vectors
Activations.c:14:13: note: vectorized 2 loops in function.
Activations.c:14:13: note: ***** Analysis failed with vector mode V8SF
Activations.c:14:13: note: ***** The result for vector mode V32QI would be the same
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V16QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V16QI
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V8QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V8QI
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V4QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V4QI
Activations.c:28:19: missed: couldn't vectorize loop
Activations.c:28:19: missed: not vectorized: unsupported control flow in loop.
Activations.c:28:19: optimized: loop vectorized using 32 byte vectors
Activations.c:28:19: optimized: loop vectorized using 16 byte vectors
Activations.c:25:13: note: vectorized 1 loops in function.
Activations.c:25:13: note: ***** Analysis failed with vector mode V8SF
Activations.c:25:13: note: ***** The result for vector mode V32QI would be the same
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V16QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V16QI
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V8QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V8QI
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V4QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V4QI
Activations.c:38:1: note: ***** Analysis failed with vector mode VOID
Activations.c:5:19: missed: couldn't vectorize loop
Activations.c:9:21: missed: statement clobbers memory: _36 = expf (_18);
Activations.c:5:19: missed: couldn't vectorize loop
Activations.c:9:21: missed: statement clobbers memory: _12 = expf (_11);
Activations.c:3:13: note: vectorized 0 loops in function.
Activations.c:9:21: missed: statement clobbers memory: _12 = expf (_11);
Activations.c:9:21: missed: statement clobbers memory: _36 = expf (_18);
Activations.c:11:1: note: ***** Analysis failed with vector mode V8SF
Activations.c:11:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SF
Activations.c:44:2: note: ***** Analysis failed with vector mode VOID
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:4:18: note: vectorized 0 loops in function.
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_4, inputs_15, _3);
PULSE.c:8:2: missed: statement clobbers memory: _5 (layer_17);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_12, pretmp_26, _9);
PULSE.c:13:28: note: ***** Analysis failed with vector mode V8SI
PULSE.c:13:28: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:22:2: missed: statement clobbers memory: _1 (layer_10(D));
PULSE.c:22:2: missed: statement clobbers memory: _14 (_2);
PULSE.c:22:2: missed: statement clobbers memory: _21 (_15);
PULSE.c:22:2: missed: statement clobbers memory: _28 (_22);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_29);
PULSE.c:26:3: missed: statement clobbers memory: memset (_34, 0, _32);
PULSE.c:26:3: missed: statement clobbers memory: memset (_27, 0, _25);
PULSE.c:26:3: missed: statement clobbers memory: memset (_20, 0, _18);
PULSE.c:26:3: missed: statement clobbers memory: memset (_7, 0, _5);
PULSE.c:28:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:28:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:39: missed: statement clobbers memory: _7 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:32:6: note: vectorized 1 loops in function.
PULSE.c:34:8: missed: statement clobbers memory: _1 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_2);
PULSE.c:39:39: missed: statement clobbers memory: _7 = rand ();
PULSE.c:40:40: missed: statement clobbers memory: _9 = rand ();
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8SI
PULSE.c:32:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:66:16: missed: couldn't vectorize loop
PULSE.c:66:16: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:69:17: missed: couldn't vectorize loop
PULSE.c:69:17: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:78:19: missed: couldn't vectorize loop
PULSE.c:78:19: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:39: missed: statement clobbers memory: _83 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:55:22: missed: couldn't vectorize loop
PULSE.c:55:22: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:47:6: note: vectorized 1 loops in function.
PULSE.c:48:1: missed: statement clobbers memory: saved_stack.7_47 = __builtin_stack_save ();
PULSE.c:49:46: missed: statement clobbers memory: _1 = PULSE_GetLossFunctionPtr (loss_function_48(D));
PULSE.c:50:42: missed: statement clobbers memory: TMP_52 = aligned_alloc (64, _4);
PULSE.c:57:14: missed: statement clobbers memory: _72 = _5 (output_207, TMP_PTR_206);
PULSE.c:65:15: missed: statement clobbers memory: random.4_56 = __builtin_alloca_with_align (_10, 32);
PULSE.c:34:8: missed: statement clobbers memory: _75 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_76);
PULSE.c:39:39: missed: statement clobbers memory: _83 = rand ();
PULSE.c:40:40: missed: statement clobbers memory: _86 = rand ();
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_112, inputs_107, _111);
PULSE.c:8:2: missed: statement clobbers memory: _113 (layer_108);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_119, _118, _117);
PULSE.c:72:11: missed: statement clobbers memory: loss_65 = PULSE_GetLoss_50 (_23, _22, _18, _17);
PULSE.c:22:2: missed: statement clobbers memory: _100 (output_208);
PULSE.c:22:2: missed: statement clobbers memory: _122 (_101);
PULSE.c:22:2: missed: statement clobbers memory: _129 (_123);
PULSE.c:22:2: missed: statement clobbers memory: _136 (_130);
PULSE.c:22:2: missed: statement clobbers memory: _143 (_137);
PULSE.c:22:2: missed: statement clobbers memory: _150 (_144);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_151);
PULSE.c:26:3: missed: statement clobbers memory: memset (_156, 0, _154);
PULSE.c:26:3: missed: statement clobbers memory: memset (_149, 0, _147);
PULSE.c:26:3: missed: statement clobbers memory: memset (_142, 0, _140);
PULSE.c:26:3: missed: statement clobbers memory: memset (_135, 0, _133);
PULSE.c:26:3: missed: statement clobbers memory: memset (_128, 0, _126);
PULSE.c:26:3: missed: statement clobbers memory: memset (_106, 0, _104);
PULSE.c:80:6: missed: statement clobbers memory: _27 (current_212, args);
PULSE.c:86:4: missed: statement clobbers memory: printf ("Epoch: %d | Item: %d | Loss: %.10f | Batch Loss: %.10f\r", i_209, j_210, _31, _30);
PULSE.c:47:6: missed: statement clobbers memory: __builtin_stack_restore (saved_stack.7_47);
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4DI
PULSE.c:47:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:96:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:96:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:130:20: missed: couldn't vectorize loop
PULSE.c:130:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:130:20: missed: couldn't vectorize loop
PULSE.c:130:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:109:20: missed: couldn't vectorize loop
PULSE.c:109:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:109:20: missed: couldn't vectorize loop
PULSE.c:109:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:98:13: note: vectorized 0 loops in function.
PULSE.c:101:39: missed: statement clobbers memory: layers_41 = malloc (_3);
PULSE.c:104:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:123:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:124:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:125:46: missed: statement clobbers memory: WEIGHTS_86 = aligned_alloc (64, 0);
PULSE.c:115:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.106_94];
PULSE.c:116:21: missed: statement clobbers memory: _75 = PULSE_GetDenseWeightsSize (args);
PULSE.c:117:16: missed: statement clobbers memory: _78 = PULSE_GetDenseIOSize (args);
PULSE.c:118:19: missed: statement clobbers memory: _81 = PULSE_GetDenseFixesSize (args);
PULSE.c:115:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})_224];
PULSE.c:116:21: missed: statement clobbers memory: _210 = PULSE_GetDenseWeightsSize (args);
PULSE.c:117:16: missed: statement clobbers memory: _213 = PULSE_GetDenseIOSize (args);
PULSE.c:118:19: missed: statement clobbers memory: _216 = PULSE_GetDenseFixesSize (args);
PULSE.c:123:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:124:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:125:46: missed: statement clobbers memory: WEIGHTS_46 = aligned_alloc (64, _5);
PULSE.c:126:41: missed: statement clobbers memory: IO_48 = aligned_alloc (64, _7);
PULSE.c:136:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.108_96];
PULSE.c:137:17: missed: statement clobbers memory: *_10 = PULSE_CreateDenseLayer (args, WEIGHTS_PTR_152, IO_PTR_153, 0B); [return slot optimization]
PULSE.c:138:20: missed: statement clobbers memory: _64 = PULSE_GetDenseWeightsSize (args);
PULSE.c:139:15: missed: statement clobbers memory: _67 = PULSE_GetDenseIOSize (args);
PULSE.c:136:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.108_124];
PULSE.c:137:17: missed: statement clobbers memory: *_106 = PULSE_CreateDenseLayer (args, WEIGHTS_PTR_148, IO_PTR_143, 0B); [return slot optimization]
PULSE.c:138:20: missed: statement clobbers memory: _95 = PULSE_GetDenseWeightsSize (args);
PULSE.c:139:15: missed: statement clobbers memory: _59 = PULSE_GetDenseIOSize (args);
PULSE.c:145:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:146:9: note: ***** Analysis succeeded with vector mode V8SI
PULSE.c:146:9: note: SLPing BB part
PULSE.c:146:9: note: Costing subgraph: 
PULSE.c:146:9: note: node 0x3c04b28 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:146:9: note: op template: <retval>.weights_size = WEIGHTS_SIZE_164;
PULSE.c:146:9: note: 	stmt 0 <retval>.weights_size = WEIGHTS_SIZE_164;
PULSE.c:146:9: note: 	stmt 1 <retval>.io_size = IO_SIZE_167;
PULSE.c:146:9: note: 	children 0x3c04ba8
PULSE.c:146:9: note: node 0x3c04ba8 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:146:9: note: op template: WEIGHTS_SIZE_164 = PHI <WEIGHTS_SIZE_161(36), 0(25)>
PULSE.c:146:9: note: 	stmt 0 WEIGHTS_SIZE_164 = PHI <WEIGHTS_SIZE_161(36), 0(25)>
PULSE.c:146:9: note: 	stmt 1 IO_SIZE_167 = PHI <IO_SIZE_160(36), 0(25)>
PULSE.c:146:9: note: 	children 0x3c04c28 0x3c05428
PULSE.c:146:9: note: node 0x3c04c28 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:146:9: note: op template: WEIGHTS_SIZE_161 = PHI <WEIGHTS_SIZE_20(11), WEIGHTS_SIZE_206(76)>
PULSE.c:146:9: note: 	stmt 0 WEIGHTS_SIZE_161 = PHI <WEIGHTS_SIZE_20(11), WEIGHTS_SIZE_206(76)>
PULSE.c:146:9: note: 	stmt 1 IO_SIZE_160 = PHI <IO_SIZE_22(11), IO_SIZE_192(76)>
PULSE.c:146:9: note: 	children 0x3c04ca8 0x3c05028
PULSE.c:146:9: note: node 0x3c04ca8 (max_nunits=2, refcnt=3) vector(2) unsigned int
PULSE.c:146:9: note: op template: WEIGHTS_SIZE_20 = PHI <WEIGHTS_SIZE_73(77), WEIGHTS_SIZE_76(10)>
PULSE.c:146:9: note: 	stmt 0 WEIGHTS_SIZE_20 = PHI <WEIGHTS_SIZE_73(77), WEIGHTS_SIZE_76(10)>
PULSE.c:146:9: note: 	stmt 1 IO_SIZE_22 = PHI <IO_SIZE_147(77), IO_SIZE_79(10)>
PULSE.c:146:9: note: 	children 0x3c04d28 0x3c04e28
PULSE.c:146:9: note: node 0x3c04d28 (max_nunits=2, refcnt=2) vector(2) unsigned int
PULSE.c:146:9: note: op template: WEIGHTS_SIZE_73 = PHI <WEIGHTS_SIZE_20(73), 0(71)>
PULSE.c:146:9: note: 	stmt 0 WEIGHTS_SIZE_73 = PHI <WEIGHTS_SIZE_20(73), 0(71)>
PULSE.c:146:9: note: 	stmt 1 IO_SIZE_147 = PHI <IO_SIZE_22(73), 0(71)>
PULSE.c:146:9: note: 	children 0x3c04ca8 0x3c04da8
PULSE.c:146:9: note: node (constant) 0x3c04da8 (max_nunits=1, refcnt=1) vector(2) unsigned int
PULSE.c:146:9: note: 	{ 0, 0 }
PULSE.c:146:9: note: node 0x3c04e28 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:146:9: note: op template: WEIGHTS_SIZE_76 = WEIGHTS_SIZE_73 + _75;
PULSE.c:146:9: note: 	stmt 0 WEIGHTS_SIZE_76 = WEIGHTS_SIZE_73 + _75;
PULSE.c:146:9: note: 	stmt 1 IO_SIZE_79 = _78 + IO_SIZE_147;
PULSE.c:146:9: note: 	children 0x3c04d28 0x3c04fa8
PULSE.c:146:9: note: node (external) 0x3c04fa8 (max_nunits=1, refcnt=1) vector(2) unsigned int
PULSE.c:146:9: note: 	{ _75, _78 }
PULSE.c:146:9: note: node 0x3c05028 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:146:9: note: op template: WEIGHTS_SIZE_206 = PHI <WEIGHTS_SIZE_238(68)>
PULSE.c:146:9: note: 	stmt 0 WEIGHTS_SIZE_206 = PHI <WEIGHTS_SIZE_238(68)>
PULSE.c:146:9: note: 	stmt 1 IO_SIZE_192 = PHI <IO_SIZE_239(68)>
PULSE.c:146:9: note: 	children 0x3c050a8
PULSE.c:146:9: note: node 0x3c050a8 (max_nunits=2, refcnt=2) vector(2) unsigned int
PULSE.c:146:9: note: op template: WEIGHTS_SIZE_238 = PHI <WEIGHTS_SIZE_211(59), WEIGHTS_SIZE_200(57)>
PULSE.c:146:9: note: 	stmt 0 WEIGHTS_SIZE_238 = PHI <WEIGHTS_SIZE_211(59), WEIGHTS_SIZE_200(57)>
PULSE.c:146:9: note: 	stmt 1 IO_SIZE_239 = PHI <IO_SIZE_214(59), IO_SIZE_201(57)>
PULSE.c:146:9: note: 	children 0x3c05128 0x3c05228
PULSE.c:146:9: note: node 0x3c05128 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:146:9: note: op template: WEIGHTS_SIZE_211 = WEIGHTS_SIZE_200 + _210;
PULSE.c:146:9: note: 	stmt 0 WEIGHTS_SIZE_211 = WEIGHTS_SIZE_200 + _210;
PULSE.c:146:9: note: 	stmt 1 IO_SIZE_214 = _213 + IO_SIZE_201;
PULSE.c:146:9: note: 	children 0x3c05228 0x3c053a8
PULSE.c:146:9: note: node 0x3c05228 (max_nunits=2, refcnt=2) vector(2) unsigned int
PULSE.c:146:9: note: op template: WEIGHTS_SIZE_200 = PHI <WEIGHTS_SIZE_243(72), WEIGHTS_SIZE_238(69)>
PULSE.c:146:9: note: 	stmt 0 WEIGHTS_SIZE_200 = PHI <WEIGHTS_SIZE_243(72), WEIGHTS_SIZE_238(69)>
PULSE.c:146:9: note: 	stmt 1 IO_SIZE_201 = PHI <IO_SIZE_244(72), IO_SIZE_239(69)>
PULSE.c:146:9: note: 	children 0x3c052a8 0x3c050a8
PULSE.c:146:9: note: node 0x3c052a8 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:146:9: note: op template: WEIGHTS_SIZE_243 = PHI <WEIGHTS_SIZE_20(74)>
PULSE.c:146:9: note: 	stmt 0 WEIGHTS_SIZE_243 = PHI <WEIGHTS_SIZE_20(74)>
PULSE.c:146:9: note: 	stmt 1 IO_SIZE_244 = PHI <IO_SIZE_22(74)>
PULSE.c:146:9: note: 	children 0x3c04ca8
PULSE.c:146:9: note: node (external) 0x3c053a8 (max_nunits=1, refcnt=1) vector(2) unsigned int
PULSE.c:146:9: note: 	{ _210, _213 }
PULSE.c:146:9: note: node (constant) 0x3c05428 (max_nunits=1, refcnt=1) vector(2) unsigned int
PULSE.c:146:9: note: 	{ 0, 0 }
PULSE.c:146:9: note: Cost model analysis: 
PULSE.c:146:9: note: Cost model analysis for part in loop 0:
  Vector cost: 48
  Scalar cost: 48
PULSE.c:146:9: note: Cost model analysis for part in loop 1:
  Vector cost: 44
  Scalar cost: 24
PULSE.c:146:9: missed: not vectorized: vectorization is not profitable.
PULSE.c:146:9: note: Costing subgraph: 
PULSE.c:146:9: note: node 0x3c05528 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:146:9: note: op template: <retval>.layers = layers_41;
PULSE.c:146:9: note: 	stmt 0 <retval>.layers = layers_41;
PULSE.c:146:9: note: 	stmt 1 <retval>.weights = WEIGHTS_163;
PULSE.c:146:9: note: 	children 0x3c05628
PULSE.c:146:9: note: node (external) 0x3c05628 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:146:9: note: 	{ layers_41, WEIGHTS_163 }
PULSE.c:146:9: note: Cost model analysis: 
PULSE.c:146:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:146:9: note: Costing subgraph: 
PULSE.c:146:9: note: node 0x3c056a8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:146:9: note: op template: <retval>.io = 0B;
PULSE.c:146:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:146:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:146:9: note: 	children 0x3c05728
PULSE.c:146:9: note: node (constant) 0x3c05728 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:146:9: note: 	{ 0B, 0B }
PULSE.c:146:9: note: Cost model analysis: 
PULSE.c:146:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:146:9: note: Basic block will be vectorized using SLP
PULSE.c:146:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:146:9: note: Vectorizing SLP tree:
PULSE.c:146:9: note: node 0x3c05528 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:146:9: note: op template: <retval>.layers = layers_41;
PULSE.c:146:9: note: 	stmt 0 <retval>.layers = layers_41;
PULSE.c:146:9: note: 	stmt 1 <retval>.weights = WEIGHTS_163;
PULSE.c:146:9: note: 	children 0x3c05628
PULSE.c:146:9: note: node (external) 0x3c05628 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:146:9: note: 	{ layers_41, WEIGHTS_163 }
PULSE.c:146:9: note: ------>vectorizing SLP node starting from: <retval>.layers = layers_41;
PULSE.c:146:9: note: vect_is_simple_use: operand WEIGHTS_163 = PHI <WEIGHTS_46(36), WEIGHTS_86(25)>, type of def: internal
PULSE.c:146:9: note: conflicting alias set types.
PULSE.c:146:9: note: transform store. ncopies = 1
PULSE.c:146:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.layers
PULSE.c:146:9: note: created &<retval>
PULSE.c:146:9: note: add new stmt: MEM <vector(2) long unsigned int> [(void *)&<retval>] = _19;
PULSE.c:146:9: note: vectorizing stmts using SLP.
PULSE.c:146:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:146:9: note: Vectorizing SLP tree:
PULSE.c:146:9: note: node 0x3c056a8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:146:9: note: op template: <retval>.io = 0B;
PULSE.c:146:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:146:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:146:9: note: 	children 0x3c05728
PULSE.c:146:9: note: node (constant) 0x3c05728 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:146:9: note: 	{ 0B, 0B }
PULSE.c:146:9: note: ------>vectorizing SLP node starting from: <retval>.io = 0B;
PULSE.c:146:9: note: vect_is_simple_use: operand 0B, type of def: constant
PULSE.c:146:9: note: transform store. ncopies = 1
PULSE.c:146:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.io
PULSE.c:146:9: note: created &<retval>.io
PULSE.c:146:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval> + 16B] = { 0, 0 };
PULSE.c:146:9: note: vectorizing stmts using SLP.
PULSE.c:146:9: note: ***** The result for vector mode V32QI would be the same
PULSE.c:152:2: missed: statement clobbers memory: free (_1);
PULSE.c:153:2: missed: statement clobbers memory: free (_2);
PULSE.c:154:2: missed: statement clobbers memory: free (_3);
PULSE.c:155:2: missed: statement clobbers memory: free (_4);
PULSE.c:156:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:156:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Loss.c:15:19: optimized: loop vectorized using 32 byte vectors
Loss.c:15:19: optimized:  loop versioned for vectorization because of possible aliasing
Loss.c:15:19: optimized: loop vectorized using 16 byte vectors
Loss.c:12:23: note: vectorized 1 loops in function.
Loss.c:12:23: note: ***** Analysis failed with vector mode V8SF
Loss.c:12:23: note: ***** The result for vector mode V32QI would be the same
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V16QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V16QI
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V8QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V8QI
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V4QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V4QI
Loss.c:7:19: missed: couldn't vectorize loop
Loss.c:8:38: missed: not vectorized: unsupported use in stmt.
Loss.c:4:23: note: vectorized 0 loops in function.
Loss.c:9:13: note: ***** Analysis failed with vector mode V8SF
Loss.c:9:13: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SF
Loss.c:23:2: note: ***** Analysis failed with vector mode VOID
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _28 (pretmp_91, _51, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:19: missed: not vectorized: no vectype for stmt: _19 = *_18;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:163:10: note: ***** Analysis failed with vector mode V4DI
Dense.c:163:10: note: ***** The result for vector mode V32QI would be the same
Dense.c:163:10: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:163:10: note: ***** Analysis failed with vector mode V16QI
Dense.c:163:10: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:163:10: note: ***** Analysis failed with vector mode V8QI
Dense.c:163:10: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:163:10: note: ***** Analysis failed with vector mode V4QI
Dense.c:145:10: optimized: loop vectorized using 32 byte vectors
Dense.c:145:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:145:10: optimized: loop vectorized using 16 byte vectors
Dense.c:136:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
Dense.c:129:10: optimized: loop vectorized using 32 byte vectors
Dense.c:129:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:129:10: optimized: loop vectorized using 16 byte vectors
Dense.c:120:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
Dense.c:111:13: note: vectorized 2 loops in function.
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_28, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
Dense.c:111:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:111:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:111:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:111:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:111:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:111:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:111:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:111:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:90:23: missed: couldn't vectorize loop
Dense.c:92:31: missed: not vectorized: no vectype for stmt: _119 = this_40(D)->outputs;
 scalar_type: float *
Dense.c:90:23: missed: couldn't vectorize loop
Dense.c:90:23: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:95:16: missed: couldn't vectorize loop
Dense.c:95:16: missed: not vectorized: unsupported control flow in loop.
Dense.c:83:13: note: vectorized 0 loops in function.
Dense.c:86:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:97:16: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:97:16: note: SLPing BB part
Dense.c:97:16: note: Costing subgraph: 
Dense.c:97:16: note: node 0x8a90148 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:97:16: note: op template: _16 = (sizetype) j_73;
Dense.c:97:16: note: 	stmt 0 _16 = (sizetype) j_73;
Dense.c:97:16: note: 	stmt 1 _111 = (sizetype) wi_74;
Dense.c:97:16: note: 	children 0x8a90248
Dense.c:97:16: note: node (external) 0x8a90248 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:97:16: note: 	{ j_73, wi_74 }
Dense.c:97:16: note: Cost model analysis: 
Dense.c:97:16: note: Scalar 1 and vector 2 loop part do not match up, skipping scalar part
Dense.c:97:16: note: Cost model analysis for part in loop 2:
  Vector cost: 36
  Scalar cost: 8
Dense.c:97:16: missed: not vectorized: vectorization is not profitable.
Dense.c:97:16: note: ***** The result for vector mode V32QI would be the same
Dense.c:97:16: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:97:16: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:97:16: note: SLPing BB part
Dense.c:97:16: note: Costing subgraph: 
Dense.c:97:16: note: node 0x8a90148 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:97:16: note: op template: _16 = (sizetype) j_73;
Dense.c:97:16: note: 	stmt 0 _16 = (sizetype) j_73;
Dense.c:97:16: note: 	stmt 1 _111 = (sizetype) wi_74;
Dense.c:97:16: note: 	children 0x8a901c8
Dense.c:97:16: note: node (external) 0x8a901c8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:97:16: note: 	{ j_73, wi_74 }
Dense.c:97:16: note: Cost model analysis: 
Dense.c:97:16: note: Scalar 1 and vector 2 loop part do not match up, skipping scalar part
Dense.c:97:16: note: Cost model analysis for part in loop 2:
  Vector cost: 36
  Scalar cost: 8
Dense.c:97:16: missed: not vectorized: vectorization is not profitable.
Dense.c:97:16: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:90:50: note: ***** Analysis failed with vector mode V8QI
Dense.c:90:50: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:90:50: note: ***** Analysis failed with vector mode V4QI
Dense.c:62:15: missed: couldn't vectorize loop
Dense.c:62:15: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:75:11: optimized: loop vectorized using 32 byte vectors
Dense.c:75:11: optimized: loop vectorized using 16 byte vectors
Dense.c:66:11: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:881:10: missed: not vectorized: no vectype for stmt: _53 = MEM[(__m256 * {ref-all})w_ptr_82];
 scalar_type: __m256
Dense.c:54:13: note: vectorized 1 loops in function.
Dense.c:57:2: missed: statement clobbers memory: memcpy (_4, dense$baiases_63, _3);
Dense.c:79:2: missed: statement clobbers memory: _27 (pretmp_163, _68, 0);
Dense.c:54:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:54:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:154:107: note: ***** Analysis failed with vector mode V8SI
Dense.c:154:107: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
Dense.c:155:85: note: ***** Analysis failed with vector mode V8SI
Dense.c:155:85: note: ***** The result for vector mode V32QI would be the same
Dense.c:155:85: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:155:85: note: ***** Analysis failed with vector mode V16QI
Dense.c:155:85: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:155:85: note: ***** Analysis failed with vector mode V8QI
Dense.c:155:85: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:155:85: note: ***** Analysis failed with vector mode V4QI
Dense.c:156:105: note: ***** Analysis failed with vector mode V8SI
Dense.c:156:105: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
Dense.c:178:19: missed: couldn't vectorize loop
Dense.c:178:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:168:13: note: vectorized 0 loops in function.
Dense.c:179:38: missed: statement clobbers memory: _5 = rand ();
Dense.c:179:72: missed: statement clobbers memory: _69 = sqrt (_12);
Dense.c:192:19: missed: statement clobbers memory: _24 = PULSE_GetActivationFunctionPtr (_23);
Dense.c:195:34: missed: statement clobbers memory: _28 = aligned_alloc (64, _27);
Dense.c:211:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:212:4: missed: statement clobbers memory: exit (1);
Dense.c:216:9: note: ***** Analysis succeeded with vector mode V8SI
Dense.c:216:9: note: SLPing BB part
Dense.c:216:9: note: Costing subgraph: 
Dense.c:216:9: note: node 0x8916348 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:216:9: note: op template: <retval>.n_inputs = args$n_inputs_45;
Dense.c:216:9: note: 	stmt 0 <retval>.n_inputs = args$n_inputs_45;
Dense.c:216:9: note: 	stmt 1 <retval>.n_outputs = args$n_outputs_46;
Dense.c:216:9: note: 	children 0x89163c8
Dense.c:216:9: note: node 0x89163c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:216:9: note: op template: args$n_inputs_45 = args.n_inputs;
Dense.c:216:9: note: 	stmt 0 args$n_inputs_45 = args.n_inputs;
Dense.c:216:9: note: 	stmt 1 args$n_outputs_46 = args.n_outputs;
Dense.c:216:9: note: Cost model analysis: 
Dense.c:216:9: note: Cost model analysis for part in loop 0:
  Vector cost: 40
  Scalar cost: 56
Dense.c:216:9: note: Costing subgraph: 
Dense.c:216:9: note: node 0x89164c8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:216:9: note: op template: <retval>.inputs = IO_34(D);
Dense.c:216:9: note: 	stmt 0 <retval>.inputs = IO_34(D);
Dense.c:216:9: note: 	stmt 1 <retval>.outputs = _22;
Dense.c:216:9: note: 	children 0x8916548
Dense.c:216:9: note: node (external) 0x8916548 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:216:9: note: 	{ IO_34(D), _22 }
Dense.c:216:9: note: Cost model analysis: 
Dense.c:216:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:216:9: note: Costing subgraph: 
Dense.c:216:9: note: node 0x8916648 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:216:9: note: op template: <retval>.feed = layer$feed_33;
Dense.c:216:9: note: 	stmt 0 <retval>.feed = layer$feed_33;
Dense.c:216:9: note: 	stmt 1 <retval>.back = layer$back_43;
Dense.c:216:9: note: 	stmt 2 <retval>.fix = layer$fix_44;
Dense.c:216:9: note: 	stmt 3 <retval>.activate = _25;
Dense.c:216:9: note: 	children 0x8916748
Dense.c:216:9: note: node (external) 0x8916748 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:216:9: note: 	{ layer$feed_33, layer$back_43, layer$fix_44, _25 }
Dense.c:216:9: note: Cost model analysis: 
Dense.c:216:9: note: Cost model analysis for part in loop 0:
  Vector cost: 64
  Scalar cost: 64
Dense.c:216:9: note: Costing subgraph: 
Dense.c:216:9: note: node 0x8916848 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:216:9: note: op template: <retval>.parent = 0B;
Dense.c:216:9: note: 	stmt 0 <retval>.parent = 0B;
Dense.c:216:9: note: 	stmt 1 <retval>.child = 0B;
Dense.c:216:9: note: 	children 0x89168c8
Dense.c:216:9: note: node (constant) 0x89168c8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:216:9: note: 	{ 0B, 0B }
Dense.c:216:9: note: Cost model analysis: 
Dense.c:216:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:216:9: note: Costing subgraph: 
Dense.c:216:9: note: node 0x8916948 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:216:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_32(D);
Dense.c:216:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_32(D);
Dense.c:216:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&<retval> + 96B] = _4;
Dense.c:216:9: note: 	children 0x89169c8
Dense.c:216:9: note: node (external) 0x89169c8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:216:9: note: 	{ MODEL_32(D), _4 }
Dense.c:216:9: note: Cost model analysis: 
Dense.c:216:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:216:9: note: Basic block will be vectorized using SLP
Dense.c:216:9: optimized: basic block part vectorized using 8 byte vectors
Dense.c:216:9: note: Vectorizing SLP tree:
Dense.c:216:9: note: node 0x8916348 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:216:9: note: op template: <retval>.n_inputs = args$n_inputs_45;
Dense.c:216:9: note: 	stmt 0 <retval>.n_inputs = args$n_inputs_45;
Dense.c:216:9: note: 	stmt 1 <retval>.n_outputs = args$n_outputs_46;
Dense.c:216:9: note: 	children 0x89163c8
Dense.c:216:9: note: node 0x89163c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:216:9: note: op template: args$n_inputs_45 = args.n_inputs;
Dense.c:216:9: note: 	stmt 0 args$n_inputs_45 = args.n_inputs;
Dense.c:216:9: note: 	stmt 1 args$n_outputs_46 = args.n_outputs;
Dense.c:216:9: note: ------>vectorizing SLP node starting from: args$n_inputs_45 = args.n_inputs;
Dense.c:216:9: note: transform load. ncopies = 1
Dense.c:216:9: note: conflicting alias set types.
Dense.c:216:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: args.n_inputs
Dense.c:216:9: note: created &args
Dense.c:216:9: note: add new stmt: vect_args_n_inputs_45.484_9 = MEM <vector(2) unsigned int> [(void *)&args];
Dense.c:216:9: note: extracting lane for live stmt args$n_inputs_45 = args.n_inputs;
Dense.c:216:9: note: extracting lane for live stmt args$n_outputs_46 = args.n_outputs;
Dense.c:216:9: note: ------>vectorizing SLP node starting from: <retval>.n_inputs = args$n_inputs_45;
Dense.c:216:9: note: vect_is_simple_use: operand args.n_inputs, type of def: internal
Dense.c:216:9: note: vect_is_simple_use: operand args.n_outputs, type of def: internal
Dense.c:216:9: note: transform store. ncopies = 1
Dense.c:216:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: <retval>.n_inputs
Dense.c:216:9: note: created &<retval>.n_inputs
Dense.c:216:9: note: add new stmt: MEM <vector(2) unsigned int> [(unsigned int *)&<retval> + 128B] = vect_args_n_inputs_45.484_9;
Dense.c:216:9: note: vectorizing stmts using SLP.
Dense.c:216:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:216:9: note: Vectorizing SLP tree:
Dense.c:216:9: note: node 0x89164c8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:216:9: note: op template: <retval>.inputs = IO_34(D);
Dense.c:216:9: note: 	stmt 0 <retval>.inputs = IO_34(D);
Dense.c:216:9: note: 	stmt 1 <retval>.outputs = _22;
Dense.c:216:9: note: 	children 0x8916548
Dense.c:216:9: note: node (external) 0x8916548 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:216:9: note: 	{ IO_34(D), _22 }
Dense.c:216:9: note: ------>vectorizing SLP node starting from: <retval>.inputs = IO_34(D);
Dense.c:216:9: note: vect_is_simple_use: operand IO_34(D) + _21, type of def: internal
Dense.c:216:9: note: transform store. ncopies = 1
Dense.c:216:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.inputs
Dense.c:216:9: note: created &<retval>
Dense.c:216:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval>] = _83;
Dense.c:216:9: note: vectorizing stmts using SLP.
Dense.c:216:9: optimized: basic block part vectorized using 32 byte vectors
Dense.c:216:9: note: Vectorizing SLP tree:
Dense.c:216:9: note: node 0x8916648 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:216:9: note: op template: <retval>.feed = layer$feed_33;
Dense.c:216:9: note: 	stmt 0 <retval>.feed = layer$feed_33;
Dense.c:216:9: note: 	stmt 1 <retval>.back = layer$back_43;
Dense.c:216:9: note: 	stmt 2 <retval>.fix = layer$fix_44;
Dense.c:216:9: note: 	stmt 3 <retval>.activate = _25;
Dense.c:216:9: note: 	children 0x8916748
Dense.c:216:9: note: node (external) 0x8916748 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:216:9: note: 	{ layer$feed_33, layer$back_43, layer$fix_44, _25 }
Dense.c:216:9: note: ------>vectorizing SLP node starting from: <retval>.feed = layer$feed_33;
Dense.c:216:9: note: vect_is_simple_use: operand layer$back_43 = PHI <layer$back_49(D)(11), _BackDense(9), _SIMD_BackDense(15)>, type of def: internal
Dense.c:216:9: note: vect_is_simple_use: operand layer$fix_44 = PHI <layer$fix_50(D)(11), _FixDense(9), _SIMD_FixDense(15)>, type of def: internal
Dense.c:216:9: note: vect_is_simple_use: operand (void (*<T91a>) (float *, unsigned int, char)) _24, type of def: internal
Dense.c:216:9: note: conflicting alias set types.
Dense.c:216:9: note: transform store. ncopies = 1
Dense.c:216:9: note: create vector_type-pointer variable to type: vector(4) long unsigned int  vectorizing a pointer ref: <retval>.feed
Dense.c:216:9: note: created &<retval>.feed
Dense.c:216:9: note: add new stmt: MEM <vector(4) long unsigned int> [(void *)&<retval> + 32B] = _89;
Dense.c:216:9: note: vectorizing stmts using SLP.
Dense.c:216:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:216:9: note: Vectorizing SLP tree:
Dense.c:216:9: note: node 0x8916848 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:216:9: note: op template: <retval>.parent = 0B;
Dense.c:216:9: note: 	stmt 0 <retval>.parent = 0B;
Dense.c:216:9: note: 	stmt 1 <retval>.child = 0B;
Dense.c:216:9: note: 	children 0x89168c8
Dense.c:216:9: note: node (constant) 0x89168c8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:216:9: note: 	{ 0B, 0B }
Dense.c:216:9: note: ------>vectorizing SLP node starting from: <retval>.parent = 0B;
Dense.c:216:9: note: vect_is_simple_use: operand 0B, type of def: constant
Dense.c:216:9: note: transform store. ncopies = 1
Dense.c:216:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.parent
Dense.c:216:9: note: created &<retval>.parent
Dense.c:216:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer * *)&<retval> + 72B] = { 0, 0 };
Dense.c:216:9: note: vectorizing stmts using SLP.
Dense.c:216:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:216:9: note: Vectorizing SLP tree:
Dense.c:216:9: note: node 0x8916948 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:216:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_32(D);
Dense.c:216:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_32(D);
Dense.c:216:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&<retval> + 96B] = _4;
Dense.c:216:9: note: 	children 0x89169c8
Dense.c:216:9: note: node (external) 0x89169c8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:216:9: note: 	{ MODEL_32(D), _4 }
Dense.c:216:9: note: ------>vectorizing SLP node starting from: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_32(D);
Dense.c:216:9: note: vect_is_simple_use: operand MODEL_32(D) + _3, type of def: internal
Dense.c:216:9: note: transform store. ncopies = 1
Dense.c:216:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B]
Dense.c:216:9: note: created &MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B]
Dense.c:216:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer *)&<retval> + 88B] = _94;
Dense.c:216:9: note: vectorizing stmts using SLP.
Dense.c:216:9: note: ***** The result for vector mode V32QI would be the same
Activations.c:17:19: optimized: loop vectorized using 32 byte vectors
Activations.c:17:19: optimized: loop vectorized using 16 byte vectors
Activations.c:17:19: optimized: loop vectorized using 32 byte vectors
Activations.c:17:19: optimized: loop vectorized using 16 byte vectors
Activations.c:14:13: note: vectorized 2 loops in function.
Activations.c:14:13: note: ***** Analysis failed with vector mode V8SF
Activations.c:14:13: note: ***** The result for vector mode V32QI would be the same
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V16QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V16QI
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V8QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V8QI
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V4QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V4QI
Activations.c:28:19: missed: couldn't vectorize loop
Activations.c:28:19: missed: not vectorized: unsupported control flow in loop.
Activations.c:28:19: optimized: loop vectorized using 32 byte vectors
Activations.c:28:19: optimized: loop vectorized using 16 byte vectors
Activations.c:25:13: note: vectorized 1 loops in function.
Activations.c:25:13: note: ***** Analysis failed with vector mode V8SF
Activations.c:25:13: note: ***** The result for vector mode V32QI would be the same
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V16QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V16QI
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V8QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V8QI
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V4QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V4QI
Activations.c:38:1: note: ***** Analysis failed with vector mode VOID
Activations.c:5:19: missed: couldn't vectorize loop
Activations.c:9:21: missed: statement clobbers memory: _36 = expf (_18);
Activations.c:5:19: missed: couldn't vectorize loop
Activations.c:9:21: missed: statement clobbers memory: _12 = expf (_11);
Activations.c:3:13: note: vectorized 0 loops in function.
Activations.c:9:21: missed: statement clobbers memory: _12 = expf (_11);
Activations.c:9:21: missed: statement clobbers memory: _36 = expf (_18);
Activations.c:11:1: note: ***** Analysis failed with vector mode V8SF
Activations.c:11:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SF
Activations.c:44:2: note: ***** Analysis failed with vector mode VOID
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:4:18: note: vectorized 0 loops in function.
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_4, inputs_15, _3);
PULSE.c:8:2: missed: statement clobbers memory: _5 (layer_17);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_12, pretmp_26, _9);
PULSE.c:13:28: note: ***** Analysis failed with vector mode V8SI
PULSE.c:13:28: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:22:2: missed: statement clobbers memory: _1 (layer_10(D));
PULSE.c:22:2: missed: statement clobbers memory: _14 (_2);
PULSE.c:22:2: missed: statement clobbers memory: _21 (_15);
PULSE.c:22:2: missed: statement clobbers memory: _28 (_22);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_29);
PULSE.c:26:3: missed: statement clobbers memory: memset (_34, 0, _32);
PULSE.c:26:3: missed: statement clobbers memory: memset (_27, 0, _25);
PULSE.c:26:3: missed: statement clobbers memory: memset (_20, 0, _18);
PULSE.c:26:3: missed: statement clobbers memory: memset (_7, 0, _5);
PULSE.c:28:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:28:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:39: missed: statement clobbers memory: _7 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:32:6: note: vectorized 1 loops in function.
PULSE.c:34:8: missed: statement clobbers memory: _1 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_2);
PULSE.c:39:39: missed: statement clobbers memory: _7 = rand ();
PULSE.c:40:40: missed: statement clobbers memory: _9 = rand ();
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8SI
PULSE.c:32:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:66:16: missed: couldn't vectorize loop
PULSE.c:66:16: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:69:17: missed: couldn't vectorize loop
PULSE.c:69:17: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:78:19: missed: couldn't vectorize loop
PULSE.c:78:19: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:39: missed: statement clobbers memory: _83 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:55:22: missed: couldn't vectorize loop
PULSE.c:55:22: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:47:6: note: vectorized 1 loops in function.
PULSE.c:48:1: missed: statement clobbers memory: saved_stack.7_47 = __builtin_stack_save ();
PULSE.c:49:46: missed: statement clobbers memory: _1 = PULSE_GetLossFunctionPtr (loss_function_48(D));
PULSE.c:50:42: missed: statement clobbers memory: TMP_52 = aligned_alloc (64, _4);
PULSE.c:57:14: missed: statement clobbers memory: _72 = _5 (output_207, TMP_PTR_206);
PULSE.c:65:15: missed: statement clobbers memory: random.4_56 = __builtin_alloca_with_align (_10, 32);
PULSE.c:34:8: missed: statement clobbers memory: _75 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_76);
PULSE.c:39:39: missed: statement clobbers memory: _83 = rand ();
PULSE.c:40:40: missed: statement clobbers memory: _86 = rand ();
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_112, inputs_107, _111);
PULSE.c:8:2: missed: statement clobbers memory: _113 (layer_108);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_119, _118, _117);
PULSE.c:72:11: missed: statement clobbers memory: loss_65 = PULSE_GetLoss_50 (_23, _22, _18, _17);
PULSE.c:22:2: missed: statement clobbers memory: _100 (output_208);
PULSE.c:22:2: missed: statement clobbers memory: _122 (_101);
PULSE.c:22:2: missed: statement clobbers memory: _129 (_123);
PULSE.c:22:2: missed: statement clobbers memory: _136 (_130);
PULSE.c:22:2: missed: statement clobbers memory: _143 (_137);
PULSE.c:22:2: missed: statement clobbers memory: _150 (_144);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_151);
PULSE.c:26:3: missed: statement clobbers memory: memset (_156, 0, _154);
PULSE.c:26:3: missed: statement clobbers memory: memset (_149, 0, _147);
PULSE.c:26:3: missed: statement clobbers memory: memset (_142, 0, _140);
PULSE.c:26:3: missed: statement clobbers memory: memset (_135, 0, _133);
PULSE.c:26:3: missed: statement clobbers memory: memset (_128, 0, _126);
PULSE.c:26:3: missed: statement clobbers memory: memset (_106, 0, _104);
PULSE.c:80:6: missed: statement clobbers memory: _27 (current_212, args);
PULSE.c:86:4: missed: statement clobbers memory: printf ("Epoch: %d | Item: %d | Loss: %.10f | Batch Loss: %.10f\r", i_209, j_210, _31, _30);
PULSE.c:47:6: missed: statement clobbers memory: __builtin_stack_restore (saved_stack.7_47);
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4DI
PULSE.c:47:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:96:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:96:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:130:20: missed: couldn't vectorize loop
PULSE.c:130:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:130:20: missed: couldn't vectorize loop
PULSE.c:130:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:109:20: missed: couldn't vectorize loop
PULSE.c:109:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:109:20: missed: couldn't vectorize loop
PULSE.c:109:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:98:13: note: vectorized 0 loops in function.
PULSE.c:101:39: missed: statement clobbers memory: layers_41 = malloc (_3);
PULSE.c:104:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:123:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:124:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:125:46: missed: statement clobbers memory: WEIGHTS_86 = aligned_alloc (64, 0);
PULSE.c:115:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.106_94];
PULSE.c:116:21: missed: statement clobbers memory: _75 = PULSE_GetDenseWeightsSize (args);
PULSE.c:117:16: missed: statement clobbers memory: _78 = PULSE_GetDenseIOSize (args);
PULSE.c:118:19: missed: statement clobbers memory: _81 = PULSE_GetDenseFixesSize (args);
PULSE.c:115:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})_224];
PULSE.c:116:21: missed: statement clobbers memory: _210 = PULSE_GetDenseWeightsSize (args);
PULSE.c:117:16: missed: statement clobbers memory: _213 = PULSE_GetDenseIOSize (args);
PULSE.c:118:19: missed: statement clobbers memory: _216 = PULSE_GetDenseFixesSize (args);
PULSE.c:123:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:124:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:125:46: missed: statement clobbers memory: WEIGHTS_46 = aligned_alloc (64, _5);
PULSE.c:126:41: missed: statement clobbers memory: IO_48 = aligned_alloc (64, _7);
PULSE.c:136:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.108_96];
PULSE.c:137:17: missed: statement clobbers memory: *_10 = PULSE_CreateDenseLayer (args, WEIGHTS_PTR_152, IO_PTR_153, 0B); [return slot optimization]
PULSE.c:138:20: missed: statement clobbers memory: _64 = PULSE_GetDenseWeightsSize (args);
PULSE.c:139:15: missed: statement clobbers memory: _67 = PULSE_GetDenseIOSize (args);
PULSE.c:136:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.108_124];
PULSE.c:137:17: missed: statement clobbers memory: *_106 = PULSE_CreateDenseLayer (args, WEIGHTS_PTR_148, IO_PTR_143, 0B); [return slot optimization]
PULSE.c:138:20: missed: statement clobbers memory: _95 = PULSE_GetDenseWeightsSize (args);
PULSE.c:139:15: missed: statement clobbers memory: _59 = PULSE_GetDenseIOSize (args);
PULSE.c:145:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:146:9: note: ***** Analysis succeeded with vector mode V8SI
PULSE.c:146:9: note: SLPing BB part
PULSE.c:146:9: note: Costing subgraph: 
PULSE.c:146:9: note: node 0x31214c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:146:9: note: op template: <retval>.weights_size = WEIGHTS_SIZE_164;
PULSE.c:146:9: note: 	stmt 0 <retval>.weights_size = WEIGHTS_SIZE_164;
PULSE.c:146:9: note: 	stmt 1 <retval>.io_size = IO_SIZE_167;
PULSE.c:146:9: note: 	children 0x3121548
PULSE.c:146:9: note: node 0x3121548 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:146:9: note: op template: WEIGHTS_SIZE_164 = PHI <WEIGHTS_SIZE_161(36), 0(25)>
PULSE.c:146:9: note: 	stmt 0 WEIGHTS_SIZE_164 = PHI <WEIGHTS_SIZE_161(36), 0(25)>
PULSE.c:146:9: note: 	stmt 1 IO_SIZE_167 = PHI <IO_SIZE_160(36), 0(25)>
PULSE.c:146:9: note: 	children 0x31215c8 0x3121dc8
PULSE.c:146:9: note: node 0x31215c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:146:9: note: op template: WEIGHTS_SIZE_161 = PHI <WEIGHTS_SIZE_20(11), WEIGHTS_SIZE_206(76)>
PULSE.c:146:9: note: 	stmt 0 WEIGHTS_SIZE_161 = PHI <WEIGHTS_SIZE_20(11), WEIGHTS_SIZE_206(76)>
PULSE.c:146:9: note: 	stmt 1 IO_SIZE_160 = PHI <IO_SIZE_22(11), IO_SIZE_192(76)>
PULSE.c:146:9: note: 	children 0x3121648 0x31219c8
PULSE.c:146:9: note: node 0x3121648 (max_nunits=2, refcnt=3) vector(2) unsigned int
PULSE.c:146:9: note: op template: WEIGHTS_SIZE_20 = PHI <WEIGHTS_SIZE_73(77), WEIGHTS_SIZE_76(10)>
PULSE.c:146:9: note: 	stmt 0 WEIGHTS_SIZE_20 = PHI <WEIGHTS_SIZE_73(77), WEIGHTS_SIZE_76(10)>
PULSE.c:146:9: note: 	stmt 1 IO_SIZE_22 = PHI <IO_SIZE_147(77), IO_SIZE_79(10)>
PULSE.c:146:9: note: 	children 0x31216c8 0x31217c8
PULSE.c:146:9: note: node 0x31216c8 (max_nunits=2, refcnt=2) vector(2) unsigned int
PULSE.c:146:9: note: op template: WEIGHTS_SIZE_73 = PHI <WEIGHTS_SIZE_20(73), 0(71)>
PULSE.c:146:9: note: 	stmt 0 WEIGHTS_SIZE_73 = PHI <WEIGHTS_SIZE_20(73), 0(71)>
PULSE.c:146:9: note: 	stmt 1 IO_SIZE_147 = PHI <IO_SIZE_22(73), 0(71)>
PULSE.c:146:9: note: 	children 0x3121648 0x3121748
PULSE.c:146:9: note: node (constant) 0x3121748 (max_nunits=1, refcnt=1) vector(2) unsigned int
PULSE.c:146:9: note: 	{ 0, 0 }
PULSE.c:146:9: note: node 0x31217c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:146:9: note: op template: WEIGHTS_SIZE_76 = WEIGHTS_SIZE_73 + _75;
PULSE.c:146:9: note: 	stmt 0 WEIGHTS_SIZE_76 = WEIGHTS_SIZE_73 + _75;
PULSE.c:146:9: note: 	stmt 1 IO_SIZE_79 = _78 + IO_SIZE_147;
PULSE.c:146:9: note: 	children 0x31216c8 0x3121948
PULSE.c:146:9: note: node (external) 0x3121948 (max_nunits=1, refcnt=1) vector(2) unsigned int
PULSE.c:146:9: note: 	{ _75, _78 }
PULSE.c:146:9: note: node 0x31219c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:146:9: note: op template: WEIGHTS_SIZE_206 = PHI <WEIGHTS_SIZE_238(68)>
PULSE.c:146:9: note: 	stmt 0 WEIGHTS_SIZE_206 = PHI <WEIGHTS_SIZE_238(68)>
PULSE.c:146:9: note: 	stmt 1 IO_SIZE_192 = PHI <IO_SIZE_239(68)>
PULSE.c:146:9: note: 	children 0x3121a48
PULSE.c:146:9: note: node 0x3121a48 (max_nunits=2, refcnt=2) vector(2) unsigned int
PULSE.c:146:9: note: op template: WEIGHTS_SIZE_238 = PHI <WEIGHTS_SIZE_211(59), WEIGHTS_SIZE_200(57)>
PULSE.c:146:9: note: 	stmt 0 WEIGHTS_SIZE_238 = PHI <WEIGHTS_SIZE_211(59), WEIGHTS_SIZE_200(57)>
PULSE.c:146:9: note: 	stmt 1 IO_SIZE_239 = PHI <IO_SIZE_214(59), IO_SIZE_201(57)>
PULSE.c:146:9: note: 	children 0x3121ac8 0x3121bc8
PULSE.c:146:9: note: node 0x3121ac8 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:146:9: note: op template: WEIGHTS_SIZE_211 = WEIGHTS_SIZE_200 + _210;
PULSE.c:146:9: note: 	stmt 0 WEIGHTS_SIZE_211 = WEIGHTS_SIZE_200 + _210;
PULSE.c:146:9: note: 	stmt 1 IO_SIZE_214 = _213 + IO_SIZE_201;
PULSE.c:146:9: note: 	children 0x3121bc8 0x3121d48
PULSE.c:146:9: note: node 0x3121bc8 (max_nunits=2, refcnt=2) vector(2) unsigned int
PULSE.c:146:9: note: op template: WEIGHTS_SIZE_200 = PHI <WEIGHTS_SIZE_243(72), WEIGHTS_SIZE_238(69)>
PULSE.c:146:9: note: 	stmt 0 WEIGHTS_SIZE_200 = PHI <WEIGHTS_SIZE_243(72), WEIGHTS_SIZE_238(69)>
PULSE.c:146:9: note: 	stmt 1 IO_SIZE_201 = PHI <IO_SIZE_244(72), IO_SIZE_239(69)>
PULSE.c:146:9: note: 	children 0x3121c48 0x3121a48
PULSE.c:146:9: note: node 0x3121c48 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:146:9: note: op template: WEIGHTS_SIZE_243 = PHI <WEIGHTS_SIZE_20(74)>
PULSE.c:146:9: note: 	stmt 0 WEIGHTS_SIZE_243 = PHI <WEIGHTS_SIZE_20(74)>
PULSE.c:146:9: note: 	stmt 1 IO_SIZE_244 = PHI <IO_SIZE_22(74)>
PULSE.c:146:9: note: 	children 0x3121648
PULSE.c:146:9: note: node (external) 0x3121d48 (max_nunits=1, refcnt=1) vector(2) unsigned int
PULSE.c:146:9: note: 	{ _210, _213 }
PULSE.c:146:9: note: node (constant) 0x3121dc8 (max_nunits=1, refcnt=1) vector(2) unsigned int
PULSE.c:146:9: note: 	{ 0, 0 }
PULSE.c:146:9: note: Cost model analysis: 
PULSE.c:146:9: note: Cost model analysis for part in loop 0:
  Vector cost: 48
  Scalar cost: 48
PULSE.c:146:9: note: Cost model analysis for part in loop 1:
  Vector cost: 44
  Scalar cost: 24
PULSE.c:146:9: missed: not vectorized: vectorization is not profitable.
PULSE.c:146:9: note: Costing subgraph: 
PULSE.c:146:9: note: node 0x3121ec8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:146:9: note: op template: <retval>.layers = layers_41;
PULSE.c:146:9: note: 	stmt 0 <retval>.layers = layers_41;
PULSE.c:146:9: note: 	stmt 1 <retval>.weights = WEIGHTS_163;
PULSE.c:146:9: note: 	children 0x3121fc8
PULSE.c:146:9: note: node (external) 0x3121fc8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:146:9: note: 	{ layers_41, WEIGHTS_163 }
PULSE.c:146:9: note: Cost model analysis: 
PULSE.c:146:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:146:9: note: Costing subgraph: 
PULSE.c:146:9: note: node 0x3122048 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:146:9: note: op template: <retval>.io = 0B;
PULSE.c:146:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:146:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:146:9: note: 	children 0x31220c8
PULSE.c:146:9: note: node (constant) 0x31220c8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:146:9: note: 	{ 0B, 0B }
PULSE.c:146:9: note: Cost model analysis: 
PULSE.c:146:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:146:9: note: Basic block will be vectorized using SLP
PULSE.c:146:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:146:9: note: Vectorizing SLP tree:
PULSE.c:146:9: note: node 0x3121ec8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:146:9: note: op template: <retval>.layers = layers_41;
PULSE.c:146:9: note: 	stmt 0 <retval>.layers = layers_41;
PULSE.c:146:9: note: 	stmt 1 <retval>.weights = WEIGHTS_163;
PULSE.c:146:9: note: 	children 0x3121fc8
PULSE.c:146:9: note: node (external) 0x3121fc8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:146:9: note: 	{ layers_41, WEIGHTS_163 }
PULSE.c:146:9: note: ------>vectorizing SLP node starting from: <retval>.layers = layers_41;
PULSE.c:146:9: note: vect_is_simple_use: operand WEIGHTS_163 = PHI <WEIGHTS_46(36), WEIGHTS_86(25)>, type of def: internal
PULSE.c:146:9: note: conflicting alias set types.
PULSE.c:146:9: note: transform store. ncopies = 1
PULSE.c:146:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.layers
PULSE.c:146:9: note: created &<retval>
PULSE.c:146:9: note: add new stmt: MEM <vector(2) long unsigned int> [(void *)&<retval>] = _19;
PULSE.c:146:9: note: vectorizing stmts using SLP.
PULSE.c:146:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:146:9: note: Vectorizing SLP tree:
PULSE.c:146:9: note: node 0x3122048 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:146:9: note: op template: <retval>.io = 0B;
PULSE.c:146:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:146:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:146:9: note: 	children 0x31220c8
PULSE.c:146:9: note: node (constant) 0x31220c8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:146:9: note: 	{ 0B, 0B }
PULSE.c:146:9: note: ------>vectorizing SLP node starting from: <retval>.io = 0B;
PULSE.c:146:9: note: vect_is_simple_use: operand 0B, type of def: constant
PULSE.c:146:9: note: transform store. ncopies = 1
PULSE.c:146:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.io
PULSE.c:146:9: note: created &<retval>.io
PULSE.c:146:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval> + 16B] = { 0, 0 };
PULSE.c:146:9: note: vectorizing stmts using SLP.
PULSE.c:146:9: note: ***** The result for vector mode V32QI would be the same
PULSE.c:152:2: missed: statement clobbers memory: free (_1);
PULSE.c:153:2: missed: statement clobbers memory: free (_2);
PULSE.c:154:2: missed: statement clobbers memory: free (_3);
PULSE.c:155:2: missed: statement clobbers memory: free (_4);
PULSE.c:156:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:156:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Loss.c:15:19: optimized: loop vectorized using 32 byte vectors
Loss.c:15:19: optimized:  loop versioned for vectorization because of possible aliasing
Loss.c:15:19: optimized: loop vectorized using 16 byte vectors
Loss.c:12:23: note: vectorized 1 loops in function.
Loss.c:12:23: note: ***** Analysis failed with vector mode V8SF
Loss.c:12:23: note: ***** The result for vector mode V32QI would be the same
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V16QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V16QI
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V8QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V8QI
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V4QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V4QI
Loss.c:7:19: missed: couldn't vectorize loop
Loss.c:8:38: missed: not vectorized: unsupported use in stmt.
Loss.c:4:23: note: vectorized 0 loops in function.
Loss.c:9:13: note: ***** Analysis failed with vector mode V8SF
Loss.c:9:13: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SF
Loss.c:23:2: note: ***** Analysis failed with vector mode VOID
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _28 (pretmp_91, _51, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:19: missed: not vectorized: no vectype for stmt: _19 = *_18;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:163:10: note: ***** Analysis failed with vector mode V4DI
Dense.c:163:10: note: ***** The result for vector mode V32QI would be the same
Dense.c:163:10: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:163:10: note: ***** Analysis failed with vector mode V16QI
Dense.c:163:10: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:163:10: note: ***** Analysis failed with vector mode V8QI
Dense.c:163:10: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:163:10: note: ***** Analysis failed with vector mode V4QI
Dense.c:145:10: optimized: loop vectorized using 32 byte vectors
Dense.c:145:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:145:10: optimized: loop vectorized using 16 byte vectors
Dense.c:136:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
Dense.c:129:10: optimized: loop vectorized using 32 byte vectors
Dense.c:129:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:129:10: optimized: loop vectorized using 16 byte vectors
Dense.c:120:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
Dense.c:111:13: note: vectorized 2 loops in function.
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_28, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
Dense.c:111:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:111:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:111:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:111:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:111:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:111:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:111:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:111:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:90:23: missed: couldn't vectorize loop
Dense.c:92:31: missed: not vectorized: no vectype for stmt: _119 = this_40(D)->outputs;
 scalar_type: float *
Dense.c:90:23: missed: couldn't vectorize loop
Dense.c:90:23: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:95:16: missed: couldn't vectorize loop
Dense.c:95:16: missed: not vectorized: unsupported control flow in loop.
Dense.c:83:13: note: vectorized 0 loops in function.
Dense.c:86:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:97:16: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:97:16: note: SLPing BB part
Dense.c:97:16: note: Costing subgraph: 
Dense.c:97:16: note: node 0x89784b8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:97:16: note: op template: _16 = (sizetype) j_73;
Dense.c:97:16: note: 	stmt 0 _16 = (sizetype) j_73;
Dense.c:97:16: note: 	stmt 1 _111 = (sizetype) wi_74;
Dense.c:97:16: note: 	children 0x89785b8
Dense.c:97:16: note: node (external) 0x89785b8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:97:16: note: 	{ j_73, wi_74 }
Dense.c:97:16: note: Cost model analysis: 
Dense.c:97:16: note: Scalar 1 and vector 2 loop part do not match up, skipping scalar part
Dense.c:97:16: note: Cost model analysis for part in loop 2:
  Vector cost: 36
  Scalar cost: 8
Dense.c:97:16: missed: not vectorized: vectorization is not profitable.
Dense.c:97:16: note: ***** The result for vector mode V32QI would be the same
Dense.c:97:16: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:97:16: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:97:16: note: SLPing BB part
Dense.c:97:16: note: Costing subgraph: 
Dense.c:97:16: note: node 0x89784b8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:97:16: note: op template: _16 = (sizetype) j_73;
Dense.c:97:16: note: 	stmt 0 _16 = (sizetype) j_73;
Dense.c:97:16: note: 	stmt 1 _111 = (sizetype) wi_74;
Dense.c:97:16: note: 	children 0x8978538
Dense.c:97:16: note: node (external) 0x8978538 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:97:16: note: 	{ j_73, wi_74 }
Dense.c:97:16: note: Cost model analysis: 
Dense.c:97:16: note: Scalar 1 and vector 2 loop part do not match up, skipping scalar part
Dense.c:97:16: note: Cost model analysis for part in loop 2:
  Vector cost: 36
  Scalar cost: 8
Dense.c:97:16: missed: not vectorized: vectorization is not profitable.
Dense.c:97:16: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:90:50: note: ***** Analysis failed with vector mode V8QI
Dense.c:90:50: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:90:50: note: ***** Analysis failed with vector mode V4QI
Dense.c:62:15: missed: couldn't vectorize loop
Dense.c:62:15: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:75:11: optimized: loop vectorized using 32 byte vectors
Dense.c:75:11: optimized: loop vectorized using 16 byte vectors
Dense.c:66:11: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:881:10: missed: not vectorized: no vectype for stmt: _53 = MEM[(__m256 * {ref-all})w_ptr_82];
 scalar_type: __m256
Dense.c:54:13: note: vectorized 1 loops in function.
Dense.c:57:2: missed: statement clobbers memory: memcpy (_4, dense$baiases_63, _3);
Dense.c:79:2: missed: statement clobbers memory: _27 (pretmp_163, _68, 0);
Dense.c:54:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:54:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:154:107: note: ***** Analysis failed with vector mode V8SI
Dense.c:154:107: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
Dense.c:155:85: note: ***** Analysis failed with vector mode V8SI
Dense.c:155:85: note: ***** The result for vector mode V32QI would be the same
Dense.c:155:85: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:155:85: note: ***** Analysis failed with vector mode V16QI
Dense.c:155:85: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:155:85: note: ***** Analysis failed with vector mode V8QI
Dense.c:155:85: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:155:85: note: ***** Analysis failed with vector mode V4QI
Dense.c:156:105: note: ***** Analysis failed with vector mode V8SI
Dense.c:156:105: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
Dense.c:178:19: missed: couldn't vectorize loop
Dense.c:178:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:168:13: note: vectorized 0 loops in function.
Dense.c:179:38: missed: statement clobbers memory: _5 = rand ();
Dense.c:179:72: missed: statement clobbers memory: _69 = sqrt (_12);
Dense.c:192:19: missed: statement clobbers memory: _24 = PULSE_GetActivationFunctionPtr (_23);
Dense.c:195:34: missed: statement clobbers memory: _28 = aligned_alloc (64, _27);
Dense.c:211:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:212:4: missed: statement clobbers memory: exit (1);
Dense.c:216:9: note: ***** Analysis succeeded with vector mode V8SI
Dense.c:216:9: note: SLPing BB part
Dense.c:216:9: note: Costing subgraph: 
Dense.c:216:9: note: node 0x8927c58 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:216:9: note: op template: <retval>.n_inputs = args$n_inputs_45;
Dense.c:216:9: note: 	stmt 0 <retval>.n_inputs = args$n_inputs_45;
Dense.c:216:9: note: 	stmt 1 <retval>.n_outputs = args$n_outputs_46;
Dense.c:216:9: note: 	children 0x8927cd8
Dense.c:216:9: note: node 0x8927cd8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:216:9: note: op template: args$n_inputs_45 = args.n_inputs;
Dense.c:216:9: note: 	stmt 0 args$n_inputs_45 = args.n_inputs;
Dense.c:216:9: note: 	stmt 1 args$n_outputs_46 = args.n_outputs;
Dense.c:216:9: note: Cost model analysis: 
Dense.c:216:9: note: Cost model analysis for part in loop 0:
  Vector cost: 40
  Scalar cost: 56
Dense.c:216:9: note: Costing subgraph: 
Dense.c:216:9: note: node 0x8927dd8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:216:9: note: op template: <retval>.inputs = IO_34(D);
Dense.c:216:9: note: 	stmt 0 <retval>.inputs = IO_34(D);
Dense.c:216:9: note: 	stmt 1 <retval>.outputs = _22;
Dense.c:216:9: note: 	children 0x8927e58
Dense.c:216:9: note: node (external) 0x8927e58 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:216:9: note: 	{ IO_34(D), _22 }
Dense.c:216:9: note: Cost model analysis: 
Dense.c:216:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:216:9: note: Costing subgraph: 
Dense.c:216:9: note: node 0x8927f58 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:216:9: note: op template: <retval>.feed = layer$feed_33;
Dense.c:216:9: note: 	stmt 0 <retval>.feed = layer$feed_33;
Dense.c:216:9: note: 	stmt 1 <retval>.back = layer$back_43;
Dense.c:216:9: note: 	stmt 2 <retval>.fix = layer$fix_44;
Dense.c:216:9: note: 	stmt 3 <retval>.activate = _25;
Dense.c:216:9: note: 	children 0x8928058
Dense.c:216:9: note: node (external) 0x8928058 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:216:9: note: 	{ layer$feed_33, layer$back_43, layer$fix_44, _25 }
Dense.c:216:9: note: Cost model analysis: 
Dense.c:216:9: note: Cost model analysis for part in loop 0:
  Vector cost: 64
  Scalar cost: 64
Dense.c:216:9: note: Costing subgraph: 
Dense.c:216:9: note: node 0x8928158 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:216:9: note: op template: <retval>.parent = 0B;
Dense.c:216:9: note: 	stmt 0 <retval>.parent = 0B;
Dense.c:216:9: note: 	stmt 1 <retval>.child = 0B;
Dense.c:216:9: note: 	children 0x89281d8
Dense.c:216:9: note: node (constant) 0x89281d8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:216:9: note: 	{ 0B, 0B }
Dense.c:216:9: note: Cost model analysis: 
Dense.c:216:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:216:9: note: Costing subgraph: 
Dense.c:216:9: note: node 0x8928258 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:216:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_32(D);
Dense.c:216:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_32(D);
Dense.c:216:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&<retval> + 96B] = _4;
Dense.c:216:9: note: 	children 0x89282d8
Dense.c:216:9: note: node (external) 0x89282d8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:216:9: note: 	{ MODEL_32(D), _4 }
Dense.c:216:9: note: Cost model analysis: 
Dense.c:216:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:216:9: note: Basic block will be vectorized using SLP
Dense.c:216:9: optimized: basic block part vectorized using 8 byte vectors
Dense.c:216:9: note: Vectorizing SLP tree:
Dense.c:216:9: note: node 0x8927c58 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:216:9: note: op template: <retval>.n_inputs = args$n_inputs_45;
Dense.c:216:9: note: 	stmt 0 <retval>.n_inputs = args$n_inputs_45;
Dense.c:216:9: note: 	stmt 1 <retval>.n_outputs = args$n_outputs_46;
Dense.c:216:9: note: 	children 0x8927cd8
Dense.c:216:9: note: node 0x8927cd8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:216:9: note: op template: args$n_inputs_45 = args.n_inputs;
Dense.c:216:9: note: 	stmt 0 args$n_inputs_45 = args.n_inputs;
Dense.c:216:9: note: 	stmt 1 args$n_outputs_46 = args.n_outputs;
Dense.c:216:9: note: ------>vectorizing SLP node starting from: args$n_inputs_45 = args.n_inputs;
Dense.c:216:9: note: transform load. ncopies = 1
Dense.c:216:9: note: conflicting alias set types.
Dense.c:216:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: args.n_inputs
Dense.c:216:9: note: created &args
Dense.c:216:9: note: add new stmt: vect_args_n_inputs_45.484_9 = MEM <vector(2) unsigned int> [(void *)&args];
Dense.c:216:9: note: extracting lane for live stmt args$n_inputs_45 = args.n_inputs;
Dense.c:216:9: note: extracting lane for live stmt args$n_outputs_46 = args.n_outputs;
Dense.c:216:9: note: ------>vectorizing SLP node starting from: <retval>.n_inputs = args$n_inputs_45;
Dense.c:216:9: note: vect_is_simple_use: operand args.n_inputs, type of def: internal
Dense.c:216:9: note: vect_is_simple_use: operand args.n_outputs, type of def: internal
Dense.c:216:9: note: transform store. ncopies = 1
Dense.c:216:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: <retval>.n_inputs
Dense.c:216:9: note: created &<retval>.n_inputs
Dense.c:216:9: note: add new stmt: MEM <vector(2) unsigned int> [(unsigned int *)&<retval> + 128B] = vect_args_n_inputs_45.484_9;
Dense.c:216:9: note: vectorizing stmts using SLP.
Dense.c:216:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:216:9: note: Vectorizing SLP tree:
Dense.c:216:9: note: node 0x8927dd8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:216:9: note: op template: <retval>.inputs = IO_34(D);
Dense.c:216:9: note: 	stmt 0 <retval>.inputs = IO_34(D);
Dense.c:216:9: note: 	stmt 1 <retval>.outputs = _22;
Dense.c:216:9: note: 	children 0x8927e58
Dense.c:216:9: note: node (external) 0x8927e58 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:216:9: note: 	{ IO_34(D), _22 }
Dense.c:216:9: note: ------>vectorizing SLP node starting from: <retval>.inputs = IO_34(D);
Dense.c:216:9: note: vect_is_simple_use: operand IO_34(D) + _21, type of def: internal
Dense.c:216:9: note: transform store. ncopies = 1
Dense.c:216:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.inputs
Dense.c:216:9: note: created &<retval>
Dense.c:216:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval>] = _83;
Dense.c:216:9: note: vectorizing stmts using SLP.
Dense.c:216:9: optimized: basic block part vectorized using 32 byte vectors
Dense.c:216:9: note: Vectorizing SLP tree:
Dense.c:216:9: note: node 0x8927f58 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:216:9: note: op template: <retval>.feed = layer$feed_33;
Dense.c:216:9: note: 	stmt 0 <retval>.feed = layer$feed_33;
Dense.c:216:9: note: 	stmt 1 <retval>.back = layer$back_43;
Dense.c:216:9: note: 	stmt 2 <retval>.fix = layer$fix_44;
Dense.c:216:9: note: 	stmt 3 <retval>.activate = _25;
Dense.c:216:9: note: 	children 0x8928058
Dense.c:216:9: note: node (external) 0x8928058 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:216:9: note: 	{ layer$feed_33, layer$back_43, layer$fix_44, _25 }
Dense.c:216:9: note: ------>vectorizing SLP node starting from: <retval>.feed = layer$feed_33;
Dense.c:216:9: note: vect_is_simple_use: operand layer$back_43 = PHI <layer$back_49(D)(11), _BackDense(9), _SIMD_BackDense(15)>, type of def: internal
Dense.c:216:9: note: vect_is_simple_use: operand layer$fix_44 = PHI <layer$fix_50(D)(11), _FixDense(9), _SIMD_FixDense(15)>, type of def: internal
Dense.c:216:9: note: vect_is_simple_use: operand (void (*<T91a>) (float *, unsigned int, char)) _24, type of def: internal
Dense.c:216:9: note: conflicting alias set types.
Dense.c:216:9: note: transform store. ncopies = 1
Dense.c:216:9: note: create vector_type-pointer variable to type: vector(4) long unsigned int  vectorizing a pointer ref: <retval>.feed
Dense.c:216:9: note: created &<retval>.feed
Dense.c:216:9: note: add new stmt: MEM <vector(4) long unsigned int> [(void *)&<retval> + 32B] = _89;
Dense.c:216:9: note: vectorizing stmts using SLP.
Dense.c:216:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:216:9: note: Vectorizing SLP tree:
Dense.c:216:9: note: node 0x8928158 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:216:9: note: op template: <retval>.parent = 0B;
Dense.c:216:9: note: 	stmt 0 <retval>.parent = 0B;
Dense.c:216:9: note: 	stmt 1 <retval>.child = 0B;
Dense.c:216:9: note: 	children 0x89281d8
Dense.c:216:9: note: node (constant) 0x89281d8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:216:9: note: 	{ 0B, 0B }
Dense.c:216:9: note: ------>vectorizing SLP node starting from: <retval>.parent = 0B;
Dense.c:216:9: note: vect_is_simple_use: operand 0B, type of def: constant
Dense.c:216:9: note: transform store. ncopies = 1
Dense.c:216:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.parent
Dense.c:216:9: note: created &<retval>.parent
Dense.c:216:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer * *)&<retval> + 72B] = { 0, 0 };
Dense.c:216:9: note: vectorizing stmts using SLP.
Dense.c:216:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:216:9: note: Vectorizing SLP tree:
Dense.c:216:9: note: node 0x8928258 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:216:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_32(D);
Dense.c:216:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_32(D);
Dense.c:216:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&<retval> + 96B] = _4;
Dense.c:216:9: note: 	children 0x89282d8
Dense.c:216:9: note: node (external) 0x89282d8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:216:9: note: 	{ MODEL_32(D), _4 }
Dense.c:216:9: note: ------>vectorizing SLP node starting from: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_32(D);
Dense.c:216:9: note: vect_is_simple_use: operand MODEL_32(D) + _3, type of def: internal
Dense.c:216:9: note: transform store. ncopies = 1
Dense.c:216:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B]
Dense.c:216:9: note: created &MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B]
Dense.c:216:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer *)&<retval> + 88B] = _94;
Dense.c:216:9: note: vectorizing stmts using SLP.
Dense.c:216:9: note: ***** The result for vector mode V32QI would be the same
Activations.c:17:19: optimized: loop vectorized using 32 byte vectors
Activations.c:17:19: optimized: loop vectorized using 16 byte vectors
Activations.c:17:19: optimized: loop vectorized using 32 byte vectors
Activations.c:17:19: optimized: loop vectorized using 16 byte vectors
Activations.c:14:13: note: vectorized 2 loops in function.
Activations.c:14:13: note: ***** Analysis failed with vector mode V8SF
Activations.c:14:13: note: ***** The result for vector mode V32QI would be the same
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V16QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V16QI
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V8QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V8QI
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V4QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V4QI
Activations.c:28:19: missed: couldn't vectorize loop
Activations.c:28:19: missed: not vectorized: unsupported control flow in loop.
Activations.c:28:19: optimized: loop vectorized using 32 byte vectors
Activations.c:28:19: optimized: loop vectorized using 16 byte vectors
Activations.c:25:13: note: vectorized 1 loops in function.
Activations.c:25:13: note: ***** Analysis failed with vector mode V8SF
Activations.c:25:13: note: ***** The result for vector mode V32QI would be the same
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V16QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V16QI
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V8QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V8QI
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V4QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V4QI
Activations.c:38:1: note: ***** Analysis failed with vector mode VOID
Activations.c:5:19: missed: couldn't vectorize loop
Activations.c:9:21: missed: statement clobbers memory: _36 = expf (_18);
Activations.c:5:19: missed: couldn't vectorize loop
Activations.c:9:21: missed: statement clobbers memory: _12 = expf (_11);
Activations.c:3:13: note: vectorized 0 loops in function.
Activations.c:9:21: missed: statement clobbers memory: _12 = expf (_11);
Activations.c:9:21: missed: statement clobbers memory: _36 = expf (_18);
Activations.c:11:1: note: ***** Analysis failed with vector mode V8SF
Activations.c:11:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SF
Activations.c:44:2: note: ***** Analysis failed with vector mode VOID
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:4:18: note: vectorized 0 loops in function.
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_4, inputs_15, _3);
PULSE.c:8:2: missed: statement clobbers memory: _5 (layer_17);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_12, pretmp_26, _9);
PULSE.c:13:28: note: ***** Analysis failed with vector mode V8SI
PULSE.c:13:28: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:22:2: missed: statement clobbers memory: _1 (layer_10(D));
PULSE.c:22:2: missed: statement clobbers memory: _14 (_2);
PULSE.c:22:2: missed: statement clobbers memory: _21 (_15);
PULSE.c:22:2: missed: statement clobbers memory: _28 (_22);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_29);
PULSE.c:26:3: missed: statement clobbers memory: memset (_34, 0, _32);
PULSE.c:26:3: missed: statement clobbers memory: memset (_27, 0, _25);
PULSE.c:26:3: missed: statement clobbers memory: memset (_20, 0, _18);
PULSE.c:26:3: missed: statement clobbers memory: memset (_7, 0, _5);
PULSE.c:28:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:28:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:39: missed: statement clobbers memory: _7 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:32:6: note: vectorized 1 loops in function.
PULSE.c:34:8: missed: statement clobbers memory: _1 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_2);
PULSE.c:39:39: missed: statement clobbers memory: _7 = rand ();
PULSE.c:40:40: missed: statement clobbers memory: _9 = rand ();
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8SI
PULSE.c:32:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:67:16: missed: couldn't vectorize loop
PULSE.c:67:16: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:70:17: missed: couldn't vectorize loop
PULSE.c:70:17: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:79:19: missed: couldn't vectorize loop
PULSE.c:79:19: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:39: missed: statement clobbers memory: _84 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:56:22: missed: couldn't vectorize loop
PULSE.c:56:22: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:47:6: note: vectorized 1 loops in function.
PULSE.c:48:1: missed: statement clobbers memory: saved_stack.7_46 = __builtin_stack_save ();
PULSE.c:49:46: missed: statement clobbers memory: _1 = PULSE_GetLossFunctionPtr (loss_function_47(D));
PULSE.c:50:42: missed: statement clobbers memory: TMP_51 = aligned_alloc (64, _3);
PULSE.c:52:2: missed: statement clobbers memory: printf ("%d", model$fixes_size_55);
PULSE.c:58:14: missed: statement clobbers memory: _73 = _4 (output_208, TMP_PTR_207);
PULSE.c:66:15: missed: statement clobbers memory: random.4_57 = __builtin_alloca_with_align (_9, 32);
PULSE.c:34:8: missed: statement clobbers memory: _76 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_77);
PULSE.c:39:39: missed: statement clobbers memory: _84 = rand ();
PULSE.c:40:40: missed: statement clobbers memory: _87 = rand ();
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_113, inputs_108, _112);
PULSE.c:8:2: missed: statement clobbers memory: _114 (layer_109);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_120, _119, _118);
PULSE.c:73:11: missed: statement clobbers memory: loss_66 = PULSE_GetLoss_49 (_22, _21, _17, _16);
PULSE.c:22:2: missed: statement clobbers memory: _101 (output_209);
PULSE.c:22:2: missed: statement clobbers memory: _123 (_102);
PULSE.c:22:2: missed: statement clobbers memory: _130 (_124);
PULSE.c:22:2: missed: statement clobbers memory: _137 (_131);
PULSE.c:22:2: missed: statement clobbers memory: _144 (_138);
PULSE.c:22:2: missed: statement clobbers memory: _151 (_145);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_152);
PULSE.c:26:3: missed: statement clobbers memory: memset (_157, 0, _155);
PULSE.c:26:3: missed: statement clobbers memory: memset (_150, 0, _148);
PULSE.c:26:3: missed: statement clobbers memory: memset (_143, 0, _141);
PULSE.c:26:3: missed: statement clobbers memory: memset (_136, 0, _134);
PULSE.c:26:3: missed: statement clobbers memory: memset (_129, 0, _127);
PULSE.c:26:3: missed: statement clobbers memory: memset (_107, 0, _105);
PULSE.c:81:6: missed: statement clobbers memory: _26 (current_213, args);
PULSE.c:87:4: missed: statement clobbers memory: printf ("Epoch: %d | Item: %d | Loss: %.10f | Batch Loss: %.10f\r", i_210, j_211, _30, _29);
PULSE.c:47:6: missed: statement clobbers memory: __builtin_stack_restore (saved_stack.7_46);
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4DI
PULSE.c:47:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:97:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:97:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:131:20: missed: couldn't vectorize loop
PULSE.c:131:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:131:20: missed: couldn't vectorize loop
PULSE.c:131:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:110:20: missed: couldn't vectorize loop
PULSE.c:110:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:110:20: missed: couldn't vectorize loop
PULSE.c:110:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:99:13: note: vectorized 0 loops in function.
PULSE.c:102:39: missed: statement clobbers memory: layers_41 = malloc (_3);
PULSE.c:105:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:124:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:125:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:126:46: missed: statement clobbers memory: WEIGHTS_86 = aligned_alloc (64, 0);
PULSE.c:116:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.107_94];
PULSE.c:117:21: missed: statement clobbers memory: _75 = PULSE_GetDenseWeightsSize (args);
PULSE.c:118:16: missed: statement clobbers memory: _78 = PULSE_GetDenseIOSize (args);
PULSE.c:119:19: missed: statement clobbers memory: _81 = PULSE_GetDenseFixesSize (args);
PULSE.c:116:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})_224];
PULSE.c:117:21: missed: statement clobbers memory: _210 = PULSE_GetDenseWeightsSize (args);
PULSE.c:118:16: missed: statement clobbers memory: _213 = PULSE_GetDenseIOSize (args);
PULSE.c:119:19: missed: statement clobbers memory: _216 = PULSE_GetDenseFixesSize (args);
PULSE.c:124:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:125:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:126:46: missed: statement clobbers memory: WEIGHTS_46 = aligned_alloc (64, _5);
PULSE.c:127:41: missed: statement clobbers memory: IO_48 = aligned_alloc (64, _7);
PULSE.c:137:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.109_96];
PULSE.c:138:17: missed: statement clobbers memory: *_10 = PULSE_CreateDenseLayer (args, WEIGHTS_PTR_152, IO_PTR_153, 0B); [return slot optimization]
PULSE.c:139:20: missed: statement clobbers memory: _64 = PULSE_GetDenseWeightsSize (args);
PULSE.c:140:15: missed: statement clobbers memory: _67 = PULSE_GetDenseIOSize (args);
PULSE.c:137:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.109_124];
PULSE.c:138:17: missed: statement clobbers memory: *_106 = PULSE_CreateDenseLayer (args, WEIGHTS_PTR_148, IO_PTR_143, 0B); [return slot optimization]
PULSE.c:139:20: missed: statement clobbers memory: _95 = PULSE_GetDenseWeightsSize (args);
PULSE.c:140:15: missed: statement clobbers memory: _59 = PULSE_GetDenseIOSize (args);
PULSE.c:146:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:147:9: note: ***** Analysis succeeded with vector mode V8SI
PULSE.c:147:9: note: SLPing BB part
PULSE.c:147:9: note: Costing subgraph: 
PULSE.c:147:9: note: node 0x33924e8 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:147:9: note: op template: <retval>.weights_size = WEIGHTS_SIZE_164;
PULSE.c:147:9: note: 	stmt 0 <retval>.weights_size = WEIGHTS_SIZE_164;
PULSE.c:147:9: note: 	stmt 1 <retval>.io_size = IO_SIZE_167;
PULSE.c:147:9: note: 	children 0x3392568
PULSE.c:147:9: note: node 0x3392568 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:147:9: note: op template: WEIGHTS_SIZE_164 = PHI <WEIGHTS_SIZE_161(36), 0(25)>
PULSE.c:147:9: note: 	stmt 0 WEIGHTS_SIZE_164 = PHI <WEIGHTS_SIZE_161(36), 0(25)>
PULSE.c:147:9: note: 	stmt 1 IO_SIZE_167 = PHI <IO_SIZE_160(36), 0(25)>
PULSE.c:147:9: note: 	children 0x33925e8 0x3392de8
PULSE.c:147:9: note: node 0x33925e8 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:147:9: note: op template: WEIGHTS_SIZE_161 = PHI <WEIGHTS_SIZE_20(11), WEIGHTS_SIZE_206(76)>
PULSE.c:147:9: note: 	stmt 0 WEIGHTS_SIZE_161 = PHI <WEIGHTS_SIZE_20(11), WEIGHTS_SIZE_206(76)>
PULSE.c:147:9: note: 	stmt 1 IO_SIZE_160 = PHI <IO_SIZE_22(11), IO_SIZE_192(76)>
PULSE.c:147:9: note: 	children 0x3392668 0x33929e8
PULSE.c:147:9: note: node 0x3392668 (max_nunits=2, refcnt=3) vector(2) unsigned int
PULSE.c:147:9: note: op template: WEIGHTS_SIZE_20 = PHI <WEIGHTS_SIZE_73(77), WEIGHTS_SIZE_76(10)>
PULSE.c:147:9: note: 	stmt 0 WEIGHTS_SIZE_20 = PHI <WEIGHTS_SIZE_73(77), WEIGHTS_SIZE_76(10)>
PULSE.c:147:9: note: 	stmt 1 IO_SIZE_22 = PHI <IO_SIZE_147(77), IO_SIZE_79(10)>
PULSE.c:147:9: note: 	children 0x33926e8 0x33927e8
PULSE.c:147:9: note: node 0x33926e8 (max_nunits=2, refcnt=2) vector(2) unsigned int
PULSE.c:147:9: note: op template: WEIGHTS_SIZE_73 = PHI <WEIGHTS_SIZE_20(73), 0(71)>
PULSE.c:147:9: note: 	stmt 0 WEIGHTS_SIZE_73 = PHI <WEIGHTS_SIZE_20(73), 0(71)>
PULSE.c:147:9: note: 	stmt 1 IO_SIZE_147 = PHI <IO_SIZE_22(73), 0(71)>
PULSE.c:147:9: note: 	children 0x3392668 0x3392768
PULSE.c:147:9: note: node (constant) 0x3392768 (max_nunits=1, refcnt=1) vector(2) unsigned int
PULSE.c:147:9: note: 	{ 0, 0 }
PULSE.c:147:9: note: node 0x33927e8 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:147:9: note: op template: WEIGHTS_SIZE_76 = WEIGHTS_SIZE_73 + _75;
PULSE.c:147:9: note: 	stmt 0 WEIGHTS_SIZE_76 = WEIGHTS_SIZE_73 + _75;
PULSE.c:147:9: note: 	stmt 1 IO_SIZE_79 = _78 + IO_SIZE_147;
PULSE.c:147:9: note: 	children 0x33926e8 0x3392968
PULSE.c:147:9: note: node (external) 0x3392968 (max_nunits=1, refcnt=1) vector(2) unsigned int
PULSE.c:147:9: note: 	{ _75, _78 }
PULSE.c:147:9: note: node 0x33929e8 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:147:9: note: op template: WEIGHTS_SIZE_206 = PHI <WEIGHTS_SIZE_238(68)>
PULSE.c:147:9: note: 	stmt 0 WEIGHTS_SIZE_206 = PHI <WEIGHTS_SIZE_238(68)>
PULSE.c:147:9: note: 	stmt 1 IO_SIZE_192 = PHI <IO_SIZE_239(68)>
PULSE.c:147:9: note: 	children 0x3392a68
PULSE.c:147:9: note: node 0x3392a68 (max_nunits=2, refcnt=2) vector(2) unsigned int
PULSE.c:147:9: note: op template: WEIGHTS_SIZE_238 = PHI <WEIGHTS_SIZE_211(59), WEIGHTS_SIZE_200(57)>
PULSE.c:147:9: note: 	stmt 0 WEIGHTS_SIZE_238 = PHI <WEIGHTS_SIZE_211(59), WEIGHTS_SIZE_200(57)>
PULSE.c:147:9: note: 	stmt 1 IO_SIZE_239 = PHI <IO_SIZE_214(59), IO_SIZE_201(57)>
PULSE.c:147:9: note: 	children 0x3392ae8 0x3392be8
PULSE.c:147:9: note: node 0x3392ae8 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:147:9: note: op template: WEIGHTS_SIZE_211 = WEIGHTS_SIZE_200 + _210;
PULSE.c:147:9: note: 	stmt 0 WEIGHTS_SIZE_211 = WEIGHTS_SIZE_200 + _210;
PULSE.c:147:9: note: 	stmt 1 IO_SIZE_214 = _213 + IO_SIZE_201;
PULSE.c:147:9: note: 	children 0x3392be8 0x3392d68
PULSE.c:147:9: note: node 0x3392be8 (max_nunits=2, refcnt=2) vector(2) unsigned int
PULSE.c:147:9: note: op template: WEIGHTS_SIZE_200 = PHI <WEIGHTS_SIZE_243(72), WEIGHTS_SIZE_238(69)>
PULSE.c:147:9: note: 	stmt 0 WEIGHTS_SIZE_200 = PHI <WEIGHTS_SIZE_243(72), WEIGHTS_SIZE_238(69)>
PULSE.c:147:9: note: 	stmt 1 IO_SIZE_201 = PHI <IO_SIZE_244(72), IO_SIZE_239(69)>
PULSE.c:147:9: note: 	children 0x3392c68 0x3392a68
PULSE.c:147:9: note: node 0x3392c68 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:147:9: note: op template: WEIGHTS_SIZE_243 = PHI <WEIGHTS_SIZE_20(74)>
PULSE.c:147:9: note: 	stmt 0 WEIGHTS_SIZE_243 = PHI <WEIGHTS_SIZE_20(74)>
PULSE.c:147:9: note: 	stmt 1 IO_SIZE_244 = PHI <IO_SIZE_22(74)>
PULSE.c:147:9: note: 	children 0x3392668
PULSE.c:147:9: note: node (external) 0x3392d68 (max_nunits=1, refcnt=1) vector(2) unsigned int
PULSE.c:147:9: note: 	{ _210, _213 }
PULSE.c:147:9: note: node (constant) 0x3392de8 (max_nunits=1, refcnt=1) vector(2) unsigned int
PULSE.c:147:9: note: 	{ 0, 0 }
PULSE.c:147:9: note: Cost model analysis: 
PULSE.c:147:9: note: Cost model analysis for part in loop 0:
  Vector cost: 48
  Scalar cost: 48
PULSE.c:147:9: note: Cost model analysis for part in loop 1:
  Vector cost: 44
  Scalar cost: 24
PULSE.c:147:9: missed: not vectorized: vectorization is not profitable.
PULSE.c:147:9: note: Costing subgraph: 
PULSE.c:147:9: note: node 0x3392ee8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:147:9: note: op template: <retval>.layers = layers_41;
PULSE.c:147:9: note: 	stmt 0 <retval>.layers = layers_41;
PULSE.c:147:9: note: 	stmt 1 <retval>.weights = WEIGHTS_163;
PULSE.c:147:9: note: 	children 0x3392fe8
PULSE.c:147:9: note: node (external) 0x3392fe8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:147:9: note: 	{ layers_41, WEIGHTS_163 }
PULSE.c:147:9: note: Cost model analysis: 
PULSE.c:147:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:147:9: note: Costing subgraph: 
PULSE.c:147:9: note: node 0x3393068 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:147:9: note: op template: <retval>.io = 0B;
PULSE.c:147:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:147:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:147:9: note: 	children 0x33930e8
PULSE.c:147:9: note: node (constant) 0x33930e8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:147:9: note: 	{ 0B, 0B }
PULSE.c:147:9: note: Cost model analysis: 
PULSE.c:147:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:147:9: note: Basic block will be vectorized using SLP
PULSE.c:147:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:147:9: note: Vectorizing SLP tree:
PULSE.c:147:9: note: node 0x3392ee8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:147:9: note: op template: <retval>.layers = layers_41;
PULSE.c:147:9: note: 	stmt 0 <retval>.layers = layers_41;
PULSE.c:147:9: note: 	stmt 1 <retval>.weights = WEIGHTS_163;
PULSE.c:147:9: note: 	children 0x3392fe8
PULSE.c:147:9: note: node (external) 0x3392fe8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:147:9: note: 	{ layers_41, WEIGHTS_163 }
PULSE.c:147:9: note: ------>vectorizing SLP node starting from: <retval>.layers = layers_41;
PULSE.c:147:9: note: vect_is_simple_use: operand WEIGHTS_163 = PHI <WEIGHTS_46(36), WEIGHTS_86(25)>, type of def: internal
PULSE.c:147:9: note: conflicting alias set types.
PULSE.c:147:9: note: transform store. ncopies = 1
PULSE.c:147:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.layers
PULSE.c:147:9: note: created &<retval>
PULSE.c:147:9: note: add new stmt: MEM <vector(2) long unsigned int> [(void *)&<retval>] = _19;
PULSE.c:147:9: note: vectorizing stmts using SLP.
PULSE.c:147:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:147:9: note: Vectorizing SLP tree:
PULSE.c:147:9: note: node 0x3393068 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:147:9: note: op template: <retval>.io = 0B;
PULSE.c:147:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:147:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:147:9: note: 	children 0x33930e8
PULSE.c:147:9: note: node (constant) 0x33930e8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:147:9: note: 	{ 0B, 0B }
PULSE.c:147:9: note: ------>vectorizing SLP node starting from: <retval>.io = 0B;
PULSE.c:147:9: note: vect_is_simple_use: operand 0B, type of def: constant
PULSE.c:147:9: note: transform store. ncopies = 1
PULSE.c:147:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.io
PULSE.c:147:9: note: created &<retval>.io
PULSE.c:147:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval> + 16B] = { 0, 0 };
PULSE.c:147:9: note: vectorizing stmts using SLP.
PULSE.c:147:9: note: ***** The result for vector mode V32QI would be the same
PULSE.c:153:2: missed: statement clobbers memory: free (_1);
PULSE.c:154:2: missed: statement clobbers memory: free (_2);
PULSE.c:155:2: missed: statement clobbers memory: free (_3);
PULSE.c:156:2: missed: statement clobbers memory: free (_4);
PULSE.c:157:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:157:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Loss.c:15:19: optimized: loop vectorized using 32 byte vectors
Loss.c:15:19: optimized:  loop versioned for vectorization because of possible aliasing
Loss.c:15:19: optimized: loop vectorized using 16 byte vectors
Loss.c:12:23: note: vectorized 1 loops in function.
Loss.c:12:23: note: ***** Analysis failed with vector mode V8SF
Loss.c:12:23: note: ***** The result for vector mode V32QI would be the same
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V16QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V16QI
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V8QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V8QI
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V4QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V4QI
Loss.c:7:19: missed: couldn't vectorize loop
Loss.c:8:38: missed: not vectorized: unsupported use in stmt.
Loss.c:4:23: note: vectorized 0 loops in function.
Loss.c:9:13: note: ***** Analysis failed with vector mode V8SF
Loss.c:9:13: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SF
Loss.c:23:2: note: ***** Analysis failed with vector mode VOID
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:4:18: note: vectorized 0 loops in function.
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_4, inputs_15, _3);
PULSE.c:8:2: missed: statement clobbers memory: _5 (layer_17);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_12, pretmp_26, _9);
PULSE.c:13:28: note: ***** Analysis failed with vector mode V8SI
PULSE.c:13:28: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:22:2: missed: statement clobbers memory: _1 (layer_10(D));
PULSE.c:22:2: missed: statement clobbers memory: _14 (_2);
PULSE.c:22:2: missed: statement clobbers memory: _21 (_15);
PULSE.c:22:2: missed: statement clobbers memory: _28 (_22);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_29);
PULSE.c:26:3: missed: statement clobbers memory: memset (_34, 0, _32);
PULSE.c:26:3: missed: statement clobbers memory: memset (_27, 0, _25);
PULSE.c:26:3: missed: statement clobbers memory: memset (_20, 0, _18);
PULSE.c:26:3: missed: statement clobbers memory: memset (_7, 0, _5);
PULSE.c:28:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:28:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:39: missed: statement clobbers memory: _7 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:32:6: note: vectorized 1 loops in function.
PULSE.c:34:8: missed: statement clobbers memory: _1 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_2);
PULSE.c:39:39: missed: statement clobbers memory: _7 = rand ();
PULSE.c:40:40: missed: statement clobbers memory: _9 = rand ();
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8SI
PULSE.c:32:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:67:16: missed: couldn't vectorize loop
PULSE.c:67:16: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:70:17: missed: couldn't vectorize loop
PULSE.c:70:17: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:79:19: missed: couldn't vectorize loop
PULSE.c:79:19: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:39: missed: statement clobbers memory: _84 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:56:22: missed: couldn't vectorize loop
PULSE.c:56:22: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:47:6: note: vectorized 1 loops in function.
PULSE.c:48:1: missed: statement clobbers memory: saved_stack.7_46 = __builtin_stack_save ();
PULSE.c:49:46: missed: statement clobbers memory: _1 = PULSE_GetLossFunctionPtr (loss_function_47(D));
PULSE.c:50:42: missed: statement clobbers memory: TMP_51 = aligned_alloc (64, _3);
PULSE.c:52:2: missed: statement clobbers memory: printf ("\n%d\n", model$fixes_size_55);
PULSE.c:58:14: missed: statement clobbers memory: _73 = _4 (output_208, TMP_PTR_207);
PULSE.c:66:15: missed: statement clobbers memory: random.4_57 = __builtin_alloca_with_align (_9, 32);
PULSE.c:34:8: missed: statement clobbers memory: _76 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_77);
PULSE.c:39:39: missed: statement clobbers memory: _84 = rand ();
PULSE.c:40:40: missed: statement clobbers memory: _87 = rand ();
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_113, inputs_108, _112);
PULSE.c:8:2: missed: statement clobbers memory: _114 (layer_109);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_120, _119, _118);
PULSE.c:73:11: missed: statement clobbers memory: loss_66 = PULSE_GetLoss_49 (_22, _21, _17, _16);
PULSE.c:22:2: missed: statement clobbers memory: _101 (output_209);
PULSE.c:22:2: missed: statement clobbers memory: _123 (_102);
PULSE.c:22:2: missed: statement clobbers memory: _130 (_124);
PULSE.c:22:2: missed: statement clobbers memory: _137 (_131);
PULSE.c:22:2: missed: statement clobbers memory: _144 (_138);
PULSE.c:22:2: missed: statement clobbers memory: _151 (_145);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_152);
PULSE.c:26:3: missed: statement clobbers memory: memset (_157, 0, _155);
PULSE.c:26:3: missed: statement clobbers memory: memset (_150, 0, _148);
PULSE.c:26:3: missed: statement clobbers memory: memset (_143, 0, _141);
PULSE.c:26:3: missed: statement clobbers memory: memset (_136, 0, _134);
PULSE.c:26:3: missed: statement clobbers memory: memset (_129, 0, _127);
PULSE.c:26:3: missed: statement clobbers memory: memset (_107, 0, _105);
PULSE.c:81:6: missed: statement clobbers memory: _26 (current_213, args);
PULSE.c:87:4: missed: statement clobbers memory: printf ("Epoch: %d | Item: %d | Loss: %.10f | Batch Loss: %.10f\r", i_210, j_211, _30, _29);
PULSE.c:47:6: missed: statement clobbers memory: __builtin_stack_restore (saved_stack.7_46);
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4DI
PULSE.c:47:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:97:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:97:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:131:20: missed: couldn't vectorize loop
PULSE.c:131:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:131:20: missed: couldn't vectorize loop
PULSE.c:131:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:110:20: missed: couldn't vectorize loop
PULSE.c:110:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:110:20: missed: couldn't vectorize loop
PULSE.c:110:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:99:13: note: vectorized 0 loops in function.
PULSE.c:102:39: missed: statement clobbers memory: layers_41 = malloc (_3);
PULSE.c:105:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:124:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:125:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:126:46: missed: statement clobbers memory: WEIGHTS_86 = aligned_alloc (64, 0);
PULSE.c:116:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.107_94];
PULSE.c:117:21: missed: statement clobbers memory: _75 = PULSE_GetDenseWeightsSize (args);
PULSE.c:118:16: missed: statement clobbers memory: _78 = PULSE_GetDenseIOSize (args);
PULSE.c:119:19: missed: statement clobbers memory: _81 = PULSE_GetDenseFixesSize (args);
PULSE.c:116:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})_224];
PULSE.c:117:21: missed: statement clobbers memory: _210 = PULSE_GetDenseWeightsSize (args);
PULSE.c:118:16: missed: statement clobbers memory: _213 = PULSE_GetDenseIOSize (args);
PULSE.c:119:19: missed: statement clobbers memory: _216 = PULSE_GetDenseFixesSize (args);
PULSE.c:124:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:125:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:126:46: missed: statement clobbers memory: WEIGHTS_46 = aligned_alloc (64, _5);
PULSE.c:127:41: missed: statement clobbers memory: IO_48 = aligned_alloc (64, _7);
PULSE.c:137:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.109_96];
PULSE.c:138:17: missed: statement clobbers memory: *_10 = PULSE_CreateDenseLayer (args, WEIGHTS_PTR_152, IO_PTR_153, 0B); [return slot optimization]
PULSE.c:139:20: missed: statement clobbers memory: _64 = PULSE_GetDenseWeightsSize (args);
PULSE.c:140:15: missed: statement clobbers memory: _67 = PULSE_GetDenseIOSize (args);
PULSE.c:137:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.109_124];
PULSE.c:138:17: missed: statement clobbers memory: *_106 = PULSE_CreateDenseLayer (args, WEIGHTS_PTR_148, IO_PTR_143, 0B); [return slot optimization]
PULSE.c:139:20: missed: statement clobbers memory: _95 = PULSE_GetDenseWeightsSize (args);
PULSE.c:140:15: missed: statement clobbers memory: _59 = PULSE_GetDenseIOSize (args);
PULSE.c:146:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:147:9: note: ***** Analysis succeeded with vector mode V8SI
PULSE.c:147:9: note: SLPing BB part
PULSE.c:147:9: note: Costing subgraph: 
PULSE.c:147:9: note: node 0x3e854f8 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:147:9: note: op template: <retval>.weights_size = WEIGHTS_SIZE_164;
PULSE.c:147:9: note: 	stmt 0 <retval>.weights_size = WEIGHTS_SIZE_164;
PULSE.c:147:9: note: 	stmt 1 <retval>.io_size = IO_SIZE_167;
PULSE.c:147:9: note: 	children 0x3e85578
PULSE.c:147:9: note: node 0x3e85578 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:147:9: note: op template: WEIGHTS_SIZE_164 = PHI <WEIGHTS_SIZE_161(36), 0(25)>
PULSE.c:147:9: note: 	stmt 0 WEIGHTS_SIZE_164 = PHI <WEIGHTS_SIZE_161(36), 0(25)>
PULSE.c:147:9: note: 	stmt 1 IO_SIZE_167 = PHI <IO_SIZE_160(36), 0(25)>
PULSE.c:147:9: note: 	children 0x3e855f8 0x3e85df8
PULSE.c:147:9: note: node 0x3e855f8 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:147:9: note: op template: WEIGHTS_SIZE_161 = PHI <WEIGHTS_SIZE_20(11), WEIGHTS_SIZE_206(76)>
PULSE.c:147:9: note: 	stmt 0 WEIGHTS_SIZE_161 = PHI <WEIGHTS_SIZE_20(11), WEIGHTS_SIZE_206(76)>
PULSE.c:147:9: note: 	stmt 1 IO_SIZE_160 = PHI <IO_SIZE_22(11), IO_SIZE_192(76)>
PULSE.c:147:9: note: 	children 0x3e85678 0x3e859f8
PULSE.c:147:9: note: node 0x3e85678 (max_nunits=2, refcnt=3) vector(2) unsigned int
PULSE.c:147:9: note: op template: WEIGHTS_SIZE_20 = PHI <WEIGHTS_SIZE_73(77), WEIGHTS_SIZE_76(10)>
PULSE.c:147:9: note: 	stmt 0 WEIGHTS_SIZE_20 = PHI <WEIGHTS_SIZE_73(77), WEIGHTS_SIZE_76(10)>
PULSE.c:147:9: note: 	stmt 1 IO_SIZE_22 = PHI <IO_SIZE_147(77), IO_SIZE_79(10)>
PULSE.c:147:9: note: 	children 0x3e856f8 0x3e857f8
PULSE.c:147:9: note: node 0x3e856f8 (max_nunits=2, refcnt=2) vector(2) unsigned int
PULSE.c:147:9: note: op template: WEIGHTS_SIZE_73 = PHI <WEIGHTS_SIZE_20(73), 0(71)>
PULSE.c:147:9: note: 	stmt 0 WEIGHTS_SIZE_73 = PHI <WEIGHTS_SIZE_20(73), 0(71)>
PULSE.c:147:9: note: 	stmt 1 IO_SIZE_147 = PHI <IO_SIZE_22(73), 0(71)>
PULSE.c:147:9: note: 	children 0x3e85678 0x3e85778
PULSE.c:147:9: note: node (constant) 0x3e85778 (max_nunits=1, refcnt=1) vector(2) unsigned int
PULSE.c:147:9: note: 	{ 0, 0 }
PULSE.c:147:9: note: node 0x3e857f8 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:147:9: note: op template: WEIGHTS_SIZE_76 = WEIGHTS_SIZE_73 + _75;
PULSE.c:147:9: note: 	stmt 0 WEIGHTS_SIZE_76 = WEIGHTS_SIZE_73 + _75;
PULSE.c:147:9: note: 	stmt 1 IO_SIZE_79 = _78 + IO_SIZE_147;
PULSE.c:147:9: note: 	children 0x3e856f8 0x3e85978
PULSE.c:147:9: note: node (external) 0x3e85978 (max_nunits=1, refcnt=1) vector(2) unsigned int
PULSE.c:147:9: note: 	{ _75, _78 }
PULSE.c:147:9: note: node 0x3e859f8 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:147:9: note: op template: WEIGHTS_SIZE_206 = PHI <WEIGHTS_SIZE_238(68)>
PULSE.c:147:9: note: 	stmt 0 WEIGHTS_SIZE_206 = PHI <WEIGHTS_SIZE_238(68)>
PULSE.c:147:9: note: 	stmt 1 IO_SIZE_192 = PHI <IO_SIZE_239(68)>
PULSE.c:147:9: note: 	children 0x3e85a78
PULSE.c:147:9: note: node 0x3e85a78 (max_nunits=2, refcnt=2) vector(2) unsigned int
PULSE.c:147:9: note: op template: WEIGHTS_SIZE_238 = PHI <WEIGHTS_SIZE_211(59), WEIGHTS_SIZE_200(57)>
PULSE.c:147:9: note: 	stmt 0 WEIGHTS_SIZE_238 = PHI <WEIGHTS_SIZE_211(59), WEIGHTS_SIZE_200(57)>
PULSE.c:147:9: note: 	stmt 1 IO_SIZE_239 = PHI <IO_SIZE_214(59), IO_SIZE_201(57)>
PULSE.c:147:9: note: 	children 0x3e85af8 0x3e85bf8
PULSE.c:147:9: note: node 0x3e85af8 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:147:9: note: op template: WEIGHTS_SIZE_211 = WEIGHTS_SIZE_200 + _210;
PULSE.c:147:9: note: 	stmt 0 WEIGHTS_SIZE_211 = WEIGHTS_SIZE_200 + _210;
PULSE.c:147:9: note: 	stmt 1 IO_SIZE_214 = _213 + IO_SIZE_201;
PULSE.c:147:9: note: 	children 0x3e85bf8 0x3e85d78
PULSE.c:147:9: note: node 0x3e85bf8 (max_nunits=2, refcnt=2) vector(2) unsigned int
PULSE.c:147:9: note: op template: WEIGHTS_SIZE_200 = PHI <WEIGHTS_SIZE_243(72), WEIGHTS_SIZE_238(69)>
PULSE.c:147:9: note: 	stmt 0 WEIGHTS_SIZE_200 = PHI <WEIGHTS_SIZE_243(72), WEIGHTS_SIZE_238(69)>
PULSE.c:147:9: note: 	stmt 1 IO_SIZE_201 = PHI <IO_SIZE_244(72), IO_SIZE_239(69)>
PULSE.c:147:9: note: 	children 0x3e85c78 0x3e85a78
PULSE.c:147:9: note: node 0x3e85c78 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:147:9: note: op template: WEIGHTS_SIZE_243 = PHI <WEIGHTS_SIZE_20(74)>
PULSE.c:147:9: note: 	stmt 0 WEIGHTS_SIZE_243 = PHI <WEIGHTS_SIZE_20(74)>
PULSE.c:147:9: note: 	stmt 1 IO_SIZE_244 = PHI <IO_SIZE_22(74)>
PULSE.c:147:9: note: 	children 0x3e85678
PULSE.c:147:9: note: node (external) 0x3e85d78 (max_nunits=1, refcnt=1) vector(2) unsigned int
PULSE.c:147:9: note: 	{ _210, _213 }
PULSE.c:147:9: note: node (constant) 0x3e85df8 (max_nunits=1, refcnt=1) vector(2) unsigned int
PULSE.c:147:9: note: 	{ 0, 0 }
PULSE.c:147:9: note: Cost model analysis: 
PULSE.c:147:9: note: Cost model analysis for part in loop 0:
  Vector cost: 48
  Scalar cost: 48
PULSE.c:147:9: note: Cost model analysis for part in loop 1:
  Vector cost: 44
  Scalar cost: 24
PULSE.c:147:9: missed: not vectorized: vectorization is not profitable.
PULSE.c:147:9: note: Costing subgraph: 
PULSE.c:147:9: note: node 0x3e85ef8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:147:9: note: op template: <retval>.layers = layers_41;
PULSE.c:147:9: note: 	stmt 0 <retval>.layers = layers_41;
PULSE.c:147:9: note: 	stmt 1 <retval>.weights = WEIGHTS_163;
PULSE.c:147:9: note: 	children 0x3e85ff8
PULSE.c:147:9: note: node (external) 0x3e85ff8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:147:9: note: 	{ layers_41, WEIGHTS_163 }
PULSE.c:147:9: note: Cost model analysis: 
PULSE.c:147:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:147:9: note: Costing subgraph: 
PULSE.c:147:9: note: node 0x3e86078 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:147:9: note: op template: <retval>.io = 0B;
PULSE.c:147:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:147:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:147:9: note: 	children 0x3e860f8
PULSE.c:147:9: note: node (constant) 0x3e860f8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:147:9: note: 	{ 0B, 0B }
PULSE.c:147:9: note: Cost model analysis: 
PULSE.c:147:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:147:9: note: Basic block will be vectorized using SLP
PULSE.c:147:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:147:9: note: Vectorizing SLP tree:
PULSE.c:147:9: note: node 0x3e85ef8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:147:9: note: op template: <retval>.layers = layers_41;
PULSE.c:147:9: note: 	stmt 0 <retval>.layers = layers_41;
PULSE.c:147:9: note: 	stmt 1 <retval>.weights = WEIGHTS_163;
PULSE.c:147:9: note: 	children 0x3e85ff8
PULSE.c:147:9: note: node (external) 0x3e85ff8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:147:9: note: 	{ layers_41, WEIGHTS_163 }
PULSE.c:147:9: note: ------>vectorizing SLP node starting from: <retval>.layers = layers_41;
PULSE.c:147:9: note: vect_is_simple_use: operand WEIGHTS_163 = PHI <WEIGHTS_46(36), WEIGHTS_86(25)>, type of def: internal
PULSE.c:147:9: note: conflicting alias set types.
PULSE.c:147:9: note: transform store. ncopies = 1
PULSE.c:147:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.layers
PULSE.c:147:9: note: created &<retval>
PULSE.c:147:9: note: add new stmt: MEM <vector(2) long unsigned int> [(void *)&<retval>] = _19;
PULSE.c:147:9: note: vectorizing stmts using SLP.
PULSE.c:147:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:147:9: note: Vectorizing SLP tree:
PULSE.c:147:9: note: node 0x3e86078 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:147:9: note: op template: <retval>.io = 0B;
PULSE.c:147:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:147:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:147:9: note: 	children 0x3e860f8
PULSE.c:147:9: note: node (constant) 0x3e860f8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:147:9: note: 	{ 0B, 0B }
PULSE.c:147:9: note: ------>vectorizing SLP node starting from: <retval>.io = 0B;
PULSE.c:147:9: note: vect_is_simple_use: operand 0B, type of def: constant
PULSE.c:147:9: note: transform store. ncopies = 1
PULSE.c:147:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.io
PULSE.c:147:9: note: created &<retval>.io
PULSE.c:147:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval> + 16B] = { 0, 0 };
PULSE.c:147:9: note: vectorizing stmts using SLP.
PULSE.c:147:9: note: ***** The result for vector mode V32QI would be the same
PULSE.c:153:2: missed: statement clobbers memory: free (_1);
PULSE.c:154:2: missed: statement clobbers memory: free (_2);
PULSE.c:155:2: missed: statement clobbers memory: free (_3);
PULSE.c:156:2: missed: statement clobbers memory: free (_4);
PULSE.c:157:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:157:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _28 (pretmp_91, _51, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:19: missed: not vectorized: no vectype for stmt: _19 = *_18;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:165:10: note: ***** Analysis failed with vector mode V4DI
Dense.c:165:10: note: ***** The result for vector mode V32QI would be the same
Dense.c:165:10: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:165:10: note: ***** Analysis failed with vector mode V16QI
Dense.c:165:10: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:165:10: note: ***** Analysis failed with vector mode V8QI
Dense.c:165:10: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:165:10: note: ***** Analysis failed with vector mode V4QI
Dense.c:145:10: optimized: loop vectorized using 32 byte vectors
Dense.c:145:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:145:10: optimized: loop vectorized using 16 byte vectors
Dense.c:136:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
Dense.c:129:10: optimized: loop vectorized using 32 byte vectors
Dense.c:129:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:129:10: optimized: loop vectorized using 16 byte vectors
Dense.c:120:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
Dense.c:111:13: note: vectorized 2 loops in function.
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_28, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
Dense.c:111:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:111:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:111:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:111:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:111:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:111:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:111:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:111:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:90:23: missed: couldn't vectorize loop
Dense.c:92:31: missed: not vectorized: no vectype for stmt: _119 = this_40(D)->outputs;
 scalar_type: float *
Dense.c:90:23: missed: couldn't vectorize loop
Dense.c:90:23: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:95:16: missed: couldn't vectorize loop
Dense.c:95:16: missed: not vectorized: unsupported control flow in loop.
Dense.c:83:13: note: vectorized 0 loops in function.
Dense.c:86:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:97:16: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:97:16: note: SLPing BB part
Dense.c:97:16: note: Costing subgraph: 
Dense.c:97:16: note: node 0x7c62028 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:97:16: note: op template: _16 = (sizetype) j_73;
Dense.c:97:16: note: 	stmt 0 _16 = (sizetype) j_73;
Dense.c:97:16: note: 	stmt 1 _111 = (sizetype) wi_74;
Dense.c:97:16: note: 	children 0x7c62128
Dense.c:97:16: note: node (external) 0x7c62128 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:97:16: note: 	{ j_73, wi_74 }
Dense.c:97:16: note: Cost model analysis: 
Dense.c:97:16: note: Scalar 1 and vector 2 loop part do not match up, skipping scalar part
Dense.c:97:16: note: Cost model analysis for part in loop 2:
  Vector cost: 36
  Scalar cost: 8
Dense.c:97:16: missed: not vectorized: vectorization is not profitable.
Dense.c:97:16: note: ***** The result for vector mode V32QI would be the same
Dense.c:97:16: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:97:16: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:97:16: note: SLPing BB part
Dense.c:97:16: note: Costing subgraph: 
Dense.c:97:16: note: node 0x7c62028 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:97:16: note: op template: _16 = (sizetype) j_73;
Dense.c:97:16: note: 	stmt 0 _16 = (sizetype) j_73;
Dense.c:97:16: note: 	stmt 1 _111 = (sizetype) wi_74;
Dense.c:97:16: note: 	children 0x7c620a8
Dense.c:97:16: note: node (external) 0x7c620a8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:97:16: note: 	{ j_73, wi_74 }
Dense.c:97:16: note: Cost model analysis: 
Dense.c:97:16: note: Scalar 1 and vector 2 loop part do not match up, skipping scalar part
Dense.c:97:16: note: Cost model analysis for part in loop 2:
  Vector cost: 36
  Scalar cost: 8
Dense.c:97:16: missed: not vectorized: vectorization is not profitable.
Dense.c:97:16: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:90:50: note: ***** Analysis failed with vector mode V8QI
Dense.c:90:50: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:90:50: note: ***** Analysis failed with vector mode V4QI
Dense.c:62:15: missed: couldn't vectorize loop
Dense.c:62:15: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:75:11: optimized: loop vectorized using 32 byte vectors
Dense.c:75:11: optimized: loop vectorized using 16 byte vectors
Dense.c:66:11: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:881:10: missed: not vectorized: no vectype for stmt: _53 = MEM[(__m256 * {ref-all})w_ptr_82];
 scalar_type: __m256
Dense.c:54:13: note: vectorized 1 loops in function.
Dense.c:57:2: missed: statement clobbers memory: memcpy (_4, dense$baiases_63, _3);
Dense.c:79:2: missed: statement clobbers memory: _27 (pretmp_163, _68, 0);
Dense.c:54:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:54:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:154:107: note: ***** Analysis failed with vector mode V8SI
Dense.c:154:107: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
Dense.c:155:85: note: ***** Analysis failed with vector mode V8SI
Dense.c:155:85: note: ***** The result for vector mode V32QI would be the same
Dense.c:155:85: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:155:85: note: ***** Analysis failed with vector mode V16QI
Dense.c:155:85: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:155:85: note: ***** Analysis failed with vector mode V8QI
Dense.c:155:85: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:155:85: note: ***** Analysis failed with vector mode V4QI
Dense.c:156:105: note: ***** Analysis failed with vector mode V8SI
Dense.c:156:105: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
Dense.c:179:19: missed: couldn't vectorize loop
Dense.c:179:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:169:13: note: vectorized 0 loops in function.
Dense.c:180:38: missed: statement clobbers memory: _5 = rand ();
Dense.c:180:72: missed: statement clobbers memory: _69 = sqrt (_12);
Dense.c:193:19: missed: statement clobbers memory: _24 = PULSE_GetActivationFunctionPtr (_23);
Dense.c:196:34: missed: statement clobbers memory: _28 = aligned_alloc (64, _27);
Dense.c:212:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:213:4: missed: statement clobbers memory: exit (1);
Dense.c:217:9: note: ***** Analysis succeeded with vector mode V8SI
Dense.c:217:9: note: SLPing BB part
Dense.c:217:9: note: Costing subgraph: 
Dense.c:217:9: note: node 0x7c720b8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:217:9: note: op template: <retval>.n_inputs = args$n_inputs_45;
Dense.c:217:9: note: 	stmt 0 <retval>.n_inputs = args$n_inputs_45;
Dense.c:217:9: note: 	stmt 1 <retval>.n_outputs = args$n_outputs_46;
Dense.c:217:9: note: 	children 0x7c72138
Dense.c:217:9: note: node 0x7c72138 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:217:9: note: op template: args$n_inputs_45 = args.n_inputs;
Dense.c:217:9: note: 	stmt 0 args$n_inputs_45 = args.n_inputs;
Dense.c:217:9: note: 	stmt 1 args$n_outputs_46 = args.n_outputs;
Dense.c:217:9: note: Cost model analysis: 
Dense.c:217:9: note: Cost model analysis for part in loop 0:
  Vector cost: 40
  Scalar cost: 56
Dense.c:217:9: note: Costing subgraph: 
Dense.c:217:9: note: node 0x7c72238 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:217:9: note: op template: <retval>.inputs = IO_34(D);
Dense.c:217:9: note: 	stmt 0 <retval>.inputs = IO_34(D);
Dense.c:217:9: note: 	stmt 1 <retval>.outputs = _22;
Dense.c:217:9: note: 	children 0x7c722b8
Dense.c:217:9: note: node (external) 0x7c722b8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:217:9: note: 	{ IO_34(D), _22 }
Dense.c:217:9: note: Cost model analysis: 
Dense.c:217:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:217:9: note: Costing subgraph: 
Dense.c:217:9: note: node 0x7c723b8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:217:9: note: op template: <retval>.feed = layer$feed_33;
Dense.c:217:9: note: 	stmt 0 <retval>.feed = layer$feed_33;
Dense.c:217:9: note: 	stmt 1 <retval>.back = layer$back_43;
Dense.c:217:9: note: 	stmt 2 <retval>.fix = layer$fix_44;
Dense.c:217:9: note: 	stmt 3 <retval>.activate = _25;
Dense.c:217:9: note: 	children 0x7c724b8
Dense.c:217:9: note: node (external) 0x7c724b8 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:217:9: note: 	{ layer$feed_33, layer$back_43, layer$fix_44, _25 }
Dense.c:217:9: note: Cost model analysis: 
Dense.c:217:9: note: Cost model analysis for part in loop 0:
  Vector cost: 64
  Scalar cost: 64
Dense.c:217:9: note: Costing subgraph: 
Dense.c:217:9: note: node 0x7c725b8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:217:9: note: op template: <retval>.parent = 0B;
Dense.c:217:9: note: 	stmt 0 <retval>.parent = 0B;
Dense.c:217:9: note: 	stmt 1 <retval>.child = 0B;
Dense.c:217:9: note: 	children 0x7c72638
Dense.c:217:9: note: node (constant) 0x7c72638 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:217:9: note: 	{ 0B, 0B }
Dense.c:217:9: note: Cost model analysis: 
Dense.c:217:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:217:9: note: Costing subgraph: 
Dense.c:217:9: note: node 0x7c726b8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:217:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_32(D);
Dense.c:217:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_32(D);
Dense.c:217:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&<retval> + 96B] = _4;
Dense.c:217:9: note: 	children 0x7c72738
Dense.c:217:9: note: node (external) 0x7c72738 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:217:9: note: 	{ MODEL_32(D), _4 }
Dense.c:217:9: note: Cost model analysis: 
Dense.c:217:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:217:9: note: Basic block will be vectorized using SLP
Dense.c:217:9: optimized: basic block part vectorized using 8 byte vectors
Dense.c:217:9: note: Vectorizing SLP tree:
Dense.c:217:9: note: node 0x7c720b8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:217:9: note: op template: <retval>.n_inputs = args$n_inputs_45;
Dense.c:217:9: note: 	stmt 0 <retval>.n_inputs = args$n_inputs_45;
Dense.c:217:9: note: 	stmt 1 <retval>.n_outputs = args$n_outputs_46;
Dense.c:217:9: note: 	children 0x7c72138
Dense.c:217:9: note: node 0x7c72138 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:217:9: note: op template: args$n_inputs_45 = args.n_inputs;
Dense.c:217:9: note: 	stmt 0 args$n_inputs_45 = args.n_inputs;
Dense.c:217:9: note: 	stmt 1 args$n_outputs_46 = args.n_outputs;
Dense.c:217:9: note: ------>vectorizing SLP node starting from: args$n_inputs_45 = args.n_inputs;
Dense.c:217:9: note: transform load. ncopies = 1
Dense.c:217:9: note: conflicting alias set types.
Dense.c:217:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: args.n_inputs
Dense.c:217:9: note: created &args
Dense.c:217:9: note: add new stmt: vect_args_n_inputs_45.484_9 = MEM <vector(2) unsigned int> [(void *)&args];
Dense.c:217:9: note: extracting lane for live stmt args$n_inputs_45 = args.n_inputs;
Dense.c:217:9: note: extracting lane for live stmt args$n_outputs_46 = args.n_outputs;
Dense.c:217:9: note: ------>vectorizing SLP node starting from: <retval>.n_inputs = args$n_inputs_45;
Dense.c:217:9: note: vect_is_simple_use: operand args.n_inputs, type of def: internal
Dense.c:217:9: note: vect_is_simple_use: operand args.n_outputs, type of def: internal
Dense.c:217:9: note: transform store. ncopies = 1
Dense.c:217:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: <retval>.n_inputs
Dense.c:217:9: note: created &<retval>.n_inputs
Dense.c:217:9: note: add new stmt: MEM <vector(2) unsigned int> [(unsigned int *)&<retval> + 128B] = vect_args_n_inputs_45.484_9;
Dense.c:217:9: note: vectorizing stmts using SLP.
Dense.c:217:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:217:9: note: Vectorizing SLP tree:
Dense.c:217:9: note: node 0x7c72238 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:217:9: note: op template: <retval>.inputs = IO_34(D);
Dense.c:217:9: note: 	stmt 0 <retval>.inputs = IO_34(D);
Dense.c:217:9: note: 	stmt 1 <retval>.outputs = _22;
Dense.c:217:9: note: 	children 0x7c722b8
Dense.c:217:9: note: node (external) 0x7c722b8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:217:9: note: 	{ IO_34(D), _22 }
Dense.c:217:9: note: ------>vectorizing SLP node starting from: <retval>.inputs = IO_34(D);
Dense.c:217:9: note: vect_is_simple_use: operand IO_34(D) + _21, type of def: internal
Dense.c:217:9: note: transform store. ncopies = 1
Dense.c:217:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.inputs
Dense.c:217:9: note: created &<retval>
Dense.c:217:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval>] = _83;
Dense.c:217:9: note: vectorizing stmts using SLP.
Dense.c:217:9: optimized: basic block part vectorized using 32 byte vectors
Dense.c:217:9: note: Vectorizing SLP tree:
Dense.c:217:9: note: node 0x7c723b8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:217:9: note: op template: <retval>.feed = layer$feed_33;
Dense.c:217:9: note: 	stmt 0 <retval>.feed = layer$feed_33;
Dense.c:217:9: note: 	stmt 1 <retval>.back = layer$back_43;
Dense.c:217:9: note: 	stmt 2 <retval>.fix = layer$fix_44;
Dense.c:217:9: note: 	stmt 3 <retval>.activate = _25;
Dense.c:217:9: note: 	children 0x7c724b8
Dense.c:217:9: note: node (external) 0x7c724b8 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:217:9: note: 	{ layer$feed_33, layer$back_43, layer$fix_44, _25 }
Dense.c:217:9: note: ------>vectorizing SLP node starting from: <retval>.feed = layer$feed_33;
Dense.c:217:9: note: vect_is_simple_use: operand layer$back_43 = PHI <layer$back_49(D)(11), _BackDense(9), _SIMD_BackDense(15)>, type of def: internal
Dense.c:217:9: note: vect_is_simple_use: operand layer$fix_44 = PHI <layer$fix_50(D)(11), _FixDense(9), _SIMD_FixDense(15)>, type of def: internal
Dense.c:217:9: note: vect_is_simple_use: operand (void (*<T91a>) (float *, unsigned int, char)) _24, type of def: internal
Dense.c:217:9: note: conflicting alias set types.
Dense.c:217:9: note: transform store. ncopies = 1
Dense.c:217:9: note: create vector_type-pointer variable to type: vector(4) long unsigned int  vectorizing a pointer ref: <retval>.feed
Dense.c:217:9: note: created &<retval>.feed
Dense.c:217:9: note: add new stmt: MEM <vector(4) long unsigned int> [(void *)&<retval> + 32B] = _89;
Dense.c:217:9: note: vectorizing stmts using SLP.
Dense.c:217:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:217:9: note: Vectorizing SLP tree:
Dense.c:217:9: note: node 0x7c725b8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:217:9: note: op template: <retval>.parent = 0B;
Dense.c:217:9: note: 	stmt 0 <retval>.parent = 0B;
Dense.c:217:9: note: 	stmt 1 <retval>.child = 0B;
Dense.c:217:9: note: 	children 0x7c72638
Dense.c:217:9: note: node (constant) 0x7c72638 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:217:9: note: 	{ 0B, 0B }
Dense.c:217:9: note: ------>vectorizing SLP node starting from: <retval>.parent = 0B;
Dense.c:217:9: note: vect_is_simple_use: operand 0B, type of def: constant
Dense.c:217:9: note: transform store. ncopies = 1
Dense.c:217:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.parent
Dense.c:217:9: note: created &<retval>.parent
Dense.c:217:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer * *)&<retval> + 72B] = { 0, 0 };
Dense.c:217:9: note: vectorizing stmts using SLP.
Dense.c:217:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:217:9: note: Vectorizing SLP tree:
Dense.c:217:9: note: node 0x7c726b8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:217:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_32(D);
Dense.c:217:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_32(D);
Dense.c:217:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&<retval> + 96B] = _4;
Dense.c:217:9: note: 	children 0x7c72738
Dense.c:217:9: note: node (external) 0x7c72738 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:217:9: note: 	{ MODEL_32(D), _4 }
Dense.c:217:9: note: ------>vectorizing SLP node starting from: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_32(D);
Dense.c:217:9: note: vect_is_simple_use: operand MODEL_32(D) + _3, type of def: internal
Dense.c:217:9: note: transform store. ncopies = 1
Dense.c:217:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B]
Dense.c:217:9: note: created &MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B]
Dense.c:217:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer *)&<retval> + 88B] = _94;
Dense.c:217:9: note: vectorizing stmts using SLP.
Dense.c:217:9: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _28 (pretmp_91, _51, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:19: missed: not vectorized: no vectype for stmt: _19 = *_18;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:165:10: note: ***** Analysis failed with vector mode V4DI
Dense.c:165:10: note: ***** The result for vector mode V32QI would be the same
Dense.c:165:10: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:165:10: note: ***** Analysis failed with vector mode V16QI
Dense.c:165:10: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:165:10: note: ***** Analysis failed with vector mode V8QI
Dense.c:165:10: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:165:10: note: ***** Analysis failed with vector mode V4QI
Dense.c:145:10: optimized: loop vectorized using 32 byte vectors
Dense.c:145:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:145:10: optimized: loop vectorized using 16 byte vectors
Dense.c:136:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
Dense.c:129:10: optimized: loop vectorized using 32 byte vectors
Dense.c:129:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:129:10: optimized: loop vectorized using 16 byte vectors
Dense.c:120:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
Dense.c:111:13: note: vectorized 2 loops in function.
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_28, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
Dense.c:111:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:111:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:111:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:111:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:111:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:111:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:111:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:111:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:90:23: missed: couldn't vectorize loop
Dense.c:92:31: missed: not vectorized: no vectype for stmt: _119 = this_40(D)->outputs;
 scalar_type: float *
Dense.c:90:23: missed: couldn't vectorize loop
Dense.c:90:23: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:95:16: missed: couldn't vectorize loop
Dense.c:95:16: missed: not vectorized: unsupported control flow in loop.
Dense.c:83:13: note: vectorized 0 loops in function.
Dense.c:86:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:97:16: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:97:16: note: SLPing BB part
Dense.c:97:16: note: Costing subgraph: 
Dense.c:97:16: note: node 0x8985458 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:97:16: note: op template: _16 = (sizetype) j_73;
Dense.c:97:16: note: 	stmt 0 _16 = (sizetype) j_73;
Dense.c:97:16: note: 	stmt 1 _111 = (sizetype) wi_74;
Dense.c:97:16: note: 	children 0x8985558
Dense.c:97:16: note: node (external) 0x8985558 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:97:16: note: 	{ j_73, wi_74 }
Dense.c:97:16: note: Cost model analysis: 
Dense.c:97:16: note: Scalar 1 and vector 2 loop part do not match up, skipping scalar part
Dense.c:97:16: note: Cost model analysis for part in loop 2:
  Vector cost: 36
  Scalar cost: 8
Dense.c:97:16: missed: not vectorized: vectorization is not profitable.
Dense.c:97:16: note: ***** The result for vector mode V32QI would be the same
Dense.c:97:16: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:97:16: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:97:16: note: SLPing BB part
Dense.c:97:16: note: Costing subgraph: 
Dense.c:97:16: note: node 0x8985458 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:97:16: note: op template: _16 = (sizetype) j_73;
Dense.c:97:16: note: 	stmt 0 _16 = (sizetype) j_73;
Dense.c:97:16: note: 	stmt 1 _111 = (sizetype) wi_74;
Dense.c:97:16: note: 	children 0x89854d8
Dense.c:97:16: note: node (external) 0x89854d8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:97:16: note: 	{ j_73, wi_74 }
Dense.c:97:16: note: Cost model analysis: 
Dense.c:97:16: note: Scalar 1 and vector 2 loop part do not match up, skipping scalar part
Dense.c:97:16: note: Cost model analysis for part in loop 2:
  Vector cost: 36
  Scalar cost: 8
Dense.c:97:16: missed: not vectorized: vectorization is not profitable.
Dense.c:97:16: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:90:50: note: ***** Analysis failed with vector mode V8QI
Dense.c:90:50: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:90:50: note: ***** Analysis failed with vector mode V4QI
Dense.c:62:15: missed: couldn't vectorize loop
Dense.c:62:15: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:75:11: optimized: loop vectorized using 32 byte vectors
Dense.c:75:11: optimized: loop vectorized using 16 byte vectors
Dense.c:66:11: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:881:10: missed: not vectorized: no vectype for stmt: _53 = MEM[(__m256 * {ref-all})w_ptr_82];
 scalar_type: __m256
Dense.c:54:13: note: vectorized 1 loops in function.
Dense.c:57:2: missed: statement clobbers memory: memcpy (_4, dense$baiases_63, _3);
Dense.c:79:2: missed: statement clobbers memory: _27 (pretmp_163, _68, 0);
Dense.c:54:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:54:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:154:107: note: ***** Analysis failed with vector mode V8SI
Dense.c:154:107: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
Dense.c:155:85: note: ***** Analysis failed with vector mode V8SI
Dense.c:155:85: note: ***** The result for vector mode V32QI would be the same
Dense.c:155:85: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:155:85: note: ***** Analysis failed with vector mode V16QI
Dense.c:155:85: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:155:85: note: ***** Analysis failed with vector mode V8QI
Dense.c:155:85: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:155:85: note: ***** Analysis failed with vector mode V4QI
Dense.c:156:105: note: ***** Analysis failed with vector mode V8SI
Dense.c:156:105: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
Dense.c:179:19: missed: couldn't vectorize loop
Dense.c:179:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:169:13: note: vectorized 0 loops in function.
Dense.c:180:38: missed: statement clobbers memory: _5 = rand ();
Dense.c:180:72: missed: statement clobbers memory: _69 = sqrt (_12);
Dense.c:193:19: missed: statement clobbers memory: _24 = PULSE_GetActivationFunctionPtr (_23);
Dense.c:196:34: missed: statement clobbers memory: _28 = aligned_alloc (64, _27);
Dense.c:212:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:213:4: missed: statement clobbers memory: exit (1);
Dense.c:217:9: note: ***** Analysis succeeded with vector mode V8SI
Dense.c:217:9: note: SLPing BB part
Dense.c:217:9: note: Costing subgraph: 
Dense.c:217:9: note: node 0x89c63c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:217:9: note: op template: <retval>.n_inputs = args$n_inputs_45;
Dense.c:217:9: note: 	stmt 0 <retval>.n_inputs = args$n_inputs_45;
Dense.c:217:9: note: 	stmt 1 <retval>.n_outputs = args$n_outputs_46;
Dense.c:217:9: note: 	children 0x89c6448
Dense.c:217:9: note: node 0x89c6448 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:217:9: note: op template: args$n_inputs_45 = args.n_inputs;
Dense.c:217:9: note: 	stmt 0 args$n_inputs_45 = args.n_inputs;
Dense.c:217:9: note: 	stmt 1 args$n_outputs_46 = args.n_outputs;
Dense.c:217:9: note: Cost model analysis: 
Dense.c:217:9: note: Cost model analysis for part in loop 0:
  Vector cost: 40
  Scalar cost: 56
Dense.c:217:9: note: Costing subgraph: 
Dense.c:217:9: note: node 0x89c6548 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:217:9: note: op template: <retval>.inputs = IO_34(D);
Dense.c:217:9: note: 	stmt 0 <retval>.inputs = IO_34(D);
Dense.c:217:9: note: 	stmt 1 <retval>.outputs = _22;
Dense.c:217:9: note: 	children 0x89c65c8
Dense.c:217:9: note: node (external) 0x89c65c8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:217:9: note: 	{ IO_34(D), _22 }
Dense.c:217:9: note: Cost model analysis: 
Dense.c:217:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:217:9: note: Costing subgraph: 
Dense.c:217:9: note: node 0x89c66c8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:217:9: note: op template: <retval>.feed = layer$feed_33;
Dense.c:217:9: note: 	stmt 0 <retval>.feed = layer$feed_33;
Dense.c:217:9: note: 	stmt 1 <retval>.back = layer$back_43;
Dense.c:217:9: note: 	stmt 2 <retval>.fix = layer$fix_44;
Dense.c:217:9: note: 	stmt 3 <retval>.activate = _25;
Dense.c:217:9: note: 	children 0x89c67c8
Dense.c:217:9: note: node (external) 0x89c67c8 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:217:9: note: 	{ layer$feed_33, layer$back_43, layer$fix_44, _25 }
Dense.c:217:9: note: Cost model analysis: 
Dense.c:217:9: note: Cost model analysis for part in loop 0:
  Vector cost: 64
  Scalar cost: 64
Dense.c:217:9: note: Costing subgraph: 
Dense.c:217:9: note: node 0x89c68c8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:217:9: note: op template: <retval>.parent = 0B;
Dense.c:217:9: note: 	stmt 0 <retval>.parent = 0B;
Dense.c:217:9: note: 	stmt 1 <retval>.child = 0B;
Dense.c:217:9: note: 	children 0x89c6948
Dense.c:217:9: note: node (constant) 0x89c6948 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:217:9: note: 	{ 0B, 0B }
Dense.c:217:9: note: Cost model analysis: 
Dense.c:217:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:217:9: note: Costing subgraph: 
Dense.c:217:9: note: node 0x89c69c8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:217:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_32(D);
Dense.c:217:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_32(D);
Dense.c:217:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&<retval> + 96B] = _4;
Dense.c:217:9: note: 	children 0x89c6a48
Dense.c:217:9: note: node (external) 0x89c6a48 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:217:9: note: 	{ MODEL_32(D), _4 }
Dense.c:217:9: note: Cost model analysis: 
Dense.c:217:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:217:9: note: Basic block will be vectorized using SLP
Dense.c:217:9: optimized: basic block part vectorized using 8 byte vectors
Dense.c:217:9: note: Vectorizing SLP tree:
Dense.c:217:9: note: node 0x89c63c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:217:9: note: op template: <retval>.n_inputs = args$n_inputs_45;
Dense.c:217:9: note: 	stmt 0 <retval>.n_inputs = args$n_inputs_45;
Dense.c:217:9: note: 	stmt 1 <retval>.n_outputs = args$n_outputs_46;
Dense.c:217:9: note: 	children 0x89c6448
Dense.c:217:9: note: node 0x89c6448 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:217:9: note: op template: args$n_inputs_45 = args.n_inputs;
Dense.c:217:9: note: 	stmt 0 args$n_inputs_45 = args.n_inputs;
Dense.c:217:9: note: 	stmt 1 args$n_outputs_46 = args.n_outputs;
Dense.c:217:9: note: ------>vectorizing SLP node starting from: args$n_inputs_45 = args.n_inputs;
Dense.c:217:9: note: transform load. ncopies = 1
Dense.c:217:9: note: conflicting alias set types.
Dense.c:217:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: args.n_inputs
Dense.c:217:9: note: created &args
Dense.c:217:9: note: add new stmt: vect_args_n_inputs_45.484_9 = MEM <vector(2) unsigned int> [(void *)&args];
Dense.c:217:9: note: extracting lane for live stmt args$n_inputs_45 = args.n_inputs;
Dense.c:217:9: note: extracting lane for live stmt args$n_outputs_46 = args.n_outputs;
Dense.c:217:9: note: ------>vectorizing SLP node starting from: <retval>.n_inputs = args$n_inputs_45;
Dense.c:217:9: note: vect_is_simple_use: operand args.n_inputs, type of def: internal
Dense.c:217:9: note: vect_is_simple_use: operand args.n_outputs, type of def: internal
Dense.c:217:9: note: transform store. ncopies = 1
Dense.c:217:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: <retval>.n_inputs
Dense.c:217:9: note: created &<retval>.n_inputs
Dense.c:217:9: note: add new stmt: MEM <vector(2) unsigned int> [(unsigned int *)&<retval> + 128B] = vect_args_n_inputs_45.484_9;
Dense.c:217:9: note: vectorizing stmts using SLP.
Dense.c:217:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:217:9: note: Vectorizing SLP tree:
Dense.c:217:9: note: node 0x89c6548 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:217:9: note: op template: <retval>.inputs = IO_34(D);
Dense.c:217:9: note: 	stmt 0 <retval>.inputs = IO_34(D);
Dense.c:217:9: note: 	stmt 1 <retval>.outputs = _22;
Dense.c:217:9: note: 	children 0x89c65c8
Dense.c:217:9: note: node (external) 0x89c65c8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:217:9: note: 	{ IO_34(D), _22 }
Dense.c:217:9: note: ------>vectorizing SLP node starting from: <retval>.inputs = IO_34(D);
Dense.c:217:9: note: vect_is_simple_use: operand IO_34(D) + _21, type of def: internal
Dense.c:217:9: note: transform store. ncopies = 1
Dense.c:217:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.inputs
Dense.c:217:9: note: created &<retval>
Dense.c:217:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval>] = _83;
Dense.c:217:9: note: vectorizing stmts using SLP.
Dense.c:217:9: optimized: basic block part vectorized using 32 byte vectors
Dense.c:217:9: note: Vectorizing SLP tree:
Dense.c:217:9: note: node 0x89c66c8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:217:9: note: op template: <retval>.feed = layer$feed_33;
Dense.c:217:9: note: 	stmt 0 <retval>.feed = layer$feed_33;
Dense.c:217:9: note: 	stmt 1 <retval>.back = layer$back_43;
Dense.c:217:9: note: 	stmt 2 <retval>.fix = layer$fix_44;
Dense.c:217:9: note: 	stmt 3 <retval>.activate = _25;
Dense.c:217:9: note: 	children 0x89c67c8
Dense.c:217:9: note: node (external) 0x89c67c8 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:217:9: note: 	{ layer$feed_33, layer$back_43, layer$fix_44, _25 }
Dense.c:217:9: note: ------>vectorizing SLP node starting from: <retval>.feed = layer$feed_33;
Dense.c:217:9: note: vect_is_simple_use: operand layer$back_43 = PHI <layer$back_49(D)(11), _BackDense(9), _SIMD_BackDense(15)>, type of def: internal
Dense.c:217:9: note: vect_is_simple_use: operand layer$fix_44 = PHI <layer$fix_50(D)(11), _FixDense(9), _SIMD_FixDense(15)>, type of def: internal
Dense.c:217:9: note: vect_is_simple_use: operand (void (*<T91a>) (float *, unsigned int, char)) _24, type of def: internal
Dense.c:217:9: note: conflicting alias set types.
Dense.c:217:9: note: transform store. ncopies = 1
Dense.c:217:9: note: create vector_type-pointer variable to type: vector(4) long unsigned int  vectorizing a pointer ref: <retval>.feed
Dense.c:217:9: note: created &<retval>.feed
Dense.c:217:9: note: add new stmt: MEM <vector(4) long unsigned int> [(void *)&<retval> + 32B] = _89;
Dense.c:217:9: note: vectorizing stmts using SLP.
Dense.c:217:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:217:9: note: Vectorizing SLP tree:
Dense.c:217:9: note: node 0x89c68c8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:217:9: note: op template: <retval>.parent = 0B;
Dense.c:217:9: note: 	stmt 0 <retval>.parent = 0B;
Dense.c:217:9: note: 	stmt 1 <retval>.child = 0B;
Dense.c:217:9: note: 	children 0x89c6948
Dense.c:217:9: note: node (constant) 0x89c6948 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:217:9: note: 	{ 0B, 0B }
Dense.c:217:9: note: ------>vectorizing SLP node starting from: <retval>.parent = 0B;
Dense.c:217:9: note: vect_is_simple_use: operand 0B, type of def: constant
Dense.c:217:9: note: transform store. ncopies = 1
Dense.c:217:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.parent
Dense.c:217:9: note: created &<retval>.parent
Dense.c:217:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer * *)&<retval> + 72B] = { 0, 0 };
Dense.c:217:9: note: vectorizing stmts using SLP.
Dense.c:217:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:217:9: note: Vectorizing SLP tree:
Dense.c:217:9: note: node 0x89c69c8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:217:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_32(D);
Dense.c:217:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_32(D);
Dense.c:217:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&<retval> + 96B] = _4;
Dense.c:217:9: note: 	children 0x89c6a48
Dense.c:217:9: note: node (external) 0x89c6a48 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:217:9: note: 	{ MODEL_32(D), _4 }
Dense.c:217:9: note: ------>vectorizing SLP node starting from: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_32(D);
Dense.c:217:9: note: vect_is_simple_use: operand MODEL_32(D) + _3, type of def: internal
Dense.c:217:9: note: transform store. ncopies = 1
Dense.c:217:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B]
Dense.c:217:9: note: created &MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B]
Dense.c:217:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer *)&<retval> + 88B] = _94;
Dense.c:217:9: note: vectorizing stmts using SLP.
Dense.c:217:9: note: ***** The result for vector mode V32QI would be the same
Activations.c:17:19: optimized: loop vectorized using 32 byte vectors
Activations.c:17:19: optimized: loop vectorized using 16 byte vectors
Activations.c:17:19: optimized: loop vectorized using 32 byte vectors
Activations.c:17:19: optimized: loop vectorized using 16 byte vectors
Activations.c:14:13: note: vectorized 2 loops in function.
Activations.c:14:13: note: ***** Analysis failed with vector mode V8SF
Activations.c:14:13: note: ***** The result for vector mode V32QI would be the same
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V16QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V16QI
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V8QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V8QI
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V4QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V4QI
Activations.c:28:19: missed: couldn't vectorize loop
Activations.c:28:19: missed: not vectorized: unsupported control flow in loop.
Activations.c:28:19: optimized: loop vectorized using 32 byte vectors
Activations.c:28:19: optimized: loop vectorized using 16 byte vectors
Activations.c:25:13: note: vectorized 1 loops in function.
Activations.c:25:13: note: ***** Analysis failed with vector mode V8SF
Activations.c:25:13: note: ***** The result for vector mode V32QI would be the same
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V16QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V16QI
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V8QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V8QI
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V4QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V4QI
Activations.c:38:1: note: ***** Analysis failed with vector mode VOID
Activations.c:5:19: missed: couldn't vectorize loop
Activations.c:9:21: missed: statement clobbers memory: _36 = expf (_18);
Activations.c:5:19: missed: couldn't vectorize loop
Activations.c:9:21: missed: statement clobbers memory: _12 = expf (_11);
Activations.c:3:13: note: vectorized 0 loops in function.
Activations.c:9:21: missed: statement clobbers memory: _12 = expf (_11);
Activations.c:9:21: missed: statement clobbers memory: _36 = expf (_18);
Activations.c:11:1: note: ***** Analysis failed with vector mode V8SF
Activations.c:11:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SF
Activations.c:44:2: note: ***** Analysis failed with vector mode VOID
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:4:18: note: vectorized 0 loops in function.
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_4, inputs_15, _3);
PULSE.c:8:2: missed: statement clobbers memory: _5 (layer_17);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_12, pretmp_26, _9);
PULSE.c:13:28: note: ***** Analysis failed with vector mode V8SI
PULSE.c:13:28: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:22:2: missed: statement clobbers memory: _1 (layer_10(D));
PULSE.c:22:2: missed: statement clobbers memory: _14 (_2);
PULSE.c:22:2: missed: statement clobbers memory: _21 (_15);
PULSE.c:22:2: missed: statement clobbers memory: _28 (_22);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_29);
PULSE.c:26:3: missed: statement clobbers memory: memset (_34, 0, _32);
PULSE.c:26:3: missed: statement clobbers memory: memset (_27, 0, _25);
PULSE.c:26:3: missed: statement clobbers memory: memset (_20, 0, _18);
PULSE.c:26:3: missed: statement clobbers memory: memset (_7, 0, _5);
PULSE.c:28:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:28:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:39: missed: statement clobbers memory: _7 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:32:6: note: vectorized 1 loops in function.
PULSE.c:34:8: missed: statement clobbers memory: _1 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_2);
PULSE.c:39:39: missed: statement clobbers memory: _7 = rand ();
PULSE.c:40:40: missed: statement clobbers memory: _9 = rand ();
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8SI
PULSE.c:32:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:67:16: missed: couldn't vectorize loop
PULSE.c:67:16: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:70:17: missed: couldn't vectorize loop
PULSE.c:70:17: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:79:19: missed: couldn't vectorize loop
PULSE.c:79:19: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:39: missed: statement clobbers memory: _84 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:56:22: missed: couldn't vectorize loop
PULSE.c:56:22: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:47:6: note: vectorized 1 loops in function.
PULSE.c:48:1: missed: statement clobbers memory: saved_stack.7_46 = __builtin_stack_save ();
PULSE.c:49:46: missed: statement clobbers memory: _1 = PULSE_GetLossFunctionPtr (loss_function_47(D));
PULSE.c:50:42: missed: statement clobbers memory: TMP_51 = aligned_alloc (64, _3);
PULSE.c:52:2: missed: statement clobbers memory: printf ("\n%d\n", model$fixes_size_55);
PULSE.c:58:14: missed: statement clobbers memory: _73 = _4 (output_208, TMP_PTR_207);
PULSE.c:66:15: missed: statement clobbers memory: random.4_57 = __builtin_alloca_with_align (_9, 32);
PULSE.c:34:8: missed: statement clobbers memory: _76 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_77);
PULSE.c:39:39: missed: statement clobbers memory: _84 = rand ();
PULSE.c:40:40: missed: statement clobbers memory: _87 = rand ();
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_113, inputs_108, _112);
PULSE.c:8:2: missed: statement clobbers memory: _114 (layer_109);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_120, _119, _118);
PULSE.c:73:11: missed: statement clobbers memory: loss_66 = PULSE_GetLoss_49 (_22, _21, _17, _16);
PULSE.c:22:2: missed: statement clobbers memory: _101 (output_209);
PULSE.c:22:2: missed: statement clobbers memory: _123 (_102);
PULSE.c:22:2: missed: statement clobbers memory: _130 (_124);
PULSE.c:22:2: missed: statement clobbers memory: _137 (_131);
PULSE.c:22:2: missed: statement clobbers memory: _144 (_138);
PULSE.c:22:2: missed: statement clobbers memory: _151 (_145);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_152);
PULSE.c:26:3: missed: statement clobbers memory: memset (_157, 0, _155);
PULSE.c:26:3: missed: statement clobbers memory: memset (_150, 0, _148);
PULSE.c:26:3: missed: statement clobbers memory: memset (_143, 0, _141);
PULSE.c:26:3: missed: statement clobbers memory: memset (_136, 0, _134);
PULSE.c:26:3: missed: statement clobbers memory: memset (_129, 0, _127);
PULSE.c:26:3: missed: statement clobbers memory: memset (_107, 0, _105);
PULSE.c:81:6: missed: statement clobbers memory: _26 (current_213, args);
PULSE.c:87:4: missed: statement clobbers memory: printf ("Epoch: %d | Item: %d | Loss: %.10f | Batch Loss: %.10f\r", i_210, j_211, _30, _29);
PULSE.c:47:6: missed: statement clobbers memory: __builtin_stack_restore (saved_stack.7_46);
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4DI
PULSE.c:47:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:97:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:97:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:131:20: missed: couldn't vectorize loop
PULSE.c:131:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:131:20: missed: couldn't vectorize loop
PULSE.c:131:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:110:20: missed: couldn't vectorize loop
PULSE.c:110:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:110:20: missed: couldn't vectorize loop
PULSE.c:110:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:99:13: note: vectorized 0 loops in function.
PULSE.c:102:39: missed: statement clobbers memory: layers_41 = malloc (_3);
PULSE.c:105:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:124:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:125:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:126:46: missed: statement clobbers memory: WEIGHTS_86 = aligned_alloc (64, 0);
PULSE.c:116:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.107_94];
PULSE.c:117:21: missed: statement clobbers memory: _75 = PULSE_GetDenseWeightsSize (args);
PULSE.c:118:16: missed: statement clobbers memory: _78 = PULSE_GetDenseIOSize (args);
PULSE.c:119:19: missed: statement clobbers memory: _81 = PULSE_GetDenseFixesSize (args);
PULSE.c:116:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})_224];
PULSE.c:117:21: missed: statement clobbers memory: _210 = PULSE_GetDenseWeightsSize (args);
PULSE.c:118:16: missed: statement clobbers memory: _213 = PULSE_GetDenseIOSize (args);
PULSE.c:119:19: missed: statement clobbers memory: _216 = PULSE_GetDenseFixesSize (args);
PULSE.c:124:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:125:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:126:46: missed: statement clobbers memory: WEIGHTS_46 = aligned_alloc (64, _5);
PULSE.c:127:41: missed: statement clobbers memory: IO_48 = aligned_alloc (64, _7);
PULSE.c:137:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.109_96];
PULSE.c:138:17: missed: statement clobbers memory: *_10 = PULSE_CreateDenseLayer (args, WEIGHTS_PTR_152, IO_PTR_153, 0B); [return slot optimization]
PULSE.c:139:20: missed: statement clobbers memory: _64 = PULSE_GetDenseWeightsSize (args);
PULSE.c:140:15: missed: statement clobbers memory: _67 = PULSE_GetDenseIOSize (args);
PULSE.c:137:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.109_124];
PULSE.c:138:17: missed: statement clobbers memory: *_106 = PULSE_CreateDenseLayer (args, WEIGHTS_PTR_148, IO_PTR_143, 0B); [return slot optimization]
PULSE.c:139:20: missed: statement clobbers memory: _95 = PULSE_GetDenseWeightsSize (args);
PULSE.c:140:15: missed: statement clobbers memory: _59 = PULSE_GetDenseIOSize (args);
PULSE.c:146:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:147:9: note: ***** Analysis succeeded with vector mode V8SI
PULSE.c:147:9: note: SLPing BB part
PULSE.c:147:9: note: Costing subgraph: 
PULSE.c:147:9: note: node 0x40d44e8 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:147:9: note: op template: <retval>.weights_size = WEIGHTS_SIZE_164;
PULSE.c:147:9: note: 	stmt 0 <retval>.weights_size = WEIGHTS_SIZE_164;
PULSE.c:147:9: note: 	stmt 1 <retval>.io_size = IO_SIZE_167;
PULSE.c:147:9: note: 	children 0x40d4568
PULSE.c:147:9: note: node 0x40d4568 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:147:9: note: op template: WEIGHTS_SIZE_164 = PHI <WEIGHTS_SIZE_161(36), 0(25)>
PULSE.c:147:9: note: 	stmt 0 WEIGHTS_SIZE_164 = PHI <WEIGHTS_SIZE_161(36), 0(25)>
PULSE.c:147:9: note: 	stmt 1 IO_SIZE_167 = PHI <IO_SIZE_160(36), 0(25)>
PULSE.c:147:9: note: 	children 0x40d45e8 0x40d4de8
PULSE.c:147:9: note: node 0x40d45e8 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:147:9: note: op template: WEIGHTS_SIZE_161 = PHI <WEIGHTS_SIZE_20(11), WEIGHTS_SIZE_206(76)>
PULSE.c:147:9: note: 	stmt 0 WEIGHTS_SIZE_161 = PHI <WEIGHTS_SIZE_20(11), WEIGHTS_SIZE_206(76)>
PULSE.c:147:9: note: 	stmt 1 IO_SIZE_160 = PHI <IO_SIZE_22(11), IO_SIZE_192(76)>
PULSE.c:147:9: note: 	children 0x40d4668 0x40d49e8
PULSE.c:147:9: note: node 0x40d4668 (max_nunits=2, refcnt=3) vector(2) unsigned int
PULSE.c:147:9: note: op template: WEIGHTS_SIZE_20 = PHI <WEIGHTS_SIZE_73(77), WEIGHTS_SIZE_76(10)>
PULSE.c:147:9: note: 	stmt 0 WEIGHTS_SIZE_20 = PHI <WEIGHTS_SIZE_73(77), WEIGHTS_SIZE_76(10)>
PULSE.c:147:9: note: 	stmt 1 IO_SIZE_22 = PHI <IO_SIZE_147(77), IO_SIZE_79(10)>
PULSE.c:147:9: note: 	children 0x40d46e8 0x40d47e8
PULSE.c:147:9: note: node 0x40d46e8 (max_nunits=2, refcnt=2) vector(2) unsigned int
PULSE.c:147:9: note: op template: WEIGHTS_SIZE_73 = PHI <WEIGHTS_SIZE_20(73), 0(71)>
PULSE.c:147:9: note: 	stmt 0 WEIGHTS_SIZE_73 = PHI <WEIGHTS_SIZE_20(73), 0(71)>
PULSE.c:147:9: note: 	stmt 1 IO_SIZE_147 = PHI <IO_SIZE_22(73), 0(71)>
PULSE.c:147:9: note: 	children 0x40d4668 0x40d4768
PULSE.c:147:9: note: node (constant) 0x40d4768 (max_nunits=1, refcnt=1) vector(2) unsigned int
PULSE.c:147:9: note: 	{ 0, 0 }
PULSE.c:147:9: note: node 0x40d47e8 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:147:9: note: op template: WEIGHTS_SIZE_76 = WEIGHTS_SIZE_73 + _75;
PULSE.c:147:9: note: 	stmt 0 WEIGHTS_SIZE_76 = WEIGHTS_SIZE_73 + _75;
PULSE.c:147:9: note: 	stmt 1 IO_SIZE_79 = _78 + IO_SIZE_147;
PULSE.c:147:9: note: 	children 0x40d46e8 0x40d4968
PULSE.c:147:9: note: node (external) 0x40d4968 (max_nunits=1, refcnt=1) vector(2) unsigned int
PULSE.c:147:9: note: 	{ _75, _78 }
PULSE.c:147:9: note: node 0x40d49e8 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:147:9: note: op template: WEIGHTS_SIZE_206 = PHI <WEIGHTS_SIZE_238(68)>
PULSE.c:147:9: note: 	stmt 0 WEIGHTS_SIZE_206 = PHI <WEIGHTS_SIZE_238(68)>
PULSE.c:147:9: note: 	stmt 1 IO_SIZE_192 = PHI <IO_SIZE_239(68)>
PULSE.c:147:9: note: 	children 0x40d4a68
PULSE.c:147:9: note: node 0x40d4a68 (max_nunits=2, refcnt=2) vector(2) unsigned int
PULSE.c:147:9: note: op template: WEIGHTS_SIZE_238 = PHI <WEIGHTS_SIZE_211(59), WEIGHTS_SIZE_200(57)>
PULSE.c:147:9: note: 	stmt 0 WEIGHTS_SIZE_238 = PHI <WEIGHTS_SIZE_211(59), WEIGHTS_SIZE_200(57)>
PULSE.c:147:9: note: 	stmt 1 IO_SIZE_239 = PHI <IO_SIZE_214(59), IO_SIZE_201(57)>
PULSE.c:147:9: note: 	children 0x40d4ae8 0x40d4be8
PULSE.c:147:9: note: node 0x40d4ae8 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:147:9: note: op template: WEIGHTS_SIZE_211 = WEIGHTS_SIZE_200 + _210;
PULSE.c:147:9: note: 	stmt 0 WEIGHTS_SIZE_211 = WEIGHTS_SIZE_200 + _210;
PULSE.c:147:9: note: 	stmt 1 IO_SIZE_214 = _213 + IO_SIZE_201;
PULSE.c:147:9: note: 	children 0x40d4be8 0x40d4d68
PULSE.c:147:9: note: node 0x40d4be8 (max_nunits=2, refcnt=2) vector(2) unsigned int
PULSE.c:147:9: note: op template: WEIGHTS_SIZE_200 = PHI <WEIGHTS_SIZE_243(72), WEIGHTS_SIZE_238(69)>
PULSE.c:147:9: note: 	stmt 0 WEIGHTS_SIZE_200 = PHI <WEIGHTS_SIZE_243(72), WEIGHTS_SIZE_238(69)>
PULSE.c:147:9: note: 	stmt 1 IO_SIZE_201 = PHI <IO_SIZE_244(72), IO_SIZE_239(69)>
PULSE.c:147:9: note: 	children 0x40d4c68 0x40d4a68
PULSE.c:147:9: note: node 0x40d4c68 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:147:9: note: op template: WEIGHTS_SIZE_243 = PHI <WEIGHTS_SIZE_20(74)>
PULSE.c:147:9: note: 	stmt 0 WEIGHTS_SIZE_243 = PHI <WEIGHTS_SIZE_20(74)>
PULSE.c:147:9: note: 	stmt 1 IO_SIZE_244 = PHI <IO_SIZE_22(74)>
PULSE.c:147:9: note: 	children 0x40d4668
PULSE.c:147:9: note: node (external) 0x40d4d68 (max_nunits=1, refcnt=1) vector(2) unsigned int
PULSE.c:147:9: note: 	{ _210, _213 }
PULSE.c:147:9: note: node (constant) 0x40d4de8 (max_nunits=1, refcnt=1) vector(2) unsigned int
PULSE.c:147:9: note: 	{ 0, 0 }
PULSE.c:147:9: note: Cost model analysis: 
PULSE.c:147:9: note: Cost model analysis for part in loop 0:
  Vector cost: 48
  Scalar cost: 48
PULSE.c:147:9: note: Cost model analysis for part in loop 1:
  Vector cost: 44
  Scalar cost: 24
PULSE.c:147:9: missed: not vectorized: vectorization is not profitable.
PULSE.c:147:9: note: Costing subgraph: 
PULSE.c:147:9: note: node 0x40d4ee8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:147:9: note: op template: <retval>.layers = layers_41;
PULSE.c:147:9: note: 	stmt 0 <retval>.layers = layers_41;
PULSE.c:147:9: note: 	stmt 1 <retval>.weights = WEIGHTS_163;
PULSE.c:147:9: note: 	children 0x40d4fe8
PULSE.c:147:9: note: node (external) 0x40d4fe8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:147:9: note: 	{ layers_41, WEIGHTS_163 }
PULSE.c:147:9: note: Cost model analysis: 
PULSE.c:147:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:147:9: note: Costing subgraph: 
PULSE.c:147:9: note: node 0x40d5068 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:147:9: note: op template: <retval>.io = 0B;
PULSE.c:147:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:147:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:147:9: note: 	children 0x40d50e8
PULSE.c:147:9: note: node (constant) 0x40d50e8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:147:9: note: 	{ 0B, 0B }
PULSE.c:147:9: note: Cost model analysis: 
PULSE.c:147:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:147:9: note: Basic block will be vectorized using SLP
PULSE.c:147:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:147:9: note: Vectorizing SLP tree:
PULSE.c:147:9: note: node 0x40d4ee8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:147:9: note: op template: <retval>.layers = layers_41;
PULSE.c:147:9: note: 	stmt 0 <retval>.layers = layers_41;
PULSE.c:147:9: note: 	stmt 1 <retval>.weights = WEIGHTS_163;
PULSE.c:147:9: note: 	children 0x40d4fe8
PULSE.c:147:9: note: node (external) 0x40d4fe8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:147:9: note: 	{ layers_41, WEIGHTS_163 }
PULSE.c:147:9: note: ------>vectorizing SLP node starting from: <retval>.layers = layers_41;
PULSE.c:147:9: note: vect_is_simple_use: operand WEIGHTS_163 = PHI <WEIGHTS_46(36), WEIGHTS_86(25)>, type of def: internal
PULSE.c:147:9: note: conflicting alias set types.
PULSE.c:147:9: note: transform store. ncopies = 1
PULSE.c:147:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.layers
PULSE.c:147:9: note: created &<retval>
PULSE.c:147:9: note: add new stmt: MEM <vector(2) long unsigned int> [(void *)&<retval>] = _19;
PULSE.c:147:9: note: vectorizing stmts using SLP.
PULSE.c:147:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:147:9: note: Vectorizing SLP tree:
PULSE.c:147:9: note: node 0x40d5068 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:147:9: note: op template: <retval>.io = 0B;
PULSE.c:147:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:147:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:147:9: note: 	children 0x40d50e8
PULSE.c:147:9: note: node (constant) 0x40d50e8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:147:9: note: 	{ 0B, 0B }
PULSE.c:147:9: note: ------>vectorizing SLP node starting from: <retval>.io = 0B;
PULSE.c:147:9: note: vect_is_simple_use: operand 0B, type of def: constant
PULSE.c:147:9: note: transform store. ncopies = 1
PULSE.c:147:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.io
PULSE.c:147:9: note: created &<retval>.io
PULSE.c:147:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval> + 16B] = { 0, 0 };
PULSE.c:147:9: note: vectorizing stmts using SLP.
PULSE.c:147:9: note: ***** The result for vector mode V32QI would be the same
PULSE.c:153:2: missed: statement clobbers memory: free (_1);
PULSE.c:154:2: missed: statement clobbers memory: free (_2);
PULSE.c:155:2: missed: statement clobbers memory: free (_3);
PULSE.c:156:2: missed: statement clobbers memory: free (_4);
PULSE.c:157:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:157:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Loss.c:15:19: optimized: loop vectorized using 32 byte vectors
Loss.c:15:19: optimized:  loop versioned for vectorization because of possible aliasing
Loss.c:15:19: optimized: loop vectorized using 16 byte vectors
Loss.c:12:23: note: vectorized 1 loops in function.
Loss.c:12:23: note: ***** Analysis failed with vector mode V8SF
Loss.c:12:23: note: ***** The result for vector mode V32QI would be the same
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V16QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V16QI
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V8QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V8QI
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V4QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V4QI
Loss.c:7:19: missed: couldn't vectorize loop
Loss.c:8:38: missed: not vectorized: unsupported use in stmt.
Loss.c:4:23: note: vectorized 0 loops in function.
Loss.c:9:13: note: ***** Analysis failed with vector mode V8SF
Loss.c:9:13: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SF
Loss.c:23:2: note: ***** Analysis failed with vector mode VOID
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _28 (pretmp_91, _51, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:19: missed: not vectorized: no vectype for stmt: _19 = *_18;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:165:10: note: ***** Analysis failed with vector mode V4DI
Dense.c:165:10: note: ***** The result for vector mode V32QI would be the same
Dense.c:165:10: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:165:10: note: ***** Analysis failed with vector mode V16QI
Dense.c:165:10: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:165:10: note: ***** Analysis failed with vector mode V8QI
Dense.c:165:10: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:165:10: note: ***** Analysis failed with vector mode V4QI
Dense.c:145:10: optimized: loop vectorized using 32 byte vectors
Dense.c:145:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:145:10: optimized: loop vectorized using 16 byte vectors
Dense.c:136:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
Dense.c:129:10: optimized: loop vectorized using 32 byte vectors
Dense.c:129:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:129:10: optimized: loop vectorized using 16 byte vectors
Dense.c:120:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
Dense.c:111:13: note: vectorized 2 loops in function.
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_28, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
Dense.c:111:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:111:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:111:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:111:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:111:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:111:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:111:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:111:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:90:23: missed: couldn't vectorize loop
Dense.c:92:31: missed: not vectorized: no vectype for stmt: _119 = this_40(D)->outputs;
 scalar_type: float *
Dense.c:90:23: missed: couldn't vectorize loop
Dense.c:90:23: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:95:16: missed: couldn't vectorize loop
Dense.c:95:16: missed: not vectorized: unsupported control flow in loop.
Dense.c:83:13: note: vectorized 0 loops in function.
Dense.c:86:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:97:16: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:97:16: note: SLPing BB part
Dense.c:97:16: note: Costing subgraph: 
Dense.c:97:16: note: node 0x8671348 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:97:16: note: op template: _16 = (sizetype) j_73;
Dense.c:97:16: note: 	stmt 0 _16 = (sizetype) j_73;
Dense.c:97:16: note: 	stmt 1 _111 = (sizetype) wi_74;
Dense.c:97:16: note: 	children 0x8671448
Dense.c:97:16: note: node (external) 0x8671448 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:97:16: note: 	{ j_73, wi_74 }
Dense.c:97:16: note: Cost model analysis: 
Dense.c:97:16: note: Scalar 1 and vector 2 loop part do not match up, skipping scalar part
Dense.c:97:16: note: Cost model analysis for part in loop 2:
  Vector cost: 36
  Scalar cost: 8
Dense.c:97:16: missed: not vectorized: vectorization is not profitable.
Dense.c:97:16: note: ***** The result for vector mode V32QI would be the same
Dense.c:97:16: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:97:16: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:97:16: note: SLPing BB part
Dense.c:97:16: note: Costing subgraph: 
Dense.c:97:16: note: node 0x8671348 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:97:16: note: op template: _16 = (sizetype) j_73;
Dense.c:97:16: note: 	stmt 0 _16 = (sizetype) j_73;
Dense.c:97:16: note: 	stmt 1 _111 = (sizetype) wi_74;
Dense.c:97:16: note: 	children 0x86713c8
Dense.c:97:16: note: node (external) 0x86713c8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:97:16: note: 	{ j_73, wi_74 }
Dense.c:97:16: note: Cost model analysis: 
Dense.c:97:16: note: Scalar 1 and vector 2 loop part do not match up, skipping scalar part
Dense.c:97:16: note: Cost model analysis for part in loop 2:
  Vector cost: 36
  Scalar cost: 8
Dense.c:97:16: missed: not vectorized: vectorization is not profitable.
Dense.c:97:16: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:90:50: note: ***** Analysis failed with vector mode V8QI
Dense.c:90:50: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:90:50: note: ***** Analysis failed with vector mode V4QI
Dense.c:62:15: missed: couldn't vectorize loop
Dense.c:62:15: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:75:11: optimized: loop vectorized using 32 byte vectors
Dense.c:75:11: optimized: loop vectorized using 16 byte vectors
Dense.c:66:11: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:881:10: missed: not vectorized: no vectype for stmt: _53 = MEM[(__m256 * {ref-all})w_ptr_82];
 scalar_type: __m256
Dense.c:54:13: note: vectorized 1 loops in function.
Dense.c:57:2: missed: statement clobbers memory: memcpy (_4, dense$baiases_63, _3);
Dense.c:79:2: missed: statement clobbers memory: _27 (pretmp_163, _68, 0);
Dense.c:54:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:54:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:154:107: note: ***** Analysis failed with vector mode V8SI
Dense.c:154:107: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
Dense.c:155:85: note: ***** Analysis failed with vector mode V8SI
Dense.c:155:85: note: ***** The result for vector mode V32QI would be the same
Dense.c:155:85: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:155:85: note: ***** Analysis failed with vector mode V16QI
Dense.c:155:85: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:155:85: note: ***** Analysis failed with vector mode V8QI
Dense.c:155:85: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:155:85: note: ***** Analysis failed with vector mode V4QI
Dense.c:156:105: note: ***** Analysis failed with vector mode V8SI
Dense.c:156:105: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
Dense.c:179:19: missed: couldn't vectorize loop
Dense.c:179:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:169:13: note: vectorized 0 loops in function.
Dense.c:180:38: missed: statement clobbers memory: _5 = rand ();
Dense.c:180:72: missed: statement clobbers memory: _69 = sqrt (_12);
Dense.c:193:19: missed: statement clobbers memory: _24 = PULSE_GetActivationFunctionPtr (_23);
Dense.c:196:34: missed: statement clobbers memory: _28 = aligned_alloc (64, _27);
Dense.c:212:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:213:4: missed: statement clobbers memory: exit (1);
Dense.c:217:9: note: ***** Analysis succeeded with vector mode V8SI
Dense.c:217:9: note: SLPing BB part
Dense.c:217:9: note: Costing subgraph: 
Dense.c:217:9: note: node 0x84f8228 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:217:9: note: op template: <retval>.n_inputs = args$n_inputs_45;
Dense.c:217:9: note: 	stmt 0 <retval>.n_inputs = args$n_inputs_45;
Dense.c:217:9: note: 	stmt 1 <retval>.n_outputs = args$n_outputs_46;
Dense.c:217:9: note: 	children 0x84f82a8
Dense.c:217:9: note: node 0x84f82a8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:217:9: note: op template: args$n_inputs_45 = args.n_inputs;
Dense.c:217:9: note: 	stmt 0 args$n_inputs_45 = args.n_inputs;
Dense.c:217:9: note: 	stmt 1 args$n_outputs_46 = args.n_outputs;
Dense.c:217:9: note: Cost model analysis: 
Dense.c:217:9: note: Cost model analysis for part in loop 0:
  Vector cost: 40
  Scalar cost: 56
Dense.c:217:9: note: Costing subgraph: 
Dense.c:217:9: note: node 0x84f83a8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:217:9: note: op template: <retval>.inputs = IO_34(D);
Dense.c:217:9: note: 	stmt 0 <retval>.inputs = IO_34(D);
Dense.c:217:9: note: 	stmt 1 <retval>.outputs = _22;
Dense.c:217:9: note: 	children 0x84f8428
Dense.c:217:9: note: node (external) 0x84f8428 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:217:9: note: 	{ IO_34(D), _22 }
Dense.c:217:9: note: Cost model analysis: 
Dense.c:217:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:217:9: note: Costing subgraph: 
Dense.c:217:9: note: node 0x84f8528 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:217:9: note: op template: <retval>.feed = layer$feed_33;
Dense.c:217:9: note: 	stmt 0 <retval>.feed = layer$feed_33;
Dense.c:217:9: note: 	stmt 1 <retval>.back = layer$back_43;
Dense.c:217:9: note: 	stmt 2 <retval>.fix = layer$fix_44;
Dense.c:217:9: note: 	stmt 3 <retval>.activate = _25;
Dense.c:217:9: note: 	children 0x84f8628
Dense.c:217:9: note: node (external) 0x84f8628 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:217:9: note: 	{ layer$feed_33, layer$back_43, layer$fix_44, _25 }
Dense.c:217:9: note: Cost model analysis: 
Dense.c:217:9: note: Cost model analysis for part in loop 0:
  Vector cost: 64
  Scalar cost: 64
Dense.c:217:9: note: Costing subgraph: 
Dense.c:217:9: note: node 0x84f8728 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:217:9: note: op template: <retval>.parent = 0B;
Dense.c:217:9: note: 	stmt 0 <retval>.parent = 0B;
Dense.c:217:9: note: 	stmt 1 <retval>.child = 0B;
Dense.c:217:9: note: 	children 0x84f87a8
Dense.c:217:9: note: node (constant) 0x84f87a8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:217:9: note: 	{ 0B, 0B }
Dense.c:217:9: note: Cost model analysis: 
Dense.c:217:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:217:9: note: Costing subgraph: 
Dense.c:217:9: note: node 0x84f8828 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:217:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_32(D);
Dense.c:217:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_32(D);
Dense.c:217:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&<retval> + 96B] = _4;
Dense.c:217:9: note: 	children 0x84f88a8
Dense.c:217:9: note: node (external) 0x84f88a8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:217:9: note: 	{ MODEL_32(D), _4 }
Dense.c:217:9: note: Cost model analysis: 
Dense.c:217:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:217:9: note: Basic block will be vectorized using SLP
Dense.c:217:9: optimized: basic block part vectorized using 8 byte vectors
Dense.c:217:9: note: Vectorizing SLP tree:
Dense.c:217:9: note: node 0x84f8228 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:217:9: note: op template: <retval>.n_inputs = args$n_inputs_45;
Dense.c:217:9: note: 	stmt 0 <retval>.n_inputs = args$n_inputs_45;
Dense.c:217:9: note: 	stmt 1 <retval>.n_outputs = args$n_outputs_46;
Dense.c:217:9: note: 	children 0x84f82a8
Dense.c:217:9: note: node 0x84f82a8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:217:9: note: op template: args$n_inputs_45 = args.n_inputs;
Dense.c:217:9: note: 	stmt 0 args$n_inputs_45 = args.n_inputs;
Dense.c:217:9: note: 	stmt 1 args$n_outputs_46 = args.n_outputs;
Dense.c:217:9: note: ------>vectorizing SLP node starting from: args$n_inputs_45 = args.n_inputs;
Dense.c:217:9: note: transform load. ncopies = 1
Dense.c:217:9: note: conflicting alias set types.
Dense.c:217:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: args.n_inputs
Dense.c:217:9: note: created &args
Dense.c:217:9: note: add new stmt: vect_args_n_inputs_45.484_9 = MEM <vector(2) unsigned int> [(void *)&args];
Dense.c:217:9: note: extracting lane for live stmt args$n_inputs_45 = args.n_inputs;
Dense.c:217:9: note: extracting lane for live stmt args$n_outputs_46 = args.n_outputs;
Dense.c:217:9: note: ------>vectorizing SLP node starting from: <retval>.n_inputs = args$n_inputs_45;
Dense.c:217:9: note: vect_is_simple_use: operand args.n_inputs, type of def: internal
Dense.c:217:9: note: vect_is_simple_use: operand args.n_outputs, type of def: internal
Dense.c:217:9: note: transform store. ncopies = 1
Dense.c:217:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: <retval>.n_inputs
Dense.c:217:9: note: created &<retval>.n_inputs
Dense.c:217:9: note: add new stmt: MEM <vector(2) unsigned int> [(unsigned int *)&<retval> + 128B] = vect_args_n_inputs_45.484_9;
Dense.c:217:9: note: vectorizing stmts using SLP.
Dense.c:217:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:217:9: note: Vectorizing SLP tree:
Dense.c:217:9: note: node 0x84f83a8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:217:9: note: op template: <retval>.inputs = IO_34(D);
Dense.c:217:9: note: 	stmt 0 <retval>.inputs = IO_34(D);
Dense.c:217:9: note: 	stmt 1 <retval>.outputs = _22;
Dense.c:217:9: note: 	children 0x84f8428
Dense.c:217:9: note: node (external) 0x84f8428 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:217:9: note: 	{ IO_34(D), _22 }
Dense.c:217:9: note: ------>vectorizing SLP node starting from: <retval>.inputs = IO_34(D);
Dense.c:217:9: note: vect_is_simple_use: operand IO_34(D) + _21, type of def: internal
Dense.c:217:9: note: transform store. ncopies = 1
Dense.c:217:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.inputs
Dense.c:217:9: note: created &<retval>
Dense.c:217:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval>] = _83;
Dense.c:217:9: note: vectorizing stmts using SLP.
Dense.c:217:9: optimized: basic block part vectorized using 32 byte vectors
Dense.c:217:9: note: Vectorizing SLP tree:
Dense.c:217:9: note: node 0x84f8528 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:217:9: note: op template: <retval>.feed = layer$feed_33;
Dense.c:217:9: note: 	stmt 0 <retval>.feed = layer$feed_33;
Dense.c:217:9: note: 	stmt 1 <retval>.back = layer$back_43;
Dense.c:217:9: note: 	stmt 2 <retval>.fix = layer$fix_44;
Dense.c:217:9: note: 	stmt 3 <retval>.activate = _25;
Dense.c:217:9: note: 	children 0x84f8628
Dense.c:217:9: note: node (external) 0x84f8628 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:217:9: note: 	{ layer$feed_33, layer$back_43, layer$fix_44, _25 }
Dense.c:217:9: note: ------>vectorizing SLP node starting from: <retval>.feed = layer$feed_33;
Dense.c:217:9: note: vect_is_simple_use: operand layer$back_43 = PHI <layer$back_49(D)(11), _BackDense(9), _SIMD_BackDense(15)>, type of def: internal
Dense.c:217:9: note: vect_is_simple_use: operand layer$fix_44 = PHI <layer$fix_50(D)(11), _FixDense(9), _SIMD_FixDense(15)>, type of def: internal
Dense.c:217:9: note: vect_is_simple_use: operand (void (*<T91a>) (float *, unsigned int, char)) _24, type of def: internal
Dense.c:217:9: note: conflicting alias set types.
Dense.c:217:9: note: transform store. ncopies = 1
Dense.c:217:9: note: create vector_type-pointer variable to type: vector(4) long unsigned int  vectorizing a pointer ref: <retval>.feed
Dense.c:217:9: note: created &<retval>.feed
Dense.c:217:9: note: add new stmt: MEM <vector(4) long unsigned int> [(void *)&<retval> + 32B] = _89;
Dense.c:217:9: note: vectorizing stmts using SLP.
Dense.c:217:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:217:9: note: Vectorizing SLP tree:
Dense.c:217:9: note: node 0x84f8728 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:217:9: note: op template: <retval>.parent = 0B;
Dense.c:217:9: note: 	stmt 0 <retval>.parent = 0B;
Dense.c:217:9: note: 	stmt 1 <retval>.child = 0B;
Dense.c:217:9: note: 	children 0x84f87a8
Dense.c:217:9: note: node (constant) 0x84f87a8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:217:9: note: 	{ 0B, 0B }
Dense.c:217:9: note: ------>vectorizing SLP node starting from: <retval>.parent = 0B;
Dense.c:217:9: note: vect_is_simple_use: operand 0B, type of def: constant
Dense.c:217:9: note: transform store. ncopies = 1
Dense.c:217:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.parent
Dense.c:217:9: note: created &<retval>.parent
Dense.c:217:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer * *)&<retval> + 72B] = { 0, 0 };
Dense.c:217:9: note: vectorizing stmts using SLP.
Dense.c:217:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:217:9: note: Vectorizing SLP tree:
Dense.c:217:9: note: node 0x84f8828 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:217:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_32(D);
Dense.c:217:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_32(D);
Dense.c:217:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&<retval> + 96B] = _4;
Dense.c:217:9: note: 	children 0x84f88a8
Dense.c:217:9: note: node (external) 0x84f88a8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:217:9: note: 	{ MODEL_32(D), _4 }
Dense.c:217:9: note: ------>vectorizing SLP node starting from: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_32(D);
Dense.c:217:9: note: vect_is_simple_use: operand MODEL_32(D) + _3, type of def: internal
Dense.c:217:9: note: transform store. ncopies = 1
Dense.c:217:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B]
Dense.c:217:9: note: created &MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B]
Dense.c:217:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer *)&<retval> + 88B] = _94;
Dense.c:217:9: note: vectorizing stmts using SLP.
Dense.c:217:9: note: ***** The result for vector mode V32QI would be the same
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:4:18: note: vectorized 0 loops in function.
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_4, inputs_15, _3);
PULSE.c:8:2: missed: statement clobbers memory: _5 (layer_17);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_12, pretmp_26, _9);
PULSE.c:13:28: note: ***** Analysis failed with vector mode V8SI
PULSE.c:13:28: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:22:2: missed: statement clobbers memory: _1 (layer_10(D));
PULSE.c:22:2: missed: statement clobbers memory: _14 (_2);
PULSE.c:22:2: missed: statement clobbers memory: _21 (_15);
PULSE.c:22:2: missed: statement clobbers memory: _28 (_22);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_29);
PULSE.c:26:3: missed: statement clobbers memory: memset (_34, 0, _32);
PULSE.c:26:3: missed: statement clobbers memory: memset (_27, 0, _25);
PULSE.c:26:3: missed: statement clobbers memory: memset (_20, 0, _18);
PULSE.c:26:3: missed: statement clobbers memory: memset (_7, 0, _5);
PULSE.c:28:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:28:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:39: missed: statement clobbers memory: _7 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:32:6: note: vectorized 1 loops in function.
PULSE.c:34:8: missed: statement clobbers memory: _1 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_2);
PULSE.c:39:39: missed: statement clobbers memory: _7 = rand ();
PULSE.c:40:40: missed: statement clobbers memory: _9 = rand ();
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8SI
PULSE.c:32:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:66:16: missed: couldn't vectorize loop
PULSE.c:66:16: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:69:17: missed: couldn't vectorize loop
PULSE.c:69:17: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:78:19: missed: couldn't vectorize loop
PULSE.c:78:19: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:39: missed: statement clobbers memory: _83 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:55:22: missed: couldn't vectorize loop
PULSE.c:55:22: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:47:6: note: vectorized 1 loops in function.
PULSE.c:48:1: missed: statement clobbers memory: saved_stack.7_47 = __builtin_stack_save ();
PULSE.c:49:46: missed: statement clobbers memory: _1 = PULSE_GetLossFunctionPtr (loss_function_48(D));
PULSE.c:50:42: missed: statement clobbers memory: TMP_52 = aligned_alloc (64, _4);
PULSE.c:57:14: missed: statement clobbers memory: _72 = _5 (output_207, TMP_PTR_206);
PULSE.c:65:15: missed: statement clobbers memory: random.4_56 = __builtin_alloca_with_align (_10, 32);
PULSE.c:34:8: missed: statement clobbers memory: _75 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_76);
PULSE.c:39:39: missed: statement clobbers memory: _83 = rand ();
PULSE.c:40:40: missed: statement clobbers memory: _86 = rand ();
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_112, inputs_107, _111);
PULSE.c:8:2: missed: statement clobbers memory: _113 (layer_108);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_119, _118, _117);
PULSE.c:72:11: missed: statement clobbers memory: loss_65 = PULSE_GetLoss_50 (_23, _22, _18, _17);
PULSE.c:22:2: missed: statement clobbers memory: _100 (output_208);
PULSE.c:22:2: missed: statement clobbers memory: _122 (_101);
PULSE.c:22:2: missed: statement clobbers memory: _129 (_123);
PULSE.c:22:2: missed: statement clobbers memory: _136 (_130);
PULSE.c:22:2: missed: statement clobbers memory: _143 (_137);
PULSE.c:22:2: missed: statement clobbers memory: _150 (_144);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_151);
PULSE.c:26:3: missed: statement clobbers memory: memset (_156, 0, _154);
PULSE.c:26:3: missed: statement clobbers memory: memset (_149, 0, _147);
PULSE.c:26:3: missed: statement clobbers memory: memset (_142, 0, _140);
PULSE.c:26:3: missed: statement clobbers memory: memset (_135, 0, _133);
PULSE.c:26:3: missed: statement clobbers memory: memset (_128, 0, _126);
PULSE.c:26:3: missed: statement clobbers memory: memset (_106, 0, _104);
PULSE.c:80:6: missed: statement clobbers memory: _27 (current_212, args);
PULSE.c:86:4: missed: statement clobbers memory: printf ("Epoch: %d | Item: %d | Loss: %.10f | Batch Loss: %.10f\r", i_209, j_210, _31, _30);
PULSE.c:47:6: missed: statement clobbers memory: __builtin_stack_restore (saved_stack.7_47);
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4DI
PULSE.c:47:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:96:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:96:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:131:20: missed: couldn't vectorize loop
PULSE.c:131:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:131:20: missed: couldn't vectorize loop
PULSE.c:131:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:109:20: missed: couldn't vectorize loop
PULSE.c:109:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:109:20: missed: couldn't vectorize loop
PULSE.c:109:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:98:13: note: vectorized 0 loops in function.
PULSE.c:101:39: missed: statement clobbers memory: layers_41 = malloc (_3);
PULSE.c:104:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:124:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:125:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:126:46: missed: statement clobbers memory: WEIGHTS_87 = aligned_alloc (64, 0);
PULSE.c:115:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.106_95];
PULSE.c:116:21: missed: statement clobbers memory: _75 = PULSE_GetDenseWeightsSize (args);
PULSE.c:117:16: missed: statement clobbers memory: _78 = PULSE_GetDenseIOSize (args);
PULSE.c:118:19: missed: statement clobbers memory: _81 = PULSE_GetDenseFixesSize (args);
PULSE.c:119:5: missed: statement clobbers memory: printf ("\n%d\n", FIXES_SIZE_82);
PULSE.c:115:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})_226];
PULSE.c:116:21: missed: statement clobbers memory: _211 = PULSE_GetDenseWeightsSize (args);
PULSE.c:117:16: missed: statement clobbers memory: _214 = PULSE_GetDenseIOSize (args);
PULSE.c:118:19: missed: statement clobbers memory: _217 = PULSE_GetDenseFixesSize (args);
PULSE.c:119:5: missed: statement clobbers memory: printf ("\n%d\n", FIXES_SIZE_218);
PULSE.c:124:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:125:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:126:46: missed: statement clobbers memory: WEIGHTS_46 = aligned_alloc (64, _5);
PULSE.c:127:41: missed: statement clobbers memory: IO_48 = aligned_alloc (64, _7);
PULSE.c:137:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.108_97];
PULSE.c:138:17: missed: statement clobbers memory: *_10 = PULSE_CreateDenseLayer (args, WEIGHTS_PTR_153, IO_PTR_154, 0B); [return slot optimization]
PULSE.c:139:20: missed: statement clobbers memory: _64 = PULSE_GetDenseWeightsSize (args);
PULSE.c:140:15: missed: statement clobbers memory: _67 = PULSE_GetDenseIOSize (args);
PULSE.c:137:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.108_125];
PULSE.c:138:17: missed: statement clobbers memory: *_107 = PULSE_CreateDenseLayer (args, WEIGHTS_PTR_149, IO_PTR_144, 0B); [return slot optimization]
PULSE.c:139:20: missed: statement clobbers memory: _96 = PULSE_GetDenseWeightsSize (args);
PULSE.c:140:15: missed: statement clobbers memory: _59 = PULSE_GetDenseIOSize (args);
PULSE.c:146:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:147:9: note: ***** Analysis succeeded with vector mode V8SI
PULSE.c:147:9: note: SLPing BB part
PULSE.c:147:9: note: Costing subgraph: 
PULSE.c:147:9: note: node 0x390a4f8 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:147:9: note: op template: <retval>.weights_size = WEIGHTS_SIZE_165;
PULSE.c:147:9: note: 	stmt 0 <retval>.weights_size = WEIGHTS_SIZE_165;
PULSE.c:147:9: note: 	stmt 1 <retval>.io_size = IO_SIZE_168;
PULSE.c:147:9: note: 	children 0x390a578
PULSE.c:147:9: note: node 0x390a578 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:147:9: note: op template: WEIGHTS_SIZE_165 = PHI <WEIGHTS_SIZE_162(36), 0(25)>
PULSE.c:147:9: note: 	stmt 0 WEIGHTS_SIZE_165 = PHI <WEIGHTS_SIZE_162(36), 0(25)>
PULSE.c:147:9: note: 	stmt 1 IO_SIZE_168 = PHI <IO_SIZE_161(36), 0(25)>
PULSE.c:147:9: note: 	children 0x390a5f8 0x390adf8
PULSE.c:147:9: note: node 0x390a5f8 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:147:9: note: op template: WEIGHTS_SIZE_162 = PHI <WEIGHTS_SIZE_20(11), WEIGHTS_SIZE_207(76)>
PULSE.c:147:9: note: 	stmt 0 WEIGHTS_SIZE_162 = PHI <WEIGHTS_SIZE_20(11), WEIGHTS_SIZE_207(76)>
PULSE.c:147:9: note: 	stmt 1 IO_SIZE_161 = PHI <IO_SIZE_22(11), IO_SIZE_193(76)>
PULSE.c:147:9: note: 	children 0x390a678 0x390a9f8
PULSE.c:147:9: note: node 0x390a678 (max_nunits=2, refcnt=3) vector(2) unsigned int
PULSE.c:147:9: note: op template: WEIGHTS_SIZE_20 = PHI <WEIGHTS_SIZE_73(77), WEIGHTS_SIZE_76(10)>
PULSE.c:147:9: note: 	stmt 0 WEIGHTS_SIZE_20 = PHI <WEIGHTS_SIZE_73(77), WEIGHTS_SIZE_76(10)>
PULSE.c:147:9: note: 	stmt 1 IO_SIZE_22 = PHI <IO_SIZE_148(77), IO_SIZE_79(10)>
PULSE.c:147:9: note: 	children 0x390a6f8 0x390a7f8
PULSE.c:147:9: note: node 0x390a6f8 (max_nunits=2, refcnt=2) vector(2) unsigned int
PULSE.c:147:9: note: op template: WEIGHTS_SIZE_73 = PHI <WEIGHTS_SIZE_20(73), 0(71)>
PULSE.c:147:9: note: 	stmt 0 WEIGHTS_SIZE_73 = PHI <WEIGHTS_SIZE_20(73), 0(71)>
PULSE.c:147:9: note: 	stmt 1 IO_SIZE_148 = PHI <IO_SIZE_22(73), 0(71)>
PULSE.c:147:9: note: 	children 0x390a678 0x390a778
PULSE.c:147:9: note: node (constant) 0x390a778 (max_nunits=1, refcnt=1) vector(2) unsigned int
PULSE.c:147:9: note: 	{ 0, 0 }
PULSE.c:147:9: note: node 0x390a7f8 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:147:9: note: op template: WEIGHTS_SIZE_76 = WEIGHTS_SIZE_73 + _75;
PULSE.c:147:9: note: 	stmt 0 WEIGHTS_SIZE_76 = WEIGHTS_SIZE_73 + _75;
PULSE.c:147:9: note: 	stmt 1 IO_SIZE_79 = _78 + IO_SIZE_148;
PULSE.c:147:9: note: 	children 0x390a6f8 0x390a978
PULSE.c:147:9: note: node (external) 0x390a978 (max_nunits=1, refcnt=1) vector(2) unsigned int
PULSE.c:147:9: note: 	{ _75, _78 }
PULSE.c:147:9: note: node 0x390a9f8 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:147:9: note: op template: WEIGHTS_SIZE_207 = PHI <WEIGHTS_SIZE_240(68)>
PULSE.c:147:9: note: 	stmt 0 WEIGHTS_SIZE_207 = PHI <WEIGHTS_SIZE_240(68)>
PULSE.c:147:9: note: 	stmt 1 IO_SIZE_193 = PHI <IO_SIZE_241(68)>
PULSE.c:147:9: note: 	children 0x390aa78
PULSE.c:147:9: note: node 0x390aa78 (max_nunits=2, refcnt=2) vector(2) unsigned int
PULSE.c:147:9: note: op template: WEIGHTS_SIZE_240 = PHI <WEIGHTS_SIZE_212(59), WEIGHTS_SIZE_201(57)>
PULSE.c:147:9: note: 	stmt 0 WEIGHTS_SIZE_240 = PHI <WEIGHTS_SIZE_212(59), WEIGHTS_SIZE_201(57)>
PULSE.c:147:9: note: 	stmt 1 IO_SIZE_241 = PHI <IO_SIZE_215(59), IO_SIZE_202(57)>
PULSE.c:147:9: note: 	children 0x390aaf8 0x390abf8
PULSE.c:147:9: note: node 0x390aaf8 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:147:9: note: op template: WEIGHTS_SIZE_212 = WEIGHTS_SIZE_201 + _211;
PULSE.c:147:9: note: 	stmt 0 WEIGHTS_SIZE_212 = WEIGHTS_SIZE_201 + _211;
PULSE.c:147:9: note: 	stmt 1 IO_SIZE_215 = _214 + IO_SIZE_202;
PULSE.c:147:9: note: 	children 0x390abf8 0x390ad78
PULSE.c:147:9: note: node 0x390abf8 (max_nunits=2, refcnt=2) vector(2) unsigned int
PULSE.c:147:9: note: op template: WEIGHTS_SIZE_201 = PHI <WEIGHTS_SIZE_245(72), WEIGHTS_SIZE_240(69)>
PULSE.c:147:9: note: 	stmt 0 WEIGHTS_SIZE_201 = PHI <WEIGHTS_SIZE_245(72), WEIGHTS_SIZE_240(69)>
PULSE.c:147:9: note: 	stmt 1 IO_SIZE_202 = PHI <IO_SIZE_246(72), IO_SIZE_241(69)>
PULSE.c:147:9: note: 	children 0x390ac78 0x390aa78
PULSE.c:147:9: note: node 0x390ac78 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:147:9: note: op template: WEIGHTS_SIZE_245 = PHI <WEIGHTS_SIZE_20(74)>
PULSE.c:147:9: note: 	stmt 0 WEIGHTS_SIZE_245 = PHI <WEIGHTS_SIZE_20(74)>
PULSE.c:147:9: note: 	stmt 1 IO_SIZE_246 = PHI <IO_SIZE_22(74)>
PULSE.c:147:9: note: 	children 0x390a678
PULSE.c:147:9: note: node (external) 0x390ad78 (max_nunits=1, refcnt=1) vector(2) unsigned int
PULSE.c:147:9: note: 	{ _211, _214 }
PULSE.c:147:9: note: node (constant) 0x390adf8 (max_nunits=1, refcnt=1) vector(2) unsigned int
PULSE.c:147:9: note: 	{ 0, 0 }
PULSE.c:147:9: note: Cost model analysis: 
PULSE.c:147:9: note: Cost model analysis for part in loop 0:
  Vector cost: 48
  Scalar cost: 48
PULSE.c:147:9: note: Cost model analysis for part in loop 1:
  Vector cost: 44
  Scalar cost: 24
PULSE.c:147:9: missed: not vectorized: vectorization is not profitable.
PULSE.c:147:9: note: Costing subgraph: 
PULSE.c:147:9: note: node 0x390aef8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:147:9: note: op template: <retval>.layers = layers_41;
PULSE.c:147:9: note: 	stmt 0 <retval>.layers = layers_41;
PULSE.c:147:9: note: 	stmt 1 <retval>.weights = WEIGHTS_164;
PULSE.c:147:9: note: 	children 0x390aff8
PULSE.c:147:9: note: node (external) 0x390aff8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:147:9: note: 	{ layers_41, WEIGHTS_164 }
PULSE.c:147:9: note: Cost model analysis: 
PULSE.c:147:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:147:9: note: Costing subgraph: 
PULSE.c:147:9: note: node 0x390b078 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:147:9: note: op template: <retval>.io = 0B;
PULSE.c:147:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:147:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:147:9: note: 	children 0x390b0f8
PULSE.c:147:9: note: node (constant) 0x390b0f8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:147:9: note: 	{ 0B, 0B }
PULSE.c:147:9: note: Cost model analysis: 
PULSE.c:147:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:147:9: note: Basic block will be vectorized using SLP
PULSE.c:147:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:147:9: note: Vectorizing SLP tree:
PULSE.c:147:9: note: node 0x390aef8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:147:9: note: op template: <retval>.layers = layers_41;
PULSE.c:147:9: note: 	stmt 0 <retval>.layers = layers_41;
PULSE.c:147:9: note: 	stmt 1 <retval>.weights = WEIGHTS_164;
PULSE.c:147:9: note: 	children 0x390aff8
PULSE.c:147:9: note: node (external) 0x390aff8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:147:9: note: 	{ layers_41, WEIGHTS_164 }
PULSE.c:147:9: note: ------>vectorizing SLP node starting from: <retval>.layers = layers_41;
PULSE.c:147:9: note: vect_is_simple_use: operand WEIGHTS_164 = PHI <WEIGHTS_46(36), WEIGHTS_87(25)>, type of def: internal
PULSE.c:147:9: note: conflicting alias set types.
PULSE.c:147:9: note: transform store. ncopies = 1
PULSE.c:147:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.layers
PULSE.c:147:9: note: created &<retval>
PULSE.c:147:9: note: add new stmt: MEM <vector(2) long unsigned int> [(void *)&<retval>] = _19;
PULSE.c:147:9: note: vectorizing stmts using SLP.
PULSE.c:147:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:147:9: note: Vectorizing SLP tree:
PULSE.c:147:9: note: node 0x390b078 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:147:9: note: op template: <retval>.io = 0B;
PULSE.c:147:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:147:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:147:9: note: 	children 0x390b0f8
PULSE.c:147:9: note: node (constant) 0x390b0f8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:147:9: note: 	{ 0B, 0B }
PULSE.c:147:9: note: ------>vectorizing SLP node starting from: <retval>.io = 0B;
PULSE.c:147:9: note: vect_is_simple_use: operand 0B, type of def: constant
PULSE.c:147:9: note: transform store. ncopies = 1
PULSE.c:147:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.io
PULSE.c:147:9: note: created &<retval>.io
PULSE.c:147:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval> + 16B] = { 0, 0 };
PULSE.c:147:9: note: vectorizing stmts using SLP.
PULSE.c:147:9: note: ***** The result for vector mode V32QI would be the same
PULSE.c:153:2: missed: statement clobbers memory: free (_1);
PULSE.c:154:2: missed: statement clobbers memory: free (_2);
PULSE.c:155:2: missed: statement clobbers memory: free (_3);
PULSE.c:156:2: missed: statement clobbers memory: free (_4);
PULSE.c:157:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:157:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _28 (pretmp_91, _51, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:19: missed: not vectorized: no vectype for stmt: _19 = *_18;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:165:10: note: ***** Analysis failed with vector mode V4DI
Dense.c:165:10: note: ***** The result for vector mode V32QI would be the same
Dense.c:165:10: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:165:10: note: ***** Analysis failed with vector mode V16QI
Dense.c:165:10: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:165:10: note: ***** Analysis failed with vector mode V8QI
Dense.c:165:10: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:165:10: note: ***** Analysis failed with vector mode V4QI
Dense.c:145:10: optimized: loop vectorized using 32 byte vectors
Dense.c:145:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:145:10: optimized: loop vectorized using 16 byte vectors
Dense.c:136:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
Dense.c:129:10: optimized: loop vectorized using 32 byte vectors
Dense.c:129:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:129:10: optimized: loop vectorized using 16 byte vectors
Dense.c:120:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
Dense.c:111:13: note: vectorized 2 loops in function.
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_28, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
Dense.c:111:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:111:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:111:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:111:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:111:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:111:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:111:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:111:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:90:23: missed: couldn't vectorize loop
Dense.c:92:31: missed: not vectorized: no vectype for stmt: _119 = this_40(D)->outputs;
 scalar_type: float *
Dense.c:90:23: missed: couldn't vectorize loop
Dense.c:90:23: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:95:16: missed: couldn't vectorize loop
Dense.c:95:16: missed: not vectorized: unsupported control flow in loop.
Dense.c:83:13: note: vectorized 0 loops in function.
Dense.c:86:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:97:16: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:97:16: note: SLPing BB part
Dense.c:97:16: note: Costing subgraph: 
Dense.c:97:16: note: node 0x89864a8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:97:16: note: op template: _16 = (sizetype) j_73;
Dense.c:97:16: note: 	stmt 0 _16 = (sizetype) j_73;
Dense.c:97:16: note: 	stmt 1 _111 = (sizetype) wi_74;
Dense.c:97:16: note: 	children 0x89865a8
Dense.c:97:16: note: node (external) 0x89865a8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:97:16: note: 	{ j_73, wi_74 }
Dense.c:97:16: note: Cost model analysis: 
Dense.c:97:16: note: Scalar 1 and vector 2 loop part do not match up, skipping scalar part
Dense.c:97:16: note: Cost model analysis for part in loop 2:
  Vector cost: 36
  Scalar cost: 8
Dense.c:97:16: missed: not vectorized: vectorization is not profitable.
Dense.c:97:16: note: ***** The result for vector mode V32QI would be the same
Dense.c:97:16: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:97:16: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:97:16: note: SLPing BB part
Dense.c:97:16: note: Costing subgraph: 
Dense.c:97:16: note: node 0x89864a8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:97:16: note: op template: _16 = (sizetype) j_73;
Dense.c:97:16: note: 	stmt 0 _16 = (sizetype) j_73;
Dense.c:97:16: note: 	stmt 1 _111 = (sizetype) wi_74;
Dense.c:97:16: note: 	children 0x8986528
Dense.c:97:16: note: node (external) 0x8986528 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:97:16: note: 	{ j_73, wi_74 }
Dense.c:97:16: note: Cost model analysis: 
Dense.c:97:16: note: Scalar 1 and vector 2 loop part do not match up, skipping scalar part
Dense.c:97:16: note: Cost model analysis for part in loop 2:
  Vector cost: 36
  Scalar cost: 8
Dense.c:97:16: missed: not vectorized: vectorization is not profitable.
Dense.c:97:16: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:90:50: note: ***** Analysis failed with vector mode V8QI
Dense.c:90:50: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:90:50: note: ***** Analysis failed with vector mode V4QI
Dense.c:62:15: missed: couldn't vectorize loop
Dense.c:62:15: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:75:11: optimized: loop vectorized using 32 byte vectors
Dense.c:75:11: optimized: loop vectorized using 16 byte vectors
Dense.c:66:11: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:881:10: missed: not vectorized: no vectype for stmt: _53 = MEM[(__m256 * {ref-all})w_ptr_82];
 scalar_type: __m256
Dense.c:54:13: note: vectorized 1 loops in function.
Dense.c:57:2: missed: statement clobbers memory: memcpy (_4, dense$baiases_63, _3);
Dense.c:79:2: missed: statement clobbers memory: _27 (pretmp_163, _68, 0);
Dense.c:54:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:54:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:154:107: note: ***** Analysis failed with vector mode V8SI
Dense.c:154:107: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
Dense.c:155:85: note: ***** Analysis failed with vector mode V8SI
Dense.c:155:85: note: ***** The result for vector mode V32QI would be the same
Dense.c:155:85: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:155:85: note: ***** Analysis failed with vector mode V16QI
Dense.c:155:85: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:155:85: note: ***** Analysis failed with vector mode V8QI
Dense.c:155:85: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:155:85: note: ***** Analysis failed with vector mode V4QI
Dense.c:156:107: note: ***** Analysis failed with vector mode V8SI
Dense.c:156:107: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
Dense.c:179:19: missed: couldn't vectorize loop
Dense.c:179:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:169:13: note: vectorized 0 loops in function.
Dense.c:180:38: missed: statement clobbers memory: _5 = rand ();
Dense.c:180:72: missed: statement clobbers memory: _69 = sqrt (_12);
Dense.c:193:19: missed: statement clobbers memory: _24 = PULSE_GetActivationFunctionPtr (_23);
Dense.c:196:34: missed: statement clobbers memory: _28 = aligned_alloc (64, _27);
Dense.c:212:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:213:4: missed: statement clobbers memory: exit (1);
Dense.c:217:9: note: ***** Analysis succeeded with vector mode V8SI
Dense.c:217:9: note: SLPing BB part
Dense.c:217:9: note: Costing subgraph: 
Dense.c:217:9: note: node 0x89b0638 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:217:9: note: op template: <retval>.n_inputs = args$n_inputs_45;
Dense.c:217:9: note: 	stmt 0 <retval>.n_inputs = args$n_inputs_45;
Dense.c:217:9: note: 	stmt 1 <retval>.n_outputs = args$n_outputs_46;
Dense.c:217:9: note: 	children 0x89b06b8
Dense.c:217:9: note: node 0x89b06b8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:217:9: note: op template: args$n_inputs_45 = args.n_inputs;
Dense.c:217:9: note: 	stmt 0 args$n_inputs_45 = args.n_inputs;
Dense.c:217:9: note: 	stmt 1 args$n_outputs_46 = args.n_outputs;
Dense.c:217:9: note: Cost model analysis: 
Dense.c:217:9: note: Cost model analysis for part in loop 0:
  Vector cost: 40
  Scalar cost: 56
Dense.c:217:9: note: Costing subgraph: 
Dense.c:217:9: note: node 0x89b07b8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:217:9: note: op template: <retval>.inputs = IO_34(D);
Dense.c:217:9: note: 	stmt 0 <retval>.inputs = IO_34(D);
Dense.c:217:9: note: 	stmt 1 <retval>.outputs = _22;
Dense.c:217:9: note: 	children 0x89b0838
Dense.c:217:9: note: node (external) 0x89b0838 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:217:9: note: 	{ IO_34(D), _22 }
Dense.c:217:9: note: Cost model analysis: 
Dense.c:217:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:217:9: note: Costing subgraph: 
Dense.c:217:9: note: node 0x89b0938 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:217:9: note: op template: <retval>.feed = layer$feed_33;
Dense.c:217:9: note: 	stmt 0 <retval>.feed = layer$feed_33;
Dense.c:217:9: note: 	stmt 1 <retval>.back = layer$back_43;
Dense.c:217:9: note: 	stmt 2 <retval>.fix = layer$fix_44;
Dense.c:217:9: note: 	stmt 3 <retval>.activate = _25;
Dense.c:217:9: note: 	children 0x89b0a38
Dense.c:217:9: note: node (external) 0x89b0a38 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:217:9: note: 	{ layer$feed_33, layer$back_43, layer$fix_44, _25 }
Dense.c:217:9: note: Cost model analysis: 
Dense.c:217:9: note: Cost model analysis for part in loop 0:
  Vector cost: 64
  Scalar cost: 64
Dense.c:217:9: note: Costing subgraph: 
Dense.c:217:9: note: node 0x89b0b38 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:217:9: note: op template: <retval>.parent = 0B;
Dense.c:217:9: note: 	stmt 0 <retval>.parent = 0B;
Dense.c:217:9: note: 	stmt 1 <retval>.child = 0B;
Dense.c:217:9: note: 	children 0x89b0bb8
Dense.c:217:9: note: node (constant) 0x89b0bb8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:217:9: note: 	{ 0B, 0B }
Dense.c:217:9: note: Cost model analysis: 
Dense.c:217:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:217:9: note: Costing subgraph: 
Dense.c:217:9: note: node 0x89b0c38 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:217:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_32(D);
Dense.c:217:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_32(D);
Dense.c:217:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&<retval> + 96B] = _4;
Dense.c:217:9: note: 	children 0x89b0cb8
Dense.c:217:9: note: node (external) 0x89b0cb8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:217:9: note: 	{ MODEL_32(D), _4 }
Dense.c:217:9: note: Cost model analysis: 
Dense.c:217:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:217:9: note: Basic block will be vectorized using SLP
Dense.c:217:9: optimized: basic block part vectorized using 8 byte vectors
Dense.c:217:9: note: Vectorizing SLP tree:
Dense.c:217:9: note: node 0x89b0638 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:217:9: note: op template: <retval>.n_inputs = args$n_inputs_45;
Dense.c:217:9: note: 	stmt 0 <retval>.n_inputs = args$n_inputs_45;
Dense.c:217:9: note: 	stmt 1 <retval>.n_outputs = args$n_outputs_46;
Dense.c:217:9: note: 	children 0x89b06b8
Dense.c:217:9: note: node 0x89b06b8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:217:9: note: op template: args$n_inputs_45 = args.n_inputs;
Dense.c:217:9: note: 	stmt 0 args$n_inputs_45 = args.n_inputs;
Dense.c:217:9: note: 	stmt 1 args$n_outputs_46 = args.n_outputs;
Dense.c:217:9: note: ------>vectorizing SLP node starting from: args$n_inputs_45 = args.n_inputs;
Dense.c:217:9: note: transform load. ncopies = 1
Dense.c:217:9: note: conflicting alias set types.
Dense.c:217:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: args.n_inputs
Dense.c:217:9: note: created &args
Dense.c:217:9: note: add new stmt: vect_args_n_inputs_45.484_9 = MEM <vector(2) unsigned int> [(void *)&args];
Dense.c:217:9: note: extracting lane for live stmt args$n_inputs_45 = args.n_inputs;
Dense.c:217:9: note: extracting lane for live stmt args$n_outputs_46 = args.n_outputs;
Dense.c:217:9: note: ------>vectorizing SLP node starting from: <retval>.n_inputs = args$n_inputs_45;
Dense.c:217:9: note: vect_is_simple_use: operand args.n_inputs, type of def: internal
Dense.c:217:9: note: vect_is_simple_use: operand args.n_outputs, type of def: internal
Dense.c:217:9: note: transform store. ncopies = 1
Dense.c:217:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: <retval>.n_inputs
Dense.c:217:9: note: created &<retval>.n_inputs
Dense.c:217:9: note: add new stmt: MEM <vector(2) unsigned int> [(unsigned int *)&<retval> + 128B] = vect_args_n_inputs_45.484_9;
Dense.c:217:9: note: vectorizing stmts using SLP.
Dense.c:217:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:217:9: note: Vectorizing SLP tree:
Dense.c:217:9: note: node 0x89b07b8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:217:9: note: op template: <retval>.inputs = IO_34(D);
Dense.c:217:9: note: 	stmt 0 <retval>.inputs = IO_34(D);
Dense.c:217:9: note: 	stmt 1 <retval>.outputs = _22;
Dense.c:217:9: note: 	children 0x89b0838
Dense.c:217:9: note: node (external) 0x89b0838 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:217:9: note: 	{ IO_34(D), _22 }
Dense.c:217:9: note: ------>vectorizing SLP node starting from: <retval>.inputs = IO_34(D);
Dense.c:217:9: note: vect_is_simple_use: operand IO_34(D) + _21, type of def: internal
Dense.c:217:9: note: transform store. ncopies = 1
Dense.c:217:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.inputs
Dense.c:217:9: note: created &<retval>
Dense.c:217:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval>] = _83;
Dense.c:217:9: note: vectorizing stmts using SLP.
Dense.c:217:9: optimized: basic block part vectorized using 32 byte vectors
Dense.c:217:9: note: Vectorizing SLP tree:
Dense.c:217:9: note: node 0x89b0938 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:217:9: note: op template: <retval>.feed = layer$feed_33;
Dense.c:217:9: note: 	stmt 0 <retval>.feed = layer$feed_33;
Dense.c:217:9: note: 	stmt 1 <retval>.back = layer$back_43;
Dense.c:217:9: note: 	stmt 2 <retval>.fix = layer$fix_44;
Dense.c:217:9: note: 	stmt 3 <retval>.activate = _25;
Dense.c:217:9: note: 	children 0x89b0a38
Dense.c:217:9: note: node (external) 0x89b0a38 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:217:9: note: 	{ layer$feed_33, layer$back_43, layer$fix_44, _25 }
Dense.c:217:9: note: ------>vectorizing SLP node starting from: <retval>.feed = layer$feed_33;
Dense.c:217:9: note: vect_is_simple_use: operand layer$back_43 = PHI <layer$back_49(D)(11), _BackDense(9), _SIMD_BackDense(15)>, type of def: internal
Dense.c:217:9: note: vect_is_simple_use: operand layer$fix_44 = PHI <layer$fix_50(D)(11), _FixDense(9), _SIMD_FixDense(15)>, type of def: internal
Dense.c:217:9: note: vect_is_simple_use: operand (void (*<T91a>) (float *, unsigned int, char)) _24, type of def: internal
Dense.c:217:9: note: conflicting alias set types.
Dense.c:217:9: note: transform store. ncopies = 1
Dense.c:217:9: note: create vector_type-pointer variable to type: vector(4) long unsigned int  vectorizing a pointer ref: <retval>.feed
Dense.c:217:9: note: created &<retval>.feed
Dense.c:217:9: note: add new stmt: MEM <vector(4) long unsigned int> [(void *)&<retval> + 32B] = _89;
Dense.c:217:9: note: vectorizing stmts using SLP.
Dense.c:217:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:217:9: note: Vectorizing SLP tree:
Dense.c:217:9: note: node 0x89b0b38 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:217:9: note: op template: <retval>.parent = 0B;
Dense.c:217:9: note: 	stmt 0 <retval>.parent = 0B;
Dense.c:217:9: note: 	stmt 1 <retval>.child = 0B;
Dense.c:217:9: note: 	children 0x89b0bb8
Dense.c:217:9: note: node (constant) 0x89b0bb8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:217:9: note: 	{ 0B, 0B }
Dense.c:217:9: note: ------>vectorizing SLP node starting from: <retval>.parent = 0B;
Dense.c:217:9: note: vect_is_simple_use: operand 0B, type of def: constant
Dense.c:217:9: note: transform store. ncopies = 1
Dense.c:217:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.parent
Dense.c:217:9: note: created &<retval>.parent
Dense.c:217:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer * *)&<retval> + 72B] = { 0, 0 };
Dense.c:217:9: note: vectorizing stmts using SLP.
Dense.c:217:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:217:9: note: Vectorizing SLP tree:
Dense.c:217:9: note: node 0x89b0c38 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:217:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_32(D);
Dense.c:217:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_32(D);
Dense.c:217:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&<retval> + 96B] = _4;
Dense.c:217:9: note: 	children 0x89b0cb8
Dense.c:217:9: note: node (external) 0x89b0cb8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:217:9: note: 	{ MODEL_32(D), _4 }
Dense.c:217:9: note: ------>vectorizing SLP node starting from: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_32(D);
Dense.c:217:9: note: vect_is_simple_use: operand MODEL_32(D) + _3, type of def: internal
Dense.c:217:9: note: transform store. ncopies = 1
Dense.c:217:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B]
Dense.c:217:9: note: created &MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B]
Dense.c:217:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer *)&<retval> + 88B] = _94;
Dense.c:217:9: note: vectorizing stmts using SLP.
Dense.c:217:9: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _28 (pretmp_91, _51, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:19: missed: not vectorized: no vectype for stmt: _19 = *_18;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:161:56: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:161:56: note: SLPing BB part
Dense.c:161:56: note: Costing subgraph: 
Dense.c:161:56: note: node 0x8d99258 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:161:56: note: op template: _4 = (long unsigned int) _3;
Dense.c:161:56: note: 	stmt 0 _4 = (long unsigned int) _3;
Dense.c:161:56: note: 	stmt 1 _7 = (sizetype) _2;
Dense.c:161:56: note: 	children 0x8d99358
Dense.c:161:56: note: node (external) 0x8d99358 (max_nunits=1, refcnt=1) vector(2) unsigned int
Dense.c:161:56: note: 	{ _3, _2 }
Dense.c:161:56: note: Cost model analysis: 
Dense.c:161:56: note: Cost model analysis for part in loop 0:
  Vector cost: 30
  Scalar cost: 8
Dense.c:161:56: missed: not vectorized: vectorization is not profitable.
Dense.c:161:56: note: ***** The result for vector mode V32QI would be the same
Dense.c:161:56: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:161:56: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:161:56: note: SLPing BB part
Dense.c:161:56: note: Costing subgraph: 
Dense.c:161:56: note: node 0x8d99258 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:161:56: note: op template: _4 = (long unsigned int) _3;
Dense.c:161:56: note: 	stmt 0 _4 = (long unsigned int) _3;
Dense.c:161:56: note: 	stmt 1 _7 = (sizetype) _2;
Dense.c:161:56: note: 	children 0x8d992d8
Dense.c:161:56: note: node (external) 0x8d992d8 (max_nunits=1, refcnt=1) vector(2) unsigned int
Dense.c:161:56: note: 	{ _3, _2 }
Dense.c:161:56: note: Cost model analysis: 
Dense.c:161:56: note: Cost model analysis for part in loop 0:
  Vector cost: 30
  Scalar cost: 8
Dense.c:161:56: missed: not vectorized: vectorization is not profitable.
Dense.c:161:56: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:161:56: note: ***** Analysis failed with vector mode V8QI
Dense.c:161:56: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:161:56: note: ***** Analysis failed with vector mode V4QI
Dense.c:145:10: optimized: loop vectorized using 32 byte vectors
Dense.c:145:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:145:10: optimized: loop vectorized using 16 byte vectors
Dense.c:136:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
Dense.c:129:10: optimized: loop vectorized using 32 byte vectors
Dense.c:129:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:129:10: optimized: loop vectorized using 16 byte vectors
Dense.c:120:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
Dense.c:111:13: note: vectorized 2 loops in function.
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_28, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
Dense.c:111:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:111:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:111:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:111:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:111:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:111:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:111:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:111:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:90:23: missed: couldn't vectorize loop
Dense.c:92:31: missed: not vectorized: no vectype for stmt: _119 = this_40(D)->outputs;
 scalar_type: float *
Dense.c:90:23: missed: couldn't vectorize loop
Dense.c:90:23: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:95:16: missed: couldn't vectorize loop
Dense.c:95:16: missed: not vectorized: unsupported control flow in loop.
Dense.c:83:13: note: vectorized 0 loops in function.
Dense.c:86:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:97:16: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:97:16: note: SLPing BB part
Dense.c:97:16: note: Costing subgraph: 
Dense.c:97:16: note: node 0x8d99258 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:97:16: note: op template: _16 = (sizetype) j_73;
Dense.c:97:16: note: 	stmt 0 _16 = (sizetype) j_73;
Dense.c:97:16: note: 	stmt 1 _111 = (sizetype) wi_74;
Dense.c:97:16: note: 	children 0x8d99358
Dense.c:97:16: note: node (external) 0x8d99358 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:97:16: note: 	{ j_73, wi_74 }
Dense.c:97:16: note: Cost model analysis: 
Dense.c:97:16: note: Scalar 1 and vector 2 loop part do not match up, skipping scalar part
Dense.c:97:16: note: Cost model analysis for part in loop 2:
  Vector cost: 36
  Scalar cost: 8
Dense.c:97:16: missed: not vectorized: vectorization is not profitable.
Dense.c:97:16: note: ***** The result for vector mode V32QI would be the same
Dense.c:97:16: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:97:16: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:97:16: note: SLPing BB part
Dense.c:97:16: note: Costing subgraph: 
Dense.c:97:16: note: node 0x8d99258 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:97:16: note: op template: _16 = (sizetype) j_73;
Dense.c:97:16: note: 	stmt 0 _16 = (sizetype) j_73;
Dense.c:97:16: note: 	stmt 1 _111 = (sizetype) wi_74;
Dense.c:97:16: note: 	children 0x8d992d8
Dense.c:97:16: note: node (external) 0x8d992d8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:97:16: note: 	{ j_73, wi_74 }
Dense.c:97:16: note: Cost model analysis: 
Dense.c:97:16: note: Scalar 1 and vector 2 loop part do not match up, skipping scalar part
Dense.c:97:16: note: Cost model analysis for part in loop 2:
  Vector cost: 36
  Scalar cost: 8
Dense.c:97:16: missed: not vectorized: vectorization is not profitable.
Dense.c:97:16: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:90:50: note: ***** Analysis failed with vector mode V8QI
Dense.c:90:50: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:90:50: note: ***** Analysis failed with vector mode V4QI
Dense.c:62:15: missed: couldn't vectorize loop
Dense.c:62:15: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:75:11: optimized: loop vectorized using 32 byte vectors
Dense.c:75:11: optimized: loop vectorized using 16 byte vectors
Dense.c:66:11: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:881:10: missed: not vectorized: no vectype for stmt: _53 = MEM[(__m256 * {ref-all})w_ptr_82];
 scalar_type: __m256
Dense.c:54:13: note: vectorized 1 loops in function.
Dense.c:57:2: missed: statement clobbers memory: memcpy (_4, dense$baiases_63, _3);
Dense.c:79:2: missed: statement clobbers memory: _27 (pretmp_163, _68, 0);
Dense.c:54:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:54:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:154:107: note: ***** Analysis failed with vector mode V8SI
Dense.c:154:107: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
Dense.c:155:85: note: ***** Analysis failed with vector mode V8SI
Dense.c:155:85: note: ***** The result for vector mode V32QI would be the same
Dense.c:155:85: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:155:85: note: ***** Analysis failed with vector mode V16QI
Dense.c:155:85: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:155:85: note: ***** Analysis failed with vector mode V8QI
Dense.c:155:85: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:155:85: note: ***** Analysis failed with vector mode V4QI
Dense.c:156:107: note: ***** Analysis failed with vector mode V8SI
Dense.c:156:107: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
Dense.c:175:19: missed: couldn't vectorize loop
Dense.c:175:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:165:13: note: vectorized 0 loops in function.
Dense.c:176:38: missed: statement clobbers memory: _5 = rand ();
Dense.c:176:72: missed: statement clobbers memory: _69 = sqrt (_12);
Dense.c:189:19: missed: statement clobbers memory: _24 = PULSE_GetActivationFunctionPtr (_23);
Dense.c:192:34: missed: statement clobbers memory: _28 = aligned_alloc (64, _27);
Dense.c:208:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:209:4: missed: statement clobbers memory: exit (1);
Dense.c:213:9: note: ***** Analysis succeeded with vector mode V8SI
Dense.c:213:9: note: SLPing BB part
Dense.c:213:9: note: Costing subgraph: 
Dense.c:213:9: note: node 0x8c20138 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:213:9: note: op template: <retval>.n_inputs = args$n_inputs_45;
Dense.c:213:9: note: 	stmt 0 <retval>.n_inputs = args$n_inputs_45;
Dense.c:213:9: note: 	stmt 1 <retval>.n_outputs = args$n_outputs_46;
Dense.c:213:9: note: 	children 0x8c201b8
Dense.c:213:9: note: node 0x8c201b8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:213:9: note: op template: args$n_inputs_45 = args.n_inputs;
Dense.c:213:9: note: 	stmt 0 args$n_inputs_45 = args.n_inputs;
Dense.c:213:9: note: 	stmt 1 args$n_outputs_46 = args.n_outputs;
Dense.c:213:9: note: Cost model analysis: 
Dense.c:213:9: note: Cost model analysis for part in loop 0:
  Vector cost: 40
  Scalar cost: 56
Dense.c:213:9: note: Costing subgraph: 
Dense.c:213:9: note: node 0x8c202b8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: op template: <retval>.inputs = IO_34(D);
Dense.c:213:9: note: 	stmt 0 <retval>.inputs = IO_34(D);
Dense.c:213:9: note: 	stmt 1 <retval>.outputs = _22;
Dense.c:213:9: note: 	children 0x8c20338
Dense.c:213:9: note: node (external) 0x8c20338 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: 	{ IO_34(D), _22 }
Dense.c:213:9: note: Cost model analysis: 
Dense.c:213:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:213:9: note: Costing subgraph: 
Dense.c:213:9: note: node 0x8c20438 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:213:9: note: op template: <retval>.feed = layer$feed_33;
Dense.c:213:9: note: 	stmt 0 <retval>.feed = layer$feed_33;
Dense.c:213:9: note: 	stmt 1 <retval>.back = layer$back_43;
Dense.c:213:9: note: 	stmt 2 <retval>.fix = layer$fix_44;
Dense.c:213:9: note: 	stmt 3 <retval>.activate = _25;
Dense.c:213:9: note: 	children 0x8c20538
Dense.c:213:9: note: node (external) 0x8c20538 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:213:9: note: 	{ layer$feed_33, layer$back_43, layer$fix_44, _25 }
Dense.c:213:9: note: Cost model analysis: 
Dense.c:213:9: note: Cost model analysis for part in loop 0:
  Vector cost: 64
  Scalar cost: 64
Dense.c:213:9: note: Costing subgraph: 
Dense.c:213:9: note: node 0x8c20638 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: op template: <retval>.parent = 0B;
Dense.c:213:9: note: 	stmt 0 <retval>.parent = 0B;
Dense.c:213:9: note: 	stmt 1 <retval>.child = 0B;
Dense.c:213:9: note: 	children 0x8c206b8
Dense.c:213:9: note: node (constant) 0x8c206b8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: 	{ 0B, 0B }
Dense.c:213:9: note: Cost model analysis: 
Dense.c:213:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:213:9: note: Costing subgraph: 
Dense.c:213:9: note: node 0x8c20738 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_32(D);
Dense.c:213:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_32(D);
Dense.c:213:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&<retval> + 96B] = _4;
Dense.c:213:9: note: 	children 0x8c207b8
Dense.c:213:9: note: node (external) 0x8c207b8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: 	{ MODEL_32(D), _4 }
Dense.c:213:9: note: Cost model analysis: 
Dense.c:213:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:213:9: note: Basic block will be vectorized using SLP
Dense.c:213:9: optimized: basic block part vectorized using 8 byte vectors
Dense.c:213:9: note: Vectorizing SLP tree:
Dense.c:213:9: note: node 0x8c20138 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:213:9: note: op template: <retval>.n_inputs = args$n_inputs_45;
Dense.c:213:9: note: 	stmt 0 <retval>.n_inputs = args$n_inputs_45;
Dense.c:213:9: note: 	stmt 1 <retval>.n_outputs = args$n_outputs_46;
Dense.c:213:9: note: 	children 0x8c201b8
Dense.c:213:9: note: node 0x8c201b8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:213:9: note: op template: args$n_inputs_45 = args.n_inputs;
Dense.c:213:9: note: 	stmt 0 args$n_inputs_45 = args.n_inputs;
Dense.c:213:9: note: 	stmt 1 args$n_outputs_46 = args.n_outputs;
Dense.c:213:9: note: ------>vectorizing SLP node starting from: args$n_inputs_45 = args.n_inputs;
Dense.c:213:9: note: transform load. ncopies = 1
Dense.c:213:9: note: conflicting alias set types.
Dense.c:213:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: args.n_inputs
Dense.c:213:9: note: created &args
Dense.c:213:9: note: add new stmt: vect_args_n_inputs_45.484_9 = MEM <vector(2) unsigned int> [(void *)&args];
Dense.c:213:9: note: extracting lane for live stmt args$n_inputs_45 = args.n_inputs;
Dense.c:213:9: note: extracting lane for live stmt args$n_outputs_46 = args.n_outputs;
Dense.c:213:9: note: ------>vectorizing SLP node starting from: <retval>.n_inputs = args$n_inputs_45;
Dense.c:213:9: note: vect_is_simple_use: operand args.n_inputs, type of def: internal
Dense.c:213:9: note: vect_is_simple_use: operand args.n_outputs, type of def: internal
Dense.c:213:9: note: transform store. ncopies = 1
Dense.c:213:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: <retval>.n_inputs
Dense.c:213:9: note: created &<retval>.n_inputs
Dense.c:213:9: note: add new stmt: MEM <vector(2) unsigned int> [(unsigned int *)&<retval> + 128B] = vect_args_n_inputs_45.484_9;
Dense.c:213:9: note: vectorizing stmts using SLP.
Dense.c:213:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:213:9: note: Vectorizing SLP tree:
Dense.c:213:9: note: node 0x8c202b8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: op template: <retval>.inputs = IO_34(D);
Dense.c:213:9: note: 	stmt 0 <retval>.inputs = IO_34(D);
Dense.c:213:9: note: 	stmt 1 <retval>.outputs = _22;
Dense.c:213:9: note: 	children 0x8c20338
Dense.c:213:9: note: node (external) 0x8c20338 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: 	{ IO_34(D), _22 }
Dense.c:213:9: note: ------>vectorizing SLP node starting from: <retval>.inputs = IO_34(D);
Dense.c:213:9: note: vect_is_simple_use: operand IO_34(D) + _21, type of def: internal
Dense.c:213:9: note: transform store. ncopies = 1
Dense.c:213:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.inputs
Dense.c:213:9: note: created &<retval>
Dense.c:213:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval>] = _83;
Dense.c:213:9: note: vectorizing stmts using SLP.
Dense.c:213:9: optimized: basic block part vectorized using 32 byte vectors
Dense.c:213:9: note: Vectorizing SLP tree:
Dense.c:213:9: note: node 0x8c20438 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:213:9: note: op template: <retval>.feed = layer$feed_33;
Dense.c:213:9: note: 	stmt 0 <retval>.feed = layer$feed_33;
Dense.c:213:9: note: 	stmt 1 <retval>.back = layer$back_43;
Dense.c:213:9: note: 	stmt 2 <retval>.fix = layer$fix_44;
Dense.c:213:9: note: 	stmt 3 <retval>.activate = _25;
Dense.c:213:9: note: 	children 0x8c20538
Dense.c:213:9: note: node (external) 0x8c20538 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:213:9: note: 	{ layer$feed_33, layer$back_43, layer$fix_44, _25 }
Dense.c:213:9: note: ------>vectorizing SLP node starting from: <retval>.feed = layer$feed_33;
Dense.c:213:9: note: vect_is_simple_use: operand layer$back_43 = PHI <layer$back_49(D)(11), _BackDense(9), _SIMD_BackDense(15)>, type of def: internal
Dense.c:213:9: note: vect_is_simple_use: operand layer$fix_44 = PHI <layer$fix_50(D)(11), _FixDense(9), _SIMD_FixDense(15)>, type of def: internal
Dense.c:213:9: note: vect_is_simple_use: operand (void (*<T91a>) (float *, unsigned int, char)) _24, type of def: internal
Dense.c:213:9: note: conflicting alias set types.
Dense.c:213:9: note: transform store. ncopies = 1
Dense.c:213:9: note: create vector_type-pointer variable to type: vector(4) long unsigned int  vectorizing a pointer ref: <retval>.feed
Dense.c:213:9: note: created &<retval>.feed
Dense.c:213:9: note: add new stmt: MEM <vector(4) long unsigned int> [(void *)&<retval> + 32B] = _89;
Dense.c:213:9: note: vectorizing stmts using SLP.
Dense.c:213:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:213:9: note: Vectorizing SLP tree:
Dense.c:213:9: note: node 0x8c20638 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: op template: <retval>.parent = 0B;
Dense.c:213:9: note: 	stmt 0 <retval>.parent = 0B;
Dense.c:213:9: note: 	stmt 1 <retval>.child = 0B;
Dense.c:213:9: note: 	children 0x8c206b8
Dense.c:213:9: note: node (constant) 0x8c206b8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: 	{ 0B, 0B }
Dense.c:213:9: note: ------>vectorizing SLP node starting from: <retval>.parent = 0B;
Dense.c:213:9: note: vect_is_simple_use: operand 0B, type of def: constant
Dense.c:213:9: note: transform store. ncopies = 1
Dense.c:213:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.parent
Dense.c:213:9: note: created &<retval>.parent
Dense.c:213:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer * *)&<retval> + 72B] = { 0, 0 };
Dense.c:213:9: note: vectorizing stmts using SLP.
Dense.c:213:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:213:9: note: Vectorizing SLP tree:
Dense.c:213:9: note: node 0x8c20738 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_32(D);
Dense.c:213:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_32(D);
Dense.c:213:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&<retval> + 96B] = _4;
Dense.c:213:9: note: 	children 0x8c207b8
Dense.c:213:9: note: node (external) 0x8c207b8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: 	{ MODEL_32(D), _4 }
Dense.c:213:9: note: ------>vectorizing SLP node starting from: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_32(D);
Dense.c:213:9: note: vect_is_simple_use: operand MODEL_32(D) + _3, type of def: internal
Dense.c:213:9: note: transform store. ncopies = 1
Dense.c:213:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B]
Dense.c:213:9: note: created &MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B]
Dense.c:213:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer *)&<retval> + 88B] = _94;
Dense.c:213:9: note: vectorizing stmts using SLP.
Dense.c:213:9: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _28 (pretmp_91, _51, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:19: missed: not vectorized: no vectype for stmt: _19 = *_18;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:161:56: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:161:56: note: SLPing BB part
Dense.c:161:56: note: Costing subgraph: 
Dense.c:161:56: note: node 0x70d0258 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:161:56: note: op template: _4 = (long unsigned int) _3;
Dense.c:161:56: note: 	stmt 0 _4 = (long unsigned int) _3;
Dense.c:161:56: note: 	stmt 1 _7 = (sizetype) _2;
Dense.c:161:56: note: 	children 0x70d0358
Dense.c:161:56: note: node (external) 0x70d0358 (max_nunits=1, refcnt=1) vector(2) unsigned int
Dense.c:161:56: note: 	{ _3, _2 }
Dense.c:161:56: note: Cost model analysis: 
Dense.c:161:56: note: Cost model analysis for part in loop 0:
  Vector cost: 30
  Scalar cost: 8
Dense.c:161:56: missed: not vectorized: vectorization is not profitable.
Dense.c:161:56: note: ***** The result for vector mode V32QI would be the same
Dense.c:161:56: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:161:56: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:161:56: note: SLPing BB part
Dense.c:161:56: note: Costing subgraph: 
Dense.c:161:56: note: node 0x70d0258 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:161:56: note: op template: _4 = (long unsigned int) _3;
Dense.c:161:56: note: 	stmt 0 _4 = (long unsigned int) _3;
Dense.c:161:56: note: 	stmt 1 _7 = (sizetype) _2;
Dense.c:161:56: note: 	children 0x70d02d8
Dense.c:161:56: note: node (external) 0x70d02d8 (max_nunits=1, refcnt=1) vector(2) unsigned int
Dense.c:161:56: note: 	{ _3, _2 }
Dense.c:161:56: note: Cost model analysis: 
Dense.c:161:56: note: Cost model analysis for part in loop 0:
  Vector cost: 30
  Scalar cost: 8
Dense.c:161:56: missed: not vectorized: vectorization is not profitable.
Dense.c:161:56: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:161:56: note: ***** Analysis failed with vector mode V8QI
Dense.c:161:56: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:161:56: note: ***** Analysis failed with vector mode V4QI
Dense.c:145:10: optimized: loop vectorized using 32 byte vectors
Dense.c:145:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:145:10: optimized: loop vectorized using 16 byte vectors
Dense.c:136:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
Dense.c:129:10: optimized: loop vectorized using 32 byte vectors
Dense.c:129:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:129:10: optimized: loop vectorized using 16 byte vectors
Dense.c:120:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
Dense.c:111:13: note: vectorized 2 loops in function.
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_28, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
Dense.c:111:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:111:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:111:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:111:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:111:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:111:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:111:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:111:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:90:23: missed: couldn't vectorize loop
Dense.c:92:31: missed: not vectorized: no vectype for stmt: _119 = this_40(D)->outputs;
 scalar_type: float *
Dense.c:90:23: missed: couldn't vectorize loop
Dense.c:90:23: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:95:16: missed: couldn't vectorize loop
Dense.c:95:16: missed: not vectorized: unsupported control flow in loop.
Dense.c:83:13: note: vectorized 0 loops in function.
Dense.c:86:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:97:16: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:97:16: note: SLPing BB part
Dense.c:97:16: note: Costing subgraph: 
Dense.c:97:16: note: node 0x70d0258 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:97:16: note: op template: _16 = (sizetype) j_73;
Dense.c:97:16: note: 	stmt 0 _16 = (sizetype) j_73;
Dense.c:97:16: note: 	stmt 1 _111 = (sizetype) wi_74;
Dense.c:97:16: note: 	children 0x70d0358
Dense.c:97:16: note: node (external) 0x70d0358 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:97:16: note: 	{ j_73, wi_74 }
Dense.c:97:16: note: Cost model analysis: 
Dense.c:97:16: note: Scalar 1 and vector 2 loop part do not match up, skipping scalar part
Dense.c:97:16: note: Cost model analysis for part in loop 2:
  Vector cost: 36
  Scalar cost: 8
Dense.c:97:16: missed: not vectorized: vectorization is not profitable.
Dense.c:97:16: note: ***** The result for vector mode V32QI would be the same
Dense.c:97:16: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:97:16: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:97:16: note: SLPing BB part
Dense.c:97:16: note: Costing subgraph: 
Dense.c:97:16: note: node 0x70d0258 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:97:16: note: op template: _16 = (sizetype) j_73;
Dense.c:97:16: note: 	stmt 0 _16 = (sizetype) j_73;
Dense.c:97:16: note: 	stmt 1 _111 = (sizetype) wi_74;
Dense.c:97:16: note: 	children 0x70d02d8
Dense.c:97:16: note: node (external) 0x70d02d8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:97:16: note: 	{ j_73, wi_74 }
Dense.c:97:16: note: Cost model analysis: 
Dense.c:97:16: note: Scalar 1 and vector 2 loop part do not match up, skipping scalar part
Dense.c:97:16: note: Cost model analysis for part in loop 2:
  Vector cost: 36
  Scalar cost: 8
Dense.c:97:16: missed: not vectorized: vectorization is not profitable.
Dense.c:97:16: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:90:50: note: ***** Analysis failed with vector mode V8QI
Dense.c:90:50: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:90:50: note: ***** Analysis failed with vector mode V4QI
Dense.c:62:15: missed: couldn't vectorize loop
Dense.c:62:15: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:75:11: optimized: loop vectorized using 32 byte vectors
Dense.c:75:11: optimized: loop vectorized using 16 byte vectors
Dense.c:66:11: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:881:10: missed: not vectorized: no vectype for stmt: _53 = MEM[(__m256 * {ref-all})w_ptr_82];
 scalar_type: __m256
Dense.c:54:13: note: vectorized 1 loops in function.
Dense.c:57:2: missed: statement clobbers memory: memcpy (_4, dense$baiases_63, _3);
Dense.c:79:2: missed: statement clobbers memory: _27 (pretmp_163, _68, 0);
Dense.c:54:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:54:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:154:107: note: ***** Analysis failed with vector mode V8SI
Dense.c:154:107: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
Dense.c:155:85: note: ***** Analysis failed with vector mode V8SI
Dense.c:155:85: note: ***** The result for vector mode V32QI would be the same
Dense.c:155:85: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:155:85: note: ***** Analysis failed with vector mode V16QI
Dense.c:155:85: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:155:85: note: ***** Analysis failed with vector mode V8QI
Dense.c:155:85: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:155:85: note: ***** Analysis failed with vector mode V4QI
Dense.c:156:107: note: ***** Analysis failed with vector mode V8SI
Dense.c:156:107: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
Dense.c:175:19: missed: couldn't vectorize loop
Dense.c:175:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:165:13: note: vectorized 0 loops in function.
Dense.c:176:38: missed: statement clobbers memory: _5 = rand ();
Dense.c:176:72: missed: statement clobbers memory: _69 = sqrt (_12);
Dense.c:189:19: missed: statement clobbers memory: _24 = PULSE_GetActivationFunctionPtr (_23);
Dense.c:192:34: missed: statement clobbers memory: _28 = aligned_alloc (64, _27);
Dense.c:208:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:209:4: missed: statement clobbers memory: exit (1);
Dense.c:213:9: note: ***** Analysis succeeded with vector mode V8SI
Dense.c:213:9: note: SLPing BB part
Dense.c:213:9: note: Costing subgraph: 
Dense.c:213:9: note: node 0x6f57138 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:213:9: note: op template: <retval>.n_inputs = args$n_inputs_45;
Dense.c:213:9: note: 	stmt 0 <retval>.n_inputs = args$n_inputs_45;
Dense.c:213:9: note: 	stmt 1 <retval>.n_outputs = args$n_outputs_46;
Dense.c:213:9: note: 	children 0x6f571b8
Dense.c:213:9: note: node 0x6f571b8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:213:9: note: op template: args$n_inputs_45 = args.n_inputs;
Dense.c:213:9: note: 	stmt 0 args$n_inputs_45 = args.n_inputs;
Dense.c:213:9: note: 	stmt 1 args$n_outputs_46 = args.n_outputs;
Dense.c:213:9: note: Cost model analysis: 
Dense.c:213:9: note: Cost model analysis for part in loop 0:
  Vector cost: 40
  Scalar cost: 56
Dense.c:213:9: note: Costing subgraph: 
Dense.c:213:9: note: node 0x6f572b8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: op template: <retval>.inputs = IO_34(D);
Dense.c:213:9: note: 	stmt 0 <retval>.inputs = IO_34(D);
Dense.c:213:9: note: 	stmt 1 <retval>.outputs = _22;
Dense.c:213:9: note: 	children 0x6f57338
Dense.c:213:9: note: node (external) 0x6f57338 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: 	{ IO_34(D), _22 }
Dense.c:213:9: note: Cost model analysis: 
Dense.c:213:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:213:9: note: Costing subgraph: 
Dense.c:213:9: note: node 0x6f57438 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:213:9: note: op template: <retval>.feed = layer$feed_33;
Dense.c:213:9: note: 	stmt 0 <retval>.feed = layer$feed_33;
Dense.c:213:9: note: 	stmt 1 <retval>.back = layer$back_43;
Dense.c:213:9: note: 	stmt 2 <retval>.fix = layer$fix_44;
Dense.c:213:9: note: 	stmt 3 <retval>.activate = _25;
Dense.c:213:9: note: 	children 0x6f57538
Dense.c:213:9: note: node (external) 0x6f57538 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:213:9: note: 	{ layer$feed_33, layer$back_43, layer$fix_44, _25 }
Dense.c:213:9: note: Cost model analysis: 
Dense.c:213:9: note: Cost model analysis for part in loop 0:
  Vector cost: 64
  Scalar cost: 64
Dense.c:213:9: note: Costing subgraph: 
Dense.c:213:9: note: node 0x6f57638 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: op template: <retval>.parent = 0B;
Dense.c:213:9: note: 	stmt 0 <retval>.parent = 0B;
Dense.c:213:9: note: 	stmt 1 <retval>.child = 0B;
Dense.c:213:9: note: 	children 0x6f576b8
Dense.c:213:9: note: node (constant) 0x6f576b8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: 	{ 0B, 0B }
Dense.c:213:9: note: Cost model analysis: 
Dense.c:213:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:213:9: note: Costing subgraph: 
Dense.c:213:9: note: node 0x6f57738 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_32(D);
Dense.c:213:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_32(D);
Dense.c:213:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&<retval> + 96B] = _4;
Dense.c:213:9: note: 	children 0x6f577b8
Dense.c:213:9: note: node (external) 0x6f577b8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: 	{ MODEL_32(D), _4 }
Dense.c:213:9: note: Cost model analysis: 
Dense.c:213:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:213:9: note: Basic block will be vectorized using SLP
Dense.c:213:9: optimized: basic block part vectorized using 8 byte vectors
Dense.c:213:9: note: Vectorizing SLP tree:
Dense.c:213:9: note: node 0x6f57138 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:213:9: note: op template: <retval>.n_inputs = args$n_inputs_45;
Dense.c:213:9: note: 	stmt 0 <retval>.n_inputs = args$n_inputs_45;
Dense.c:213:9: note: 	stmt 1 <retval>.n_outputs = args$n_outputs_46;
Dense.c:213:9: note: 	children 0x6f571b8
Dense.c:213:9: note: node 0x6f571b8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:213:9: note: op template: args$n_inputs_45 = args.n_inputs;
Dense.c:213:9: note: 	stmt 0 args$n_inputs_45 = args.n_inputs;
Dense.c:213:9: note: 	stmt 1 args$n_outputs_46 = args.n_outputs;
Dense.c:213:9: note: ------>vectorizing SLP node starting from: args$n_inputs_45 = args.n_inputs;
Dense.c:213:9: note: transform load. ncopies = 1
Dense.c:213:9: note: conflicting alias set types.
Dense.c:213:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: args.n_inputs
Dense.c:213:9: note: created &args
Dense.c:213:9: note: add new stmt: vect_args_n_inputs_45.484_9 = MEM <vector(2) unsigned int> [(void *)&args];
Dense.c:213:9: note: extracting lane for live stmt args$n_inputs_45 = args.n_inputs;
Dense.c:213:9: note: extracting lane for live stmt args$n_outputs_46 = args.n_outputs;
Dense.c:213:9: note: ------>vectorizing SLP node starting from: <retval>.n_inputs = args$n_inputs_45;
Dense.c:213:9: note: vect_is_simple_use: operand args.n_inputs, type of def: internal
Dense.c:213:9: note: vect_is_simple_use: operand args.n_outputs, type of def: internal
Dense.c:213:9: note: transform store. ncopies = 1
Dense.c:213:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: <retval>.n_inputs
Dense.c:213:9: note: created &<retval>.n_inputs
Dense.c:213:9: note: add new stmt: MEM <vector(2) unsigned int> [(unsigned int *)&<retval> + 128B] = vect_args_n_inputs_45.484_9;
Dense.c:213:9: note: vectorizing stmts using SLP.
Dense.c:213:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:213:9: note: Vectorizing SLP tree:
Dense.c:213:9: note: node 0x6f572b8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: op template: <retval>.inputs = IO_34(D);
Dense.c:213:9: note: 	stmt 0 <retval>.inputs = IO_34(D);
Dense.c:213:9: note: 	stmt 1 <retval>.outputs = _22;
Dense.c:213:9: note: 	children 0x6f57338
Dense.c:213:9: note: node (external) 0x6f57338 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: 	{ IO_34(D), _22 }
Dense.c:213:9: note: ------>vectorizing SLP node starting from: <retval>.inputs = IO_34(D);
Dense.c:213:9: note: vect_is_simple_use: operand IO_34(D) + _21, type of def: internal
Dense.c:213:9: note: transform store. ncopies = 1
Dense.c:213:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.inputs
Dense.c:213:9: note: created &<retval>
Dense.c:213:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval>] = _83;
Dense.c:213:9: note: vectorizing stmts using SLP.
Dense.c:213:9: optimized: basic block part vectorized using 32 byte vectors
Dense.c:213:9: note: Vectorizing SLP tree:
Dense.c:213:9: note: node 0x6f57438 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:213:9: note: op template: <retval>.feed = layer$feed_33;
Dense.c:213:9: note: 	stmt 0 <retval>.feed = layer$feed_33;
Dense.c:213:9: note: 	stmt 1 <retval>.back = layer$back_43;
Dense.c:213:9: note: 	stmt 2 <retval>.fix = layer$fix_44;
Dense.c:213:9: note: 	stmt 3 <retval>.activate = _25;
Dense.c:213:9: note: 	children 0x6f57538
Dense.c:213:9: note: node (external) 0x6f57538 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:213:9: note: 	{ layer$feed_33, layer$back_43, layer$fix_44, _25 }
Dense.c:213:9: note: ------>vectorizing SLP node starting from: <retval>.feed = layer$feed_33;
Dense.c:213:9: note: vect_is_simple_use: operand layer$back_43 = PHI <layer$back_49(D)(11), _BackDense(9), _SIMD_BackDense(15)>, type of def: internal
Dense.c:213:9: note: vect_is_simple_use: operand layer$fix_44 = PHI <layer$fix_50(D)(11), _FixDense(9), _SIMD_FixDense(15)>, type of def: internal
Dense.c:213:9: note: vect_is_simple_use: operand (void (*<T91a>) (float *, unsigned int, char)) _24, type of def: internal
Dense.c:213:9: note: conflicting alias set types.
Dense.c:213:9: note: transform store. ncopies = 1
Dense.c:213:9: note: create vector_type-pointer variable to type: vector(4) long unsigned int  vectorizing a pointer ref: <retval>.feed
Dense.c:213:9: note: created &<retval>.feed
Dense.c:213:9: note: add new stmt: MEM <vector(4) long unsigned int> [(void *)&<retval> + 32B] = _89;
Dense.c:213:9: note: vectorizing stmts using SLP.
Dense.c:213:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:213:9: note: Vectorizing SLP tree:
Dense.c:213:9: note: node 0x6f57638 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: op template: <retval>.parent = 0B;
Dense.c:213:9: note: 	stmt 0 <retval>.parent = 0B;
Dense.c:213:9: note: 	stmt 1 <retval>.child = 0B;
Dense.c:213:9: note: 	children 0x6f576b8
Dense.c:213:9: note: node (constant) 0x6f576b8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: 	{ 0B, 0B }
Dense.c:213:9: note: ------>vectorizing SLP node starting from: <retval>.parent = 0B;
Dense.c:213:9: note: vect_is_simple_use: operand 0B, type of def: constant
Dense.c:213:9: note: transform store. ncopies = 1
Dense.c:213:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.parent
Dense.c:213:9: note: created &<retval>.parent
Dense.c:213:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer * *)&<retval> + 72B] = { 0, 0 };
Dense.c:213:9: note: vectorizing stmts using SLP.
Dense.c:213:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:213:9: note: Vectorizing SLP tree:
Dense.c:213:9: note: node 0x6f57738 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_32(D);
Dense.c:213:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_32(D);
Dense.c:213:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&<retval> + 96B] = _4;
Dense.c:213:9: note: 	children 0x6f577b8
Dense.c:213:9: note: node (external) 0x6f577b8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: 	{ MODEL_32(D), _4 }
Dense.c:213:9: note: ------>vectorizing SLP node starting from: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_32(D);
Dense.c:213:9: note: vect_is_simple_use: operand MODEL_32(D) + _3, type of def: internal
Dense.c:213:9: note: transform store. ncopies = 1
Dense.c:213:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B]
Dense.c:213:9: note: created &MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B]
Dense.c:213:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer *)&<retval> + 88B] = _94;
Dense.c:213:9: note: vectorizing stmts using SLP.
Dense.c:213:9: note: ***** The result for vector mode V32QI would be the same
Activations.c:17:19: optimized: loop vectorized using 32 byte vectors
Activations.c:17:19: optimized: loop vectorized using 16 byte vectors
Activations.c:17:19: optimized: loop vectorized using 32 byte vectors
Activations.c:17:19: optimized: loop vectorized using 16 byte vectors
Activations.c:14:13: note: vectorized 2 loops in function.
Activations.c:14:13: note: ***** Analysis failed with vector mode V8SF
Activations.c:14:13: note: ***** The result for vector mode V32QI would be the same
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V16QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V16QI
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V8QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V8QI
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V4QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V4QI
Activations.c:28:19: missed: couldn't vectorize loop
Activations.c:28:19: missed: not vectorized: unsupported control flow in loop.
Activations.c:28:19: optimized: loop vectorized using 32 byte vectors
Activations.c:28:19: optimized: loop vectorized using 16 byte vectors
Activations.c:25:13: note: vectorized 1 loops in function.
Activations.c:25:13: note: ***** Analysis failed with vector mode V8SF
Activations.c:25:13: note: ***** The result for vector mode V32QI would be the same
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V16QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V16QI
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V8QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V8QI
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V4QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V4QI
Activations.c:38:1: note: ***** Analysis failed with vector mode VOID
Activations.c:5:19: missed: couldn't vectorize loop
Activations.c:9:21: missed: statement clobbers memory: _36 = expf (_18);
Activations.c:5:19: missed: couldn't vectorize loop
Activations.c:9:21: missed: statement clobbers memory: _12 = expf (_11);
Activations.c:3:13: note: vectorized 0 loops in function.
Activations.c:9:21: missed: statement clobbers memory: _12 = expf (_11);
Activations.c:9:21: missed: statement clobbers memory: _36 = expf (_18);
Activations.c:11:1: note: ***** Analysis failed with vector mode V8SF
Activations.c:11:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SF
Activations.c:44:2: note: ***** Analysis failed with vector mode VOID
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:4:18: note: vectorized 0 loops in function.
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_4, inputs_15, _3);
PULSE.c:8:2: missed: statement clobbers memory: _5 (layer_17);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_12, pretmp_26, _9);
PULSE.c:13:28: note: ***** Analysis failed with vector mode V8SI
PULSE.c:13:28: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:22:2: missed: statement clobbers memory: _1 (layer_10(D));
PULSE.c:22:2: missed: statement clobbers memory: _14 (_2);
PULSE.c:22:2: missed: statement clobbers memory: _21 (_15);
PULSE.c:22:2: missed: statement clobbers memory: _28 (_22);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_29);
PULSE.c:26:3: missed: statement clobbers memory: memset (_34, 0, _32);
PULSE.c:26:3: missed: statement clobbers memory: memset (_27, 0, _25);
PULSE.c:26:3: missed: statement clobbers memory: memset (_20, 0, _18);
PULSE.c:26:3: missed: statement clobbers memory: memset (_7, 0, _5);
PULSE.c:28:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:28:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:39: missed: statement clobbers memory: _7 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:32:6: note: vectorized 1 loops in function.
PULSE.c:34:8: missed: statement clobbers memory: _1 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_2);
PULSE.c:39:39: missed: statement clobbers memory: _7 = rand ();
PULSE.c:40:40: missed: statement clobbers memory: _9 = rand ();
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8SI
PULSE.c:32:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:66:16: missed: couldn't vectorize loop
PULSE.c:66:16: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:69:17: missed: couldn't vectorize loop
PULSE.c:69:17: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:78:19: missed: couldn't vectorize loop
PULSE.c:78:19: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:39: missed: statement clobbers memory: _83 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:55:22: missed: couldn't vectorize loop
PULSE.c:55:22: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:47:6: note: vectorized 1 loops in function.
PULSE.c:48:1: missed: statement clobbers memory: saved_stack.7_47 = __builtin_stack_save ();
PULSE.c:49:46: missed: statement clobbers memory: _1 = PULSE_GetLossFunctionPtr (loss_function_48(D));
PULSE.c:50:42: missed: statement clobbers memory: TMP_52 = aligned_alloc (64, _4);
PULSE.c:57:14: missed: statement clobbers memory: _72 = _5 (output_207, TMP_PTR_206);
PULSE.c:65:15: missed: statement clobbers memory: random.4_56 = __builtin_alloca_with_align (_10, 32);
PULSE.c:34:8: missed: statement clobbers memory: _75 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_76);
PULSE.c:39:39: missed: statement clobbers memory: _83 = rand ();
PULSE.c:40:40: missed: statement clobbers memory: _86 = rand ();
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_112, inputs_107, _111);
PULSE.c:8:2: missed: statement clobbers memory: _113 (layer_108);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_119, _118, _117);
PULSE.c:72:11: missed: statement clobbers memory: loss_65 = PULSE_GetLoss_50 (_23, _22, _18, _17);
PULSE.c:22:2: missed: statement clobbers memory: _100 (output_208);
PULSE.c:22:2: missed: statement clobbers memory: _122 (_101);
PULSE.c:22:2: missed: statement clobbers memory: _129 (_123);
PULSE.c:22:2: missed: statement clobbers memory: _136 (_130);
PULSE.c:22:2: missed: statement clobbers memory: _143 (_137);
PULSE.c:22:2: missed: statement clobbers memory: _150 (_144);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_151);
PULSE.c:26:3: missed: statement clobbers memory: memset (_156, 0, _154);
PULSE.c:26:3: missed: statement clobbers memory: memset (_149, 0, _147);
PULSE.c:26:3: missed: statement clobbers memory: memset (_142, 0, _140);
PULSE.c:26:3: missed: statement clobbers memory: memset (_135, 0, _133);
PULSE.c:26:3: missed: statement clobbers memory: memset (_128, 0, _126);
PULSE.c:26:3: missed: statement clobbers memory: memset (_106, 0, _104);
PULSE.c:80:6: missed: statement clobbers memory: _27 (current_212, args);
PULSE.c:86:4: missed: statement clobbers memory: printf ("Epoch: %d | Item: %d | Loss: %.10f | Batch Loss: %.10f\r", i_209, j_210, _31, _30);
PULSE.c:47:6: missed: statement clobbers memory: __builtin_stack_restore (saved_stack.7_47);
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4DI
PULSE.c:47:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:96:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:96:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:131:20: missed: couldn't vectorize loop
PULSE.c:131:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:131:20: missed: couldn't vectorize loop
PULSE.c:131:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:109:20: missed: couldn't vectorize loop
PULSE.c:109:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:109:20: missed: couldn't vectorize loop
PULSE.c:109:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:98:13: note: vectorized 0 loops in function.
PULSE.c:101:39: missed: statement clobbers memory: layers_41 = malloc (_3);
PULSE.c:104:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:124:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:125:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:126:46: missed: statement clobbers memory: WEIGHTS_87 = aligned_alloc (64, 0);
PULSE.c:115:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.106_95];
PULSE.c:116:21: missed: statement clobbers memory: _75 = PULSE_GetDenseWeightsSize (args);
PULSE.c:117:16: missed: statement clobbers memory: _78 = PULSE_GetDenseIOSize (args);
PULSE.c:118:19: missed: statement clobbers memory: _81 = PULSE_GetDenseFixesSize (args);
PULSE.c:119:5: missed: statement clobbers memory: printf ("\n%d\n", FIXES_SIZE_82);
PULSE.c:115:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})_226];
PULSE.c:116:21: missed: statement clobbers memory: _211 = PULSE_GetDenseWeightsSize (args);
PULSE.c:117:16: missed: statement clobbers memory: _214 = PULSE_GetDenseIOSize (args);
PULSE.c:118:19: missed: statement clobbers memory: _217 = PULSE_GetDenseFixesSize (args);
PULSE.c:119:5: missed: statement clobbers memory: printf ("\n%d\n", FIXES_SIZE_218);
PULSE.c:124:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:125:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:126:46: missed: statement clobbers memory: WEIGHTS_46 = aligned_alloc (64, _5);
PULSE.c:127:41: missed: statement clobbers memory: IO_48 = aligned_alloc (64, _7);
PULSE.c:137:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.108_97];
PULSE.c:138:17: missed: statement clobbers memory: *_10 = PULSE_CreateDenseLayer (args, WEIGHTS_PTR_153, IO_PTR_154, 0B); [return slot optimization]
PULSE.c:139:20: missed: statement clobbers memory: _64 = PULSE_GetDenseWeightsSize (args);
PULSE.c:140:15: missed: statement clobbers memory: _67 = PULSE_GetDenseIOSize (args);
PULSE.c:137:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.108_125];
PULSE.c:138:17: missed: statement clobbers memory: *_107 = PULSE_CreateDenseLayer (args, WEIGHTS_PTR_149, IO_PTR_144, 0B); [return slot optimization]
PULSE.c:139:20: missed: statement clobbers memory: _96 = PULSE_GetDenseWeightsSize (args);
PULSE.c:140:15: missed: statement clobbers memory: _59 = PULSE_GetDenseIOSize (args);
PULSE.c:146:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:147:9: note: ***** Analysis succeeded with vector mode V8SI
PULSE.c:147:9: note: SLPing BB part
PULSE.c:147:9: note: Costing subgraph: 
PULSE.c:147:9: note: node 0x3a5a4f8 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:147:9: note: op template: <retval>.weights_size = WEIGHTS_SIZE_165;
PULSE.c:147:9: note: 	stmt 0 <retval>.weights_size = WEIGHTS_SIZE_165;
PULSE.c:147:9: note: 	stmt 1 <retval>.io_size = IO_SIZE_168;
PULSE.c:147:9: note: 	children 0x3a5a578
PULSE.c:147:9: note: node 0x3a5a578 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:147:9: note: op template: WEIGHTS_SIZE_165 = PHI <WEIGHTS_SIZE_162(36), 0(25)>
PULSE.c:147:9: note: 	stmt 0 WEIGHTS_SIZE_165 = PHI <WEIGHTS_SIZE_162(36), 0(25)>
PULSE.c:147:9: note: 	stmt 1 IO_SIZE_168 = PHI <IO_SIZE_161(36), 0(25)>
PULSE.c:147:9: note: 	children 0x3a5a5f8 0x3a5adf8
PULSE.c:147:9: note: node 0x3a5a5f8 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:147:9: note: op template: WEIGHTS_SIZE_162 = PHI <WEIGHTS_SIZE_20(11), WEIGHTS_SIZE_207(76)>
PULSE.c:147:9: note: 	stmt 0 WEIGHTS_SIZE_162 = PHI <WEIGHTS_SIZE_20(11), WEIGHTS_SIZE_207(76)>
PULSE.c:147:9: note: 	stmt 1 IO_SIZE_161 = PHI <IO_SIZE_22(11), IO_SIZE_193(76)>
PULSE.c:147:9: note: 	children 0x3a5a678 0x3a5a9f8
PULSE.c:147:9: note: node 0x3a5a678 (max_nunits=2, refcnt=3) vector(2) unsigned int
PULSE.c:147:9: note: op template: WEIGHTS_SIZE_20 = PHI <WEIGHTS_SIZE_73(77), WEIGHTS_SIZE_76(10)>
PULSE.c:147:9: note: 	stmt 0 WEIGHTS_SIZE_20 = PHI <WEIGHTS_SIZE_73(77), WEIGHTS_SIZE_76(10)>
PULSE.c:147:9: note: 	stmt 1 IO_SIZE_22 = PHI <IO_SIZE_148(77), IO_SIZE_79(10)>
PULSE.c:147:9: note: 	children 0x3a5a6f8 0x3a5a7f8
PULSE.c:147:9: note: node 0x3a5a6f8 (max_nunits=2, refcnt=2) vector(2) unsigned int
PULSE.c:147:9: note: op template: WEIGHTS_SIZE_73 = PHI <WEIGHTS_SIZE_20(73), 0(71)>
PULSE.c:147:9: note: 	stmt 0 WEIGHTS_SIZE_73 = PHI <WEIGHTS_SIZE_20(73), 0(71)>
PULSE.c:147:9: note: 	stmt 1 IO_SIZE_148 = PHI <IO_SIZE_22(73), 0(71)>
PULSE.c:147:9: note: 	children 0x3a5a678 0x3a5a778
PULSE.c:147:9: note: node (constant) 0x3a5a778 (max_nunits=1, refcnt=1) vector(2) unsigned int
PULSE.c:147:9: note: 	{ 0, 0 }
PULSE.c:147:9: note: node 0x3a5a7f8 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:147:9: note: op template: WEIGHTS_SIZE_76 = WEIGHTS_SIZE_73 + _75;
PULSE.c:147:9: note: 	stmt 0 WEIGHTS_SIZE_76 = WEIGHTS_SIZE_73 + _75;
PULSE.c:147:9: note: 	stmt 1 IO_SIZE_79 = _78 + IO_SIZE_148;
PULSE.c:147:9: note: 	children 0x3a5a6f8 0x3a5a978
PULSE.c:147:9: note: node (external) 0x3a5a978 (max_nunits=1, refcnt=1) vector(2) unsigned int
PULSE.c:147:9: note: 	{ _75, _78 }
PULSE.c:147:9: note: node 0x3a5a9f8 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:147:9: note: op template: WEIGHTS_SIZE_207 = PHI <WEIGHTS_SIZE_240(68)>
PULSE.c:147:9: note: 	stmt 0 WEIGHTS_SIZE_207 = PHI <WEIGHTS_SIZE_240(68)>
PULSE.c:147:9: note: 	stmt 1 IO_SIZE_193 = PHI <IO_SIZE_241(68)>
PULSE.c:147:9: note: 	children 0x3a5aa78
PULSE.c:147:9: note: node 0x3a5aa78 (max_nunits=2, refcnt=2) vector(2) unsigned int
PULSE.c:147:9: note: op template: WEIGHTS_SIZE_240 = PHI <WEIGHTS_SIZE_212(59), WEIGHTS_SIZE_201(57)>
PULSE.c:147:9: note: 	stmt 0 WEIGHTS_SIZE_240 = PHI <WEIGHTS_SIZE_212(59), WEIGHTS_SIZE_201(57)>
PULSE.c:147:9: note: 	stmt 1 IO_SIZE_241 = PHI <IO_SIZE_215(59), IO_SIZE_202(57)>
PULSE.c:147:9: note: 	children 0x3a5aaf8 0x3a5abf8
PULSE.c:147:9: note: node 0x3a5aaf8 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:147:9: note: op template: WEIGHTS_SIZE_212 = WEIGHTS_SIZE_201 + _211;
PULSE.c:147:9: note: 	stmt 0 WEIGHTS_SIZE_212 = WEIGHTS_SIZE_201 + _211;
PULSE.c:147:9: note: 	stmt 1 IO_SIZE_215 = _214 + IO_SIZE_202;
PULSE.c:147:9: note: 	children 0x3a5abf8 0x3a5ad78
PULSE.c:147:9: note: node 0x3a5abf8 (max_nunits=2, refcnt=2) vector(2) unsigned int
PULSE.c:147:9: note: op template: WEIGHTS_SIZE_201 = PHI <WEIGHTS_SIZE_245(72), WEIGHTS_SIZE_240(69)>
PULSE.c:147:9: note: 	stmt 0 WEIGHTS_SIZE_201 = PHI <WEIGHTS_SIZE_245(72), WEIGHTS_SIZE_240(69)>
PULSE.c:147:9: note: 	stmt 1 IO_SIZE_202 = PHI <IO_SIZE_246(72), IO_SIZE_241(69)>
PULSE.c:147:9: note: 	children 0x3a5ac78 0x3a5aa78
PULSE.c:147:9: note: node 0x3a5ac78 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:147:9: note: op template: WEIGHTS_SIZE_245 = PHI <WEIGHTS_SIZE_20(74)>
PULSE.c:147:9: note: 	stmt 0 WEIGHTS_SIZE_245 = PHI <WEIGHTS_SIZE_20(74)>
PULSE.c:147:9: note: 	stmt 1 IO_SIZE_246 = PHI <IO_SIZE_22(74)>
PULSE.c:147:9: note: 	children 0x3a5a678
PULSE.c:147:9: note: node (external) 0x3a5ad78 (max_nunits=1, refcnt=1) vector(2) unsigned int
PULSE.c:147:9: note: 	{ _211, _214 }
PULSE.c:147:9: note: node (constant) 0x3a5adf8 (max_nunits=1, refcnt=1) vector(2) unsigned int
PULSE.c:147:9: note: 	{ 0, 0 }
PULSE.c:147:9: note: Cost model analysis: 
PULSE.c:147:9: note: Cost model analysis for part in loop 0:
  Vector cost: 48
  Scalar cost: 48
PULSE.c:147:9: note: Cost model analysis for part in loop 1:
  Vector cost: 44
  Scalar cost: 24
PULSE.c:147:9: missed: not vectorized: vectorization is not profitable.
PULSE.c:147:9: note: Costing subgraph: 
PULSE.c:147:9: note: node 0x3a5aef8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:147:9: note: op template: <retval>.layers = layers_41;
PULSE.c:147:9: note: 	stmt 0 <retval>.layers = layers_41;
PULSE.c:147:9: note: 	stmt 1 <retval>.weights = WEIGHTS_164;
PULSE.c:147:9: note: 	children 0x3a5aff8
PULSE.c:147:9: note: node (external) 0x3a5aff8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:147:9: note: 	{ layers_41, WEIGHTS_164 }
PULSE.c:147:9: note: Cost model analysis: 
PULSE.c:147:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:147:9: note: Costing subgraph: 
PULSE.c:147:9: note: node 0x3a5b078 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:147:9: note: op template: <retval>.io = 0B;
PULSE.c:147:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:147:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:147:9: note: 	children 0x3a5b0f8
PULSE.c:147:9: note: node (constant) 0x3a5b0f8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:147:9: note: 	{ 0B, 0B }
PULSE.c:147:9: note: Cost model analysis: 
PULSE.c:147:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:147:9: note: Basic block will be vectorized using SLP
PULSE.c:147:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:147:9: note: Vectorizing SLP tree:
PULSE.c:147:9: note: node 0x3a5aef8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:147:9: note: op template: <retval>.layers = layers_41;
PULSE.c:147:9: note: 	stmt 0 <retval>.layers = layers_41;
PULSE.c:147:9: note: 	stmt 1 <retval>.weights = WEIGHTS_164;
PULSE.c:147:9: note: 	children 0x3a5aff8
PULSE.c:147:9: note: node (external) 0x3a5aff8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:147:9: note: 	{ layers_41, WEIGHTS_164 }
PULSE.c:147:9: note: ------>vectorizing SLP node starting from: <retval>.layers = layers_41;
PULSE.c:147:9: note: vect_is_simple_use: operand WEIGHTS_164 = PHI <WEIGHTS_46(36), WEIGHTS_87(25)>, type of def: internal
PULSE.c:147:9: note: conflicting alias set types.
PULSE.c:147:9: note: transform store. ncopies = 1
PULSE.c:147:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.layers
PULSE.c:147:9: note: created &<retval>
PULSE.c:147:9: note: add new stmt: MEM <vector(2) long unsigned int> [(void *)&<retval>] = _19;
PULSE.c:147:9: note: vectorizing stmts using SLP.
PULSE.c:147:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:147:9: note: Vectorizing SLP tree:
PULSE.c:147:9: note: node 0x3a5b078 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:147:9: note: op template: <retval>.io = 0B;
PULSE.c:147:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:147:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:147:9: note: 	children 0x3a5b0f8
PULSE.c:147:9: note: node (constant) 0x3a5b0f8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:147:9: note: 	{ 0B, 0B }
PULSE.c:147:9: note: ------>vectorizing SLP node starting from: <retval>.io = 0B;
PULSE.c:147:9: note: vect_is_simple_use: operand 0B, type of def: constant
PULSE.c:147:9: note: transform store. ncopies = 1
PULSE.c:147:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.io
PULSE.c:147:9: note: created &<retval>.io
PULSE.c:147:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval> + 16B] = { 0, 0 };
PULSE.c:147:9: note: vectorizing stmts using SLP.
PULSE.c:147:9: note: ***** The result for vector mode V32QI would be the same
PULSE.c:153:2: missed: statement clobbers memory: free (_1);
PULSE.c:154:2: missed: statement clobbers memory: free (_2);
PULSE.c:155:2: missed: statement clobbers memory: free (_3);
PULSE.c:156:2: missed: statement clobbers memory: free (_4);
PULSE.c:157:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:157:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Loss.c:15:19: optimized: loop vectorized using 32 byte vectors
Loss.c:15:19: optimized:  loop versioned for vectorization because of possible aliasing
Loss.c:15:19: optimized: loop vectorized using 16 byte vectors
Loss.c:12:23: note: vectorized 1 loops in function.
Loss.c:12:23: note: ***** Analysis failed with vector mode V8SF
Loss.c:12:23: note: ***** The result for vector mode V32QI would be the same
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V16QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V16QI
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V8QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V8QI
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V4QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V4QI
Loss.c:7:19: missed: couldn't vectorize loop
Loss.c:8:38: missed: not vectorized: unsupported use in stmt.
Loss.c:4:23: note: vectorized 0 loops in function.
Loss.c:9:13: note: ***** Analysis failed with vector mode V8SF
Loss.c:9:13: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SF
Loss.c:23:2: note: ***** Analysis failed with vector mode VOID
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:4:18: note: vectorized 0 loops in function.
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_4, inputs_15, _3);
PULSE.c:8:2: missed: statement clobbers memory: _5 (layer_17);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_12, pretmp_26, _9);
PULSE.c:13:28: note: ***** Analysis failed with vector mode V8SI
PULSE.c:13:28: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:22:2: missed: statement clobbers memory: _1 (layer_10(D));
PULSE.c:22:2: missed: statement clobbers memory: _14 (_2);
PULSE.c:22:2: missed: statement clobbers memory: _21 (_15);
PULSE.c:22:2: missed: statement clobbers memory: _28 (_22);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_29);
PULSE.c:26:3: missed: statement clobbers memory: memset (_34, 0, _32);
PULSE.c:26:3: missed: statement clobbers memory: memset (_27, 0, _25);
PULSE.c:26:3: missed: statement clobbers memory: memset (_20, 0, _18);
PULSE.c:26:3: missed: statement clobbers memory: memset (_7, 0, _5);
PULSE.c:28:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:28:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:39: missed: statement clobbers memory: _7 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:32:6: note: vectorized 1 loops in function.
PULSE.c:34:8: missed: statement clobbers memory: _1 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_2);
PULSE.c:39:39: missed: statement clobbers memory: _7 = rand ();
PULSE.c:40:40: missed: statement clobbers memory: _9 = rand ();
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8SI
PULSE.c:32:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:68:16: missed: couldn't vectorize loop
PULSE.c:68:16: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:71:17: missed: couldn't vectorize loop
PULSE.c:71:17: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:80:19: missed: couldn't vectorize loop
PULSE.c:80:19: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:39: missed: statement clobbers memory: _84 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:55:22: missed: couldn't vectorize loop
PULSE.c:55:22: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:47:6: note: vectorized 1 loops in function.
PULSE.c:48:1: missed: statement clobbers memory: saved_stack.7_47 = __builtin_stack_save ();
PULSE.c:49:46: missed: statement clobbers memory: _1 = PULSE_GetLossFunctionPtr (loss_function_48(D));
PULSE.c:50:42: missed: statement clobbers memory: TMP_52 = aligned_alloc (64, _4);
PULSE.c:57:22: missed: statement clobbers memory: ACC_72 = _5 (output_208, TMP_PTR_207);
PULSE.c:58:3: missed: statement clobbers memory: printf ("%d", ACC_72);
PULSE.c:67:15: missed: statement clobbers memory: random.4_56 = __builtin_alloca_with_align (_10, 32);
PULSE.c:34:8: missed: statement clobbers memory: _76 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_77);
PULSE.c:39:39: missed: statement clobbers memory: _84 = rand ();
PULSE.c:40:40: missed: statement clobbers memory: _87 = rand ();
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_113, inputs_108, _112);
PULSE.c:8:2: missed: statement clobbers memory: _114 (layer_109);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_120, _119, _118);
PULSE.c:74:11: missed: statement clobbers memory: loss_65 = PULSE_GetLoss_50 (_23, _22, _18, _17);
PULSE.c:22:2: missed: statement clobbers memory: _101 (output_209);
PULSE.c:22:2: missed: statement clobbers memory: _123 (_102);
PULSE.c:22:2: missed: statement clobbers memory: _130 (_124);
PULSE.c:22:2: missed: statement clobbers memory: _137 (_131);
PULSE.c:22:2: missed: statement clobbers memory: _144 (_138);
PULSE.c:22:2: missed: statement clobbers memory: _151 (_145);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_152);
PULSE.c:26:3: missed: statement clobbers memory: memset (_157, 0, _155);
PULSE.c:26:3: missed: statement clobbers memory: memset (_150, 0, _148);
PULSE.c:26:3: missed: statement clobbers memory: memset (_143, 0, _141);
PULSE.c:26:3: missed: statement clobbers memory: memset (_136, 0, _134);
PULSE.c:26:3: missed: statement clobbers memory: memset (_129, 0, _127);
PULSE.c:26:3: missed: statement clobbers memory: memset (_107, 0, _105);
PULSE.c:82:6: missed: statement clobbers memory: _27 (current_213, args);
PULSE.c:88:4: missed: statement clobbers memory: printf ("Epoch: %d | Item: %d | Loss: %.10f | Batch Loss: %.10f\r", i_210, j_211, _31, _30);
PULSE.c:47:6: missed: statement clobbers memory: __builtin_stack_restore (saved_stack.7_47);
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4DI
PULSE.c:47:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:98:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:98:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:133:20: missed: couldn't vectorize loop
PULSE.c:133:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:133:20: missed: couldn't vectorize loop
PULSE.c:133:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:111:20: missed: couldn't vectorize loop
PULSE.c:111:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:111:20: missed: couldn't vectorize loop
PULSE.c:111:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:100:13: note: vectorized 0 loops in function.
PULSE.c:103:39: missed: statement clobbers memory: layers_41 = malloc (_3);
PULSE.c:106:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:126:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:127:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:128:46: missed: statement clobbers memory: WEIGHTS_87 = aligned_alloc (64, 0);
PULSE.c:117:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.106_95];
PULSE.c:118:21: missed: statement clobbers memory: _75 = PULSE_GetDenseWeightsSize (args);
PULSE.c:119:16: missed: statement clobbers memory: _78 = PULSE_GetDenseIOSize (args);
PULSE.c:120:19: missed: statement clobbers memory: _81 = PULSE_GetDenseFixesSize (args);
PULSE.c:121:5: missed: statement clobbers memory: printf ("\n%d\n", FIXES_SIZE_82);
PULSE.c:117:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})_226];
PULSE.c:118:21: missed: statement clobbers memory: _211 = PULSE_GetDenseWeightsSize (args);
PULSE.c:119:16: missed: statement clobbers memory: _214 = PULSE_GetDenseIOSize (args);
PULSE.c:120:19: missed: statement clobbers memory: _217 = PULSE_GetDenseFixesSize (args);
PULSE.c:121:5: missed: statement clobbers memory: printf ("\n%d\n", FIXES_SIZE_218);
PULSE.c:126:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:127:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:128:46: missed: statement clobbers memory: WEIGHTS_46 = aligned_alloc (64, _5);
PULSE.c:129:41: missed: statement clobbers memory: IO_48 = aligned_alloc (64, _7);
PULSE.c:139:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.108_97];
PULSE.c:140:17: missed: statement clobbers memory: *_10 = PULSE_CreateDenseLayer (args, WEIGHTS_PTR_153, IO_PTR_154, 0B); [return slot optimization]
PULSE.c:141:20: missed: statement clobbers memory: _64 = PULSE_GetDenseWeightsSize (args);
PULSE.c:142:15: missed: statement clobbers memory: _67 = PULSE_GetDenseIOSize (args);
PULSE.c:139:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.108_125];
PULSE.c:140:17: missed: statement clobbers memory: *_107 = PULSE_CreateDenseLayer (args, WEIGHTS_PTR_149, IO_PTR_144, 0B); [return slot optimization]
PULSE.c:141:20: missed: statement clobbers memory: _96 = PULSE_GetDenseWeightsSize (args);
PULSE.c:142:15: missed: statement clobbers memory: _59 = PULSE_GetDenseIOSize (args);
PULSE.c:148:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:149:9: note: ***** Analysis succeeded with vector mode V8SI
PULSE.c:149:9: note: SLPing BB part
PULSE.c:149:9: note: Costing subgraph: 
PULSE.c:149:9: note: node 0x4129518 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:149:9: note: op template: <retval>.weights_size = WEIGHTS_SIZE_165;
PULSE.c:149:9: note: 	stmt 0 <retval>.weights_size = WEIGHTS_SIZE_165;
PULSE.c:149:9: note: 	stmt 1 <retval>.io_size = IO_SIZE_168;
PULSE.c:149:9: note: 	children 0x4129598
PULSE.c:149:9: note: node 0x4129598 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:149:9: note: op template: WEIGHTS_SIZE_165 = PHI <WEIGHTS_SIZE_162(36), 0(25)>
PULSE.c:149:9: note: 	stmt 0 WEIGHTS_SIZE_165 = PHI <WEIGHTS_SIZE_162(36), 0(25)>
PULSE.c:149:9: note: 	stmt 1 IO_SIZE_168 = PHI <IO_SIZE_161(36), 0(25)>
PULSE.c:149:9: note: 	children 0x4129618 0x4129e18
PULSE.c:149:9: note: node 0x4129618 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:149:9: note: op template: WEIGHTS_SIZE_162 = PHI <WEIGHTS_SIZE_20(11), WEIGHTS_SIZE_207(76)>
PULSE.c:149:9: note: 	stmt 0 WEIGHTS_SIZE_162 = PHI <WEIGHTS_SIZE_20(11), WEIGHTS_SIZE_207(76)>
PULSE.c:149:9: note: 	stmt 1 IO_SIZE_161 = PHI <IO_SIZE_22(11), IO_SIZE_193(76)>
PULSE.c:149:9: note: 	children 0x4129698 0x4129a18
PULSE.c:149:9: note: node 0x4129698 (max_nunits=2, refcnt=3) vector(2) unsigned int
PULSE.c:149:9: note: op template: WEIGHTS_SIZE_20 = PHI <WEIGHTS_SIZE_73(77), WEIGHTS_SIZE_76(10)>
PULSE.c:149:9: note: 	stmt 0 WEIGHTS_SIZE_20 = PHI <WEIGHTS_SIZE_73(77), WEIGHTS_SIZE_76(10)>
PULSE.c:149:9: note: 	stmt 1 IO_SIZE_22 = PHI <IO_SIZE_148(77), IO_SIZE_79(10)>
PULSE.c:149:9: note: 	children 0x4129718 0x4129818
PULSE.c:149:9: note: node 0x4129718 (max_nunits=2, refcnt=2) vector(2) unsigned int
PULSE.c:149:9: note: op template: WEIGHTS_SIZE_73 = PHI <WEIGHTS_SIZE_20(73), 0(71)>
PULSE.c:149:9: note: 	stmt 0 WEIGHTS_SIZE_73 = PHI <WEIGHTS_SIZE_20(73), 0(71)>
PULSE.c:149:9: note: 	stmt 1 IO_SIZE_148 = PHI <IO_SIZE_22(73), 0(71)>
PULSE.c:149:9: note: 	children 0x4129698 0x4129798
PULSE.c:149:9: note: node (constant) 0x4129798 (max_nunits=1, refcnt=1) vector(2) unsigned int
PULSE.c:149:9: note: 	{ 0, 0 }
PULSE.c:149:9: note: node 0x4129818 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:149:9: note: op template: WEIGHTS_SIZE_76 = WEIGHTS_SIZE_73 + _75;
PULSE.c:149:9: note: 	stmt 0 WEIGHTS_SIZE_76 = WEIGHTS_SIZE_73 + _75;
PULSE.c:149:9: note: 	stmt 1 IO_SIZE_79 = _78 + IO_SIZE_148;
PULSE.c:149:9: note: 	children 0x4129718 0x4129998
PULSE.c:149:9: note: node (external) 0x4129998 (max_nunits=1, refcnt=1) vector(2) unsigned int
PULSE.c:149:9: note: 	{ _75, _78 }
PULSE.c:149:9: note: node 0x4129a18 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:149:9: note: op template: WEIGHTS_SIZE_207 = PHI <WEIGHTS_SIZE_240(68)>
PULSE.c:149:9: note: 	stmt 0 WEIGHTS_SIZE_207 = PHI <WEIGHTS_SIZE_240(68)>
PULSE.c:149:9: note: 	stmt 1 IO_SIZE_193 = PHI <IO_SIZE_241(68)>
PULSE.c:149:9: note: 	children 0x4129a98
PULSE.c:149:9: note: node 0x4129a98 (max_nunits=2, refcnt=2) vector(2) unsigned int
PULSE.c:149:9: note: op template: WEIGHTS_SIZE_240 = PHI <WEIGHTS_SIZE_212(59), WEIGHTS_SIZE_201(57)>
PULSE.c:149:9: note: 	stmt 0 WEIGHTS_SIZE_240 = PHI <WEIGHTS_SIZE_212(59), WEIGHTS_SIZE_201(57)>
PULSE.c:149:9: note: 	stmt 1 IO_SIZE_241 = PHI <IO_SIZE_215(59), IO_SIZE_202(57)>
PULSE.c:149:9: note: 	children 0x4129b18 0x4129c18
PULSE.c:149:9: note: node 0x4129b18 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:149:9: note: op template: WEIGHTS_SIZE_212 = WEIGHTS_SIZE_201 + _211;
PULSE.c:149:9: note: 	stmt 0 WEIGHTS_SIZE_212 = WEIGHTS_SIZE_201 + _211;
PULSE.c:149:9: note: 	stmt 1 IO_SIZE_215 = _214 + IO_SIZE_202;
PULSE.c:149:9: note: 	children 0x4129c18 0x4129d98
PULSE.c:149:9: note: node 0x4129c18 (max_nunits=2, refcnt=2) vector(2) unsigned int
PULSE.c:149:9: note: op template: WEIGHTS_SIZE_201 = PHI <WEIGHTS_SIZE_245(72), WEIGHTS_SIZE_240(69)>
PULSE.c:149:9: note: 	stmt 0 WEIGHTS_SIZE_201 = PHI <WEIGHTS_SIZE_245(72), WEIGHTS_SIZE_240(69)>
PULSE.c:149:9: note: 	stmt 1 IO_SIZE_202 = PHI <IO_SIZE_246(72), IO_SIZE_241(69)>
PULSE.c:149:9: note: 	children 0x4129c98 0x4129a98
PULSE.c:149:9: note: node 0x4129c98 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:149:9: note: op template: WEIGHTS_SIZE_245 = PHI <WEIGHTS_SIZE_20(74)>
PULSE.c:149:9: note: 	stmt 0 WEIGHTS_SIZE_245 = PHI <WEIGHTS_SIZE_20(74)>
PULSE.c:149:9: note: 	stmt 1 IO_SIZE_246 = PHI <IO_SIZE_22(74)>
PULSE.c:149:9: note: 	children 0x4129698
PULSE.c:149:9: note: node (external) 0x4129d98 (max_nunits=1, refcnt=1) vector(2) unsigned int
PULSE.c:149:9: note: 	{ _211, _214 }
PULSE.c:149:9: note: node (constant) 0x4129e18 (max_nunits=1, refcnt=1) vector(2) unsigned int
PULSE.c:149:9: note: 	{ 0, 0 }
PULSE.c:149:9: note: Cost model analysis: 
PULSE.c:149:9: note: Cost model analysis for part in loop 0:
  Vector cost: 48
  Scalar cost: 48
PULSE.c:149:9: note: Cost model analysis for part in loop 1:
  Vector cost: 44
  Scalar cost: 24
PULSE.c:149:9: missed: not vectorized: vectorization is not profitable.
PULSE.c:149:9: note: Costing subgraph: 
PULSE.c:149:9: note: node 0x4129f18 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:149:9: note: op template: <retval>.layers = layers_41;
PULSE.c:149:9: note: 	stmt 0 <retval>.layers = layers_41;
PULSE.c:149:9: note: 	stmt 1 <retval>.weights = WEIGHTS_164;
PULSE.c:149:9: note: 	children 0x412a018
PULSE.c:149:9: note: node (external) 0x412a018 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:149:9: note: 	{ layers_41, WEIGHTS_164 }
PULSE.c:149:9: note: Cost model analysis: 
PULSE.c:149:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:149:9: note: Costing subgraph: 
PULSE.c:149:9: note: node 0x412a098 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:149:9: note: op template: <retval>.io = 0B;
PULSE.c:149:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:149:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:149:9: note: 	children 0x412a118
PULSE.c:149:9: note: node (constant) 0x412a118 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:149:9: note: 	{ 0B, 0B }
PULSE.c:149:9: note: Cost model analysis: 
PULSE.c:149:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:149:9: note: Basic block will be vectorized using SLP
PULSE.c:149:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:149:9: note: Vectorizing SLP tree:
PULSE.c:149:9: note: node 0x4129f18 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:149:9: note: op template: <retval>.layers = layers_41;
PULSE.c:149:9: note: 	stmt 0 <retval>.layers = layers_41;
PULSE.c:149:9: note: 	stmt 1 <retval>.weights = WEIGHTS_164;
PULSE.c:149:9: note: 	children 0x412a018
PULSE.c:149:9: note: node (external) 0x412a018 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:149:9: note: 	{ layers_41, WEIGHTS_164 }
PULSE.c:149:9: note: ------>vectorizing SLP node starting from: <retval>.layers = layers_41;
PULSE.c:149:9: note: vect_is_simple_use: operand WEIGHTS_164 = PHI <WEIGHTS_46(36), WEIGHTS_87(25)>, type of def: internal
PULSE.c:149:9: note: conflicting alias set types.
PULSE.c:149:9: note: transform store. ncopies = 1
PULSE.c:149:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.layers
PULSE.c:149:9: note: created &<retval>
PULSE.c:149:9: note: add new stmt: MEM <vector(2) long unsigned int> [(void *)&<retval>] = _19;
PULSE.c:149:9: note: vectorizing stmts using SLP.
PULSE.c:149:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:149:9: note: Vectorizing SLP tree:
PULSE.c:149:9: note: node 0x412a098 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:149:9: note: op template: <retval>.io = 0B;
PULSE.c:149:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:149:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:149:9: note: 	children 0x412a118
PULSE.c:149:9: note: node (constant) 0x412a118 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:149:9: note: 	{ 0B, 0B }
PULSE.c:149:9: note: ------>vectorizing SLP node starting from: <retval>.io = 0B;
PULSE.c:149:9: note: vect_is_simple_use: operand 0B, type of def: constant
PULSE.c:149:9: note: transform store. ncopies = 1
PULSE.c:149:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.io
PULSE.c:149:9: note: created &<retval>.io
PULSE.c:149:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval> + 16B] = { 0, 0 };
PULSE.c:149:9: note: vectorizing stmts using SLP.
PULSE.c:149:9: note: ***** The result for vector mode V32QI would be the same
PULSE.c:155:2: missed: statement clobbers memory: free (_1);
PULSE.c:156:2: missed: statement clobbers memory: free (_2);
PULSE.c:157:2: missed: statement clobbers memory: free (_3);
PULSE.c:158:2: missed: statement clobbers memory: free (_4);
PULSE.c:159:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:159:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:4:18: note: vectorized 0 loops in function.
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_4, inputs_15, _3);
PULSE.c:8:2: missed: statement clobbers memory: _5 (layer_17);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_12, pretmp_26, _9);
PULSE.c:13:28: note: ***** Analysis failed with vector mode V8SI
PULSE.c:13:28: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:22:2: missed: statement clobbers memory: _1 (layer_10(D));
PULSE.c:22:2: missed: statement clobbers memory: _14 (_2);
PULSE.c:22:2: missed: statement clobbers memory: _21 (_15);
PULSE.c:22:2: missed: statement clobbers memory: _28 (_22);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_29);
PULSE.c:26:3: missed: statement clobbers memory: memset (_34, 0, _32);
PULSE.c:26:3: missed: statement clobbers memory: memset (_27, 0, _25);
PULSE.c:26:3: missed: statement clobbers memory: memset (_20, 0, _18);
PULSE.c:26:3: missed: statement clobbers memory: memset (_7, 0, _5);
PULSE.c:28:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:28:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:39: missed: statement clobbers memory: _7 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:32:6: note: vectorized 1 loops in function.
PULSE.c:34:8: missed: statement clobbers memory: _1 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_2);
PULSE.c:39:39: missed: statement clobbers memory: _7 = rand ();
PULSE.c:40:40: missed: statement clobbers memory: _9 = rand ();
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8SI
PULSE.c:32:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:68:16: missed: couldn't vectorize loop
PULSE.c:68:16: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:71:17: missed: couldn't vectorize loop
PULSE.c:71:17: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:80:19: missed: couldn't vectorize loop
PULSE.c:80:19: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:39: missed: statement clobbers memory: _84 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:55:22: missed: couldn't vectorize loop
PULSE.c:55:22: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:47:6: note: vectorized 1 loops in function.
PULSE.c:48:1: missed: statement clobbers memory: saved_stack.7_47 = __builtin_stack_save ();
PULSE.c:49:46: missed: statement clobbers memory: _1 = PULSE_GetLossFunctionPtr (loss_function_48(D));
PULSE.c:50:42: missed: statement clobbers memory: TMP_52 = aligned_alloc (64, _4);
PULSE.c:57:22: missed: statement clobbers memory: ACC_72 = _5 (output_208, TMP_PTR_207);
PULSE.c:58:3: missed: statement clobbers memory: printf ("%d", ACC_72);
PULSE.c:67:15: missed: statement clobbers memory: random.4_56 = __builtin_alloca_with_align (_10, 32);
PULSE.c:34:8: missed: statement clobbers memory: _76 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_77);
PULSE.c:39:39: missed: statement clobbers memory: _84 = rand ();
PULSE.c:40:40: missed: statement clobbers memory: _87 = rand ();
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_113, inputs_108, _112);
PULSE.c:8:2: missed: statement clobbers memory: _114 (layer_109);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_120, _119, _118);
PULSE.c:74:11: missed: statement clobbers memory: loss_65 = PULSE_GetLoss_50 (_23, _22, _18, _17);
PULSE.c:22:2: missed: statement clobbers memory: _101 (output_209);
PULSE.c:22:2: missed: statement clobbers memory: _123 (_102);
PULSE.c:22:2: missed: statement clobbers memory: _130 (_124);
PULSE.c:22:2: missed: statement clobbers memory: _137 (_131);
PULSE.c:22:2: missed: statement clobbers memory: _144 (_138);
PULSE.c:22:2: missed: statement clobbers memory: _151 (_145);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_152);
PULSE.c:26:3: missed: statement clobbers memory: memset (_157, 0, _155);
PULSE.c:26:3: missed: statement clobbers memory: memset (_150, 0, _148);
PULSE.c:26:3: missed: statement clobbers memory: memset (_143, 0, _141);
PULSE.c:26:3: missed: statement clobbers memory: memset (_136, 0, _134);
PULSE.c:26:3: missed: statement clobbers memory: memset (_129, 0, _127);
PULSE.c:26:3: missed: statement clobbers memory: memset (_107, 0, _105);
PULSE.c:82:6: missed: statement clobbers memory: _27 (current_213, args);
PULSE.c:88:4: missed: statement clobbers memory: printf ("Epoch: %d | Item: %d | Loss: %.10f | Batch Loss: %.10f\r", i_210, j_211, _31, _30);
PULSE.c:47:6: missed: statement clobbers memory: __builtin_stack_restore (saved_stack.7_47);
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4DI
PULSE.c:47:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:98:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:98:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:132:20: missed: couldn't vectorize loop
PULSE.c:132:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:132:20: missed: couldn't vectorize loop
PULSE.c:132:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:111:20: missed: couldn't vectorize loop
PULSE.c:111:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:111:20: missed: couldn't vectorize loop
PULSE.c:111:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:100:13: note: vectorized 0 loops in function.
PULSE.c:103:39: missed: statement clobbers memory: layers_41 = malloc (_3);
PULSE.c:106:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:125:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:126:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:127:46: missed: statement clobbers memory: WEIGHTS_86 = aligned_alloc (64, 0);
PULSE.c:117:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.106_94];
PULSE.c:118:21: missed: statement clobbers memory: _75 = PULSE_GetDenseWeightsSize (args);
PULSE.c:119:16: missed: statement clobbers memory: _78 = PULSE_GetDenseIOSize (args);
PULSE.c:120:19: missed: statement clobbers memory: _81 = PULSE_GetDenseFixesSize (args);
PULSE.c:117:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})_224];
PULSE.c:118:21: missed: statement clobbers memory: _210 = PULSE_GetDenseWeightsSize (args);
PULSE.c:119:16: missed: statement clobbers memory: _213 = PULSE_GetDenseIOSize (args);
PULSE.c:120:19: missed: statement clobbers memory: _216 = PULSE_GetDenseFixesSize (args);
PULSE.c:125:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:126:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:127:46: missed: statement clobbers memory: WEIGHTS_46 = aligned_alloc (64, _5);
PULSE.c:128:41: missed: statement clobbers memory: IO_48 = aligned_alloc (64, _7);
PULSE.c:138:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.108_96];
PULSE.c:139:17: missed: statement clobbers memory: *_10 = PULSE_CreateDenseLayer (args, WEIGHTS_PTR_152, IO_PTR_153, 0B); [return slot optimization]
PULSE.c:140:20: missed: statement clobbers memory: _64 = PULSE_GetDenseWeightsSize (args);
PULSE.c:141:15: missed: statement clobbers memory: _67 = PULSE_GetDenseIOSize (args);
PULSE.c:138:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.108_124];
PULSE.c:139:17: missed: statement clobbers memory: *_106 = PULSE_CreateDenseLayer (args, WEIGHTS_PTR_148, IO_PTR_143, 0B); [return slot optimization]
PULSE.c:140:20: missed: statement clobbers memory: _95 = PULSE_GetDenseWeightsSize (args);
PULSE.c:141:15: missed: statement clobbers memory: _59 = PULSE_GetDenseIOSize (args);
PULSE.c:147:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:148:9: note: ***** Analysis succeeded with vector mode V8SI
PULSE.c:148:9: note: SLPing BB part
PULSE.c:148:9: note: Costing subgraph: 
PULSE.c:148:9: note: node 0x4de84f8 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:148:9: note: op template: <retval>.weights_size = WEIGHTS_SIZE_164;
PULSE.c:148:9: note: 	stmt 0 <retval>.weights_size = WEIGHTS_SIZE_164;
PULSE.c:148:9: note: 	stmt 1 <retval>.io_size = IO_SIZE_167;
PULSE.c:148:9: note: 	children 0x4de8578
PULSE.c:148:9: note: node 0x4de8578 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:148:9: note: op template: WEIGHTS_SIZE_164 = PHI <WEIGHTS_SIZE_161(36), 0(25)>
PULSE.c:148:9: note: 	stmt 0 WEIGHTS_SIZE_164 = PHI <WEIGHTS_SIZE_161(36), 0(25)>
PULSE.c:148:9: note: 	stmt 1 IO_SIZE_167 = PHI <IO_SIZE_160(36), 0(25)>
PULSE.c:148:9: note: 	children 0x4de85f8 0x4de8df8
PULSE.c:148:9: note: node 0x4de85f8 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:148:9: note: op template: WEIGHTS_SIZE_161 = PHI <WEIGHTS_SIZE_20(11), WEIGHTS_SIZE_206(76)>
PULSE.c:148:9: note: 	stmt 0 WEIGHTS_SIZE_161 = PHI <WEIGHTS_SIZE_20(11), WEIGHTS_SIZE_206(76)>
PULSE.c:148:9: note: 	stmt 1 IO_SIZE_160 = PHI <IO_SIZE_22(11), IO_SIZE_192(76)>
PULSE.c:148:9: note: 	children 0x4de8678 0x4de89f8
PULSE.c:148:9: note: node 0x4de8678 (max_nunits=2, refcnt=3) vector(2) unsigned int
PULSE.c:148:9: note: op template: WEIGHTS_SIZE_20 = PHI <WEIGHTS_SIZE_73(77), WEIGHTS_SIZE_76(10)>
PULSE.c:148:9: note: 	stmt 0 WEIGHTS_SIZE_20 = PHI <WEIGHTS_SIZE_73(77), WEIGHTS_SIZE_76(10)>
PULSE.c:148:9: note: 	stmt 1 IO_SIZE_22 = PHI <IO_SIZE_147(77), IO_SIZE_79(10)>
PULSE.c:148:9: note: 	children 0x4de86f8 0x4de87f8
PULSE.c:148:9: note: node 0x4de86f8 (max_nunits=2, refcnt=2) vector(2) unsigned int
PULSE.c:148:9: note: op template: WEIGHTS_SIZE_73 = PHI <WEIGHTS_SIZE_20(73), 0(71)>
PULSE.c:148:9: note: 	stmt 0 WEIGHTS_SIZE_73 = PHI <WEIGHTS_SIZE_20(73), 0(71)>
PULSE.c:148:9: note: 	stmt 1 IO_SIZE_147 = PHI <IO_SIZE_22(73), 0(71)>
PULSE.c:148:9: note: 	children 0x4de8678 0x4de8778
PULSE.c:148:9: note: node (constant) 0x4de8778 (max_nunits=1, refcnt=1) vector(2) unsigned int
PULSE.c:148:9: note: 	{ 0, 0 }
PULSE.c:148:9: note: node 0x4de87f8 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:148:9: note: op template: WEIGHTS_SIZE_76 = WEIGHTS_SIZE_73 + _75;
PULSE.c:148:9: note: 	stmt 0 WEIGHTS_SIZE_76 = WEIGHTS_SIZE_73 + _75;
PULSE.c:148:9: note: 	stmt 1 IO_SIZE_79 = _78 + IO_SIZE_147;
PULSE.c:148:9: note: 	children 0x4de86f8 0x4de8978
PULSE.c:148:9: note: node (external) 0x4de8978 (max_nunits=1, refcnt=1) vector(2) unsigned int
PULSE.c:148:9: note: 	{ _75, _78 }
PULSE.c:148:9: note: node 0x4de89f8 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:148:9: note: op template: WEIGHTS_SIZE_206 = PHI <WEIGHTS_SIZE_238(68)>
PULSE.c:148:9: note: 	stmt 0 WEIGHTS_SIZE_206 = PHI <WEIGHTS_SIZE_238(68)>
PULSE.c:148:9: note: 	stmt 1 IO_SIZE_192 = PHI <IO_SIZE_239(68)>
PULSE.c:148:9: note: 	children 0x4de8a78
PULSE.c:148:9: note: node 0x4de8a78 (max_nunits=2, refcnt=2) vector(2) unsigned int
PULSE.c:148:9: note: op template: WEIGHTS_SIZE_238 = PHI <WEIGHTS_SIZE_211(59), WEIGHTS_SIZE_200(57)>
PULSE.c:148:9: note: 	stmt 0 WEIGHTS_SIZE_238 = PHI <WEIGHTS_SIZE_211(59), WEIGHTS_SIZE_200(57)>
PULSE.c:148:9: note: 	stmt 1 IO_SIZE_239 = PHI <IO_SIZE_214(59), IO_SIZE_201(57)>
PULSE.c:148:9: note: 	children 0x4de8af8 0x4de8bf8
PULSE.c:148:9: note: node 0x4de8af8 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:148:9: note: op template: WEIGHTS_SIZE_211 = WEIGHTS_SIZE_200 + _210;
PULSE.c:148:9: note: 	stmt 0 WEIGHTS_SIZE_211 = WEIGHTS_SIZE_200 + _210;
PULSE.c:148:9: note: 	stmt 1 IO_SIZE_214 = _213 + IO_SIZE_201;
PULSE.c:148:9: note: 	children 0x4de8bf8 0x4de8d78
PULSE.c:148:9: note: node 0x4de8bf8 (max_nunits=2, refcnt=2) vector(2) unsigned int
PULSE.c:148:9: note: op template: WEIGHTS_SIZE_200 = PHI <WEIGHTS_SIZE_243(72), WEIGHTS_SIZE_238(69)>
PULSE.c:148:9: note: 	stmt 0 WEIGHTS_SIZE_200 = PHI <WEIGHTS_SIZE_243(72), WEIGHTS_SIZE_238(69)>
PULSE.c:148:9: note: 	stmt 1 IO_SIZE_201 = PHI <IO_SIZE_244(72), IO_SIZE_239(69)>
PULSE.c:148:9: note: 	children 0x4de8c78 0x4de8a78
PULSE.c:148:9: note: node 0x4de8c78 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:148:9: note: op template: WEIGHTS_SIZE_243 = PHI <WEIGHTS_SIZE_20(74)>
PULSE.c:148:9: note: 	stmt 0 WEIGHTS_SIZE_243 = PHI <WEIGHTS_SIZE_20(74)>
PULSE.c:148:9: note: 	stmt 1 IO_SIZE_244 = PHI <IO_SIZE_22(74)>
PULSE.c:148:9: note: 	children 0x4de8678
PULSE.c:148:9: note: node (external) 0x4de8d78 (max_nunits=1, refcnt=1) vector(2) unsigned int
PULSE.c:148:9: note: 	{ _210, _213 }
PULSE.c:148:9: note: node (constant) 0x4de8df8 (max_nunits=1, refcnt=1) vector(2) unsigned int
PULSE.c:148:9: note: 	{ 0, 0 }
PULSE.c:148:9: note: Cost model analysis: 
PULSE.c:148:9: note: Cost model analysis for part in loop 0:
  Vector cost: 48
  Scalar cost: 48
PULSE.c:148:9: note: Cost model analysis for part in loop 1:
  Vector cost: 44
  Scalar cost: 24
PULSE.c:148:9: missed: not vectorized: vectorization is not profitable.
PULSE.c:148:9: note: Costing subgraph: 
PULSE.c:148:9: note: node 0x4de8ef8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:148:9: note: op template: <retval>.layers = layers_41;
PULSE.c:148:9: note: 	stmt 0 <retval>.layers = layers_41;
PULSE.c:148:9: note: 	stmt 1 <retval>.weights = WEIGHTS_163;
PULSE.c:148:9: note: 	children 0x4de8ff8
PULSE.c:148:9: note: node (external) 0x4de8ff8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:148:9: note: 	{ layers_41, WEIGHTS_163 }
PULSE.c:148:9: note: Cost model analysis: 
PULSE.c:148:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:148:9: note: Costing subgraph: 
PULSE.c:148:9: note: node 0x4de9078 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:148:9: note: op template: <retval>.io = 0B;
PULSE.c:148:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:148:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:148:9: note: 	children 0x4de90f8
PULSE.c:148:9: note: node (constant) 0x4de90f8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:148:9: note: 	{ 0B, 0B }
PULSE.c:148:9: note: Cost model analysis: 
PULSE.c:148:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:148:9: note: Basic block will be vectorized using SLP
PULSE.c:148:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:148:9: note: Vectorizing SLP tree:
PULSE.c:148:9: note: node 0x4de8ef8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:148:9: note: op template: <retval>.layers = layers_41;
PULSE.c:148:9: note: 	stmt 0 <retval>.layers = layers_41;
PULSE.c:148:9: note: 	stmt 1 <retval>.weights = WEIGHTS_163;
PULSE.c:148:9: note: 	children 0x4de8ff8
PULSE.c:148:9: note: node (external) 0x4de8ff8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:148:9: note: 	{ layers_41, WEIGHTS_163 }
PULSE.c:148:9: note: ------>vectorizing SLP node starting from: <retval>.layers = layers_41;
PULSE.c:148:9: note: vect_is_simple_use: operand WEIGHTS_163 = PHI <WEIGHTS_46(36), WEIGHTS_86(25)>, type of def: internal
PULSE.c:148:9: note: conflicting alias set types.
PULSE.c:148:9: note: transform store. ncopies = 1
PULSE.c:148:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.layers
PULSE.c:148:9: note: created &<retval>
PULSE.c:148:9: note: add new stmt: MEM <vector(2) long unsigned int> [(void *)&<retval>] = _19;
PULSE.c:148:9: note: vectorizing stmts using SLP.
PULSE.c:148:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:148:9: note: Vectorizing SLP tree:
PULSE.c:148:9: note: node 0x4de9078 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:148:9: note: op template: <retval>.io = 0B;
PULSE.c:148:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:148:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:148:9: note: 	children 0x4de90f8
PULSE.c:148:9: note: node (constant) 0x4de90f8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:148:9: note: 	{ 0B, 0B }
PULSE.c:148:9: note: ------>vectorizing SLP node starting from: <retval>.io = 0B;
PULSE.c:148:9: note: vect_is_simple_use: operand 0B, type of def: constant
PULSE.c:148:9: note: transform store. ncopies = 1
PULSE.c:148:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.io
PULSE.c:148:9: note: created &<retval>.io
PULSE.c:148:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval> + 16B] = { 0, 0 };
PULSE.c:148:9: note: vectorizing stmts using SLP.
PULSE.c:148:9: note: ***** The result for vector mode V32QI would be the same
PULSE.c:154:2: missed: statement clobbers memory: free (_1);
PULSE.c:155:2: missed: statement clobbers memory: free (_2);
PULSE.c:156:2: missed: statement clobbers memory: free (_3);
PULSE.c:157:2: missed: statement clobbers memory: free (_4);
PULSE.c:158:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:158:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _28 (pretmp_91, _51, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:19: missed: not vectorized: no vectype for stmt: _19 = *_18;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:161:56: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:161:56: note: SLPing BB part
Dense.c:161:56: note: Costing subgraph: 
Dense.c:161:56: note: node 0x7573a28 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:161:56: note: op template: _4 = (long unsigned int) _3;
Dense.c:161:56: note: 	stmt 0 _4 = (long unsigned int) _3;
Dense.c:161:56: note: 	stmt 1 _7 = (sizetype) _2;
Dense.c:161:56: note: 	children 0x7573b28
Dense.c:161:56: note: node (external) 0x7573b28 (max_nunits=1, refcnt=1) vector(2) unsigned int
Dense.c:161:56: note: 	{ _3, _2 }
Dense.c:161:56: note: Cost model analysis: 
Dense.c:161:56: note: Cost model analysis for part in loop 0:
  Vector cost: 30
  Scalar cost: 8
Dense.c:161:56: missed: not vectorized: vectorization is not profitable.
Dense.c:161:56: note: ***** The result for vector mode V32QI would be the same
Dense.c:161:56: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:161:56: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:161:56: note: SLPing BB part
Dense.c:161:56: note: Costing subgraph: 
Dense.c:161:56: note: node 0x7573a28 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:161:56: note: op template: _4 = (long unsigned int) _3;
Dense.c:161:56: note: 	stmt 0 _4 = (long unsigned int) _3;
Dense.c:161:56: note: 	stmt 1 _7 = (sizetype) _2;
Dense.c:161:56: note: 	children 0x7573aa8
Dense.c:161:56: note: node (external) 0x7573aa8 (max_nunits=1, refcnt=1) vector(2) unsigned int
Dense.c:161:56: note: 	{ _3, _2 }
Dense.c:161:56: note: Cost model analysis: 
Dense.c:161:56: note: Cost model analysis for part in loop 0:
  Vector cost: 30
  Scalar cost: 8
Dense.c:161:56: missed: not vectorized: vectorization is not profitable.
Dense.c:161:56: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:161:56: note: ***** Analysis failed with vector mode V8QI
Dense.c:161:56: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:161:56: note: ***** Analysis failed with vector mode V4QI
Dense.c:145:10: optimized: loop vectorized using 32 byte vectors
Dense.c:145:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:145:10: optimized: loop vectorized using 16 byte vectors
Dense.c:136:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
Dense.c:129:10: optimized: loop vectorized using 32 byte vectors
Dense.c:129:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:129:10: optimized: loop vectorized using 16 byte vectors
Dense.c:120:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
Dense.c:111:13: note: vectorized 2 loops in function.
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_28, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
Dense.c:111:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:111:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:111:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:111:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:111:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:111:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:111:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:111:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:90:23: missed: couldn't vectorize loop
Dense.c:92:31: missed: not vectorized: no vectype for stmt: _119 = this_40(D)->outputs;
 scalar_type: float *
Dense.c:90:23: missed: couldn't vectorize loop
Dense.c:90:23: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:95:16: missed: couldn't vectorize loop
Dense.c:95:16: missed: not vectorized: unsupported control flow in loop.
Dense.c:83:13: note: vectorized 0 loops in function.
Dense.c:86:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:97:16: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:97:16: note: SLPing BB part
Dense.c:97:16: note: Costing subgraph: 
Dense.c:97:16: note: node 0x7573a28 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:97:16: note: op template: _16 = (sizetype) j_73;
Dense.c:97:16: note: 	stmt 0 _16 = (sizetype) j_73;
Dense.c:97:16: note: 	stmt 1 _111 = (sizetype) wi_74;
Dense.c:97:16: note: 	children 0x7573b28
Dense.c:97:16: note: node (external) 0x7573b28 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:97:16: note: 	{ j_73, wi_74 }
Dense.c:97:16: note: Cost model analysis: 
Dense.c:97:16: note: Scalar 1 and vector 2 loop part do not match up, skipping scalar part
Dense.c:97:16: note: Cost model analysis for part in loop 2:
  Vector cost: 36
  Scalar cost: 8
Dense.c:97:16: missed: not vectorized: vectorization is not profitable.
Dense.c:97:16: note: ***** The result for vector mode V32QI would be the same
Dense.c:97:16: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:97:16: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:97:16: note: SLPing BB part
Dense.c:97:16: note: Costing subgraph: 
Dense.c:97:16: note: node 0x7573a28 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:97:16: note: op template: _16 = (sizetype) j_73;
Dense.c:97:16: note: 	stmt 0 _16 = (sizetype) j_73;
Dense.c:97:16: note: 	stmt 1 _111 = (sizetype) wi_74;
Dense.c:97:16: note: 	children 0x7573aa8
Dense.c:97:16: note: node (external) 0x7573aa8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:97:16: note: 	{ j_73, wi_74 }
Dense.c:97:16: note: Cost model analysis: 
Dense.c:97:16: note: Scalar 1 and vector 2 loop part do not match up, skipping scalar part
Dense.c:97:16: note: Cost model analysis for part in loop 2:
  Vector cost: 36
  Scalar cost: 8
Dense.c:97:16: missed: not vectorized: vectorization is not profitable.
Dense.c:97:16: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:90:50: note: ***** Analysis failed with vector mode V8QI
Dense.c:90:50: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:90:50: note: ***** Analysis failed with vector mode V4QI
Dense.c:62:15: missed: couldn't vectorize loop
Dense.c:62:15: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:75:11: optimized: loop vectorized using 32 byte vectors
Dense.c:75:11: optimized: loop vectorized using 16 byte vectors
Dense.c:66:11: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:881:10: missed: not vectorized: no vectype for stmt: _53 = MEM[(__m256 * {ref-all})w_ptr_82];
 scalar_type: __m256
Dense.c:54:13: note: vectorized 1 loops in function.
Dense.c:57:2: missed: statement clobbers memory: memcpy (_4, dense$baiases_63, _3);
Dense.c:79:2: missed: statement clobbers memory: _27 (pretmp_163, _68, 0);
Dense.c:54:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:54:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:154:107: note: ***** Analysis failed with vector mode V8SI
Dense.c:154:107: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
Dense.c:155:85: note: ***** Analysis failed with vector mode V8SI
Dense.c:155:85: note: ***** The result for vector mode V32QI would be the same
Dense.c:155:85: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:155:85: note: ***** Analysis failed with vector mode V16QI
Dense.c:155:85: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:155:85: note: ***** Analysis failed with vector mode V8QI
Dense.c:155:85: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:155:85: note: ***** Analysis failed with vector mode V4QI
Dense.c:156:107: note: ***** Analysis failed with vector mode V8SI
Dense.c:156:107: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
Dense.c:175:19: missed: couldn't vectorize loop
Dense.c:175:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:165:13: note: vectorized 0 loops in function.
Dense.c:176:38: missed: statement clobbers memory: _5 = rand ();
Dense.c:176:72: missed: statement clobbers memory: _69 = sqrt (_12);
Dense.c:189:19: missed: statement clobbers memory: _24 = PULSE_GetActivationFunctionPtr (_23);
Dense.c:192:34: missed: statement clobbers memory: _28 = aligned_alloc (64, _27);
Dense.c:208:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:209:4: missed: statement clobbers memory: exit (1);
Dense.c:213:9: note: ***** Analysis succeeded with vector mode V8SI
Dense.c:213:9: note: SLPing BB part
Dense.c:213:9: note: Costing subgraph: 
Dense.c:213:9: note: node 0x7583ab8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:213:9: note: op template: <retval>.n_inputs = args$n_inputs_45;
Dense.c:213:9: note: 	stmt 0 <retval>.n_inputs = args$n_inputs_45;
Dense.c:213:9: note: 	stmt 1 <retval>.n_outputs = args$n_outputs_46;
Dense.c:213:9: note: 	children 0x7583b38
Dense.c:213:9: note: node 0x7583b38 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:213:9: note: op template: args$n_inputs_45 = args.n_inputs;
Dense.c:213:9: note: 	stmt 0 args$n_inputs_45 = args.n_inputs;
Dense.c:213:9: note: 	stmt 1 args$n_outputs_46 = args.n_outputs;
Dense.c:213:9: note: Cost model analysis: 
Dense.c:213:9: note: Cost model analysis for part in loop 0:
  Vector cost: 40
  Scalar cost: 56
Dense.c:213:9: note: Costing subgraph: 
Dense.c:213:9: note: node 0x7583c38 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: op template: <retval>.inputs = IO_34(D);
Dense.c:213:9: note: 	stmt 0 <retval>.inputs = IO_34(D);
Dense.c:213:9: note: 	stmt 1 <retval>.outputs = _22;
Dense.c:213:9: note: 	children 0x7583cb8
Dense.c:213:9: note: node (external) 0x7583cb8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: 	{ IO_34(D), _22 }
Dense.c:213:9: note: Cost model analysis: 
Dense.c:213:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:213:9: note: Costing subgraph: 
Dense.c:213:9: note: node 0x7583db8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:213:9: note: op template: <retval>.feed = layer$feed_33;
Dense.c:213:9: note: 	stmt 0 <retval>.feed = layer$feed_33;
Dense.c:213:9: note: 	stmt 1 <retval>.back = layer$back_43;
Dense.c:213:9: note: 	stmt 2 <retval>.fix = layer$fix_44;
Dense.c:213:9: note: 	stmt 3 <retval>.activate = _25;
Dense.c:213:9: note: 	children 0x7583eb8
Dense.c:213:9: note: node (external) 0x7583eb8 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:213:9: note: 	{ layer$feed_33, layer$back_43, layer$fix_44, _25 }
Dense.c:213:9: note: Cost model analysis: 
Dense.c:213:9: note: Cost model analysis for part in loop 0:
  Vector cost: 64
  Scalar cost: 64
Dense.c:213:9: note: Costing subgraph: 
Dense.c:213:9: note: node 0x7583fb8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: op template: <retval>.parent = 0B;
Dense.c:213:9: note: 	stmt 0 <retval>.parent = 0B;
Dense.c:213:9: note: 	stmt 1 <retval>.child = 0B;
Dense.c:213:9: note: 	children 0x7584038
Dense.c:213:9: note: node (constant) 0x7584038 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: 	{ 0B, 0B }
Dense.c:213:9: note: Cost model analysis: 
Dense.c:213:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:213:9: note: Costing subgraph: 
Dense.c:213:9: note: node 0x75840b8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_32(D);
Dense.c:213:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_32(D);
Dense.c:213:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&<retval> + 96B] = _4;
Dense.c:213:9: note: 	children 0x7584138
Dense.c:213:9: note: node (external) 0x7584138 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: 	{ MODEL_32(D), _4 }
Dense.c:213:9: note: Cost model analysis: 
Dense.c:213:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:213:9: note: Basic block will be vectorized using SLP
Dense.c:213:9: optimized: basic block part vectorized using 8 byte vectors
Dense.c:213:9: note: Vectorizing SLP tree:
Dense.c:213:9: note: node 0x7583ab8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:213:9: note: op template: <retval>.n_inputs = args$n_inputs_45;
Dense.c:213:9: note: 	stmt 0 <retval>.n_inputs = args$n_inputs_45;
Dense.c:213:9: note: 	stmt 1 <retval>.n_outputs = args$n_outputs_46;
Dense.c:213:9: note: 	children 0x7583b38
Dense.c:213:9: note: node 0x7583b38 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:213:9: note: op template: args$n_inputs_45 = args.n_inputs;
Dense.c:213:9: note: 	stmt 0 args$n_inputs_45 = args.n_inputs;
Dense.c:213:9: note: 	stmt 1 args$n_outputs_46 = args.n_outputs;
Dense.c:213:9: note: ------>vectorizing SLP node starting from: args$n_inputs_45 = args.n_inputs;
Dense.c:213:9: note: transform load. ncopies = 1
Dense.c:213:9: note: conflicting alias set types.
Dense.c:213:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: args.n_inputs
Dense.c:213:9: note: created &args
Dense.c:213:9: note: add new stmt: vect_args_n_inputs_45.484_9 = MEM <vector(2) unsigned int> [(void *)&args];
Dense.c:213:9: note: extracting lane for live stmt args$n_inputs_45 = args.n_inputs;
Dense.c:213:9: note: extracting lane for live stmt args$n_outputs_46 = args.n_outputs;
Dense.c:213:9: note: ------>vectorizing SLP node starting from: <retval>.n_inputs = args$n_inputs_45;
Dense.c:213:9: note: vect_is_simple_use: operand args.n_inputs, type of def: internal
Dense.c:213:9: note: vect_is_simple_use: operand args.n_outputs, type of def: internal
Dense.c:213:9: note: transform store. ncopies = 1
Dense.c:213:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: <retval>.n_inputs
Dense.c:213:9: note: created &<retval>.n_inputs
Dense.c:213:9: note: add new stmt: MEM <vector(2) unsigned int> [(unsigned int *)&<retval> + 128B] = vect_args_n_inputs_45.484_9;
Dense.c:213:9: note: vectorizing stmts using SLP.
Dense.c:213:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:213:9: note: Vectorizing SLP tree:
Dense.c:213:9: note: node 0x7583c38 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: op template: <retval>.inputs = IO_34(D);
Dense.c:213:9: note: 	stmt 0 <retval>.inputs = IO_34(D);
Dense.c:213:9: note: 	stmt 1 <retval>.outputs = _22;
Dense.c:213:9: note: 	children 0x7583cb8
Dense.c:213:9: note: node (external) 0x7583cb8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: 	{ IO_34(D), _22 }
Dense.c:213:9: note: ------>vectorizing SLP node starting from: <retval>.inputs = IO_34(D);
Dense.c:213:9: note: vect_is_simple_use: operand IO_34(D) + _21, type of def: internal
Dense.c:213:9: note: transform store. ncopies = 1
Dense.c:213:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.inputs
Dense.c:213:9: note: created &<retval>
Dense.c:213:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval>] = _83;
Dense.c:213:9: note: vectorizing stmts using SLP.
Dense.c:213:9: optimized: basic block part vectorized using 32 byte vectors
Dense.c:213:9: note: Vectorizing SLP tree:
Dense.c:213:9: note: node 0x7583db8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:213:9: note: op template: <retval>.feed = layer$feed_33;
Dense.c:213:9: note: 	stmt 0 <retval>.feed = layer$feed_33;
Dense.c:213:9: note: 	stmt 1 <retval>.back = layer$back_43;
Dense.c:213:9: note: 	stmt 2 <retval>.fix = layer$fix_44;
Dense.c:213:9: note: 	stmt 3 <retval>.activate = _25;
Dense.c:213:9: note: 	children 0x7583eb8
Dense.c:213:9: note: node (external) 0x7583eb8 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:213:9: note: 	{ layer$feed_33, layer$back_43, layer$fix_44, _25 }
Dense.c:213:9: note: ------>vectorizing SLP node starting from: <retval>.feed = layer$feed_33;
Dense.c:213:9: note: vect_is_simple_use: operand layer$back_43 = PHI <layer$back_49(D)(11), _BackDense(9), _SIMD_BackDense(15)>, type of def: internal
Dense.c:213:9: note: vect_is_simple_use: operand layer$fix_44 = PHI <layer$fix_50(D)(11), _FixDense(9), _SIMD_FixDense(15)>, type of def: internal
Dense.c:213:9: note: vect_is_simple_use: operand (void (*<T91a>) (float *, unsigned int, char)) _24, type of def: internal
Dense.c:213:9: note: conflicting alias set types.
Dense.c:213:9: note: transform store. ncopies = 1
Dense.c:213:9: note: create vector_type-pointer variable to type: vector(4) long unsigned int  vectorizing a pointer ref: <retval>.feed
Dense.c:213:9: note: created &<retval>.feed
Dense.c:213:9: note: add new stmt: MEM <vector(4) long unsigned int> [(void *)&<retval> + 32B] = _89;
Dense.c:213:9: note: vectorizing stmts using SLP.
Dense.c:213:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:213:9: note: Vectorizing SLP tree:
Dense.c:213:9: note: node 0x7583fb8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: op template: <retval>.parent = 0B;
Dense.c:213:9: note: 	stmt 0 <retval>.parent = 0B;
Dense.c:213:9: note: 	stmt 1 <retval>.child = 0B;
Dense.c:213:9: note: 	children 0x7584038
Dense.c:213:9: note: node (constant) 0x7584038 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: 	{ 0B, 0B }
Dense.c:213:9: note: ------>vectorizing SLP node starting from: <retval>.parent = 0B;
Dense.c:213:9: note: vect_is_simple_use: operand 0B, type of def: constant
Dense.c:213:9: note: transform store. ncopies = 1
Dense.c:213:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.parent
Dense.c:213:9: note: created &<retval>.parent
Dense.c:213:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer * *)&<retval> + 72B] = { 0, 0 };
Dense.c:213:9: note: vectorizing stmts using SLP.
Dense.c:213:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:213:9: note: Vectorizing SLP tree:
Dense.c:213:9: note: node 0x75840b8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_32(D);
Dense.c:213:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_32(D);
Dense.c:213:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&<retval> + 96B] = _4;
Dense.c:213:9: note: 	children 0x7584138
Dense.c:213:9: note: node (external) 0x7584138 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: 	{ MODEL_32(D), _4 }
Dense.c:213:9: note: ------>vectorizing SLP node starting from: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_32(D);
Dense.c:213:9: note: vect_is_simple_use: operand MODEL_32(D) + _3, type of def: internal
Dense.c:213:9: note: transform store. ncopies = 1
Dense.c:213:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B]
Dense.c:213:9: note: created &MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B]
Dense.c:213:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer *)&<retval> + 88B] = _94;
Dense.c:213:9: note: vectorizing stmts using SLP.
Dense.c:213:9: note: ***** The result for vector mode V32QI would be the same
Activations.c:17:19: optimized: loop vectorized using 32 byte vectors
Activations.c:17:19: optimized: loop vectorized using 16 byte vectors
Activations.c:17:19: optimized: loop vectorized using 32 byte vectors
Activations.c:17:19: optimized: loop vectorized using 16 byte vectors
Activations.c:14:13: note: vectorized 2 loops in function.
Activations.c:14:13: note: ***** Analysis failed with vector mode V8SF
Activations.c:14:13: note: ***** The result for vector mode V32QI would be the same
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V16QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V16QI
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V8QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V8QI
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V4QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V4QI
Activations.c:28:19: missed: couldn't vectorize loop
Activations.c:28:19: missed: not vectorized: unsupported control flow in loop.
Activations.c:28:19: optimized: loop vectorized using 32 byte vectors
Activations.c:28:19: optimized: loop vectorized using 16 byte vectors
Activations.c:25:13: note: vectorized 1 loops in function.
Activations.c:25:13: note: ***** Analysis failed with vector mode V8SF
Activations.c:25:13: note: ***** The result for vector mode V32QI would be the same
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V16QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V16QI
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V8QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V8QI
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V4QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V4QI
Activations.c:38:1: note: ***** Analysis failed with vector mode VOID
Activations.c:5:19: missed: couldn't vectorize loop
Activations.c:9:21: missed: statement clobbers memory: _36 = expf (_18);
Activations.c:5:19: missed: couldn't vectorize loop
Activations.c:9:21: missed: statement clobbers memory: _12 = expf (_11);
Activations.c:3:13: note: vectorized 0 loops in function.
Activations.c:9:21: missed: statement clobbers memory: _12 = expf (_11);
Activations.c:9:21: missed: statement clobbers memory: _36 = expf (_18);
Activations.c:11:1: note: ***** Analysis failed with vector mode V8SF
Activations.c:11:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SF
Activations.c:44:2: note: ***** Analysis failed with vector mode VOID
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:4:18: note: vectorized 0 loops in function.
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_4, inputs_15, _3);
PULSE.c:8:2: missed: statement clobbers memory: _5 (layer_17);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_12, pretmp_26, _9);
PULSE.c:13:28: note: ***** Analysis failed with vector mode V8SI
PULSE.c:13:28: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:22:2: missed: statement clobbers memory: _1 (layer_10(D));
PULSE.c:22:2: missed: statement clobbers memory: _14 (_2);
PULSE.c:22:2: missed: statement clobbers memory: _21 (_15);
PULSE.c:22:2: missed: statement clobbers memory: _28 (_22);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_29);
PULSE.c:26:3: missed: statement clobbers memory: memset (_34, 0, _32);
PULSE.c:26:3: missed: statement clobbers memory: memset (_27, 0, _25);
PULSE.c:26:3: missed: statement clobbers memory: memset (_20, 0, _18);
PULSE.c:26:3: missed: statement clobbers memory: memset (_7, 0, _5);
PULSE.c:28:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:28:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:39: missed: statement clobbers memory: _7 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:32:6: note: vectorized 1 loops in function.
PULSE.c:34:8: missed: statement clobbers memory: _1 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_2);
PULSE.c:39:39: missed: statement clobbers memory: _7 = rand ();
PULSE.c:40:40: missed: statement clobbers memory: _9 = rand ();
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8SI
PULSE.c:32:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:68:16: missed: couldn't vectorize loop
PULSE.c:68:16: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:71:17: missed: couldn't vectorize loop
PULSE.c:71:17: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:80:19: missed: couldn't vectorize loop
PULSE.c:80:19: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:39: missed: statement clobbers memory: _84 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:55:22: missed: couldn't vectorize loop
PULSE.c:55:22: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:47:6: note: vectorized 1 loops in function.
PULSE.c:48:1: missed: statement clobbers memory: saved_stack.7_47 = __builtin_stack_save ();
PULSE.c:49:46: missed: statement clobbers memory: _1 = PULSE_GetLossFunctionPtr (loss_function_48(D));
PULSE.c:50:42: missed: statement clobbers memory: TMP_52 = aligned_alloc (64, _4);
PULSE.c:57:22: missed: statement clobbers memory: ACC_72 = _5 (output_208, TMP_PTR_207);
PULSE.c:58:3: missed: statement clobbers memory: printf ("%d\n", ACC_72);
PULSE.c:67:15: missed: statement clobbers memory: random.4_56 = __builtin_alloca_with_align (_10, 32);
PULSE.c:34:8: missed: statement clobbers memory: _76 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_77);
PULSE.c:39:39: missed: statement clobbers memory: _84 = rand ();
PULSE.c:40:40: missed: statement clobbers memory: _87 = rand ();
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_113, inputs_108, _112);
PULSE.c:8:2: missed: statement clobbers memory: _114 (layer_109);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_120, _119, _118);
PULSE.c:74:11: missed: statement clobbers memory: loss_65 = PULSE_GetLoss_50 (_23, _22, _18, _17);
PULSE.c:22:2: missed: statement clobbers memory: _101 (output_209);
PULSE.c:22:2: missed: statement clobbers memory: _123 (_102);
PULSE.c:22:2: missed: statement clobbers memory: _130 (_124);
PULSE.c:22:2: missed: statement clobbers memory: _137 (_131);
PULSE.c:22:2: missed: statement clobbers memory: _144 (_138);
PULSE.c:22:2: missed: statement clobbers memory: _151 (_145);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_152);
PULSE.c:26:3: missed: statement clobbers memory: memset (_157, 0, _155);
PULSE.c:26:3: missed: statement clobbers memory: memset (_150, 0, _148);
PULSE.c:26:3: missed: statement clobbers memory: memset (_143, 0, _141);
PULSE.c:26:3: missed: statement clobbers memory: memset (_136, 0, _134);
PULSE.c:26:3: missed: statement clobbers memory: memset (_129, 0, _127);
PULSE.c:26:3: missed: statement clobbers memory: memset (_107, 0, _105);
PULSE.c:82:6: missed: statement clobbers memory: _27 (current_213, args);
PULSE.c:88:4: missed: statement clobbers memory: printf ("Epoch: %d | Item: %d | Loss: %.10f | Batch Loss: %.10f\r", i_210, j_211, _31, _30);
PULSE.c:47:6: missed: statement clobbers memory: __builtin_stack_restore (saved_stack.7_47);
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4DI
PULSE.c:47:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:98:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:98:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:132:20: missed: couldn't vectorize loop
PULSE.c:132:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:132:20: missed: couldn't vectorize loop
PULSE.c:132:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:111:20: missed: couldn't vectorize loop
PULSE.c:111:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:111:20: missed: couldn't vectorize loop
PULSE.c:111:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:100:13: note: vectorized 0 loops in function.
PULSE.c:103:39: missed: statement clobbers memory: layers_41 = malloc (_3);
PULSE.c:106:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:125:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:126:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:127:46: missed: statement clobbers memory: WEIGHTS_86 = aligned_alloc (64, 0);
PULSE.c:117:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.106_94];
PULSE.c:118:21: missed: statement clobbers memory: _75 = PULSE_GetDenseWeightsSize (args);
PULSE.c:119:16: missed: statement clobbers memory: _78 = PULSE_GetDenseIOSize (args);
PULSE.c:120:19: missed: statement clobbers memory: _81 = PULSE_GetDenseFixesSize (args);
PULSE.c:117:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})_224];
PULSE.c:118:21: missed: statement clobbers memory: _210 = PULSE_GetDenseWeightsSize (args);
PULSE.c:119:16: missed: statement clobbers memory: _213 = PULSE_GetDenseIOSize (args);
PULSE.c:120:19: missed: statement clobbers memory: _216 = PULSE_GetDenseFixesSize (args);
PULSE.c:125:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:126:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:127:46: missed: statement clobbers memory: WEIGHTS_46 = aligned_alloc (64, _5);
PULSE.c:128:41: missed: statement clobbers memory: IO_48 = aligned_alloc (64, _7);
PULSE.c:138:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.108_96];
PULSE.c:139:17: missed: statement clobbers memory: *_10 = PULSE_CreateDenseLayer (args, WEIGHTS_PTR_152, IO_PTR_153, 0B); [return slot optimization]
PULSE.c:140:20: missed: statement clobbers memory: _64 = PULSE_GetDenseWeightsSize (args);
PULSE.c:141:15: missed: statement clobbers memory: _67 = PULSE_GetDenseIOSize (args);
PULSE.c:138:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.108_124];
PULSE.c:139:17: missed: statement clobbers memory: *_106 = PULSE_CreateDenseLayer (args, WEIGHTS_PTR_148, IO_PTR_143, 0B); [return slot optimization]
PULSE.c:140:20: missed: statement clobbers memory: _95 = PULSE_GetDenseWeightsSize (args);
PULSE.c:141:15: missed: statement clobbers memory: _59 = PULSE_GetDenseIOSize (args);
PULSE.c:147:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:148:9: note: ***** Analysis succeeded with vector mode V8SI
PULSE.c:148:9: note: SLPing BB part
PULSE.c:148:9: note: Costing subgraph: 
PULSE.c:148:9: note: node 0x487c4f8 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:148:9: note: op template: <retval>.weights_size = WEIGHTS_SIZE_164;
PULSE.c:148:9: note: 	stmt 0 <retval>.weights_size = WEIGHTS_SIZE_164;
PULSE.c:148:9: note: 	stmt 1 <retval>.io_size = IO_SIZE_167;
PULSE.c:148:9: note: 	children 0x487c578
PULSE.c:148:9: note: node 0x487c578 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:148:9: note: op template: WEIGHTS_SIZE_164 = PHI <WEIGHTS_SIZE_161(36), 0(25)>
PULSE.c:148:9: note: 	stmt 0 WEIGHTS_SIZE_164 = PHI <WEIGHTS_SIZE_161(36), 0(25)>
PULSE.c:148:9: note: 	stmt 1 IO_SIZE_167 = PHI <IO_SIZE_160(36), 0(25)>
PULSE.c:148:9: note: 	children 0x487c5f8 0x487cdf8
PULSE.c:148:9: note: node 0x487c5f8 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:148:9: note: op template: WEIGHTS_SIZE_161 = PHI <WEIGHTS_SIZE_20(11), WEIGHTS_SIZE_206(76)>
PULSE.c:148:9: note: 	stmt 0 WEIGHTS_SIZE_161 = PHI <WEIGHTS_SIZE_20(11), WEIGHTS_SIZE_206(76)>
PULSE.c:148:9: note: 	stmt 1 IO_SIZE_160 = PHI <IO_SIZE_22(11), IO_SIZE_192(76)>
PULSE.c:148:9: note: 	children 0x487c678 0x487c9f8
PULSE.c:148:9: note: node 0x487c678 (max_nunits=2, refcnt=3) vector(2) unsigned int
PULSE.c:148:9: note: op template: WEIGHTS_SIZE_20 = PHI <WEIGHTS_SIZE_73(77), WEIGHTS_SIZE_76(10)>
PULSE.c:148:9: note: 	stmt 0 WEIGHTS_SIZE_20 = PHI <WEIGHTS_SIZE_73(77), WEIGHTS_SIZE_76(10)>
PULSE.c:148:9: note: 	stmt 1 IO_SIZE_22 = PHI <IO_SIZE_147(77), IO_SIZE_79(10)>
PULSE.c:148:9: note: 	children 0x487c6f8 0x487c7f8
PULSE.c:148:9: note: node 0x487c6f8 (max_nunits=2, refcnt=2) vector(2) unsigned int
PULSE.c:148:9: note: op template: WEIGHTS_SIZE_73 = PHI <WEIGHTS_SIZE_20(73), 0(71)>
PULSE.c:148:9: note: 	stmt 0 WEIGHTS_SIZE_73 = PHI <WEIGHTS_SIZE_20(73), 0(71)>
PULSE.c:148:9: note: 	stmt 1 IO_SIZE_147 = PHI <IO_SIZE_22(73), 0(71)>
PULSE.c:148:9: note: 	children 0x487c678 0x487c778
PULSE.c:148:9: note: node (constant) 0x487c778 (max_nunits=1, refcnt=1) vector(2) unsigned int
PULSE.c:148:9: note: 	{ 0, 0 }
PULSE.c:148:9: note: node 0x487c7f8 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:148:9: note: op template: WEIGHTS_SIZE_76 = WEIGHTS_SIZE_73 + _75;
PULSE.c:148:9: note: 	stmt 0 WEIGHTS_SIZE_76 = WEIGHTS_SIZE_73 + _75;
PULSE.c:148:9: note: 	stmt 1 IO_SIZE_79 = _78 + IO_SIZE_147;
PULSE.c:148:9: note: 	children 0x487c6f8 0x487c978
PULSE.c:148:9: note: node (external) 0x487c978 (max_nunits=1, refcnt=1) vector(2) unsigned int
PULSE.c:148:9: note: 	{ _75, _78 }
PULSE.c:148:9: note: node 0x487c9f8 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:148:9: note: op template: WEIGHTS_SIZE_206 = PHI <WEIGHTS_SIZE_238(68)>
PULSE.c:148:9: note: 	stmt 0 WEIGHTS_SIZE_206 = PHI <WEIGHTS_SIZE_238(68)>
PULSE.c:148:9: note: 	stmt 1 IO_SIZE_192 = PHI <IO_SIZE_239(68)>
PULSE.c:148:9: note: 	children 0x487ca78
PULSE.c:148:9: note: node 0x487ca78 (max_nunits=2, refcnt=2) vector(2) unsigned int
PULSE.c:148:9: note: op template: WEIGHTS_SIZE_238 = PHI <WEIGHTS_SIZE_211(59), WEIGHTS_SIZE_200(57)>
PULSE.c:148:9: note: 	stmt 0 WEIGHTS_SIZE_238 = PHI <WEIGHTS_SIZE_211(59), WEIGHTS_SIZE_200(57)>
PULSE.c:148:9: note: 	stmt 1 IO_SIZE_239 = PHI <IO_SIZE_214(59), IO_SIZE_201(57)>
PULSE.c:148:9: note: 	children 0x487caf8 0x487cbf8
PULSE.c:148:9: note: node 0x487caf8 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:148:9: note: op template: WEIGHTS_SIZE_211 = WEIGHTS_SIZE_200 + _210;
PULSE.c:148:9: note: 	stmt 0 WEIGHTS_SIZE_211 = WEIGHTS_SIZE_200 + _210;
PULSE.c:148:9: note: 	stmt 1 IO_SIZE_214 = _213 + IO_SIZE_201;
PULSE.c:148:9: note: 	children 0x487cbf8 0x487cd78
PULSE.c:148:9: note: node 0x487cbf8 (max_nunits=2, refcnt=2) vector(2) unsigned int
PULSE.c:148:9: note: op template: WEIGHTS_SIZE_200 = PHI <WEIGHTS_SIZE_243(72), WEIGHTS_SIZE_238(69)>
PULSE.c:148:9: note: 	stmt 0 WEIGHTS_SIZE_200 = PHI <WEIGHTS_SIZE_243(72), WEIGHTS_SIZE_238(69)>
PULSE.c:148:9: note: 	stmt 1 IO_SIZE_201 = PHI <IO_SIZE_244(72), IO_SIZE_239(69)>
PULSE.c:148:9: note: 	children 0x487cc78 0x487ca78
PULSE.c:148:9: note: node 0x487cc78 (max_nunits=2, refcnt=1) vector(2) unsigned int
PULSE.c:148:9: note: op template: WEIGHTS_SIZE_243 = PHI <WEIGHTS_SIZE_20(74)>
PULSE.c:148:9: note: 	stmt 0 WEIGHTS_SIZE_243 = PHI <WEIGHTS_SIZE_20(74)>
PULSE.c:148:9: note: 	stmt 1 IO_SIZE_244 = PHI <IO_SIZE_22(74)>
PULSE.c:148:9: note: 	children 0x487c678
PULSE.c:148:9: note: node (external) 0x487cd78 (max_nunits=1, refcnt=1) vector(2) unsigned int
PULSE.c:148:9: note: 	{ _210, _213 }
PULSE.c:148:9: note: node (constant) 0x487cdf8 (max_nunits=1, refcnt=1) vector(2) unsigned int
PULSE.c:148:9: note: 	{ 0, 0 }
PULSE.c:148:9: note: Cost model analysis: 
PULSE.c:148:9: note: Cost model analysis for part in loop 0:
  Vector cost: 48
  Scalar cost: 48
PULSE.c:148:9: note: Cost model analysis for part in loop 1:
  Vector cost: 44
  Scalar cost: 24
PULSE.c:148:9: missed: not vectorized: vectorization is not profitable.
PULSE.c:148:9: note: Costing subgraph: 
PULSE.c:148:9: note: node 0x487cef8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:148:9: note: op template: <retval>.layers = layers_41;
PULSE.c:148:9: note: 	stmt 0 <retval>.layers = layers_41;
PULSE.c:148:9: note: 	stmt 1 <retval>.weights = WEIGHTS_163;
PULSE.c:148:9: note: 	children 0x487cff8
PULSE.c:148:9: note: node (external) 0x487cff8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:148:9: note: 	{ layers_41, WEIGHTS_163 }
PULSE.c:148:9: note: Cost model analysis: 
PULSE.c:148:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:148:9: note: Costing subgraph: 
PULSE.c:148:9: note: node 0x487d078 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:148:9: note: op template: <retval>.io = 0B;
PULSE.c:148:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:148:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:148:9: note: 	children 0x487d0f8
PULSE.c:148:9: note: node (constant) 0x487d0f8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:148:9: note: 	{ 0B, 0B }
PULSE.c:148:9: note: Cost model analysis: 
PULSE.c:148:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:148:9: note: Basic block will be vectorized using SLP
PULSE.c:148:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:148:9: note: Vectorizing SLP tree:
PULSE.c:148:9: note: node 0x487cef8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:148:9: note: op template: <retval>.layers = layers_41;
PULSE.c:148:9: note: 	stmt 0 <retval>.layers = layers_41;
PULSE.c:148:9: note: 	stmt 1 <retval>.weights = WEIGHTS_163;
PULSE.c:148:9: note: 	children 0x487cff8
PULSE.c:148:9: note: node (external) 0x487cff8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:148:9: note: 	{ layers_41, WEIGHTS_163 }
PULSE.c:148:9: note: ------>vectorizing SLP node starting from: <retval>.layers = layers_41;
PULSE.c:148:9: note: vect_is_simple_use: operand WEIGHTS_163 = PHI <WEIGHTS_46(36), WEIGHTS_86(25)>, type of def: internal
PULSE.c:148:9: note: conflicting alias set types.
PULSE.c:148:9: note: transform store. ncopies = 1
PULSE.c:148:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.layers
PULSE.c:148:9: note: created &<retval>
PULSE.c:148:9: note: add new stmt: MEM <vector(2) long unsigned int> [(void *)&<retval>] = _19;
PULSE.c:148:9: note: vectorizing stmts using SLP.
PULSE.c:148:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:148:9: note: Vectorizing SLP tree:
PULSE.c:148:9: note: node 0x487d078 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:148:9: note: op template: <retval>.io = 0B;
PULSE.c:148:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:148:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:148:9: note: 	children 0x487d0f8
PULSE.c:148:9: note: node (constant) 0x487d0f8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:148:9: note: 	{ 0B, 0B }
PULSE.c:148:9: note: ------>vectorizing SLP node starting from: <retval>.io = 0B;
PULSE.c:148:9: note: vect_is_simple_use: operand 0B, type of def: constant
PULSE.c:148:9: note: transform store. ncopies = 1
PULSE.c:148:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.io
PULSE.c:148:9: note: created &<retval>.io
PULSE.c:148:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval> + 16B] = { 0, 0 };
PULSE.c:148:9: note: vectorizing stmts using SLP.
PULSE.c:148:9: note: ***** The result for vector mode V32QI would be the same
PULSE.c:154:2: missed: statement clobbers memory: free (_1);
PULSE.c:155:2: missed: statement clobbers memory: free (_2);
PULSE.c:156:2: missed: statement clobbers memory: free (_3);
PULSE.c:157:2: missed: statement clobbers memory: free (_4);
PULSE.c:158:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:158:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Loss.c:15:19: optimized: loop vectorized using 32 byte vectors
Loss.c:15:19: optimized:  loop versioned for vectorization because of possible aliasing
Loss.c:15:19: optimized: loop vectorized using 16 byte vectors
Loss.c:12:23: note: vectorized 1 loops in function.
Loss.c:12:23: note: ***** Analysis failed with vector mode V8SF
Loss.c:12:23: note: ***** The result for vector mode V32QI would be the same
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V16QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V16QI
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V8QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V8QI
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V4QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V4QI
Loss.c:7:19: missed: couldn't vectorize loop
Loss.c:8:38: missed: not vectorized: unsupported use in stmt.
Loss.c:4:23: note: vectorized 0 loops in function.
Loss.c:9:13: note: ***** Analysis failed with vector mode V8SF
Loss.c:9:13: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SF
Loss.c:23:2: note: ***** Analysis failed with vector mode VOID
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:4:18: note: vectorized 0 loops in function.
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_4, inputs_15, _3);
PULSE.c:8:2: missed: statement clobbers memory: _5 (layer_17);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_12, pretmp_26, _9);
PULSE.c:13:28: note: ***** Analysis failed with vector mode V8SI
PULSE.c:13:28: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:22:2: missed: statement clobbers memory: _1 (layer_10(D));
PULSE.c:22:2: missed: statement clobbers memory: _14 (_2);
PULSE.c:22:2: missed: statement clobbers memory: _21 (_15);
PULSE.c:22:2: missed: statement clobbers memory: _28 (_22);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_29);
PULSE.c:26:3: missed: statement clobbers memory: memset (_34, 0, _32);
PULSE.c:26:3: missed: statement clobbers memory: memset (_27, 0, _25);
PULSE.c:26:3: missed: statement clobbers memory: memset (_20, 0, _18);
PULSE.c:26:3: missed: statement clobbers memory: memset (_7, 0, _5);
PULSE.c:28:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:28:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:39: missed: statement clobbers memory: _7 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:32:6: note: vectorized 1 loops in function.
PULSE.c:34:8: missed: statement clobbers memory: _1 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_2);
PULSE.c:39:39: missed: statement clobbers memory: _7 = rand ();
PULSE.c:40:40: missed: statement clobbers memory: _9 = rand ();
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8SI
PULSE.c:32:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:68:16: missed: couldn't vectorize loop
PULSE.c:68:16: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:71:17: missed: couldn't vectorize loop
PULSE.c:71:17: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:80:19: missed: couldn't vectorize loop
PULSE.c:80:19: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:39: missed: statement clobbers memory: _84 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:55:22: missed: couldn't vectorize loop
PULSE.c:55:22: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:47:6: note: vectorized 1 loops in function.
PULSE.c:48:1: missed: statement clobbers memory: saved_stack.7_47 = __builtin_stack_save ();
PULSE.c:49:46: missed: statement clobbers memory: _1 = PULSE_GetLossFunctionPtr (loss_function_48(D));
PULSE.c:50:42: missed: statement clobbers memory: TMP_52 = aligned_alloc (64, _4);
PULSE.c:57:22: missed: statement clobbers memory: ACC_72 = _5 (output_208, TMP_PTR_207);
PULSE.c:58:3: missed: statement clobbers memory: printf ("%d\n", ACC_72);
PULSE.c:67:15: missed: statement clobbers memory: random.4_56 = __builtin_alloca_with_align (_10, 32);
PULSE.c:34:8: missed: statement clobbers memory: _76 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_77);
PULSE.c:39:39: missed: statement clobbers memory: _84 = rand ();
PULSE.c:40:40: missed: statement clobbers memory: _87 = rand ();
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_113, inputs_108, _112);
PULSE.c:8:2: missed: statement clobbers memory: _114 (layer_109);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_120, _119, _118);
PULSE.c:74:11: missed: statement clobbers memory: loss_65 = PULSE_GetLoss_50 (_23, _22, _18, _17);
PULSE.c:22:2: missed: statement clobbers memory: _101 (output_209);
PULSE.c:22:2: missed: statement clobbers memory: _123 (_102);
PULSE.c:22:2: missed: statement clobbers memory: _130 (_124);
PULSE.c:22:2: missed: statement clobbers memory: _137 (_131);
PULSE.c:22:2: missed: statement clobbers memory: _144 (_138);
PULSE.c:22:2: missed: statement clobbers memory: _151 (_145);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_152);
PULSE.c:26:3: missed: statement clobbers memory: memset (_157, 0, _155);
PULSE.c:26:3: missed: statement clobbers memory: memset (_150, 0, _148);
PULSE.c:26:3: missed: statement clobbers memory: memset (_143, 0, _141);
PULSE.c:26:3: missed: statement clobbers memory: memset (_136, 0, _134);
PULSE.c:26:3: missed: statement clobbers memory: memset (_129, 0, _127);
PULSE.c:26:3: missed: statement clobbers memory: memset (_107, 0, _105);
PULSE.c:82:6: missed: statement clobbers memory: _27 (current_213, args);
PULSE.c:88:4: missed: statement clobbers memory: printf ("Epoch: %d | Item: %d | Loss: %.10f | Batch Loss: %.10f\r", i_210, j_211, _31, _30);
PULSE.c:47:6: missed: statement clobbers memory: __builtin_stack_restore (saved_stack.7_47);
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4DI
PULSE.c:47:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:98:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:98:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:132:20: missed: couldn't vectorize loop
PULSE.c:132:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:132:20: missed: couldn't vectorize loop
PULSE.c:132:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:111:20: missed: couldn't vectorize loop
PULSE.c:111:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:111:20: missed: couldn't vectorize loop
PULSE.c:111:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:100:13: note: vectorized 0 loops in function.
PULSE.c:103:39: missed: statement clobbers memory: layers_42 = malloc (_3);
PULSE.c:106:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:125:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:126:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:127:46: missed: statement clobbers memory: WEIGHTS_88 = aligned_alloc (64, 0);
PULSE.c:117:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.106_96];
PULSE.c:118:21: missed: statement clobbers memory: _77 = PULSE_GetDenseWeightsSize (args);
PULSE.c:119:16: missed: statement clobbers memory: _80 = PULSE_GetDenseIOSize (args);
PULSE.c:120:19: missed: statement clobbers memory: _83 = PULSE_GetDenseFixesSize (args);
PULSE.c:117:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})_226];
PULSE.c:118:21: missed: statement clobbers memory: _212 = PULSE_GetDenseWeightsSize (args);
PULSE.c:119:16: missed: statement clobbers memory: _215 = PULSE_GetDenseIOSize (args);
PULSE.c:120:19: missed: statement clobbers memory: _218 = PULSE_GetDenseFixesSize (args);
PULSE.c:125:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:126:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:127:46: missed: statement clobbers memory: WEIGHTS_47 = aligned_alloc (64, _5);
PULSE.c:128:41: missed: statement clobbers memory: IO_49 = aligned_alloc (64, _7);
PULSE.c:138:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.108_98];
PULSE.c:139:17: missed: statement clobbers memory: *_10 = PULSE_CreateDenseLayer (args, WEIGHTS_PTR_154, IO_PTR_155, 0B); [return slot optimization]
PULSE.c:140:20: missed: statement clobbers memory: _66 = PULSE_GetDenseWeightsSize (args);
PULSE.c:141:15: missed: statement clobbers memory: _69 = PULSE_GetDenseIOSize (args);
PULSE.c:138:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.108_126];
PULSE.c:139:17: missed: statement clobbers memory: *_108 = PULSE_CreateDenseLayer (args, WEIGHTS_PTR_150, IO_PTR_145, 0B); [return slot optimization]
PULSE.c:140:20: missed: statement clobbers memory: _97 = PULSE_GetDenseWeightsSize (args);
PULSE.c:141:15: missed: statement clobbers memory: _61 = PULSE_GetDenseIOSize (args);
PULSE.c:147:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:148:9: note: ***** Analysis succeeded with vector mode V8SI
PULSE.c:148:9: note: SLPing BB part
PULSE.c:148:9: note: Costing subgraph: 
PULSE.c:148:9: note: node 0x4791d98 (max_nunits=4, refcnt=1) vector(4) unsigned int
PULSE.c:148:9: note: op template: <retval>.n_layers = _20;
PULSE.c:148:9: note: 	stmt 0 <retval>.n_layers = _20;
PULSE.c:148:9: note: 	stmt 1 <retval>.weights_size = WEIGHTS_SIZE_166;
PULSE.c:148:9: note: 	stmt 2 <retval>.io_size = IO_SIZE_169;
PULSE.c:148:9: note: 	stmt 3 <retval>.fixes_size = FIXES_SIZE_170;
PULSE.c:148:9: note: 	children 0x4791e98
PULSE.c:148:9: note: node (external) 0x4791e98 (max_nunits=1, refcnt=1) vector(4) unsigned int
PULSE.c:148:9: note: 	{ _20, WEIGHTS_SIZE_166, IO_SIZE_169, FIXES_SIZE_170 }
PULSE.c:148:9: note: Cost model analysis: 
PULSE.c:148:9: note: Cost model analysis for part in loop 0:
  Vector cost: 52
  Scalar cost: 64
PULSE.c:148:9: note: Costing subgraph: 
PULSE.c:148:9: note: node 0x4791f98 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:148:9: note: op template: <retval>.layers = layers_42;
PULSE.c:148:9: note: 	stmt 0 <retval>.layers = layers_42;
PULSE.c:148:9: note: 	stmt 1 <retval>.weights = WEIGHTS_165;
PULSE.c:148:9: note: 	children 0x4792098
PULSE.c:148:9: note: node (external) 0x4792098 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:148:9: note: 	{ layers_42, WEIGHTS_165 }
PULSE.c:148:9: note: Cost model analysis: 
PULSE.c:148:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:148:9: note: Costing subgraph: 
PULSE.c:148:9: note: node 0x4792118 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:148:9: note: op template: <retval>.io = 0B;
PULSE.c:148:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:148:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:148:9: note: 	children 0x4792198
PULSE.c:148:9: note: node (constant) 0x4792198 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:148:9: note: 	{ 0B, 0B }
PULSE.c:148:9: note: Cost model analysis: 
PULSE.c:148:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:148:9: note: Basic block will be vectorized using SLP
PULSE.c:148:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:148:9: note: Vectorizing SLP tree:
PULSE.c:148:9: note: node 0x4791d98 (max_nunits=4, refcnt=1) vector(4) unsigned int
PULSE.c:148:9: note: op template: <retval>.n_layers = _20;
PULSE.c:148:9: note: 	stmt 0 <retval>.n_layers = _20;
PULSE.c:148:9: note: 	stmt 1 <retval>.weights_size = WEIGHTS_SIZE_166;
PULSE.c:148:9: note: 	stmt 2 <retval>.io_size = IO_SIZE_169;
PULSE.c:148:9: note: 	stmt 3 <retval>.fixes_size = FIXES_SIZE_170;
PULSE.c:148:9: note: 	children 0x4791e98
PULSE.c:148:9: note: node (external) 0x4791e98 (max_nunits=1, refcnt=1) vector(4) unsigned int
PULSE.c:148:9: note: 	{ _20, WEIGHTS_SIZE_166, IO_SIZE_169, FIXES_SIZE_170 }
PULSE.c:148:9: note: ------>vectorizing SLP node starting from: <retval>.n_layers = _20;
PULSE.c:148:9: note: vect_is_simple_use: operand WEIGHTS_SIZE_166 = PHI <WEIGHTS_SIZE_163(36), 0(25)>, type of def: internal
PULSE.c:148:9: note: vect_is_simple_use: operand IO_SIZE_169 = PHI <IO_SIZE_162(36), 0(25)>, type of def: internal
PULSE.c:148:9: note: vect_is_simple_use: operand FIXES_SIZE_170 = PHI <FIXES_SIZE_161(36), 0(25)>, type of def: internal
PULSE.c:148:9: note: transform store. ncopies = 1
PULSE.c:148:9: note: create vector_type-pointer variable to type: vector(4) unsigned int  vectorizing a pointer ref: <retval>.n_layers
PULSE.c:148:9: note: created &<retval>.n_layers
PULSE.c:148:9: note: add new stmt: MEM <vector(4) unsigned int> [(unsigned int *)&<retval> + 32B] = _17;
PULSE.c:148:9: note: vectorizing stmts using SLP.
PULSE.c:148:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:148:9: note: Vectorizing SLP tree:
PULSE.c:148:9: note: node 0x4791f98 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:148:9: note: op template: <retval>.layers = layers_42;
PULSE.c:148:9: note: 	stmt 0 <retval>.layers = layers_42;
PULSE.c:148:9: note: 	stmt 1 <retval>.weights = WEIGHTS_165;
PULSE.c:148:9: note: 	children 0x4792098
PULSE.c:148:9: note: node (external) 0x4792098 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:148:9: note: 	{ layers_42, WEIGHTS_165 }
PULSE.c:148:9: note: ------>vectorizing SLP node starting from: <retval>.layers = layers_42;
PULSE.c:148:9: note: vect_is_simple_use: operand WEIGHTS_165 = PHI <WEIGHTS_47(36), WEIGHTS_88(25)>, type of def: internal
PULSE.c:148:9: note: conflicting alias set types.
PULSE.c:148:9: note: transform store. ncopies = 1
PULSE.c:148:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.layers
PULSE.c:148:9: note: created &<retval>
PULSE.c:148:9: note: add new stmt: MEM <vector(2) long unsigned int> [(void *)&<retval>] = _87;
PULSE.c:148:9: note: vectorizing stmts using SLP.
PULSE.c:148:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:148:9: note: Vectorizing SLP tree:
PULSE.c:148:9: note: node 0x4792118 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:148:9: note: op template: <retval>.io = 0B;
PULSE.c:148:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:148:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:148:9: note: 	children 0x4792198
PULSE.c:148:9: note: node (constant) 0x4792198 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:148:9: note: 	{ 0B, 0B }
PULSE.c:148:9: note: ------>vectorizing SLP node starting from: <retval>.io = 0B;
PULSE.c:148:9: note: vect_is_simple_use: operand 0B, type of def: constant
PULSE.c:148:9: note: transform store. ncopies = 1
PULSE.c:148:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.io
PULSE.c:148:9: note: created &<retval>.io
PULSE.c:148:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval> + 16B] = { 0, 0 };
PULSE.c:148:9: note: vectorizing stmts using SLP.
PULSE.c:148:9: note: ***** The result for vector mode V32QI would be the same
PULSE.c:154:2: missed: statement clobbers memory: free (_1);
PULSE.c:155:2: missed: statement clobbers memory: free (_2);
PULSE.c:156:2: missed: statement clobbers memory: free (_3);
PULSE.c:157:2: missed: statement clobbers memory: free (_4);
PULSE.c:158:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:158:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:4:18: note: vectorized 0 loops in function.
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_4, inputs_15, _3);
PULSE.c:8:2: missed: statement clobbers memory: _5 (layer_17);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_12, pretmp_26, _9);
PULSE.c:13:28: note: ***** Analysis failed with vector mode V8SI
PULSE.c:13:28: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:22:2: missed: statement clobbers memory: _1 (layer_10(D));
PULSE.c:22:2: missed: statement clobbers memory: _14 (_2);
PULSE.c:22:2: missed: statement clobbers memory: _21 (_15);
PULSE.c:22:2: missed: statement clobbers memory: _28 (_22);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_29);
PULSE.c:26:3: missed: statement clobbers memory: memset (_34, 0, _32);
PULSE.c:26:3: missed: statement clobbers memory: memset (_27, 0, _25);
PULSE.c:26:3: missed: statement clobbers memory: memset (_20, 0, _18);
PULSE.c:26:3: missed: statement clobbers memory: memset (_7, 0, _5);
PULSE.c:28:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:28:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:39: missed: statement clobbers memory: _7 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:32:6: note: vectorized 1 loops in function.
PULSE.c:34:8: missed: statement clobbers memory: _1 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_2);
PULSE.c:39:39: missed: statement clobbers memory: _7 = rand ();
PULSE.c:40:40: missed: statement clobbers memory: _9 = rand ();
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8SI
PULSE.c:32:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:64:16: missed: couldn't vectorize loop
PULSE.c:64:16: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:67:17: missed: couldn't vectorize loop
PULSE.c:67:17: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:76:19: missed: couldn't vectorize loop
PULSE.c:76:19: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:39: missed: statement clobbers memory: _93 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:55:15: missed: couldn't vectorize loop
PULSE.c:57:14: missed: statement clobbers memory: _81 = _8 (_11, TMP_PTR_217);
PULSE.c:47:6: note: vectorized 1 loops in function.
PULSE.c:48:1: missed: statement clobbers memory: saved_stack.8_54 = __builtin_stack_save ();
PULSE.c:49:46: missed: statement clobbers memory: _1 = PULSE_GetLossFunctionPtr (loss_function_55(D));
PULSE.c:50:42: missed: statement clobbers memory: TMP_59 = aligned_alloc (64, _4);
PULSE.c:57:14: missed: statement clobbers memory: _81 = _8 (_11, TMP_PTR_217);
PULSE.c:63:15: missed: statement clobbers memory: random.5_65 = __builtin_alloca_with_align (_16, 32);
PULSE.c:34:8: missed: statement clobbers memory: _85 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_86);
PULSE.c:39:39: missed: statement clobbers memory: _93 = rand ();
PULSE.c:40:40: missed: statement clobbers memory: _96 = rand ();
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_122, inputs_117, _121);
PULSE.c:8:2: missed: statement clobbers memory: _123 (layer_118);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_129, _128, _127);
PULSE.c:70:11: missed: statement clobbers memory: loss_74 = PULSE_GetLoss_57 (_29, _28, _24, _23);
PULSE.c:22:2: missed: statement clobbers memory: _110 (output_219);
PULSE.c:22:2: missed: statement clobbers memory: _132 (_111);
PULSE.c:22:2: missed: statement clobbers memory: _139 (_133);
PULSE.c:22:2: missed: statement clobbers memory: _146 (_140);
PULSE.c:22:2: missed: statement clobbers memory: _153 (_147);
PULSE.c:22:2: missed: statement clobbers memory: _160 (_154);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_161);
PULSE.c:26:3: missed: statement clobbers memory: memset (_166, 0, _164);
PULSE.c:26:3: missed: statement clobbers memory: memset (_159, 0, _157);
PULSE.c:26:3: missed: statement clobbers memory: memset (_152, 0, _150);
PULSE.c:26:3: missed: statement clobbers memory: memset (_145, 0, _143);
PULSE.c:26:3: missed: statement clobbers memory: memset (_138, 0, _136);
PULSE.c:26:3: missed: statement clobbers memory: memset (_116, 0, _114);
PULSE.c:78:6: missed: statement clobbers memory: _33 (current_224, args);
PULSE.c:84:4: missed: statement clobbers memory: printf ("Epoch: %d | Item: %d | Loss: %.10f | Batch Loss: %.10f\r", i_221, j_222, _37, _36);
PULSE.c:47:6: missed: statement clobbers memory: __builtin_stack_restore (saved_stack.8_54);
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4DI
PULSE.c:47:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:94:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:94:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:128:20: missed: couldn't vectorize loop
PULSE.c:128:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:128:20: missed: couldn't vectorize loop
PULSE.c:128:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:107:20: missed: couldn't vectorize loop
PULSE.c:107:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:107:20: missed: couldn't vectorize loop
PULSE.c:107:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:96:13: note: vectorized 0 loops in function.
PULSE.c:99:39: missed: statement clobbers memory: layers_42 = malloc (_3);
PULSE.c:102:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:121:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:122:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:123:46: missed: statement clobbers memory: WEIGHTS_88 = aligned_alloc (64, 0);
PULSE.c:113:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.112_96];
PULSE.c:114:21: missed: statement clobbers memory: _77 = PULSE_GetDenseWeightsSize (args);
PULSE.c:115:16: missed: statement clobbers memory: _80 = PULSE_GetDenseIOSize (args);
PULSE.c:116:19: missed: statement clobbers memory: _83 = PULSE_GetDenseFixesSize (args);
PULSE.c:113:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})_226];
PULSE.c:114:21: missed: statement clobbers memory: _212 = PULSE_GetDenseWeightsSize (args);
PULSE.c:115:16: missed: statement clobbers memory: _215 = PULSE_GetDenseIOSize (args);
PULSE.c:116:19: missed: statement clobbers memory: _218 = PULSE_GetDenseFixesSize (args);
PULSE.c:121:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:122:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:123:46: missed: statement clobbers memory: WEIGHTS_47 = aligned_alloc (64, _5);
PULSE.c:124:41: missed: statement clobbers memory: IO_49 = aligned_alloc (64, _7);
PULSE.c:134:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.114_98];
PULSE.c:135:17: missed: statement clobbers memory: *_10 = PULSE_CreateDenseLayer (args, WEIGHTS_PTR_154, IO_PTR_155, 0B); [return slot optimization]
PULSE.c:136:20: missed: statement clobbers memory: _66 = PULSE_GetDenseWeightsSize (args);
PULSE.c:137:15: missed: statement clobbers memory: _69 = PULSE_GetDenseIOSize (args);
PULSE.c:134:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.114_126];
PULSE.c:135:17: missed: statement clobbers memory: *_108 = PULSE_CreateDenseLayer (args, WEIGHTS_PTR_150, IO_PTR_145, 0B); [return slot optimization]
PULSE.c:136:20: missed: statement clobbers memory: _97 = PULSE_GetDenseWeightsSize (args);
PULSE.c:137:15: missed: statement clobbers memory: _61 = PULSE_GetDenseIOSize (args);
PULSE.c:143:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:144:9: note: ***** Analysis succeeded with vector mode V8SI
PULSE.c:144:9: note: SLPing BB part
PULSE.c:144:9: note: Costing subgraph: 
PULSE.c:144:9: note: node 0x4d8dd48 (max_nunits=4, refcnt=1) vector(4) unsigned int
PULSE.c:144:9: note: op template: <retval>.n_layers = _20;
PULSE.c:144:9: note: 	stmt 0 <retval>.n_layers = _20;
PULSE.c:144:9: note: 	stmt 1 <retval>.weights_size = WEIGHTS_SIZE_166;
PULSE.c:144:9: note: 	stmt 2 <retval>.io_size = IO_SIZE_169;
PULSE.c:144:9: note: 	stmt 3 <retval>.fixes_size = FIXES_SIZE_170;
PULSE.c:144:9: note: 	children 0x4d8de48
PULSE.c:144:9: note: node (external) 0x4d8de48 (max_nunits=1, refcnt=1) vector(4) unsigned int
PULSE.c:144:9: note: 	{ _20, WEIGHTS_SIZE_166, IO_SIZE_169, FIXES_SIZE_170 }
PULSE.c:144:9: note: Cost model analysis: 
PULSE.c:144:9: note: Cost model analysis for part in loop 0:
  Vector cost: 52
  Scalar cost: 64
PULSE.c:144:9: note: Costing subgraph: 
PULSE.c:144:9: note: node 0x4d8df48 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:144:9: note: op template: <retval>.layers = layers_42;
PULSE.c:144:9: note: 	stmt 0 <retval>.layers = layers_42;
PULSE.c:144:9: note: 	stmt 1 <retval>.weights = WEIGHTS_165;
PULSE.c:144:9: note: 	children 0x4d8e048
PULSE.c:144:9: note: node (external) 0x4d8e048 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:144:9: note: 	{ layers_42, WEIGHTS_165 }
PULSE.c:144:9: note: Cost model analysis: 
PULSE.c:144:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:144:9: note: Costing subgraph: 
PULSE.c:144:9: note: node 0x4d8e0c8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:144:9: note: op template: <retval>.io = 0B;
PULSE.c:144:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:144:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:144:9: note: 	children 0x4d8e148
PULSE.c:144:9: note: node (constant) 0x4d8e148 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:144:9: note: 	{ 0B, 0B }
PULSE.c:144:9: note: Cost model analysis: 
PULSE.c:144:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:144:9: note: Basic block will be vectorized using SLP
PULSE.c:144:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:144:9: note: Vectorizing SLP tree:
PULSE.c:144:9: note: node 0x4d8dd48 (max_nunits=4, refcnt=1) vector(4) unsigned int
PULSE.c:144:9: note: op template: <retval>.n_layers = _20;
PULSE.c:144:9: note: 	stmt 0 <retval>.n_layers = _20;
PULSE.c:144:9: note: 	stmt 1 <retval>.weights_size = WEIGHTS_SIZE_166;
PULSE.c:144:9: note: 	stmt 2 <retval>.io_size = IO_SIZE_169;
PULSE.c:144:9: note: 	stmt 3 <retval>.fixes_size = FIXES_SIZE_170;
PULSE.c:144:9: note: 	children 0x4d8de48
PULSE.c:144:9: note: node (external) 0x4d8de48 (max_nunits=1, refcnt=1) vector(4) unsigned int
PULSE.c:144:9: note: 	{ _20, WEIGHTS_SIZE_166, IO_SIZE_169, FIXES_SIZE_170 }
PULSE.c:144:9: note: ------>vectorizing SLP node starting from: <retval>.n_layers = _20;
PULSE.c:144:9: note: vect_is_simple_use: operand WEIGHTS_SIZE_166 = PHI <WEIGHTS_SIZE_163(36), 0(25)>, type of def: internal
PULSE.c:144:9: note: vect_is_simple_use: operand IO_SIZE_169 = PHI <IO_SIZE_162(36), 0(25)>, type of def: internal
PULSE.c:144:9: note: vect_is_simple_use: operand FIXES_SIZE_170 = PHI <FIXES_SIZE_161(36), 0(25)>, type of def: internal
PULSE.c:144:9: note: transform store. ncopies = 1
PULSE.c:144:9: note: create vector_type-pointer variable to type: vector(4) unsigned int  vectorizing a pointer ref: <retval>.n_layers
PULSE.c:144:9: note: created &<retval>.n_layers
PULSE.c:144:9: note: add new stmt: MEM <vector(4) unsigned int> [(unsigned int *)&<retval> + 32B] = _17;
PULSE.c:144:9: note: vectorizing stmts using SLP.
PULSE.c:144:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:144:9: note: Vectorizing SLP tree:
PULSE.c:144:9: note: node 0x4d8df48 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:144:9: note: op template: <retval>.layers = layers_42;
PULSE.c:144:9: note: 	stmt 0 <retval>.layers = layers_42;
PULSE.c:144:9: note: 	stmt 1 <retval>.weights = WEIGHTS_165;
PULSE.c:144:9: note: 	children 0x4d8e048
PULSE.c:144:9: note: node (external) 0x4d8e048 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:144:9: note: 	{ layers_42, WEIGHTS_165 }
PULSE.c:144:9: note: ------>vectorizing SLP node starting from: <retval>.layers = layers_42;
PULSE.c:144:9: note: vect_is_simple_use: operand WEIGHTS_165 = PHI <WEIGHTS_47(36), WEIGHTS_88(25)>, type of def: internal
PULSE.c:144:9: note: conflicting alias set types.
PULSE.c:144:9: note: transform store. ncopies = 1
PULSE.c:144:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.layers
PULSE.c:144:9: note: created &<retval>
PULSE.c:144:9: note: add new stmt: MEM <vector(2) long unsigned int> [(void *)&<retval>] = _87;
PULSE.c:144:9: note: vectorizing stmts using SLP.
PULSE.c:144:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:144:9: note: Vectorizing SLP tree:
PULSE.c:144:9: note: node 0x4d8e0c8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:144:9: note: op template: <retval>.io = 0B;
PULSE.c:144:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:144:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:144:9: note: 	children 0x4d8e148
PULSE.c:144:9: note: node (constant) 0x4d8e148 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:144:9: note: 	{ 0B, 0B }
PULSE.c:144:9: note: ------>vectorizing SLP node starting from: <retval>.io = 0B;
PULSE.c:144:9: note: vect_is_simple_use: operand 0B, type of def: constant
PULSE.c:144:9: note: transform store. ncopies = 1
PULSE.c:144:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.io
PULSE.c:144:9: note: created &<retval>.io
PULSE.c:144:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval> + 16B] = { 0, 0 };
PULSE.c:144:9: note: vectorizing stmts using SLP.
PULSE.c:144:9: note: ***** The result for vector mode V32QI would be the same
PULSE.c:150:2: missed: statement clobbers memory: free (_1);
PULSE.c:151:2: missed: statement clobbers memory: free (_2);
PULSE.c:152:2: missed: statement clobbers memory: free (_3);
PULSE.c:153:2: missed: statement clobbers memory: free (_4);
PULSE.c:154:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:154:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:4:18: note: vectorized 0 loops in function.
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_4, inputs_15, _3);
PULSE.c:8:2: missed: statement clobbers memory: _5 (layer_17);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_12, pretmp_26, _9);
PULSE.c:13:28: note: ***** Analysis failed with vector mode V8SI
PULSE.c:13:28: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:22:2: missed: statement clobbers memory: _1 (layer_10(D));
PULSE.c:22:2: missed: statement clobbers memory: _14 (_2);
PULSE.c:22:2: missed: statement clobbers memory: _21 (_15);
PULSE.c:22:2: missed: statement clobbers memory: _28 (_22);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_29);
PULSE.c:26:3: missed: statement clobbers memory: memset (_34, 0, _32);
PULSE.c:26:3: missed: statement clobbers memory: memset (_27, 0, _25);
PULSE.c:26:3: missed: statement clobbers memory: memset (_20, 0, _18);
PULSE.c:26:3: missed: statement clobbers memory: memset (_7, 0, _5);
PULSE.c:28:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:28:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:39: missed: statement clobbers memory: _7 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:32:6: note: vectorized 1 loops in function.
PULSE.c:34:8: missed: statement clobbers memory: _1 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_2);
PULSE.c:39:39: missed: statement clobbers memory: _7 = rand ();
PULSE.c:40:40: missed: statement clobbers memory: _9 = rand ();
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8SI
PULSE.c:32:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:64:16: missed: couldn't vectorize loop
PULSE.c:64:16: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:67:17: missed: couldn't vectorize loop
PULSE.c:67:17: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:76:19: missed: couldn't vectorize loop
PULSE.c:76:19: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:39: missed: statement clobbers memory: _93 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:55:15: missed: couldn't vectorize loop
PULSE.c:57:14: missed: statement clobbers memory: _81 = _8 (_11, TMP_PTR_217);
PULSE.c:47:6: note: vectorized 1 loops in function.
PULSE.c:48:1: missed: statement clobbers memory: saved_stack.8_54 = __builtin_stack_save ();
PULSE.c:49:46: missed: statement clobbers memory: _1 = PULSE_GetLossFunctionPtr (loss_function_55(D));
PULSE.c:50:42: missed: statement clobbers memory: TMP_59 = aligned_alloc (64, _4);
PULSE.c:57:14: missed: statement clobbers memory: _81 = _8 (_11, TMP_PTR_217);
PULSE.c:63:15: missed: statement clobbers memory: random.5_65 = __builtin_alloca_with_align (_16, 32);
PULSE.c:34:8: missed: statement clobbers memory: _85 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_86);
PULSE.c:39:39: missed: statement clobbers memory: _93 = rand ();
PULSE.c:40:40: missed: statement clobbers memory: _96 = rand ();
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_122, inputs_117, _121);
PULSE.c:8:2: missed: statement clobbers memory: _123 (layer_118);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_129, _128, _127);
PULSE.c:70:11: missed: statement clobbers memory: loss_74 = PULSE_GetLoss_57 (_29, _28, _24, _23);
PULSE.c:22:2: missed: statement clobbers memory: _110 (output_219);
PULSE.c:22:2: missed: statement clobbers memory: _132 (_111);
PULSE.c:22:2: missed: statement clobbers memory: _139 (_133);
PULSE.c:22:2: missed: statement clobbers memory: _146 (_140);
PULSE.c:22:2: missed: statement clobbers memory: _153 (_147);
PULSE.c:22:2: missed: statement clobbers memory: _160 (_154);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_161);
PULSE.c:26:3: missed: statement clobbers memory: memset (_166, 0, _164);
PULSE.c:26:3: missed: statement clobbers memory: memset (_159, 0, _157);
PULSE.c:26:3: missed: statement clobbers memory: memset (_152, 0, _150);
PULSE.c:26:3: missed: statement clobbers memory: memset (_145, 0, _143);
PULSE.c:26:3: missed: statement clobbers memory: memset (_138, 0, _136);
PULSE.c:26:3: missed: statement clobbers memory: memset (_116, 0, _114);
PULSE.c:78:6: missed: statement clobbers memory: _33 (current_224, args);
PULSE.c:84:4: missed: statement clobbers memory: printf ("Epoch: %d | Item: %d | Loss: %.10f | Batch Loss: %.10f\r", i_221, j_222, _37, _36);
PULSE.c:47:6: missed: statement clobbers memory: __builtin_stack_restore (saved_stack.8_54);
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4DI
PULSE.c:47:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:94:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:94:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:128:20: missed: couldn't vectorize loop
PULSE.c:128:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:128:20: missed: couldn't vectorize loop
PULSE.c:128:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:107:20: missed: couldn't vectorize loop
PULSE.c:107:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:107:20: missed: couldn't vectorize loop
PULSE.c:107:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:96:13: note: vectorized 0 loops in function.
PULSE.c:99:39: missed: statement clobbers memory: layers_42 = malloc (_3);
PULSE.c:102:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:121:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:122:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:123:46: missed: statement clobbers memory: WEIGHTS_88 = aligned_alloc (64, 0);
PULSE.c:113:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.112_96];
PULSE.c:114:21: missed: statement clobbers memory: _77 = PULSE_GetDenseWeightsSize (args);
PULSE.c:115:16: missed: statement clobbers memory: _80 = PULSE_GetDenseIOSize (args);
PULSE.c:116:19: missed: statement clobbers memory: _83 = PULSE_GetDenseFixesSize (args);
PULSE.c:113:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})_226];
PULSE.c:114:21: missed: statement clobbers memory: _212 = PULSE_GetDenseWeightsSize (args);
PULSE.c:115:16: missed: statement clobbers memory: _215 = PULSE_GetDenseIOSize (args);
PULSE.c:116:19: missed: statement clobbers memory: _218 = PULSE_GetDenseFixesSize (args);
PULSE.c:121:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:122:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:123:46: missed: statement clobbers memory: WEIGHTS_47 = aligned_alloc (64, _5);
PULSE.c:124:41: missed: statement clobbers memory: IO_49 = aligned_alloc (64, _7);
PULSE.c:134:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.114_98];
PULSE.c:135:17: missed: statement clobbers memory: *_10 = PULSE_CreateDenseLayer (args, WEIGHTS_PTR_154, IO_PTR_155, 0B); [return slot optimization]
PULSE.c:136:20: missed: statement clobbers memory: _66 = PULSE_GetDenseWeightsSize (args);
PULSE.c:137:15: missed: statement clobbers memory: _69 = PULSE_GetDenseIOSize (args);
PULSE.c:134:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.114_126];
PULSE.c:135:17: missed: statement clobbers memory: *_108 = PULSE_CreateDenseLayer (args, WEIGHTS_PTR_150, IO_PTR_145, 0B); [return slot optimization]
PULSE.c:136:20: missed: statement clobbers memory: _97 = PULSE_GetDenseWeightsSize (args);
PULSE.c:137:15: missed: statement clobbers memory: _61 = PULSE_GetDenseIOSize (args);
PULSE.c:143:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:144:9: note: ***** Analysis succeeded with vector mode V8SI
PULSE.c:144:9: note: SLPing BB part
PULSE.c:144:9: note: Costing subgraph: 
PULSE.c:144:9: note: node 0x32fcd48 (max_nunits=4, refcnt=1) vector(4) unsigned int
PULSE.c:144:9: note: op template: <retval>.n_layers = _20;
PULSE.c:144:9: note: 	stmt 0 <retval>.n_layers = _20;
PULSE.c:144:9: note: 	stmt 1 <retval>.weights_size = WEIGHTS_SIZE_166;
PULSE.c:144:9: note: 	stmt 2 <retval>.io_size = IO_SIZE_169;
PULSE.c:144:9: note: 	stmt 3 <retval>.fixes_size = FIXES_SIZE_170;
PULSE.c:144:9: note: 	children 0x32fce48
PULSE.c:144:9: note: node (external) 0x32fce48 (max_nunits=1, refcnt=1) vector(4) unsigned int
PULSE.c:144:9: note: 	{ _20, WEIGHTS_SIZE_166, IO_SIZE_169, FIXES_SIZE_170 }
PULSE.c:144:9: note: Cost model analysis: 
PULSE.c:144:9: note: Cost model analysis for part in loop 0:
  Vector cost: 52
  Scalar cost: 64
PULSE.c:144:9: note: Costing subgraph: 
PULSE.c:144:9: note: node 0x32fcf48 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:144:9: note: op template: <retval>.layers = layers_42;
PULSE.c:144:9: note: 	stmt 0 <retval>.layers = layers_42;
PULSE.c:144:9: note: 	stmt 1 <retval>.weights = WEIGHTS_165;
PULSE.c:144:9: note: 	children 0x32fd048
PULSE.c:144:9: note: node (external) 0x32fd048 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:144:9: note: 	{ layers_42, WEIGHTS_165 }
PULSE.c:144:9: note: Cost model analysis: 
PULSE.c:144:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:144:9: note: Costing subgraph: 
PULSE.c:144:9: note: node 0x32fd0c8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:144:9: note: op template: <retval>.io = 0B;
PULSE.c:144:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:144:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:144:9: note: 	children 0x32fd148
PULSE.c:144:9: note: node (constant) 0x32fd148 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:144:9: note: 	{ 0B, 0B }
PULSE.c:144:9: note: Cost model analysis: 
PULSE.c:144:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:144:9: note: Basic block will be vectorized using SLP
PULSE.c:144:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:144:9: note: Vectorizing SLP tree:
PULSE.c:144:9: note: node 0x32fcd48 (max_nunits=4, refcnt=1) vector(4) unsigned int
PULSE.c:144:9: note: op template: <retval>.n_layers = _20;
PULSE.c:144:9: note: 	stmt 0 <retval>.n_layers = _20;
PULSE.c:144:9: note: 	stmt 1 <retval>.weights_size = WEIGHTS_SIZE_166;
PULSE.c:144:9: note: 	stmt 2 <retval>.io_size = IO_SIZE_169;
PULSE.c:144:9: note: 	stmt 3 <retval>.fixes_size = FIXES_SIZE_170;
PULSE.c:144:9: note: 	children 0x32fce48
PULSE.c:144:9: note: node (external) 0x32fce48 (max_nunits=1, refcnt=1) vector(4) unsigned int
PULSE.c:144:9: note: 	{ _20, WEIGHTS_SIZE_166, IO_SIZE_169, FIXES_SIZE_170 }
PULSE.c:144:9: note: ------>vectorizing SLP node starting from: <retval>.n_layers = _20;
PULSE.c:144:9: note: vect_is_simple_use: operand WEIGHTS_SIZE_166 = PHI <WEIGHTS_SIZE_163(36), 0(25)>, type of def: internal
PULSE.c:144:9: note: vect_is_simple_use: operand IO_SIZE_169 = PHI <IO_SIZE_162(36), 0(25)>, type of def: internal
PULSE.c:144:9: note: vect_is_simple_use: operand FIXES_SIZE_170 = PHI <FIXES_SIZE_161(36), 0(25)>, type of def: internal
PULSE.c:144:9: note: transform store. ncopies = 1
PULSE.c:144:9: note: create vector_type-pointer variable to type: vector(4) unsigned int  vectorizing a pointer ref: <retval>.n_layers
PULSE.c:144:9: note: created &<retval>.n_layers
PULSE.c:144:9: note: add new stmt: MEM <vector(4) unsigned int> [(unsigned int *)&<retval> + 32B] = _17;
PULSE.c:144:9: note: vectorizing stmts using SLP.
PULSE.c:144:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:144:9: note: Vectorizing SLP tree:
PULSE.c:144:9: note: node 0x32fcf48 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:144:9: note: op template: <retval>.layers = layers_42;
PULSE.c:144:9: note: 	stmt 0 <retval>.layers = layers_42;
PULSE.c:144:9: note: 	stmt 1 <retval>.weights = WEIGHTS_165;
PULSE.c:144:9: note: 	children 0x32fd048
PULSE.c:144:9: note: node (external) 0x32fd048 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:144:9: note: 	{ layers_42, WEIGHTS_165 }
PULSE.c:144:9: note: ------>vectorizing SLP node starting from: <retval>.layers = layers_42;
PULSE.c:144:9: note: vect_is_simple_use: operand WEIGHTS_165 = PHI <WEIGHTS_47(36), WEIGHTS_88(25)>, type of def: internal
PULSE.c:144:9: note: conflicting alias set types.
PULSE.c:144:9: note: transform store. ncopies = 1
PULSE.c:144:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.layers
PULSE.c:144:9: note: created &<retval>
PULSE.c:144:9: note: add new stmt: MEM <vector(2) long unsigned int> [(void *)&<retval>] = _87;
PULSE.c:144:9: note: vectorizing stmts using SLP.
PULSE.c:144:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:144:9: note: Vectorizing SLP tree:
PULSE.c:144:9: note: node 0x32fd0c8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:144:9: note: op template: <retval>.io = 0B;
PULSE.c:144:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:144:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:144:9: note: 	children 0x32fd148
PULSE.c:144:9: note: node (constant) 0x32fd148 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:144:9: note: 	{ 0B, 0B }
PULSE.c:144:9: note: ------>vectorizing SLP node starting from: <retval>.io = 0B;
PULSE.c:144:9: note: vect_is_simple_use: operand 0B, type of def: constant
PULSE.c:144:9: note: transform store. ncopies = 1
PULSE.c:144:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.io
PULSE.c:144:9: note: created &<retval>.io
PULSE.c:144:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval> + 16B] = { 0, 0 };
PULSE.c:144:9: note: vectorizing stmts using SLP.
PULSE.c:144:9: note: ***** The result for vector mode V32QI would be the same
PULSE.c:150:2: missed: statement clobbers memory: free (_1);
PULSE.c:151:2: missed: statement clobbers memory: free (_2);
PULSE.c:152:2: missed: statement clobbers memory: free (_3);
PULSE.c:153:2: missed: statement clobbers memory: free (_4);
PULSE.c:154:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:154:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:4:18: note: vectorized 0 loops in function.
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_4, inputs_15, _3);
PULSE.c:8:2: missed: statement clobbers memory: _5 (layer_17);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_12, pretmp_26, _9);
PULSE.c:13:28: note: ***** Analysis failed with vector mode V8SI
PULSE.c:13:28: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:22:2: missed: statement clobbers memory: _1 (layer_10(D));
PULSE.c:22:2: missed: statement clobbers memory: _14 (_2);
PULSE.c:22:2: missed: statement clobbers memory: _21 (_15);
PULSE.c:22:2: missed: statement clobbers memory: _28 (_22);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_29);
PULSE.c:26:3: missed: statement clobbers memory: memset (_34, 0, _32);
PULSE.c:26:3: missed: statement clobbers memory: memset (_27, 0, _25);
PULSE.c:26:3: missed: statement clobbers memory: memset (_20, 0, _18);
PULSE.c:26:3: missed: statement clobbers memory: memset (_7, 0, _5);
PULSE.c:28:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:28:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:39: missed: statement clobbers memory: _7 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:32:6: note: vectorized 1 loops in function.
PULSE.c:34:8: missed: statement clobbers memory: _1 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_2);
PULSE.c:39:39: missed: statement clobbers memory: _7 = rand ();
PULSE.c:40:40: missed: statement clobbers memory: _9 = rand ();
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8SI
PULSE.c:32:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:63:16: missed: couldn't vectorize loop
PULSE.c:63:16: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:66:17: missed: couldn't vectorize loop
PULSE.c:66:17: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:75:19: missed: couldn't vectorize loop
PULSE.c:75:19: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:39: missed: statement clobbers memory: _91 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:55:15: missed: couldn't vectorize loop
PULSE.c:57:14: missed: statement clobbers memory: _80 = _11 (_10, TMP_PTR_214);
PULSE.c:47:6: note: vectorized 1 loops in function.
PULSE.c:48:1: missed: statement clobbers memory: saved_stack.8_53 = __builtin_stack_save ();
PULSE.c:49:46: missed: statement clobbers memory: _1 = PULSE_GetLossFunctionPtr (loss_function_54(D));
PULSE.c:50:42: missed: statement clobbers memory: TMP_58 = aligned_alloc (64, _4);
PULSE.c:57:14: missed: statement clobbers memory: _80 = _11 (_10, TMP_PTR_214);
PULSE.c:62:15: missed: statement clobbers memory: random.5_64 = __builtin_alloca_with_align (_16, 32);
PULSE.c:34:8: missed: statement clobbers memory: _83 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_84);
PULSE.c:39:39: missed: statement clobbers memory: _91 = rand ();
PULSE.c:40:40: missed: statement clobbers memory: _94 = rand ();
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_120, inputs_115, _119);
PULSE.c:8:2: missed: statement clobbers memory: _121 (layer_116);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_127, _126, _125);
PULSE.c:69:11: missed: statement clobbers memory: loss_73 = PULSE_GetLoss_56 (_29, _28, _24, _23);
PULSE.c:22:2: missed: statement clobbers memory: _108 (output_60);
PULSE.c:22:2: missed: statement clobbers memory: _130 (_109);
PULSE.c:22:2: missed: statement clobbers memory: _137 (_131);
PULSE.c:22:2: missed: statement clobbers memory: _144 (_138);
PULSE.c:22:2: missed: statement clobbers memory: _151 (_145);
PULSE.c:22:2: missed: statement clobbers memory: _158 (_152);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_159);
PULSE.c:26:3: missed: statement clobbers memory: memset (_164, 0, _162);
PULSE.c:26:3: missed: statement clobbers memory: memset (_157, 0, _155);
PULSE.c:26:3: missed: statement clobbers memory: memset (_150, 0, _148);
PULSE.c:26:3: missed: statement clobbers memory: memset (_143, 0, _141);
PULSE.c:26:3: missed: statement clobbers memory: memset (_136, 0, _134);
PULSE.c:26:3: missed: statement clobbers memory: memset (_114, 0, _112);
PULSE.c:77:6: missed: statement clobbers memory: _33 (current_219, args);
PULSE.c:83:4: missed: statement clobbers memory: printf ("Epoch: %d | Item: %d | Loss: %.10f | Batch Loss: %.10f\r", i_216, j_217, _37, _36);
PULSE.c:47:6: missed: statement clobbers memory: __builtin_stack_restore (saved_stack.8_53);
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4DI
PULSE.c:47:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:93:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:93:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:127:20: missed: couldn't vectorize loop
PULSE.c:127:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:127:20: missed: couldn't vectorize loop
PULSE.c:127:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:106:20: missed: couldn't vectorize loop
PULSE.c:106:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:106:20: missed: couldn't vectorize loop
PULSE.c:106:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:95:13: note: vectorized 0 loops in function.
PULSE.c:98:39: missed: statement clobbers memory: layers_42 = malloc (_3);
PULSE.c:101:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:120:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:121:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:122:46: missed: statement clobbers memory: WEIGHTS_88 = aligned_alloc (64, 0);
PULSE.c:112:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.115_96];
PULSE.c:113:21: missed: statement clobbers memory: _77 = PULSE_GetDenseWeightsSize (args);
PULSE.c:114:16: missed: statement clobbers memory: _80 = PULSE_GetDenseIOSize (args);
PULSE.c:115:19: missed: statement clobbers memory: _83 = PULSE_GetDenseFixesSize (args);
PULSE.c:112:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})_226];
PULSE.c:113:21: missed: statement clobbers memory: _212 = PULSE_GetDenseWeightsSize (args);
PULSE.c:114:16: missed: statement clobbers memory: _215 = PULSE_GetDenseIOSize (args);
PULSE.c:115:19: missed: statement clobbers memory: _218 = PULSE_GetDenseFixesSize (args);
PULSE.c:120:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:121:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:122:46: missed: statement clobbers memory: WEIGHTS_47 = aligned_alloc (64, _5);
PULSE.c:123:41: missed: statement clobbers memory: IO_49 = aligned_alloc (64, _7);
PULSE.c:133:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.117_98];
PULSE.c:134:17: missed: statement clobbers memory: *_10 = PULSE_CreateDenseLayer (args, WEIGHTS_PTR_154, IO_PTR_155, 0B); [return slot optimization]
PULSE.c:135:20: missed: statement clobbers memory: _66 = PULSE_GetDenseWeightsSize (args);
PULSE.c:136:15: missed: statement clobbers memory: _69 = PULSE_GetDenseIOSize (args);
PULSE.c:133:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.117_126];
PULSE.c:134:17: missed: statement clobbers memory: *_108 = PULSE_CreateDenseLayer (args, WEIGHTS_PTR_150, IO_PTR_145, 0B); [return slot optimization]
PULSE.c:135:20: missed: statement clobbers memory: _97 = PULSE_GetDenseWeightsSize (args);
PULSE.c:136:15: missed: statement clobbers memory: _61 = PULSE_GetDenseIOSize (args);
PULSE.c:142:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:143:9: note: ***** Analysis succeeded with vector mode V8SI
PULSE.c:143:9: note: SLPing BB part
PULSE.c:143:9: note: Costing subgraph: 
PULSE.c:143:9: note: node 0x4a79e58 (max_nunits=4, refcnt=1) vector(4) unsigned int
PULSE.c:143:9: note: op template: <retval>.n_layers = _20;
PULSE.c:143:9: note: 	stmt 0 <retval>.n_layers = _20;
PULSE.c:143:9: note: 	stmt 1 <retval>.weights_size = WEIGHTS_SIZE_166;
PULSE.c:143:9: note: 	stmt 2 <retval>.io_size = IO_SIZE_169;
PULSE.c:143:9: note: 	stmt 3 <retval>.fixes_size = FIXES_SIZE_170;
PULSE.c:143:9: note: 	children 0x4a79f58
PULSE.c:143:9: note: node (external) 0x4a79f58 (max_nunits=1, refcnt=1) vector(4) unsigned int
PULSE.c:143:9: note: 	{ _20, WEIGHTS_SIZE_166, IO_SIZE_169, FIXES_SIZE_170 }
PULSE.c:143:9: note: Cost model analysis: 
PULSE.c:143:9: note: Cost model analysis for part in loop 0:
  Vector cost: 52
  Scalar cost: 64
PULSE.c:143:9: note: Costing subgraph: 
PULSE.c:143:9: note: node 0x4a7a058 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:143:9: note: op template: <retval>.layers = layers_42;
PULSE.c:143:9: note: 	stmt 0 <retval>.layers = layers_42;
PULSE.c:143:9: note: 	stmt 1 <retval>.weights = WEIGHTS_165;
PULSE.c:143:9: note: 	children 0x4a7a158
PULSE.c:143:9: note: node (external) 0x4a7a158 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:143:9: note: 	{ layers_42, WEIGHTS_165 }
PULSE.c:143:9: note: Cost model analysis: 
PULSE.c:143:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:143:9: note: Costing subgraph: 
PULSE.c:143:9: note: node 0x4a7a1d8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:143:9: note: op template: <retval>.io = 0B;
PULSE.c:143:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:143:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:143:9: note: 	children 0x4a7a258
PULSE.c:143:9: note: node (constant) 0x4a7a258 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:143:9: note: 	{ 0B, 0B }
PULSE.c:143:9: note: Cost model analysis: 
PULSE.c:143:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:143:9: note: Basic block will be vectorized using SLP
PULSE.c:143:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:143:9: note: Vectorizing SLP tree:
PULSE.c:143:9: note: node 0x4a79e58 (max_nunits=4, refcnt=1) vector(4) unsigned int
PULSE.c:143:9: note: op template: <retval>.n_layers = _20;
PULSE.c:143:9: note: 	stmt 0 <retval>.n_layers = _20;
PULSE.c:143:9: note: 	stmt 1 <retval>.weights_size = WEIGHTS_SIZE_166;
PULSE.c:143:9: note: 	stmt 2 <retval>.io_size = IO_SIZE_169;
PULSE.c:143:9: note: 	stmt 3 <retval>.fixes_size = FIXES_SIZE_170;
PULSE.c:143:9: note: 	children 0x4a79f58
PULSE.c:143:9: note: node (external) 0x4a79f58 (max_nunits=1, refcnt=1) vector(4) unsigned int
PULSE.c:143:9: note: 	{ _20, WEIGHTS_SIZE_166, IO_SIZE_169, FIXES_SIZE_170 }
PULSE.c:143:9: note: ------>vectorizing SLP node starting from: <retval>.n_layers = _20;
PULSE.c:143:9: note: vect_is_simple_use: operand WEIGHTS_SIZE_166 = PHI <WEIGHTS_SIZE_163(36), 0(25)>, type of def: internal
PULSE.c:143:9: note: vect_is_simple_use: operand IO_SIZE_169 = PHI <IO_SIZE_162(36), 0(25)>, type of def: internal
PULSE.c:143:9: note: vect_is_simple_use: operand FIXES_SIZE_170 = PHI <FIXES_SIZE_161(36), 0(25)>, type of def: internal
PULSE.c:143:9: note: transform store. ncopies = 1
PULSE.c:143:9: note: create vector_type-pointer variable to type: vector(4) unsigned int  vectorizing a pointer ref: <retval>.n_layers
PULSE.c:143:9: note: created &<retval>.n_layers
PULSE.c:143:9: note: add new stmt: MEM <vector(4) unsigned int> [(unsigned int *)&<retval> + 32B] = _17;
PULSE.c:143:9: note: vectorizing stmts using SLP.
PULSE.c:143:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:143:9: note: Vectorizing SLP tree:
PULSE.c:143:9: note: node 0x4a7a058 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:143:9: note: op template: <retval>.layers = layers_42;
PULSE.c:143:9: note: 	stmt 0 <retval>.layers = layers_42;
PULSE.c:143:9: note: 	stmt 1 <retval>.weights = WEIGHTS_165;
PULSE.c:143:9: note: 	children 0x4a7a158
PULSE.c:143:9: note: node (external) 0x4a7a158 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:143:9: note: 	{ layers_42, WEIGHTS_165 }
PULSE.c:143:9: note: ------>vectorizing SLP node starting from: <retval>.layers = layers_42;
PULSE.c:143:9: note: vect_is_simple_use: operand WEIGHTS_165 = PHI <WEIGHTS_47(36), WEIGHTS_88(25)>, type of def: internal
PULSE.c:143:9: note: conflicting alias set types.
PULSE.c:143:9: note: transform store. ncopies = 1
PULSE.c:143:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.layers
PULSE.c:143:9: note: created &<retval>
PULSE.c:143:9: note: add new stmt: MEM <vector(2) long unsigned int> [(void *)&<retval>] = _87;
PULSE.c:143:9: note: vectorizing stmts using SLP.
PULSE.c:143:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:143:9: note: Vectorizing SLP tree:
PULSE.c:143:9: note: node 0x4a7a1d8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:143:9: note: op template: <retval>.io = 0B;
PULSE.c:143:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:143:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:143:9: note: 	children 0x4a7a258
PULSE.c:143:9: note: node (constant) 0x4a7a258 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:143:9: note: 	{ 0B, 0B }
PULSE.c:143:9: note: ------>vectorizing SLP node starting from: <retval>.io = 0B;
PULSE.c:143:9: note: vect_is_simple_use: operand 0B, type of def: constant
PULSE.c:143:9: note: transform store. ncopies = 1
PULSE.c:143:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.io
PULSE.c:143:9: note: created &<retval>.io
PULSE.c:143:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval> + 16B] = { 0, 0 };
PULSE.c:143:9: note: vectorizing stmts using SLP.
PULSE.c:143:9: note: ***** The result for vector mode V32QI would be the same
PULSE.c:149:2: missed: statement clobbers memory: free (_1);
PULSE.c:150:2: missed: statement clobbers memory: free (_2);
PULSE.c:151:2: missed: statement clobbers memory: free (_3);
PULSE.c:152:2: missed: statement clobbers memory: free (_4);
PULSE.c:153:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:153:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _28 (pretmp_91, _51, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:19: missed: not vectorized: no vectype for stmt: _19 = *_18;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:161:56: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:161:56: note: SLPing BB part
Dense.c:161:56: note: Costing subgraph: 
Dense.c:161:56: note: node 0x8e48728 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:161:56: note: op template: _4 = (long unsigned int) _3;
Dense.c:161:56: note: 	stmt 0 _4 = (long unsigned int) _3;
Dense.c:161:56: note: 	stmt 1 _7 = (sizetype) _2;
Dense.c:161:56: note: 	children 0x8e48828
Dense.c:161:56: note: node (external) 0x8e48828 (max_nunits=1, refcnt=1) vector(2) unsigned int
Dense.c:161:56: note: 	{ _3, _2 }
Dense.c:161:56: note: Cost model analysis: 
Dense.c:161:56: note: Cost model analysis for part in loop 0:
  Vector cost: 30
  Scalar cost: 8
Dense.c:161:56: missed: not vectorized: vectorization is not profitable.
Dense.c:161:56: note: ***** The result for vector mode V32QI would be the same
Dense.c:161:56: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:161:56: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:161:56: note: SLPing BB part
Dense.c:161:56: note: Costing subgraph: 
Dense.c:161:56: note: node 0x8e48728 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:161:56: note: op template: _4 = (long unsigned int) _3;
Dense.c:161:56: note: 	stmt 0 _4 = (long unsigned int) _3;
Dense.c:161:56: note: 	stmt 1 _7 = (sizetype) _2;
Dense.c:161:56: note: 	children 0x8e487a8
Dense.c:161:56: note: node (external) 0x8e487a8 (max_nunits=1, refcnt=1) vector(2) unsigned int
Dense.c:161:56: note: 	{ _3, _2 }
Dense.c:161:56: note: Cost model analysis: 
Dense.c:161:56: note: Cost model analysis for part in loop 0:
  Vector cost: 30
  Scalar cost: 8
Dense.c:161:56: missed: not vectorized: vectorization is not profitable.
Dense.c:161:56: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:161:56: note: ***** Analysis failed with vector mode V8QI
Dense.c:161:56: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:161:56: note: ***** Analysis failed with vector mode V4QI
Dense.c:145:10: optimized: loop vectorized using 32 byte vectors
Dense.c:145:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:145:10: optimized: loop vectorized using 16 byte vectors
Dense.c:136:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
Dense.c:129:10: optimized: loop vectorized using 32 byte vectors
Dense.c:129:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:129:10: optimized: loop vectorized using 16 byte vectors
Dense.c:120:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
Dense.c:111:13: note: vectorized 2 loops in function.
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_28, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
Dense.c:111:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:111:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:111:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:111:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:111:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:111:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:111:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:111:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:90:23: missed: couldn't vectorize loop
Dense.c:92:31: missed: not vectorized: no vectype for stmt: _119 = this_40(D)->outputs;
 scalar_type: float *
Dense.c:90:23: missed: couldn't vectorize loop
Dense.c:90:23: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:95:16: missed: couldn't vectorize loop
Dense.c:95:16: missed: not vectorized: unsupported control flow in loop.
Dense.c:83:13: note: vectorized 0 loops in function.
Dense.c:86:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:97:16: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:97:16: note: SLPing BB part
Dense.c:97:16: note: Costing subgraph: 
Dense.c:97:16: note: node 0x8e48728 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:97:16: note: op template: _16 = (sizetype) j_73;
Dense.c:97:16: note: 	stmt 0 _16 = (sizetype) j_73;
Dense.c:97:16: note: 	stmt 1 _111 = (sizetype) wi_74;
Dense.c:97:16: note: 	children 0x8e48828
Dense.c:97:16: note: node (external) 0x8e48828 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:97:16: note: 	{ j_73, wi_74 }
Dense.c:97:16: note: Cost model analysis: 
Dense.c:97:16: note: Scalar 1 and vector 2 loop part do not match up, skipping scalar part
Dense.c:97:16: note: Cost model analysis for part in loop 2:
  Vector cost: 36
  Scalar cost: 8
Dense.c:97:16: missed: not vectorized: vectorization is not profitable.
Dense.c:97:16: note: ***** The result for vector mode V32QI would be the same
Dense.c:97:16: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:97:16: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:97:16: note: SLPing BB part
Dense.c:97:16: note: Costing subgraph: 
Dense.c:97:16: note: node 0x8e48728 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:97:16: note: op template: _16 = (sizetype) j_73;
Dense.c:97:16: note: 	stmt 0 _16 = (sizetype) j_73;
Dense.c:97:16: note: 	stmt 1 _111 = (sizetype) wi_74;
Dense.c:97:16: note: 	children 0x8e487a8
Dense.c:97:16: note: node (external) 0x8e487a8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:97:16: note: 	{ j_73, wi_74 }
Dense.c:97:16: note: Cost model analysis: 
Dense.c:97:16: note: Scalar 1 and vector 2 loop part do not match up, skipping scalar part
Dense.c:97:16: note: Cost model analysis for part in loop 2:
  Vector cost: 36
  Scalar cost: 8
Dense.c:97:16: missed: not vectorized: vectorization is not profitable.
Dense.c:97:16: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:90:50: note: ***** Analysis failed with vector mode V8QI
Dense.c:90:50: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:90:50: note: ***** Analysis failed with vector mode V4QI
Dense.c:62:15: missed: couldn't vectorize loop
Dense.c:62:15: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:75:11: optimized: loop vectorized using 32 byte vectors
Dense.c:75:11: optimized: loop vectorized using 16 byte vectors
Dense.c:66:11: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:881:10: missed: not vectorized: no vectype for stmt: _53 = MEM[(__m256 * {ref-all})w_ptr_82];
 scalar_type: __m256
Dense.c:54:13: note: vectorized 1 loops in function.
Dense.c:57:2: missed: statement clobbers memory: memcpy (_4, dense$baiases_63, _3);
Dense.c:79:2: missed: statement clobbers memory: _27 (pretmp_163, _68, 0);
Dense.c:54:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:54:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:154:107: note: ***** Analysis failed with vector mode V8SI
Dense.c:154:107: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
Dense.c:155:85: note: ***** Analysis failed with vector mode V8SI
Dense.c:155:85: note: ***** The result for vector mode V32QI would be the same
Dense.c:155:85: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:155:85: note: ***** Analysis failed with vector mode V16QI
Dense.c:155:85: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:155:85: note: ***** Analysis failed with vector mode V8QI
Dense.c:155:85: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:155:85: note: ***** Analysis failed with vector mode V4QI
Dense.c:156:107: note: ***** Analysis failed with vector mode V8SI
Dense.c:156:107: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
Dense.c:172:19: missed: couldn't vectorize loop
Dense.c:172:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:165:13: note: vectorized 0 loops in function.
Dense.c:173:38: missed: statement clobbers memory: _5 = rand ();
Dense.c:173:72: missed: statement clobbers memory: _65 = sqrt (_12);
Dense.c:186:19: missed: statement clobbers memory: _24 = PULSE_GetActivationFunctionPtr (_23);
Dense.c:202:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:203:4: missed: statement clobbers memory: exit (1);
Dense.c:207:9: note: ***** Analysis succeeded with vector mode V8SI
Dense.c:207:9: note: SLPing BB part
Dense.c:207:9: note: Costing subgraph: 
Dense.c:207:9: note: node 0x8ccddf8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:207:9: note: op template: <retval>.n_inputs = args$n_inputs_41;
Dense.c:207:9: note: 	stmt 0 <retval>.n_inputs = args$n_inputs_41;
Dense.c:207:9: note: 	stmt 1 <retval>.n_outputs = args$n_outputs_42;
Dense.c:207:9: note: 	children 0x8ccde78
Dense.c:207:9: note: node 0x8ccde78 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:207:9: note: op template: args$n_inputs_41 = args.n_inputs;
Dense.c:207:9: note: 	stmt 0 args$n_inputs_41 = args.n_inputs;
Dense.c:207:9: note: 	stmt 1 args$n_outputs_42 = args.n_outputs;
Dense.c:207:9: note: Cost model analysis: 
Dense.c:207:9: note: Cost model analysis for part in loop 0:
  Vector cost: 40
  Scalar cost: 56
Dense.c:207:9: note: Costing subgraph: 
Dense.c:207:9: note: node 0x8ccdf78 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:207:9: note: op template: <retval>.inputs = IO_31(D);
Dense.c:207:9: note: 	stmt 0 <retval>.inputs = IO_31(D);
Dense.c:207:9: note: 	stmt 1 <retval>.outputs = _22;
Dense.c:207:9: note: 	children 0x8ccdff8
Dense.c:207:9: note: node (external) 0x8ccdff8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:207:9: note: 	{ IO_31(D), _22 }
Dense.c:207:9: note: Cost model analysis: 
Dense.c:207:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:207:9: note: Costing subgraph: 
Dense.c:207:9: note: node 0x8cce0f8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:207:9: note: op template: <retval>.feed = layer$feed_30;
Dense.c:207:9: note: 	stmt 0 <retval>.feed = layer$feed_30;
Dense.c:207:9: note: 	stmt 1 <retval>.back = layer$back_39;
Dense.c:207:9: note: 	stmt 2 <retval>.fix = layer$fix_40;
Dense.c:207:9: note: 	stmt 3 <retval>.activate = _25;
Dense.c:207:9: note: 	children 0x8cce1f8
Dense.c:207:9: note: node (external) 0x8cce1f8 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:207:9: note: 	{ layer$feed_30, layer$back_39, layer$fix_40, _25 }
Dense.c:207:9: note: Cost model analysis: 
Dense.c:207:9: note: Cost model analysis for part in loop 0:
  Vector cost: 64
  Scalar cost: 64
Dense.c:207:9: note: Costing subgraph: 
Dense.c:207:9: note: node 0x8cce2f8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:207:9: note: op template: <retval>.parent = 0B;
Dense.c:207:9: note: 	stmt 0 <retval>.parent = 0B;
Dense.c:207:9: note: 	stmt 1 <retval>.child = 0B;
Dense.c:207:9: note: 	children 0x8cce378
Dense.c:207:9: note: node (constant) 0x8cce378 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:207:9: note: 	{ 0B, 0B }
Dense.c:207:9: note: Cost model analysis: 
Dense.c:207:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:207:9: note: Costing subgraph: 
Dense.c:207:9: note: node 0x8cce3f8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:207:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_29(D);
Dense.c:207:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_29(D);
Dense.c:207:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&<retval> + 96B] = _4;
Dense.c:207:9: note: 	children 0x8cce478
Dense.c:207:9: note: node (external) 0x8cce478 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:207:9: note: 	{ MODEL_29(D), _4 }
Dense.c:207:9: note: Cost model analysis: 
Dense.c:207:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:207:9: note: Basic block will be vectorized using SLP
Dense.c:207:9: optimized: basic block part vectorized using 8 byte vectors
Dense.c:207:9: note: Vectorizing SLP tree:
Dense.c:207:9: note: node 0x8ccddf8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:207:9: note: op template: <retval>.n_inputs = args$n_inputs_41;
Dense.c:207:9: note: 	stmt 0 <retval>.n_inputs = args$n_inputs_41;
Dense.c:207:9: note: 	stmt 1 <retval>.n_outputs = args$n_outputs_42;
Dense.c:207:9: note: 	children 0x8ccde78
Dense.c:207:9: note: node 0x8ccde78 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:207:9: note: op template: args$n_inputs_41 = args.n_inputs;
Dense.c:207:9: note: 	stmt 0 args$n_inputs_41 = args.n_inputs;
Dense.c:207:9: note: 	stmt 1 args$n_outputs_42 = args.n_outputs;
Dense.c:207:9: note: ------>vectorizing SLP node starting from: args$n_inputs_41 = args.n_inputs;
Dense.c:207:9: note: transform load. ncopies = 1
Dense.c:207:9: note: conflicting alias set types.
Dense.c:207:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: args.n_inputs
Dense.c:207:9: note: created &args
Dense.c:207:9: note: add new stmt: vect_args_n_inputs_41.484_26 = MEM <vector(2) unsigned int> [(void *)&args];
Dense.c:207:9: note: extracting lane for live stmt args$n_inputs_41 = args.n_inputs;
Dense.c:207:9: note: extracting lane for live stmt args$n_outputs_42 = args.n_outputs;
Dense.c:207:9: note: ------>vectorizing SLP node starting from: <retval>.n_inputs = args$n_inputs_41;
Dense.c:207:9: note: vect_is_simple_use: operand args.n_inputs, type of def: internal
Dense.c:207:9: note: vect_is_simple_use: operand args.n_outputs, type of def: internal
Dense.c:207:9: note: transform store. ncopies = 1
Dense.c:207:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: <retval>.n_inputs
Dense.c:207:9: note: created &<retval>.n_inputs
Dense.c:207:9: note: add new stmt: MEM <vector(2) unsigned int> [(unsigned int *)&<retval> + 128B] = vect_args_n_inputs_41.484_26;
Dense.c:207:9: note: vectorizing stmts using SLP.
Dense.c:207:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:207:9: note: Vectorizing SLP tree:
Dense.c:207:9: note: node 0x8ccdf78 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:207:9: note: op template: <retval>.inputs = IO_31(D);
Dense.c:207:9: note: 	stmt 0 <retval>.inputs = IO_31(D);
Dense.c:207:9: note: 	stmt 1 <retval>.outputs = _22;
Dense.c:207:9: note: 	children 0x8ccdff8
Dense.c:207:9: note: node (external) 0x8ccdff8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:207:9: note: 	{ IO_31(D), _22 }
Dense.c:207:9: note: ------>vectorizing SLP node starting from: <retval>.inputs = IO_31(D);
Dense.c:207:9: note: vect_is_simple_use: operand IO_31(D) + _21, type of def: internal
Dense.c:207:9: note: transform store. ncopies = 1
Dense.c:207:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.inputs
Dense.c:207:9: note: created &<retval>
Dense.c:207:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval>] = _78;
Dense.c:207:9: note: vectorizing stmts using SLP.
Dense.c:207:9: optimized: basic block part vectorized using 32 byte vectors
Dense.c:207:9: note: Vectorizing SLP tree:
Dense.c:207:9: note: node 0x8cce0f8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:207:9: note: op template: <retval>.feed = layer$feed_30;
Dense.c:207:9: note: 	stmt 0 <retval>.feed = layer$feed_30;
Dense.c:207:9: note: 	stmt 1 <retval>.back = layer$back_39;
Dense.c:207:9: note: 	stmt 2 <retval>.fix = layer$fix_40;
Dense.c:207:9: note: 	stmt 3 <retval>.activate = _25;
Dense.c:207:9: note: 	children 0x8cce1f8
Dense.c:207:9: note: node (external) 0x8cce1f8 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:207:9: note: 	{ layer$feed_30, layer$back_39, layer$fix_40, _25 }
Dense.c:207:9: note: ------>vectorizing SLP node starting from: <retval>.feed = layer$feed_30;
Dense.c:207:9: note: vect_is_simple_use: operand layer$back_39 = PHI <layer$back_45(D)(11), _BackDense(9), _SIMD_BackDense(15)>, type of def: internal
Dense.c:207:9: note: vect_is_simple_use: operand layer$fix_40 = PHI <layer$fix_46(D)(11), _FixDense(9), _SIMD_FixDense(15)>, type of def: internal
Dense.c:207:9: note: vect_is_simple_use: operand (void (*<T91a>) (float *, unsigned int, char)) _24, type of def: internal
Dense.c:207:9: note: conflicting alias set types.
Dense.c:207:9: note: transform store. ncopies = 1
Dense.c:207:9: note: create vector_type-pointer variable to type: vector(4) long unsigned int  vectorizing a pointer ref: <retval>.feed
Dense.c:207:9: note: created &<retval>.feed
Dense.c:207:9: note: add new stmt: MEM <vector(4) long unsigned int> [(void *)&<retval> + 32B] = _84;
Dense.c:207:9: note: vectorizing stmts using SLP.
Dense.c:207:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:207:9: note: Vectorizing SLP tree:
Dense.c:207:9: note: node 0x8cce2f8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:207:9: note: op template: <retval>.parent = 0B;
Dense.c:207:9: note: 	stmt 0 <retval>.parent = 0B;
Dense.c:207:9: note: 	stmt 1 <retval>.child = 0B;
Dense.c:207:9: note: 	children 0x8cce378
Dense.c:207:9: note: node (constant) 0x8cce378 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:207:9: note: 	{ 0B, 0B }
Dense.c:207:9: note: ------>vectorizing SLP node starting from: <retval>.parent = 0B;
Dense.c:207:9: note: vect_is_simple_use: operand 0B, type of def: constant
Dense.c:207:9: note: transform store. ncopies = 1
Dense.c:207:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.parent
Dense.c:207:9: note: created &<retval>.parent
Dense.c:207:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer * *)&<retval> + 72B] = { 0, 0 };
Dense.c:207:9: note: vectorizing stmts using SLP.
Dense.c:207:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:207:9: note: Vectorizing SLP tree:
Dense.c:207:9: note: node 0x8cce3f8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:207:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_29(D);
Dense.c:207:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_29(D);
Dense.c:207:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&<retval> + 96B] = _4;
Dense.c:207:9: note: 	children 0x8cce478
Dense.c:207:9: note: node (external) 0x8cce478 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:207:9: note: 	{ MODEL_29(D), _4 }
Dense.c:207:9: note: ------>vectorizing SLP node starting from: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_29(D);
Dense.c:207:9: note: vect_is_simple_use: operand MODEL_29(D) + _3, type of def: internal
Dense.c:207:9: note: transform store. ncopies = 1
Dense.c:207:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B]
Dense.c:207:9: note: created &MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B]
Dense.c:207:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer *)&<retval> + 88B] = _89;
Dense.c:207:9: note: vectorizing stmts using SLP.
Dense.c:207:9: note: ***** The result for vector mode V32QI would be the same
Activations.c:17:19: optimized: loop vectorized using 32 byte vectors
Activations.c:17:19: optimized: loop vectorized using 16 byte vectors
Activations.c:17:19: optimized: loop vectorized using 32 byte vectors
Activations.c:17:19: optimized: loop vectorized using 16 byte vectors
Activations.c:14:13: note: vectorized 2 loops in function.
Activations.c:14:13: note: ***** Analysis failed with vector mode V8SF
Activations.c:14:13: note: ***** The result for vector mode V32QI would be the same
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V16QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V16QI
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V8QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V8QI
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V4QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V4QI
Activations.c:28:19: missed: couldn't vectorize loop
Activations.c:28:19: missed: not vectorized: unsupported control flow in loop.
Activations.c:28:19: optimized: loop vectorized using 32 byte vectors
Activations.c:28:19: optimized: loop vectorized using 16 byte vectors
Activations.c:25:13: note: vectorized 1 loops in function.
Activations.c:25:13: note: ***** Analysis failed with vector mode V8SF
Activations.c:25:13: note: ***** The result for vector mode V32QI would be the same
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V16QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V16QI
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V8QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V8QI
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V4QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V4QI
Activations.c:38:1: note: ***** Analysis failed with vector mode VOID
Activations.c:5:19: missed: couldn't vectorize loop
Activations.c:9:21: missed: statement clobbers memory: _36 = expf (_18);
Activations.c:5:19: missed: couldn't vectorize loop
Activations.c:9:21: missed: statement clobbers memory: _12 = expf (_11);
Activations.c:3:13: note: vectorized 0 loops in function.
Activations.c:9:21: missed: statement clobbers memory: _12 = expf (_11);
Activations.c:9:21: missed: statement clobbers memory: _36 = expf (_18);
Activations.c:11:1: note: ***** Analysis failed with vector mode V8SF
Activations.c:11:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SF
Activations.c:44:2: note: ***** Analysis failed with vector mode VOID
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:4:18: note: vectorized 0 loops in function.
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_4, inputs_15, _3);
PULSE.c:8:2: missed: statement clobbers memory: _5 (layer_17);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_12, pretmp_26, _9);
PULSE.c:13:28: note: ***** Analysis failed with vector mode V8SI
PULSE.c:13:28: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:22:2: missed: statement clobbers memory: _1 (layer_10(D));
PULSE.c:22:2: missed: statement clobbers memory: _14 (_2);
PULSE.c:22:2: missed: statement clobbers memory: _21 (_15);
PULSE.c:22:2: missed: statement clobbers memory: _28 (_22);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_29);
PULSE.c:26:3: missed: statement clobbers memory: memset (_34, 0, _32);
PULSE.c:26:3: missed: statement clobbers memory: memset (_27, 0, _25);
PULSE.c:26:3: missed: statement clobbers memory: memset (_20, 0, _18);
PULSE.c:26:3: missed: statement clobbers memory: memset (_7, 0, _5);
PULSE.c:28:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:28:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:39: missed: statement clobbers memory: _7 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:32:6: note: vectorized 1 loops in function.
PULSE.c:34:8: missed: statement clobbers memory: _1 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_2);
PULSE.c:39:39: missed: statement clobbers memory: _7 = rand ();
PULSE.c:40:40: missed: statement clobbers memory: _9 = rand ();
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8SI
PULSE.c:32:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:63:16: missed: couldn't vectorize loop
PULSE.c:63:16: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:66:17: missed: couldn't vectorize loop
PULSE.c:66:17: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:75:19: missed: couldn't vectorize loop
PULSE.c:75:19: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:39: missed: statement clobbers memory: _91 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:55:15: missed: couldn't vectorize loop
PULSE.c:57:14: missed: statement clobbers memory: _80 = _11 (_10, TMP_PTR_214);
PULSE.c:47:6: note: vectorized 1 loops in function.
PULSE.c:48:1: missed: statement clobbers memory: saved_stack.8_53 = __builtin_stack_save ();
PULSE.c:49:46: missed: statement clobbers memory: _1 = PULSE_GetLossFunctionPtr (loss_function_54(D));
PULSE.c:50:42: missed: statement clobbers memory: TMP_58 = aligned_alloc (64, _4);
PULSE.c:57:14: missed: statement clobbers memory: _80 = _11 (_10, TMP_PTR_214);
PULSE.c:62:15: missed: statement clobbers memory: random.5_64 = __builtin_alloca_with_align (_16, 32);
PULSE.c:34:8: missed: statement clobbers memory: _83 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_84);
PULSE.c:39:39: missed: statement clobbers memory: _91 = rand ();
PULSE.c:40:40: missed: statement clobbers memory: _94 = rand ();
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_120, inputs_115, _119);
PULSE.c:8:2: missed: statement clobbers memory: _121 (layer_116);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_127, _126, _125);
PULSE.c:69:11: missed: statement clobbers memory: loss_73 = PULSE_GetLoss_56 (_29, _28, _24, _23);
PULSE.c:22:2: missed: statement clobbers memory: _108 (output_60);
PULSE.c:22:2: missed: statement clobbers memory: _130 (_109);
PULSE.c:22:2: missed: statement clobbers memory: _137 (_131);
PULSE.c:22:2: missed: statement clobbers memory: _144 (_138);
PULSE.c:22:2: missed: statement clobbers memory: _151 (_145);
PULSE.c:22:2: missed: statement clobbers memory: _158 (_152);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_159);
PULSE.c:26:3: missed: statement clobbers memory: memset (_164, 0, _162);
PULSE.c:26:3: missed: statement clobbers memory: memset (_157, 0, _155);
PULSE.c:26:3: missed: statement clobbers memory: memset (_150, 0, _148);
PULSE.c:26:3: missed: statement clobbers memory: memset (_143, 0, _141);
PULSE.c:26:3: missed: statement clobbers memory: memset (_136, 0, _134);
PULSE.c:26:3: missed: statement clobbers memory: memset (_114, 0, _112);
PULSE.c:77:6: missed: statement clobbers memory: _33 (current_219, args);
PULSE.c:83:4: missed: statement clobbers memory: printf ("Epoch: %d | Item: %d | Loss: %.10f | Batch Loss: %.10f\r", i_216, j_217, _37, _36);
PULSE.c:47:6: missed: statement clobbers memory: __builtin_stack_restore (saved_stack.8_53);
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4DI
PULSE.c:47:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:93:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:93:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:127:20: missed: couldn't vectorize loop
PULSE.c:127:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:127:20: missed: couldn't vectorize loop
PULSE.c:127:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:106:20: missed: couldn't vectorize loop
PULSE.c:106:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:106:20: missed: couldn't vectorize loop
PULSE.c:106:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:95:13: note: vectorized 0 loops in function.
PULSE.c:98:39: missed: statement clobbers memory: layers_42 = malloc (_3);
PULSE.c:101:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:120:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:121:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:122:46: missed: statement clobbers memory: WEIGHTS_88 = aligned_alloc (64, 0);
PULSE.c:112:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.115_96];
PULSE.c:113:21: missed: statement clobbers memory: _77 = PULSE_GetDenseWeightsSize (args);
PULSE.c:114:16: missed: statement clobbers memory: _80 = PULSE_GetDenseIOSize (args);
PULSE.c:115:19: missed: statement clobbers memory: _83 = PULSE_GetDenseFixesSize (args);
PULSE.c:112:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})_226];
PULSE.c:113:21: missed: statement clobbers memory: _212 = PULSE_GetDenseWeightsSize (args);
PULSE.c:114:16: missed: statement clobbers memory: _215 = PULSE_GetDenseIOSize (args);
PULSE.c:115:19: missed: statement clobbers memory: _218 = PULSE_GetDenseFixesSize (args);
PULSE.c:120:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:121:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:122:46: missed: statement clobbers memory: WEIGHTS_47 = aligned_alloc (64, _5);
PULSE.c:123:41: missed: statement clobbers memory: IO_49 = aligned_alloc (64, _7);
PULSE.c:133:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.117_98];
PULSE.c:134:17: missed: statement clobbers memory: *_10 = PULSE_CreateDenseLayer (args, WEIGHTS_PTR_154, IO_PTR_155); [return slot optimization]
PULSE.c:135:20: missed: statement clobbers memory: _66 = PULSE_GetDenseWeightsSize (args);
PULSE.c:136:15: missed: statement clobbers memory: _69 = PULSE_GetDenseIOSize (args);
PULSE.c:133:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.117_126];
PULSE.c:134:17: missed: statement clobbers memory: *_108 = PULSE_CreateDenseLayer (args, WEIGHTS_PTR_150, IO_PTR_145); [return slot optimization]
PULSE.c:135:20: missed: statement clobbers memory: _97 = PULSE_GetDenseWeightsSize (args);
PULSE.c:136:15: missed: statement clobbers memory: _61 = PULSE_GetDenseIOSize (args);
PULSE.c:142:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:143:9: note: ***** Analysis succeeded with vector mode V8SI
PULSE.c:143:9: note: SLPing BB part
PULSE.c:143:9: note: Costing subgraph: 
PULSE.c:143:9: note: node 0x3bb9d28 (max_nunits=4, refcnt=1) vector(4) unsigned int
PULSE.c:143:9: note: op template: <retval>.n_layers = _20;
PULSE.c:143:9: note: 	stmt 0 <retval>.n_layers = _20;
PULSE.c:143:9: note: 	stmt 1 <retval>.weights_size = WEIGHTS_SIZE_166;
PULSE.c:143:9: note: 	stmt 2 <retval>.io_size = IO_SIZE_169;
PULSE.c:143:9: note: 	stmt 3 <retval>.fixes_size = FIXES_SIZE_170;
PULSE.c:143:9: note: 	children 0x3bb9e28
PULSE.c:143:9: note: node (external) 0x3bb9e28 (max_nunits=1, refcnt=1) vector(4) unsigned int
PULSE.c:143:9: note: 	{ _20, WEIGHTS_SIZE_166, IO_SIZE_169, FIXES_SIZE_170 }
PULSE.c:143:9: note: Cost model analysis: 
PULSE.c:143:9: note: Cost model analysis for part in loop 0:
  Vector cost: 52
  Scalar cost: 64
PULSE.c:143:9: note: Costing subgraph: 
PULSE.c:143:9: note: node 0x3bb9f28 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:143:9: note: op template: <retval>.layers = layers_42;
PULSE.c:143:9: note: 	stmt 0 <retval>.layers = layers_42;
PULSE.c:143:9: note: 	stmt 1 <retval>.weights = WEIGHTS_165;
PULSE.c:143:9: note: 	children 0x3bba028
PULSE.c:143:9: note: node (external) 0x3bba028 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:143:9: note: 	{ layers_42, WEIGHTS_165 }
PULSE.c:143:9: note: Cost model analysis: 
PULSE.c:143:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:143:9: note: Costing subgraph: 
PULSE.c:143:9: note: node 0x3bba0a8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:143:9: note: op template: <retval>.io = 0B;
PULSE.c:143:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:143:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:143:9: note: 	children 0x3bba128
PULSE.c:143:9: note: node (constant) 0x3bba128 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:143:9: note: 	{ 0B, 0B }
PULSE.c:143:9: note: Cost model analysis: 
PULSE.c:143:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:143:9: note: Basic block will be vectorized using SLP
PULSE.c:143:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:143:9: note: Vectorizing SLP tree:
PULSE.c:143:9: note: node 0x3bb9d28 (max_nunits=4, refcnt=1) vector(4) unsigned int
PULSE.c:143:9: note: op template: <retval>.n_layers = _20;
PULSE.c:143:9: note: 	stmt 0 <retval>.n_layers = _20;
PULSE.c:143:9: note: 	stmt 1 <retval>.weights_size = WEIGHTS_SIZE_166;
PULSE.c:143:9: note: 	stmt 2 <retval>.io_size = IO_SIZE_169;
PULSE.c:143:9: note: 	stmt 3 <retval>.fixes_size = FIXES_SIZE_170;
PULSE.c:143:9: note: 	children 0x3bb9e28
PULSE.c:143:9: note: node (external) 0x3bb9e28 (max_nunits=1, refcnt=1) vector(4) unsigned int
PULSE.c:143:9: note: 	{ _20, WEIGHTS_SIZE_166, IO_SIZE_169, FIXES_SIZE_170 }
PULSE.c:143:9: note: ------>vectorizing SLP node starting from: <retval>.n_layers = _20;
PULSE.c:143:9: note: vect_is_simple_use: operand WEIGHTS_SIZE_166 = PHI <WEIGHTS_SIZE_163(36), 0(25)>, type of def: internal
PULSE.c:143:9: note: vect_is_simple_use: operand IO_SIZE_169 = PHI <IO_SIZE_162(36), 0(25)>, type of def: internal
PULSE.c:143:9: note: vect_is_simple_use: operand FIXES_SIZE_170 = PHI <FIXES_SIZE_161(36), 0(25)>, type of def: internal
PULSE.c:143:9: note: transform store. ncopies = 1
PULSE.c:143:9: note: create vector_type-pointer variable to type: vector(4) unsigned int  vectorizing a pointer ref: <retval>.n_layers
PULSE.c:143:9: note: created &<retval>.n_layers
PULSE.c:143:9: note: add new stmt: MEM <vector(4) unsigned int> [(unsigned int *)&<retval> + 32B] = _17;
PULSE.c:143:9: note: vectorizing stmts using SLP.
PULSE.c:143:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:143:9: note: Vectorizing SLP tree:
PULSE.c:143:9: note: node 0x3bb9f28 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:143:9: note: op template: <retval>.layers = layers_42;
PULSE.c:143:9: note: 	stmt 0 <retval>.layers = layers_42;
PULSE.c:143:9: note: 	stmt 1 <retval>.weights = WEIGHTS_165;
PULSE.c:143:9: note: 	children 0x3bba028
PULSE.c:143:9: note: node (external) 0x3bba028 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:143:9: note: 	{ layers_42, WEIGHTS_165 }
PULSE.c:143:9: note: ------>vectorizing SLP node starting from: <retval>.layers = layers_42;
PULSE.c:143:9: note: vect_is_simple_use: operand WEIGHTS_165 = PHI <WEIGHTS_47(36), WEIGHTS_88(25)>, type of def: internal
PULSE.c:143:9: note: conflicting alias set types.
PULSE.c:143:9: note: transform store. ncopies = 1
PULSE.c:143:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.layers
PULSE.c:143:9: note: created &<retval>
PULSE.c:143:9: note: add new stmt: MEM <vector(2) long unsigned int> [(void *)&<retval>] = _87;
PULSE.c:143:9: note: vectorizing stmts using SLP.
PULSE.c:143:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:143:9: note: Vectorizing SLP tree:
PULSE.c:143:9: note: node 0x3bba0a8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:143:9: note: op template: <retval>.io = 0B;
PULSE.c:143:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:143:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:143:9: note: 	children 0x3bba128
PULSE.c:143:9: note: node (constant) 0x3bba128 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:143:9: note: 	{ 0B, 0B }
PULSE.c:143:9: note: ------>vectorizing SLP node starting from: <retval>.io = 0B;
PULSE.c:143:9: note: vect_is_simple_use: operand 0B, type of def: constant
PULSE.c:143:9: note: transform store. ncopies = 1
PULSE.c:143:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.io
PULSE.c:143:9: note: created &<retval>.io
PULSE.c:143:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval> + 16B] = { 0, 0 };
PULSE.c:143:9: note: vectorizing stmts using SLP.
PULSE.c:143:9: note: ***** The result for vector mode V32QI would be the same
PULSE.c:149:2: missed: statement clobbers memory: free (_1);
PULSE.c:150:2: missed: statement clobbers memory: free (_2);
PULSE.c:151:2: missed: statement clobbers memory: free (_3);
PULSE.c:152:2: missed: statement clobbers memory: free (_4);
PULSE.c:153:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:153:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Loss.c:15:19: optimized: loop vectorized using 32 byte vectors
Loss.c:15:19: optimized:  loop versioned for vectorization because of possible aliasing
Loss.c:15:19: optimized: loop vectorized using 16 byte vectors
Loss.c:12:23: note: vectorized 1 loops in function.
Loss.c:12:23: note: ***** Analysis failed with vector mode V8SF
Loss.c:12:23: note: ***** The result for vector mode V32QI would be the same
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V16QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V16QI
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V8QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V8QI
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V4QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V4QI
Loss.c:7:19: missed: couldn't vectorize loop
Loss.c:8:38: missed: not vectorized: unsupported use in stmt.
Loss.c:4:23: note: vectorized 0 loops in function.
Loss.c:9:13: note: ***** Analysis failed with vector mode V8SF
Loss.c:9:13: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SF
Loss.c:23:2: note: ***** Analysis failed with vector mode VOID
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:4:18: note: vectorized 0 loops in function.
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_4, inputs_15, _3);
PULSE.c:8:2: missed: statement clobbers memory: _5 (layer_17);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_12, pretmp_26, _9);
PULSE.c:13:28: note: ***** Analysis failed with vector mode V8SI
PULSE.c:13:28: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:22:2: missed: statement clobbers memory: _1 (layer_10(D));
PULSE.c:22:2: missed: statement clobbers memory: _14 (_2);
PULSE.c:22:2: missed: statement clobbers memory: _21 (_15);
PULSE.c:22:2: missed: statement clobbers memory: _28 (_22);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_29);
PULSE.c:26:3: missed: statement clobbers memory: memset (_34, 0, _32);
PULSE.c:26:3: missed: statement clobbers memory: memset (_27, 0, _25);
PULSE.c:26:3: missed: statement clobbers memory: memset (_20, 0, _18);
PULSE.c:26:3: missed: statement clobbers memory: memset (_7, 0, _5);
PULSE.c:28:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:28:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:39: missed: statement clobbers memory: _7 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:32:6: note: vectorized 1 loops in function.
PULSE.c:34:8: missed: statement clobbers memory: _1 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_2);
PULSE.c:39:39: missed: statement clobbers memory: _7 = rand ();
PULSE.c:40:40: missed: statement clobbers memory: _9 = rand ();
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8SI
PULSE.c:32:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:60:16: missed: couldn't vectorize loop
PULSE.c:60:16: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:63:17: missed: couldn't vectorize loop
PULSE.c:63:17: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:72:19: missed: couldn't vectorize loop
PULSE.c:72:19: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:39: missed: statement clobbers memory: _92 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:55:15: missed: couldn't vectorize loop
PULSE.c:56:14: missed: statement clobbers memory: _81 = _11 (_10, TMP_PTR_215);
PULSE.c:47:6: note: vectorized 1 loops in function.
PULSE.c:48:1: missed: statement clobbers memory: saved_stack.8_53 = __builtin_stack_save ();
PULSE.c:49:46: missed: statement clobbers memory: _1 = PULSE_GetLossFunctionPtr (loss_function_54(D));
PULSE.c:50:42: missed: statement clobbers memory: TMP_58 = aligned_alloc (64, _4);
PULSE.c:56:14: missed: statement clobbers memory: _81 = _11 (_10, TMP_PTR_215);
PULSE.c:59:15: missed: statement clobbers memory: random.5_64 = __builtin_alloca_with_align (_16, 32);
PULSE.c:34:8: missed: statement clobbers memory: _84 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_85);
PULSE.c:39:39: missed: statement clobbers memory: _92 = rand ();
PULSE.c:40:40: missed: statement clobbers memory: _95 = rand ();
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_121, inputs_116, _120);
PULSE.c:8:2: missed: statement clobbers memory: _122 (layer_117);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_128, _127, _126);
PULSE.c:66:11: missed: statement clobbers memory: loss_74 = PULSE_GetLoss_56 (_29, _28, _24, _23);
PULSE.c:22:2: missed: statement clobbers memory: _109 (output_60);
PULSE.c:22:2: missed: statement clobbers memory: _131 (_110);
PULSE.c:22:2: missed: statement clobbers memory: _138 (_132);
PULSE.c:22:2: missed: statement clobbers memory: _145 (_139);
PULSE.c:22:2: missed: statement clobbers memory: _152 (_146);
PULSE.c:22:2: missed: statement clobbers memory: _159 (_153);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_160);
PULSE.c:26:3: missed: statement clobbers memory: memset (_165, 0, _163);
PULSE.c:26:3: missed: statement clobbers memory: memset (_158, 0, _156);
PULSE.c:26:3: missed: statement clobbers memory: memset (_151, 0, _149);
PULSE.c:26:3: missed: statement clobbers memory: memset (_144, 0, _142);
PULSE.c:26:3: missed: statement clobbers memory: memset (_137, 0, _135);
PULSE.c:26:3: missed: statement clobbers memory: memset (_115, 0, _113);
PULSE.c:74:6: missed: statement clobbers memory: _33 (current_220, args);
PULSE.c:80:4: missed: statement clobbers memory: printf ("Epoch: %d | Item: %d | Loss: %.10f | Batch Loss: %.10f\r", i_217, j_218, _37, _36);
PULSE.c:84:2: missed: statement clobbers memory: free (TMP_58);
PULSE.c:47:6: missed: statement clobbers memory: __builtin_stack_restore (saved_stack.8_53);
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4DI
PULSE.c:47:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:91:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:91:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:125:20: missed: couldn't vectorize loop
PULSE.c:125:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:125:20: missed: couldn't vectorize loop
PULSE.c:125:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:104:20: missed: couldn't vectorize loop
PULSE.c:104:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:104:20: missed: couldn't vectorize loop
PULSE.c:104:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:93:13: note: vectorized 0 loops in function.
PULSE.c:96:39: missed: statement clobbers memory: layers_42 = malloc (_3);
PULSE.c:99:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:118:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:119:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:120:46: missed: statement clobbers memory: WEIGHTS_88 = aligned_alloc (64, 0);
PULSE.c:110:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.115_96];
PULSE.c:111:21: missed: statement clobbers memory: _77 = PULSE_GetDenseWeightsSize (args);
PULSE.c:112:16: missed: statement clobbers memory: _80 = PULSE_GetDenseIOSize (args);
PULSE.c:113:19: missed: statement clobbers memory: _83 = PULSE_GetDenseFixesSize (args);
PULSE.c:110:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})_226];
PULSE.c:111:21: missed: statement clobbers memory: _212 = PULSE_GetDenseWeightsSize (args);
PULSE.c:112:16: missed: statement clobbers memory: _215 = PULSE_GetDenseIOSize (args);
PULSE.c:113:19: missed: statement clobbers memory: _218 = PULSE_GetDenseFixesSize (args);
PULSE.c:118:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:119:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:120:46: missed: statement clobbers memory: WEIGHTS_47 = aligned_alloc (64, _5);
PULSE.c:121:41: missed: statement clobbers memory: IO_49 = aligned_alloc (64, _7);
PULSE.c:131:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.117_98];
PULSE.c:132:17: missed: statement clobbers memory: *_10 = PULSE_CreateDenseLayer (args, WEIGHTS_PTR_154, IO_PTR_155); [return slot optimization]
PULSE.c:133:20: missed: statement clobbers memory: _66 = PULSE_GetDenseWeightsSize (args);
PULSE.c:134:15: missed: statement clobbers memory: _69 = PULSE_GetDenseIOSize (args);
PULSE.c:131:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.117_126];
PULSE.c:132:17: missed: statement clobbers memory: *_108 = PULSE_CreateDenseLayer (args, WEIGHTS_PTR_150, IO_PTR_145); [return slot optimization]
PULSE.c:133:20: missed: statement clobbers memory: _97 = PULSE_GetDenseWeightsSize (args);
PULSE.c:134:15: missed: statement clobbers memory: _61 = PULSE_GetDenseIOSize (args);
PULSE.c:140:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:141:9: note: ***** Analysis succeeded with vector mode V8SI
PULSE.c:141:9: note: SLPing BB part
PULSE.c:141:9: note: Costing subgraph: 
PULSE.c:141:9: note: node 0x3137d18 (max_nunits=4, refcnt=1) vector(4) unsigned int
PULSE.c:141:9: note: op template: <retval>.n_layers = _20;
PULSE.c:141:9: note: 	stmt 0 <retval>.n_layers = _20;
PULSE.c:141:9: note: 	stmt 1 <retval>.weights_size = WEIGHTS_SIZE_166;
PULSE.c:141:9: note: 	stmt 2 <retval>.io_size = IO_SIZE_169;
PULSE.c:141:9: note: 	stmt 3 <retval>.fixes_size = FIXES_SIZE_170;
PULSE.c:141:9: note: 	children 0x3137e18
PULSE.c:141:9: note: node (external) 0x3137e18 (max_nunits=1, refcnt=1) vector(4) unsigned int
PULSE.c:141:9: note: 	{ _20, WEIGHTS_SIZE_166, IO_SIZE_169, FIXES_SIZE_170 }
PULSE.c:141:9: note: Cost model analysis: 
PULSE.c:141:9: note: Cost model analysis for part in loop 0:
  Vector cost: 52
  Scalar cost: 64
PULSE.c:141:9: note: Costing subgraph: 
PULSE.c:141:9: note: node 0x3137f18 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:141:9: note: op template: <retval>.layers = layers_42;
PULSE.c:141:9: note: 	stmt 0 <retval>.layers = layers_42;
PULSE.c:141:9: note: 	stmt 1 <retval>.weights = WEIGHTS_165;
PULSE.c:141:9: note: 	children 0x3138018
PULSE.c:141:9: note: node (external) 0x3138018 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:141:9: note: 	{ layers_42, WEIGHTS_165 }
PULSE.c:141:9: note: Cost model analysis: 
PULSE.c:141:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:141:9: note: Costing subgraph: 
PULSE.c:141:9: note: node 0x3138098 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:141:9: note: op template: <retval>.io = 0B;
PULSE.c:141:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:141:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:141:9: note: 	children 0x3138118
PULSE.c:141:9: note: node (constant) 0x3138118 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:141:9: note: 	{ 0B, 0B }
PULSE.c:141:9: note: Cost model analysis: 
PULSE.c:141:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:141:9: note: Basic block will be vectorized using SLP
PULSE.c:141:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:141:9: note: Vectorizing SLP tree:
PULSE.c:141:9: note: node 0x3137d18 (max_nunits=4, refcnt=1) vector(4) unsigned int
PULSE.c:141:9: note: op template: <retval>.n_layers = _20;
PULSE.c:141:9: note: 	stmt 0 <retval>.n_layers = _20;
PULSE.c:141:9: note: 	stmt 1 <retval>.weights_size = WEIGHTS_SIZE_166;
PULSE.c:141:9: note: 	stmt 2 <retval>.io_size = IO_SIZE_169;
PULSE.c:141:9: note: 	stmt 3 <retval>.fixes_size = FIXES_SIZE_170;
PULSE.c:141:9: note: 	children 0x3137e18
PULSE.c:141:9: note: node (external) 0x3137e18 (max_nunits=1, refcnt=1) vector(4) unsigned int
PULSE.c:141:9: note: 	{ _20, WEIGHTS_SIZE_166, IO_SIZE_169, FIXES_SIZE_170 }
PULSE.c:141:9: note: ------>vectorizing SLP node starting from: <retval>.n_layers = _20;
PULSE.c:141:9: note: vect_is_simple_use: operand WEIGHTS_SIZE_166 = PHI <WEIGHTS_SIZE_163(36), 0(25)>, type of def: internal
PULSE.c:141:9: note: vect_is_simple_use: operand IO_SIZE_169 = PHI <IO_SIZE_162(36), 0(25)>, type of def: internal
PULSE.c:141:9: note: vect_is_simple_use: operand FIXES_SIZE_170 = PHI <FIXES_SIZE_161(36), 0(25)>, type of def: internal
PULSE.c:141:9: note: transform store. ncopies = 1
PULSE.c:141:9: note: create vector_type-pointer variable to type: vector(4) unsigned int  vectorizing a pointer ref: <retval>.n_layers
PULSE.c:141:9: note: created &<retval>.n_layers
PULSE.c:141:9: note: add new stmt: MEM <vector(4) unsigned int> [(unsigned int *)&<retval> + 32B] = _17;
PULSE.c:141:9: note: vectorizing stmts using SLP.
PULSE.c:141:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:141:9: note: Vectorizing SLP tree:
PULSE.c:141:9: note: node 0x3137f18 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:141:9: note: op template: <retval>.layers = layers_42;
PULSE.c:141:9: note: 	stmt 0 <retval>.layers = layers_42;
PULSE.c:141:9: note: 	stmt 1 <retval>.weights = WEIGHTS_165;
PULSE.c:141:9: note: 	children 0x3138018
PULSE.c:141:9: note: node (external) 0x3138018 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:141:9: note: 	{ layers_42, WEIGHTS_165 }
PULSE.c:141:9: note: ------>vectorizing SLP node starting from: <retval>.layers = layers_42;
PULSE.c:141:9: note: vect_is_simple_use: operand WEIGHTS_165 = PHI <WEIGHTS_47(36), WEIGHTS_88(25)>, type of def: internal
PULSE.c:141:9: note: conflicting alias set types.
PULSE.c:141:9: note: transform store. ncopies = 1
PULSE.c:141:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.layers
PULSE.c:141:9: note: created &<retval>
PULSE.c:141:9: note: add new stmt: MEM <vector(2) long unsigned int> [(void *)&<retval>] = _87;
PULSE.c:141:9: note: vectorizing stmts using SLP.
PULSE.c:141:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:141:9: note: Vectorizing SLP tree:
PULSE.c:141:9: note: node 0x3138098 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:141:9: note: op template: <retval>.io = 0B;
PULSE.c:141:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:141:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:141:9: note: 	children 0x3138118
PULSE.c:141:9: note: node (constant) 0x3138118 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:141:9: note: 	{ 0B, 0B }
PULSE.c:141:9: note: ------>vectorizing SLP node starting from: <retval>.io = 0B;
PULSE.c:141:9: note: vect_is_simple_use: operand 0B, type of def: constant
PULSE.c:141:9: note: transform store. ncopies = 1
PULSE.c:141:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.io
PULSE.c:141:9: note: created &<retval>.io
PULSE.c:141:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval> + 16B] = { 0, 0 };
PULSE.c:141:9: note: vectorizing stmts using SLP.
PULSE.c:141:9: note: ***** The result for vector mode V32QI would be the same
PULSE.c:147:2: missed: statement clobbers memory: free (_1);
PULSE.c:148:2: missed: statement clobbers memory: free (_2);
PULSE.c:149:2: missed: statement clobbers memory: free (_3);
PULSE.c:150:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:150:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _28 (pretmp_91, _51, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:19: missed: not vectorized: no vectype for stmt: _19 = *_18;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:167:56: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:167:56: note: SLPing BB part
Dense.c:167:56: note: Costing subgraph: 
Dense.c:167:56: note: node 0x7b0d108 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:167:56: note: op template: _4 = (long unsigned int) _3;
Dense.c:167:56: note: 	stmt 0 _4 = (long unsigned int) _3;
Dense.c:167:56: note: 	stmt 1 _7 = (sizetype) _2;
Dense.c:167:56: note: 	children 0x7b0d208
Dense.c:167:56: note: node (external) 0x7b0d208 (max_nunits=1, refcnt=1) vector(2) unsigned int
Dense.c:167:56: note: 	{ _3, _2 }
Dense.c:167:56: note: Cost model analysis: 
Dense.c:167:56: note: Cost model analysis for part in loop 0:
  Vector cost: 30
  Scalar cost: 8
Dense.c:167:56: missed: not vectorized: vectorization is not profitable.
Dense.c:167:56: note: ***** The result for vector mode V32QI would be the same
Dense.c:167:56: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:167:56: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:167:56: note: SLPing BB part
Dense.c:167:56: note: Costing subgraph: 
Dense.c:167:56: note: node 0x7b0d108 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:167:56: note: op template: _4 = (long unsigned int) _3;
Dense.c:167:56: note: 	stmt 0 _4 = (long unsigned int) _3;
Dense.c:167:56: note: 	stmt 1 _7 = (sizetype) _2;
Dense.c:167:56: note: 	children 0x7b0d188
Dense.c:167:56: note: node (external) 0x7b0d188 (max_nunits=1, refcnt=1) vector(2) unsigned int
Dense.c:167:56: note: 	{ _3, _2 }
Dense.c:167:56: note: Cost model analysis: 
Dense.c:167:56: note: Cost model analysis for part in loop 0:
  Vector cost: 30
  Scalar cost: 8
Dense.c:167:56: missed: not vectorized: vectorization is not profitable.
Dense.c:167:56: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:167:56: note: ***** Analysis failed with vector mode V8QI
Dense.c:167:56: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:167:56: note: ***** Analysis failed with vector mode V4QI
Dense.c:151:10: optimized: loop vectorized using 32 byte vectors
Dense.c:151:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:151:10: optimized: loop vectorized using 16 byte vectors
Dense.c:142:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
Dense.c:135:10: optimized: loop vectorized using 32 byte vectors
Dense.c:135:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:135:10: optimized: loop vectorized using 16 byte vectors
Dense.c:126:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
Dense.c:117:13: note: vectorized 2 loops in function.
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_28, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
Dense.c:117:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:117:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:117:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:117:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:117:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:117:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:117:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:117:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:90:23: missed: couldn't vectorize loop
Dense.c:90:23: missed: not vectorized: unsupported outerloop form.
Dense.c:108:11: missed: couldn't vectorize loop
Dense.c:112:17: missed: not vectorized: no vectype for stmt: _123 = _130->errors;
 scalar_type: float *
Dense.c:90:23: missed: couldn't vectorize loop
Dense.c:90:23: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:108:11: missed: couldn't vectorize loop
Dense.c:112:17: missed: not vectorized: no vectype for stmt: _34 = _33->errors;
 scalar_type: float *
Dense.c:95:16: missed: couldn't vectorize loop
Dense.c:95:16: missed: not vectorized: unsupported control flow in loop.
Dense.c:83:13: note: vectorized 0 loops in function.
Dense.c:86:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:97:16: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:97:16: note: SLPing BB part
Dense.c:97:16: note: Costing subgraph: 
Dense.c:97:16: note: node 0x7b0d408 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:97:16: note: op template: _16 = (sizetype) j_97;
Dense.c:97:16: note: 	stmt 0 _16 = (sizetype) j_97;
Dense.c:97:16: note: 	stmt 1 _141 = (sizetype) wi_100;
Dense.c:97:16: note: 	children 0x7b0d508
Dense.c:97:16: note: node (external) 0x7b0d508 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:97:16: note: 	{ j_97, wi_100 }
Dense.c:97:16: note: Cost model analysis: 
Dense.c:97:16: note: Scalar 1 and vector 2 loop part do not match up, skipping scalar part
Dense.c:97:16: note: Cost model analysis for part in loop 2:
  Vector cost: 36
  Scalar cost: 8
Dense.c:97:16: missed: not vectorized: vectorization is not profitable.
Dense.c:97:16: note: ***** The result for vector mode V32QI would be the same
Dense.c:97:16: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:97:16: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:97:16: note: SLPing BB part
Dense.c:97:16: note: Costing subgraph: 
Dense.c:97:16: note: node 0x7b0d308 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:97:16: note: op template: _16 = (sizetype) j_97;
Dense.c:97:16: note: 	stmt 0 _16 = (sizetype) j_97;
Dense.c:97:16: note: 	stmt 1 _141 = (sizetype) wi_100;
Dense.c:97:16: note: 	children 0x7b0d708
Dense.c:97:16: note: node (external) 0x7b0d708 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:97:16: note: 	{ j_97, wi_100 }
Dense.c:97:16: note: Cost model analysis: 
Dense.c:97:16: note: Scalar 1 and vector 2 loop part do not match up, skipping scalar part
Dense.c:97:16: note: Cost model analysis for part in loop 2:
  Vector cost: 36
  Scalar cost: 8
Dense.c:97:16: missed: not vectorized: vectorization is not profitable.
Dense.c:97:16: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:112:29: note: ***** Analysis failed with vector mode V8QI
Dense.c:112:29: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:90:50: note: ***** Analysis failed with vector mode V4QI
Dense.c:62:15: missed: couldn't vectorize loop
Dense.c:62:15: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:75:11: optimized: loop vectorized using 32 byte vectors
Dense.c:75:11: optimized: loop vectorized using 16 byte vectors
Dense.c:66:11: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:881:10: missed: not vectorized: no vectype for stmt: _53 = MEM[(__m256 * {ref-all})w_ptr_82];
 scalar_type: __m256
Dense.c:54:13: note: vectorized 1 loops in function.
Dense.c:57:2: missed: statement clobbers memory: memcpy (_4, dense$baiases_63, _3);
Dense.c:79:2: missed: statement clobbers memory: _27 (pretmp_163, _68, 0);
Dense.c:54:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:54:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:160:107: note: ***** Analysis failed with vector mode V8SI
Dense.c:160:107: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
Dense.c:161:85: note: ***** Analysis failed with vector mode V8SI
Dense.c:161:85: note: ***** The result for vector mode V32QI would be the same
Dense.c:161:85: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:161:85: note: ***** Analysis failed with vector mode V16QI
Dense.c:161:85: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:161:85: note: ***** Analysis failed with vector mode V8QI
Dense.c:161:85: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:161:85: note: ***** Analysis failed with vector mode V4QI
Dense.c:162:107: note: ***** Analysis failed with vector mode V8SI
Dense.c:162:107: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
Dense.c:178:19: missed: couldn't vectorize loop
Dense.c:178:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:171:13: note: vectorized 0 loops in function.
Dense.c:179:38: missed: statement clobbers memory: _5 = rand ();
Dense.c:179:72: missed: statement clobbers memory: _65 = sqrt (_12);
Dense.c:192:19: missed: statement clobbers memory: _24 = PULSE_GetActivationFunctionPtr (_23);
Dense.c:208:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:209:4: missed: statement clobbers memory: exit (1);
Dense.c:213:9: note: ***** Analysis succeeded with vector mode V8SI
Dense.c:213:9: note: SLPing BB part
Dense.c:213:9: note: Costing subgraph: 
Dense.c:213:9: note: node 0x7a14e58 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:213:9: note: op template: <retval>.n_inputs = args$n_inputs_41;
Dense.c:213:9: note: 	stmt 0 <retval>.n_inputs = args$n_inputs_41;
Dense.c:213:9: note: 	stmt 1 <retval>.n_outputs = args$n_outputs_42;
Dense.c:213:9: note: 	children 0x7a14ed8
Dense.c:213:9: note: node 0x7a14ed8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:213:9: note: op template: args$n_inputs_41 = args.n_inputs;
Dense.c:213:9: note: 	stmt 0 args$n_inputs_41 = args.n_inputs;
Dense.c:213:9: note: 	stmt 1 args$n_outputs_42 = args.n_outputs;
Dense.c:213:9: note: Cost model analysis: 
Dense.c:213:9: note: Cost model analysis for part in loop 0:
  Vector cost: 40
  Scalar cost: 56
Dense.c:213:9: note: Costing subgraph: 
Dense.c:213:9: note: node 0x7a14fd8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: op template: <retval>.inputs = IO_31(D);
Dense.c:213:9: note: 	stmt 0 <retval>.inputs = IO_31(D);
Dense.c:213:9: note: 	stmt 1 <retval>.outputs = _22;
Dense.c:213:9: note: 	children 0x7a15058
Dense.c:213:9: note: node (external) 0x7a15058 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: 	{ IO_31(D), _22 }
Dense.c:213:9: note: Cost model analysis: 
Dense.c:213:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:213:9: note: Costing subgraph: 
Dense.c:213:9: note: node 0x7a15158 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:213:9: note: op template: <retval>.feed = layer$feed_30;
Dense.c:213:9: note: 	stmt 0 <retval>.feed = layer$feed_30;
Dense.c:213:9: note: 	stmt 1 <retval>.back = layer$back_39;
Dense.c:213:9: note: 	stmt 2 <retval>.fix = layer$fix_40;
Dense.c:213:9: note: 	stmt 3 <retval>.activate = _25;
Dense.c:213:9: note: 	children 0x7a15258
Dense.c:213:9: note: node (external) 0x7a15258 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:213:9: note: 	{ layer$feed_30, layer$back_39, layer$fix_40, _25 }
Dense.c:213:9: note: Cost model analysis: 
Dense.c:213:9: note: Cost model analysis for part in loop 0:
  Vector cost: 64
  Scalar cost: 64
Dense.c:213:9: note: Costing subgraph: 
Dense.c:213:9: note: node 0x7a15358 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: op template: <retval>.parent = 0B;
Dense.c:213:9: note: 	stmt 0 <retval>.parent = 0B;
Dense.c:213:9: note: 	stmt 1 <retval>.child = 0B;
Dense.c:213:9: note: 	children 0x7a153d8
Dense.c:213:9: note: node (constant) 0x7a153d8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: 	{ 0B, 0B }
Dense.c:213:9: note: Cost model analysis: 
Dense.c:213:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:213:9: note: Costing subgraph: 
Dense.c:213:9: note: node 0x7a15458 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_29(D);
Dense.c:213:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_29(D);
Dense.c:213:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&<retval> + 96B] = _4;
Dense.c:213:9: note: 	children 0x7a154d8
Dense.c:213:9: note: node (external) 0x7a154d8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: 	{ MODEL_29(D), _4 }
Dense.c:213:9: note: Cost model analysis: 
Dense.c:213:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:213:9: note: Basic block will be vectorized using SLP
Dense.c:213:9: optimized: basic block part vectorized using 8 byte vectors
Dense.c:213:9: note: Vectorizing SLP tree:
Dense.c:213:9: note: node 0x7a14e58 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:213:9: note: op template: <retval>.n_inputs = args$n_inputs_41;
Dense.c:213:9: note: 	stmt 0 <retval>.n_inputs = args$n_inputs_41;
Dense.c:213:9: note: 	stmt 1 <retval>.n_outputs = args$n_outputs_42;
Dense.c:213:9: note: 	children 0x7a14ed8
Dense.c:213:9: note: node 0x7a14ed8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:213:9: note: op template: args$n_inputs_41 = args.n_inputs;
Dense.c:213:9: note: 	stmt 0 args$n_inputs_41 = args.n_inputs;
Dense.c:213:9: note: 	stmt 1 args$n_outputs_42 = args.n_outputs;
Dense.c:213:9: note: ------>vectorizing SLP node starting from: args$n_inputs_41 = args.n_inputs;
Dense.c:213:9: note: transform load. ncopies = 1
Dense.c:213:9: note: conflicting alias set types.
Dense.c:213:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: args.n_inputs
Dense.c:213:9: note: created &args
Dense.c:213:9: note: add new stmt: vect_args_n_inputs_41.500_26 = MEM <vector(2) unsigned int> [(void *)&args];
Dense.c:213:9: note: extracting lane for live stmt args$n_inputs_41 = args.n_inputs;
Dense.c:213:9: note: extracting lane for live stmt args$n_outputs_42 = args.n_outputs;
Dense.c:213:9: note: ------>vectorizing SLP node starting from: <retval>.n_inputs = args$n_inputs_41;
Dense.c:213:9: note: vect_is_simple_use: operand args.n_inputs, type of def: internal
Dense.c:213:9: note: vect_is_simple_use: operand args.n_outputs, type of def: internal
Dense.c:213:9: note: transform store. ncopies = 1
Dense.c:213:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: <retval>.n_inputs
Dense.c:213:9: note: created &<retval>.n_inputs
Dense.c:213:9: note: add new stmt: MEM <vector(2) unsigned int> [(unsigned int *)&<retval> + 128B] = vect_args_n_inputs_41.500_26;
Dense.c:213:9: note: vectorizing stmts using SLP.
Dense.c:213:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:213:9: note: Vectorizing SLP tree:
Dense.c:213:9: note: node 0x7a14fd8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: op template: <retval>.inputs = IO_31(D);
Dense.c:213:9: note: 	stmt 0 <retval>.inputs = IO_31(D);
Dense.c:213:9: note: 	stmt 1 <retval>.outputs = _22;
Dense.c:213:9: note: 	children 0x7a15058
Dense.c:213:9: note: node (external) 0x7a15058 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: 	{ IO_31(D), _22 }
Dense.c:213:9: note: ------>vectorizing SLP node starting from: <retval>.inputs = IO_31(D);
Dense.c:213:9: note: vect_is_simple_use: operand IO_31(D) + _21, type of def: internal
Dense.c:213:9: note: transform store. ncopies = 1
Dense.c:213:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.inputs
Dense.c:213:9: note: created &<retval>
Dense.c:213:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval>] = _78;
Dense.c:213:9: note: vectorizing stmts using SLP.
Dense.c:213:9: optimized: basic block part vectorized using 32 byte vectors
Dense.c:213:9: note: Vectorizing SLP tree:
Dense.c:213:9: note: node 0x7a15158 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:213:9: note: op template: <retval>.feed = layer$feed_30;
Dense.c:213:9: note: 	stmt 0 <retval>.feed = layer$feed_30;
Dense.c:213:9: note: 	stmt 1 <retval>.back = layer$back_39;
Dense.c:213:9: note: 	stmt 2 <retval>.fix = layer$fix_40;
Dense.c:213:9: note: 	stmt 3 <retval>.activate = _25;
Dense.c:213:9: note: 	children 0x7a15258
Dense.c:213:9: note: node (external) 0x7a15258 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:213:9: note: 	{ layer$feed_30, layer$back_39, layer$fix_40, _25 }
Dense.c:213:9: note: ------>vectorizing SLP node starting from: <retval>.feed = layer$feed_30;
Dense.c:213:9: note: vect_is_simple_use: operand layer$back_39 = PHI <layer$back_45(D)(11), _BackDense(9), _SIMD_BackDense(15)>, type of def: internal
Dense.c:213:9: note: vect_is_simple_use: operand layer$fix_40 = PHI <layer$fix_46(D)(11), _FixDense(9), _SIMD_FixDense(15)>, type of def: internal
Dense.c:213:9: note: vect_is_simple_use: operand (void (*<T91a>) (float *, unsigned int, char)) _24, type of def: internal
Dense.c:213:9: note: conflicting alias set types.
Dense.c:213:9: note: transform store. ncopies = 1
Dense.c:213:9: note: create vector_type-pointer variable to type: vector(4) long unsigned int  vectorizing a pointer ref: <retval>.feed
Dense.c:213:9: note: created &<retval>.feed
Dense.c:213:9: note: add new stmt: MEM <vector(4) long unsigned int> [(void *)&<retval> + 32B] = _84;
Dense.c:213:9: note: vectorizing stmts using SLP.
Dense.c:213:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:213:9: note: Vectorizing SLP tree:
Dense.c:213:9: note: node 0x7a15358 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: op template: <retval>.parent = 0B;
Dense.c:213:9: note: 	stmt 0 <retval>.parent = 0B;
Dense.c:213:9: note: 	stmt 1 <retval>.child = 0B;
Dense.c:213:9: note: 	children 0x7a153d8
Dense.c:213:9: note: node (constant) 0x7a153d8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: 	{ 0B, 0B }
Dense.c:213:9: note: ------>vectorizing SLP node starting from: <retval>.parent = 0B;
Dense.c:213:9: note: vect_is_simple_use: operand 0B, type of def: constant
Dense.c:213:9: note: transform store. ncopies = 1
Dense.c:213:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.parent
Dense.c:213:9: note: created &<retval>.parent
Dense.c:213:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer * *)&<retval> + 72B] = { 0, 0 };
Dense.c:213:9: note: vectorizing stmts using SLP.
Dense.c:213:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:213:9: note: Vectorizing SLP tree:
Dense.c:213:9: note: node 0x7a15458 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_29(D);
Dense.c:213:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_29(D);
Dense.c:213:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&<retval> + 96B] = _4;
Dense.c:213:9: note: 	children 0x7a154d8
Dense.c:213:9: note: node (external) 0x7a154d8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: 	{ MODEL_29(D), _4 }
Dense.c:213:9: note: ------>vectorizing SLP node starting from: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_29(D);
Dense.c:213:9: note: vect_is_simple_use: operand MODEL_29(D) + _3, type of def: internal
Dense.c:213:9: note: transform store. ncopies = 1
Dense.c:213:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B]
Dense.c:213:9: note: created &MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B]
Dense.c:213:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer *)&<retval> + 88B] = _89;
Dense.c:213:9: note: vectorizing stmts using SLP.
Dense.c:213:9: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _28 (pretmp_91, _51, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:19: missed: not vectorized: no vectype for stmt: _19 = *_18;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:167:56: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:167:56: note: SLPing BB part
Dense.c:167:56: note: Costing subgraph: 
Dense.c:167:56: note: node 0x8bd5b38 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:167:56: note: op template: _4 = (long unsigned int) _3;
Dense.c:167:56: note: 	stmt 0 _4 = (long unsigned int) _3;
Dense.c:167:56: note: 	stmt 1 _7 = (sizetype) _2;
Dense.c:167:56: note: 	children 0x8bd5c38
Dense.c:167:56: note: node (external) 0x8bd5c38 (max_nunits=1, refcnt=1) vector(2) unsigned int
Dense.c:167:56: note: 	{ _3, _2 }
Dense.c:167:56: note: Cost model analysis: 
Dense.c:167:56: note: Cost model analysis for part in loop 0:
  Vector cost: 30
  Scalar cost: 8
Dense.c:167:56: missed: not vectorized: vectorization is not profitable.
Dense.c:167:56: note: ***** The result for vector mode V32QI would be the same
Dense.c:167:56: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:167:56: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:167:56: note: SLPing BB part
Dense.c:167:56: note: Costing subgraph: 
Dense.c:167:56: note: node 0x8bd5b38 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:167:56: note: op template: _4 = (long unsigned int) _3;
Dense.c:167:56: note: 	stmt 0 _4 = (long unsigned int) _3;
Dense.c:167:56: note: 	stmt 1 _7 = (sizetype) _2;
Dense.c:167:56: note: 	children 0x8bd5bb8
Dense.c:167:56: note: node (external) 0x8bd5bb8 (max_nunits=1, refcnt=1) vector(2) unsigned int
Dense.c:167:56: note: 	{ _3, _2 }
Dense.c:167:56: note: Cost model analysis: 
Dense.c:167:56: note: Cost model analysis for part in loop 0:
  Vector cost: 30
  Scalar cost: 8
Dense.c:167:56: missed: not vectorized: vectorization is not profitable.
Dense.c:167:56: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:167:56: note: ***** Analysis failed with vector mode V8QI
Dense.c:167:56: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:167:56: note: ***** Analysis failed with vector mode V4QI
Dense.c:151:10: optimized: loop vectorized using 32 byte vectors
Dense.c:151:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:151:10: optimized: loop vectorized using 16 byte vectors
Dense.c:142:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
Dense.c:135:10: optimized: loop vectorized using 32 byte vectors
Dense.c:135:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:135:10: optimized: loop vectorized using 16 byte vectors
Dense.c:126:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
Dense.c:117:13: note: vectorized 2 loops in function.
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_28, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
Dense.c:117:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:117:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:117:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:117:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:117:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:117:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:117:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:117:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:90:23: missed: couldn't vectorize loop
Dense.c:92:31: missed: not vectorized: no vectype for stmt: _121 = this_41(D)->outputs;
 scalar_type: float *
Dense.c:90:23: missed: couldn't vectorize loop
Dense.c:90:23: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:95:16: missed: couldn't vectorize loop
Dense.c:95:16: missed: not vectorized: unsupported control flow in loop.
Dense.c:83:13: note: vectorized 0 loops in function.
Dense.c:86:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:97:16: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:97:16: note: SLPing BB part
Dense.c:97:16: note: Costing subgraph: 
Dense.c:97:16: note: node 0x8bd5b38 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:97:16: note: op template: _16 = (sizetype) j_75;
Dense.c:97:16: note: 	stmt 0 _16 = (sizetype) j_75;
Dense.c:97:16: note: 	stmt 1 _113 = (sizetype) wi_76;
Dense.c:97:16: note: 	children 0x8bd5c38
Dense.c:97:16: note: node (external) 0x8bd5c38 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:97:16: note: 	{ j_75, wi_76 }
Dense.c:97:16: note: Cost model analysis: 
Dense.c:97:16: note: Scalar 1 and vector 2 loop part do not match up, skipping scalar part
Dense.c:97:16: note: Cost model analysis for part in loop 2:
  Vector cost: 36
  Scalar cost: 8
Dense.c:97:16: missed: not vectorized: vectorization is not profitable.
Dense.c:97:16: note: ***** The result for vector mode V32QI would be the same
Dense.c:97:16: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:97:16: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:97:16: note: SLPing BB part
Dense.c:97:16: note: Costing subgraph: 
Dense.c:97:16: note: node 0x8bd5b38 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:97:16: note: op template: _16 = (sizetype) j_75;
Dense.c:97:16: note: 	stmt 0 _16 = (sizetype) j_75;
Dense.c:97:16: note: 	stmt 1 _113 = (sizetype) wi_76;
Dense.c:97:16: note: 	children 0x8bd5bb8
Dense.c:97:16: note: node (external) 0x8bd5bb8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:97:16: note: 	{ j_75, wi_76 }
Dense.c:97:16: note: Cost model analysis: 
Dense.c:97:16: note: Scalar 1 and vector 2 loop part do not match up, skipping scalar part
Dense.c:97:16: note: Cost model analysis for part in loop 2:
  Vector cost: 36
  Scalar cost: 8
Dense.c:97:16: missed: not vectorized: vectorization is not profitable.
Dense.c:97:16: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:90:50: note: ***** Analysis failed with vector mode V8QI
Dense.c:90:50: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:90:50: note: ***** Analysis failed with vector mode V4QI
Dense.c:62:15: missed: couldn't vectorize loop
Dense.c:62:15: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:75:11: optimized: loop vectorized using 32 byte vectors
Dense.c:75:11: optimized: loop vectorized using 16 byte vectors
Dense.c:66:11: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:881:10: missed: not vectorized: no vectype for stmt: _53 = MEM[(__m256 * {ref-all})w_ptr_82];
 scalar_type: __m256
Dense.c:54:13: note: vectorized 1 loops in function.
Dense.c:57:2: missed: statement clobbers memory: memcpy (_4, dense$baiases_63, _3);
Dense.c:79:2: missed: statement clobbers memory: _27 (pretmp_163, _68, 0);
Dense.c:54:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:54:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:160:107: note: ***** Analysis failed with vector mode V8SI
Dense.c:160:107: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
Dense.c:161:85: note: ***** Analysis failed with vector mode V8SI
Dense.c:161:85: note: ***** The result for vector mode V32QI would be the same
Dense.c:161:85: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:161:85: note: ***** Analysis failed with vector mode V16QI
Dense.c:161:85: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:161:85: note: ***** Analysis failed with vector mode V8QI
Dense.c:161:85: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:161:85: note: ***** Analysis failed with vector mode V4QI
Dense.c:162:107: note: ***** Analysis failed with vector mode V8SI
Dense.c:162:107: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
Dense.c:178:19: missed: couldn't vectorize loop
Dense.c:178:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:171:13: note: vectorized 0 loops in function.
Dense.c:179:38: missed: statement clobbers memory: _5 = rand ();
Dense.c:179:72: missed: statement clobbers memory: _65 = sqrt (_12);
Dense.c:192:19: missed: statement clobbers memory: _24 = PULSE_GetActivationFunctionPtr (_23);
Dense.c:208:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:209:4: missed: statement clobbers memory: exit (1);
Dense.c:213:9: note: ***** Analysis succeeded with vector mode V8SI
Dense.c:213:9: note: SLPing BB part
Dense.c:213:9: note: Costing subgraph: 
Dense.c:213:9: note: node 0x8b75248 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:213:9: note: op template: <retval>.n_inputs = args$n_inputs_41;
Dense.c:213:9: note: 	stmt 0 <retval>.n_inputs = args$n_inputs_41;
Dense.c:213:9: note: 	stmt 1 <retval>.n_outputs = args$n_outputs_42;
Dense.c:213:9: note: 	children 0x8b752c8
Dense.c:213:9: note: node 0x8b752c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:213:9: note: op template: args$n_inputs_41 = args.n_inputs;
Dense.c:213:9: note: 	stmt 0 args$n_inputs_41 = args.n_inputs;
Dense.c:213:9: note: 	stmt 1 args$n_outputs_42 = args.n_outputs;
Dense.c:213:9: note: Cost model analysis: 
Dense.c:213:9: note: Cost model analysis for part in loop 0:
  Vector cost: 40
  Scalar cost: 56
Dense.c:213:9: note: Costing subgraph: 
Dense.c:213:9: note: node 0x8b753c8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: op template: <retval>.inputs = IO_31(D);
Dense.c:213:9: note: 	stmt 0 <retval>.inputs = IO_31(D);
Dense.c:213:9: note: 	stmt 1 <retval>.outputs = _22;
Dense.c:213:9: note: 	children 0x8b75448
Dense.c:213:9: note: node (external) 0x8b75448 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: 	{ IO_31(D), _22 }
Dense.c:213:9: note: Cost model analysis: 
Dense.c:213:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:213:9: note: Costing subgraph: 
Dense.c:213:9: note: node 0x8b75548 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:213:9: note: op template: <retval>.feed = layer$feed_30;
Dense.c:213:9: note: 	stmt 0 <retval>.feed = layer$feed_30;
Dense.c:213:9: note: 	stmt 1 <retval>.back = layer$back_39;
Dense.c:213:9: note: 	stmt 2 <retval>.fix = layer$fix_40;
Dense.c:213:9: note: 	stmt 3 <retval>.activate = _25;
Dense.c:213:9: note: 	children 0x8b75648
Dense.c:213:9: note: node (external) 0x8b75648 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:213:9: note: 	{ layer$feed_30, layer$back_39, layer$fix_40, _25 }
Dense.c:213:9: note: Cost model analysis: 
Dense.c:213:9: note: Cost model analysis for part in loop 0:
  Vector cost: 64
  Scalar cost: 64
Dense.c:213:9: note: Costing subgraph: 
Dense.c:213:9: note: node 0x8b75748 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: op template: <retval>.parent = 0B;
Dense.c:213:9: note: 	stmt 0 <retval>.parent = 0B;
Dense.c:213:9: note: 	stmt 1 <retval>.child = 0B;
Dense.c:213:9: note: 	children 0x8b757c8
Dense.c:213:9: note: node (constant) 0x8b757c8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: 	{ 0B, 0B }
Dense.c:213:9: note: Cost model analysis: 
Dense.c:213:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:213:9: note: Costing subgraph: 
Dense.c:213:9: note: node 0x8b75848 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_29(D);
Dense.c:213:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_29(D);
Dense.c:213:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&<retval> + 96B] = _4;
Dense.c:213:9: note: 	children 0x8b758c8
Dense.c:213:9: note: node (external) 0x8b758c8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: 	{ MODEL_29(D), _4 }
Dense.c:213:9: note: Cost model analysis: 
Dense.c:213:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:213:9: note: Basic block will be vectorized using SLP
Dense.c:213:9: optimized: basic block part vectorized using 8 byte vectors
Dense.c:213:9: note: Vectorizing SLP tree:
Dense.c:213:9: note: node 0x8b75248 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:213:9: note: op template: <retval>.n_inputs = args$n_inputs_41;
Dense.c:213:9: note: 	stmt 0 <retval>.n_inputs = args$n_inputs_41;
Dense.c:213:9: note: 	stmt 1 <retval>.n_outputs = args$n_outputs_42;
Dense.c:213:9: note: 	children 0x8b752c8
Dense.c:213:9: note: node 0x8b752c8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:213:9: note: op template: args$n_inputs_41 = args.n_inputs;
Dense.c:213:9: note: 	stmt 0 args$n_inputs_41 = args.n_inputs;
Dense.c:213:9: note: 	stmt 1 args$n_outputs_42 = args.n_outputs;
Dense.c:213:9: note: ------>vectorizing SLP node starting from: args$n_inputs_41 = args.n_inputs;
Dense.c:213:9: note: transform load. ncopies = 1
Dense.c:213:9: note: conflicting alias set types.
Dense.c:213:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: args.n_inputs
Dense.c:213:9: note: created &args
Dense.c:213:9: note: add new stmt: vect_args_n_inputs_41.484_26 = MEM <vector(2) unsigned int> [(void *)&args];
Dense.c:213:9: note: extracting lane for live stmt args$n_inputs_41 = args.n_inputs;
Dense.c:213:9: note: extracting lane for live stmt args$n_outputs_42 = args.n_outputs;
Dense.c:213:9: note: ------>vectorizing SLP node starting from: <retval>.n_inputs = args$n_inputs_41;
Dense.c:213:9: note: vect_is_simple_use: operand args.n_inputs, type of def: internal
Dense.c:213:9: note: vect_is_simple_use: operand args.n_outputs, type of def: internal
Dense.c:213:9: note: transform store. ncopies = 1
Dense.c:213:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: <retval>.n_inputs
Dense.c:213:9: note: created &<retval>.n_inputs
Dense.c:213:9: note: add new stmt: MEM <vector(2) unsigned int> [(unsigned int *)&<retval> + 128B] = vect_args_n_inputs_41.484_26;
Dense.c:213:9: note: vectorizing stmts using SLP.
Dense.c:213:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:213:9: note: Vectorizing SLP tree:
Dense.c:213:9: note: node 0x8b753c8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: op template: <retval>.inputs = IO_31(D);
Dense.c:213:9: note: 	stmt 0 <retval>.inputs = IO_31(D);
Dense.c:213:9: note: 	stmt 1 <retval>.outputs = _22;
Dense.c:213:9: note: 	children 0x8b75448
Dense.c:213:9: note: node (external) 0x8b75448 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: 	{ IO_31(D), _22 }
Dense.c:213:9: note: ------>vectorizing SLP node starting from: <retval>.inputs = IO_31(D);
Dense.c:213:9: note: vect_is_simple_use: operand IO_31(D) + _21, type of def: internal
Dense.c:213:9: note: transform store. ncopies = 1
Dense.c:213:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.inputs
Dense.c:213:9: note: created &<retval>
Dense.c:213:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval>] = _78;
Dense.c:213:9: note: vectorizing stmts using SLP.
Dense.c:213:9: optimized: basic block part vectorized using 32 byte vectors
Dense.c:213:9: note: Vectorizing SLP tree:
Dense.c:213:9: note: node 0x8b75548 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:213:9: note: op template: <retval>.feed = layer$feed_30;
Dense.c:213:9: note: 	stmt 0 <retval>.feed = layer$feed_30;
Dense.c:213:9: note: 	stmt 1 <retval>.back = layer$back_39;
Dense.c:213:9: note: 	stmt 2 <retval>.fix = layer$fix_40;
Dense.c:213:9: note: 	stmt 3 <retval>.activate = _25;
Dense.c:213:9: note: 	children 0x8b75648
Dense.c:213:9: note: node (external) 0x8b75648 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:213:9: note: 	{ layer$feed_30, layer$back_39, layer$fix_40, _25 }
Dense.c:213:9: note: ------>vectorizing SLP node starting from: <retval>.feed = layer$feed_30;
Dense.c:213:9: note: vect_is_simple_use: operand layer$back_39 = PHI <layer$back_45(D)(11), _BackDense(9), _SIMD_BackDense(15)>, type of def: internal
Dense.c:213:9: note: vect_is_simple_use: operand layer$fix_40 = PHI <layer$fix_46(D)(11), _FixDense(9), _SIMD_FixDense(15)>, type of def: internal
Dense.c:213:9: note: vect_is_simple_use: operand (void (*<T91a>) (float *, unsigned int, char)) _24, type of def: internal
Dense.c:213:9: note: conflicting alias set types.
Dense.c:213:9: note: transform store. ncopies = 1
Dense.c:213:9: note: create vector_type-pointer variable to type: vector(4) long unsigned int  vectorizing a pointer ref: <retval>.feed
Dense.c:213:9: note: created &<retval>.feed
Dense.c:213:9: note: add new stmt: MEM <vector(4) long unsigned int> [(void *)&<retval> + 32B] = _84;
Dense.c:213:9: note: vectorizing stmts using SLP.
Dense.c:213:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:213:9: note: Vectorizing SLP tree:
Dense.c:213:9: note: node 0x8b75748 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: op template: <retval>.parent = 0B;
Dense.c:213:9: note: 	stmt 0 <retval>.parent = 0B;
Dense.c:213:9: note: 	stmt 1 <retval>.child = 0B;
Dense.c:213:9: note: 	children 0x8b757c8
Dense.c:213:9: note: node (constant) 0x8b757c8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: 	{ 0B, 0B }
Dense.c:213:9: note: ------>vectorizing SLP node starting from: <retval>.parent = 0B;
Dense.c:213:9: note: vect_is_simple_use: operand 0B, type of def: constant
Dense.c:213:9: note: transform store. ncopies = 1
Dense.c:213:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.parent
Dense.c:213:9: note: created &<retval>.parent
Dense.c:213:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer * *)&<retval> + 72B] = { 0, 0 };
Dense.c:213:9: note: vectorizing stmts using SLP.
Dense.c:213:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:213:9: note: Vectorizing SLP tree:
Dense.c:213:9: note: node 0x8b75848 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_29(D);
Dense.c:213:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_29(D);
Dense.c:213:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&<retval> + 96B] = _4;
Dense.c:213:9: note: 	children 0x8b758c8
Dense.c:213:9: note: node (external) 0x8b758c8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: 	{ MODEL_29(D), _4 }
Dense.c:213:9: note: ------>vectorizing SLP node starting from: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_29(D);
Dense.c:213:9: note: vect_is_simple_use: operand MODEL_29(D) + _3, type of def: internal
Dense.c:213:9: note: transform store. ncopies = 1
Dense.c:213:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B]
Dense.c:213:9: note: created &MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B]
Dense.c:213:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer *)&<retval> + 88B] = _89;
Dense.c:213:9: note: vectorizing stmts using SLP.
Dense.c:213:9: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _28 (pretmp_91, _51, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:19: missed: not vectorized: no vectype for stmt: _19 = *_18;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:167:56: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:167:56: note: SLPing BB part
Dense.c:167:56: note: Costing subgraph: 
Dense.c:167:56: note: node 0x7634e98 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:167:56: note: op template: _4 = (long unsigned int) _3;
Dense.c:167:56: note: 	stmt 0 _4 = (long unsigned int) _3;
Dense.c:167:56: note: 	stmt 1 _7 = (sizetype) _2;
Dense.c:167:56: note: 	children 0x7634f98
Dense.c:167:56: note: node (external) 0x7634f98 (max_nunits=1, refcnt=1) vector(2) unsigned int
Dense.c:167:56: note: 	{ _3, _2 }
Dense.c:167:56: note: Cost model analysis: 
Dense.c:167:56: note: Cost model analysis for part in loop 0:
  Vector cost: 30
  Scalar cost: 8
Dense.c:167:56: missed: not vectorized: vectorization is not profitable.
Dense.c:167:56: note: ***** The result for vector mode V32QI would be the same
Dense.c:167:56: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:167:56: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:167:56: note: SLPing BB part
Dense.c:167:56: note: Costing subgraph: 
Dense.c:167:56: note: node 0x7634e98 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:167:56: note: op template: _4 = (long unsigned int) _3;
Dense.c:167:56: note: 	stmt 0 _4 = (long unsigned int) _3;
Dense.c:167:56: note: 	stmt 1 _7 = (sizetype) _2;
Dense.c:167:56: note: 	children 0x7634f18
Dense.c:167:56: note: node (external) 0x7634f18 (max_nunits=1, refcnt=1) vector(2) unsigned int
Dense.c:167:56: note: 	{ _3, _2 }
Dense.c:167:56: note: Cost model analysis: 
Dense.c:167:56: note: Cost model analysis for part in loop 0:
  Vector cost: 30
  Scalar cost: 8
Dense.c:167:56: missed: not vectorized: vectorization is not profitable.
Dense.c:167:56: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:167:56: note: ***** Analysis failed with vector mode V8QI
Dense.c:167:56: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:167:56: note: ***** Analysis failed with vector mode V4QI
Dense.c:151:10: optimized: loop vectorized using 32 byte vectors
Dense.c:151:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:151:10: optimized: loop vectorized using 16 byte vectors
Dense.c:142:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
Dense.c:135:10: optimized: loop vectorized using 32 byte vectors
Dense.c:135:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:135:10: optimized: loop vectorized using 16 byte vectors
Dense.c:126:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
Dense.c:117:13: note: vectorized 2 loops in function.
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_28, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
Dense.c:117:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:117:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:117:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:117:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:117:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:117:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:117:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:117:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:90:23: missed: couldn't vectorize loop
Dense.c:92:31: missed: not vectorized: no vectype for stmt: _121 = this_41(D)->outputs;
 scalar_type: float *
Dense.c:90:23: missed: couldn't vectorize loop
Dense.c:90:23: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:95:16: missed: couldn't vectorize loop
Dense.c:95:16: missed: not vectorized: unsupported control flow in loop.
Dense.c:83:13: note: vectorized 0 loops in function.
Dense.c:86:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:97:16: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:97:16: note: SLPing BB part
Dense.c:97:16: note: Costing subgraph: 
Dense.c:97:16: note: node 0x7634e98 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:97:16: note: op template: _16 = (sizetype) j_75;
Dense.c:97:16: note: 	stmt 0 _16 = (sizetype) j_75;
Dense.c:97:16: note: 	stmt 1 _113 = (sizetype) wi_76;
Dense.c:97:16: note: 	children 0x7634f98
Dense.c:97:16: note: node (external) 0x7634f98 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:97:16: note: 	{ j_75, wi_76 }
Dense.c:97:16: note: Cost model analysis: 
Dense.c:97:16: note: Scalar 1 and vector 2 loop part do not match up, skipping scalar part
Dense.c:97:16: note: Cost model analysis for part in loop 2:
  Vector cost: 36
  Scalar cost: 8
Dense.c:97:16: missed: not vectorized: vectorization is not profitable.
Dense.c:97:16: note: ***** The result for vector mode V32QI would be the same
Dense.c:97:16: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:97:16: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:97:16: note: SLPing BB part
Dense.c:97:16: note: Costing subgraph: 
Dense.c:97:16: note: node 0x7634e98 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:97:16: note: op template: _16 = (sizetype) j_75;
Dense.c:97:16: note: 	stmt 0 _16 = (sizetype) j_75;
Dense.c:97:16: note: 	stmt 1 _113 = (sizetype) wi_76;
Dense.c:97:16: note: 	children 0x7634f18
Dense.c:97:16: note: node (external) 0x7634f18 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:97:16: note: 	{ j_75, wi_76 }
Dense.c:97:16: note: Cost model analysis: 
Dense.c:97:16: note: Scalar 1 and vector 2 loop part do not match up, skipping scalar part
Dense.c:97:16: note: Cost model analysis for part in loop 2:
  Vector cost: 36
  Scalar cost: 8
Dense.c:97:16: missed: not vectorized: vectorization is not profitable.
Dense.c:97:16: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:90:50: note: ***** Analysis failed with vector mode V8QI
Dense.c:90:50: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:90:50: note: ***** Analysis failed with vector mode V4QI
Dense.c:62:15: missed: couldn't vectorize loop
Dense.c:62:15: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:75:11: optimized: loop vectorized using 32 byte vectors
Dense.c:75:11: optimized: loop vectorized using 16 byte vectors
Dense.c:66:11: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:881:10: missed: not vectorized: no vectype for stmt: _53 = MEM[(__m256 * {ref-all})w_ptr_82];
 scalar_type: __m256
Dense.c:54:13: note: vectorized 1 loops in function.
Dense.c:57:2: missed: statement clobbers memory: memcpy (_4, dense$baiases_63, _3);
Dense.c:79:2: missed: statement clobbers memory: _27 (pretmp_163, _68, 0);
Dense.c:54:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:54:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:160:107: note: ***** Analysis failed with vector mode V8SI
Dense.c:160:107: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
Dense.c:161:85: note: ***** Analysis failed with vector mode V8SI
Dense.c:161:85: note: ***** The result for vector mode V32QI would be the same
Dense.c:161:85: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:161:85: note: ***** Analysis failed with vector mode V16QI
Dense.c:161:85: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:161:85: note: ***** Analysis failed with vector mode V8QI
Dense.c:161:85: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:161:85: note: ***** Analysis failed with vector mode V4QI
Dense.c:162:107: note: ***** Analysis failed with vector mode V8SI
Dense.c:162:107: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
Dense.c:178:19: missed: couldn't vectorize loop
Dense.c:178:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:171:13: note: vectorized 0 loops in function.
Dense.c:179:38: missed: statement clobbers memory: _5 = rand ();
Dense.c:179:72: missed: statement clobbers memory: _65 = sqrt (_12);
Dense.c:192:19: missed: statement clobbers memory: _24 = PULSE_GetActivationFunctionPtr (_23);
Dense.c:208:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:209:4: missed: statement clobbers memory: exit (1);
Dense.c:213:9: note: ***** Analysis succeeded with vector mode V8SI
Dense.c:213:9: note: SLPing BB part
Dense.c:213:9: note: Costing subgraph: 
Dense.c:213:9: note: node 0x7455508 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:213:9: note: op template: <retval>.n_inputs = args$n_inputs_41;
Dense.c:213:9: note: 	stmt 0 <retval>.n_inputs = args$n_inputs_41;
Dense.c:213:9: note: 	stmt 1 <retval>.n_outputs = args$n_outputs_42;
Dense.c:213:9: note: 	children 0x7455588
Dense.c:213:9: note: node 0x7455588 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:213:9: note: op template: args$n_inputs_41 = args.n_inputs;
Dense.c:213:9: note: 	stmt 0 args$n_inputs_41 = args.n_inputs;
Dense.c:213:9: note: 	stmt 1 args$n_outputs_42 = args.n_outputs;
Dense.c:213:9: note: Cost model analysis: 
Dense.c:213:9: note: Cost model analysis for part in loop 0:
  Vector cost: 40
  Scalar cost: 56
Dense.c:213:9: note: Costing subgraph: 
Dense.c:213:9: note: node 0x7455688 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: op template: <retval>.inputs = IO_31(D);
Dense.c:213:9: note: 	stmt 0 <retval>.inputs = IO_31(D);
Dense.c:213:9: note: 	stmt 1 <retval>.outputs = _22;
Dense.c:213:9: note: 	children 0x7455708
Dense.c:213:9: note: node (external) 0x7455708 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: 	{ IO_31(D), _22 }
Dense.c:213:9: note: Cost model analysis: 
Dense.c:213:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:213:9: note: Costing subgraph: 
Dense.c:213:9: note: node 0x7455808 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:213:9: note: op template: <retval>.feed = layer$feed_30;
Dense.c:213:9: note: 	stmt 0 <retval>.feed = layer$feed_30;
Dense.c:213:9: note: 	stmt 1 <retval>.back = layer$back_39;
Dense.c:213:9: note: 	stmt 2 <retval>.fix = layer$fix_40;
Dense.c:213:9: note: 	stmt 3 <retval>.activate = _25;
Dense.c:213:9: note: 	children 0x7455908
Dense.c:213:9: note: node (external) 0x7455908 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:213:9: note: 	{ layer$feed_30, layer$back_39, layer$fix_40, _25 }
Dense.c:213:9: note: Cost model analysis: 
Dense.c:213:9: note: Cost model analysis for part in loop 0:
  Vector cost: 64
  Scalar cost: 64
Dense.c:213:9: note: Costing subgraph: 
Dense.c:213:9: note: node 0x7455a08 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: op template: <retval>.parent = 0B;
Dense.c:213:9: note: 	stmt 0 <retval>.parent = 0B;
Dense.c:213:9: note: 	stmt 1 <retval>.child = 0B;
Dense.c:213:9: note: 	children 0x7455a88
Dense.c:213:9: note: node (constant) 0x7455a88 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: 	{ 0B, 0B }
Dense.c:213:9: note: Cost model analysis: 
Dense.c:213:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:213:9: note: Costing subgraph: 
Dense.c:213:9: note: node 0x7455b08 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_29(D);
Dense.c:213:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_29(D);
Dense.c:213:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&<retval> + 96B] = _4;
Dense.c:213:9: note: 	children 0x7455b88
Dense.c:213:9: note: node (external) 0x7455b88 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: 	{ MODEL_29(D), _4 }
Dense.c:213:9: note: Cost model analysis: 
Dense.c:213:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:213:9: note: Basic block will be vectorized using SLP
Dense.c:213:9: optimized: basic block part vectorized using 8 byte vectors
Dense.c:213:9: note: Vectorizing SLP tree:
Dense.c:213:9: note: node 0x7455508 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:213:9: note: op template: <retval>.n_inputs = args$n_inputs_41;
Dense.c:213:9: note: 	stmt 0 <retval>.n_inputs = args$n_inputs_41;
Dense.c:213:9: note: 	stmt 1 <retval>.n_outputs = args$n_outputs_42;
Dense.c:213:9: note: 	children 0x7455588
Dense.c:213:9: note: node 0x7455588 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:213:9: note: op template: args$n_inputs_41 = args.n_inputs;
Dense.c:213:9: note: 	stmt 0 args$n_inputs_41 = args.n_inputs;
Dense.c:213:9: note: 	stmt 1 args$n_outputs_42 = args.n_outputs;
Dense.c:213:9: note: ------>vectorizing SLP node starting from: args$n_inputs_41 = args.n_inputs;
Dense.c:213:9: note: transform load. ncopies = 1
Dense.c:213:9: note: conflicting alias set types.
Dense.c:213:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: args.n_inputs
Dense.c:213:9: note: created &args
Dense.c:213:9: note: add new stmt: vect_args_n_inputs_41.484_26 = MEM <vector(2) unsigned int> [(void *)&args];
Dense.c:213:9: note: extracting lane for live stmt args$n_inputs_41 = args.n_inputs;
Dense.c:213:9: note: extracting lane for live stmt args$n_outputs_42 = args.n_outputs;
Dense.c:213:9: note: ------>vectorizing SLP node starting from: <retval>.n_inputs = args$n_inputs_41;
Dense.c:213:9: note: vect_is_simple_use: operand args.n_inputs, type of def: internal
Dense.c:213:9: note: vect_is_simple_use: operand args.n_outputs, type of def: internal
Dense.c:213:9: note: transform store. ncopies = 1
Dense.c:213:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: <retval>.n_inputs
Dense.c:213:9: note: created &<retval>.n_inputs
Dense.c:213:9: note: add new stmt: MEM <vector(2) unsigned int> [(unsigned int *)&<retval> + 128B] = vect_args_n_inputs_41.484_26;
Dense.c:213:9: note: vectorizing stmts using SLP.
Dense.c:213:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:213:9: note: Vectorizing SLP tree:
Dense.c:213:9: note: node 0x7455688 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: op template: <retval>.inputs = IO_31(D);
Dense.c:213:9: note: 	stmt 0 <retval>.inputs = IO_31(D);
Dense.c:213:9: note: 	stmt 1 <retval>.outputs = _22;
Dense.c:213:9: note: 	children 0x7455708
Dense.c:213:9: note: node (external) 0x7455708 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: 	{ IO_31(D), _22 }
Dense.c:213:9: note: ------>vectorizing SLP node starting from: <retval>.inputs = IO_31(D);
Dense.c:213:9: note: vect_is_simple_use: operand IO_31(D) + _21, type of def: internal
Dense.c:213:9: note: transform store. ncopies = 1
Dense.c:213:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.inputs
Dense.c:213:9: note: created &<retval>
Dense.c:213:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval>] = _78;
Dense.c:213:9: note: vectorizing stmts using SLP.
Dense.c:213:9: optimized: basic block part vectorized using 32 byte vectors
Dense.c:213:9: note: Vectorizing SLP tree:
Dense.c:213:9: note: node 0x7455808 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:213:9: note: op template: <retval>.feed = layer$feed_30;
Dense.c:213:9: note: 	stmt 0 <retval>.feed = layer$feed_30;
Dense.c:213:9: note: 	stmt 1 <retval>.back = layer$back_39;
Dense.c:213:9: note: 	stmt 2 <retval>.fix = layer$fix_40;
Dense.c:213:9: note: 	stmt 3 <retval>.activate = _25;
Dense.c:213:9: note: 	children 0x7455908
Dense.c:213:9: note: node (external) 0x7455908 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:213:9: note: 	{ layer$feed_30, layer$back_39, layer$fix_40, _25 }
Dense.c:213:9: note: ------>vectorizing SLP node starting from: <retval>.feed = layer$feed_30;
Dense.c:213:9: note: vect_is_simple_use: operand layer$back_39 = PHI <layer$back_45(D)(11), _BackDense(9), _SIMD_BackDense(15)>, type of def: internal
Dense.c:213:9: note: vect_is_simple_use: operand layer$fix_40 = PHI <layer$fix_46(D)(11), _FixDense(9), _SIMD_FixDense(15)>, type of def: internal
Dense.c:213:9: note: vect_is_simple_use: operand (void (*<T91a>) (float *, unsigned int, char)) _24, type of def: internal
Dense.c:213:9: note: conflicting alias set types.
Dense.c:213:9: note: transform store. ncopies = 1
Dense.c:213:9: note: create vector_type-pointer variable to type: vector(4) long unsigned int  vectorizing a pointer ref: <retval>.feed
Dense.c:213:9: note: created &<retval>.feed
Dense.c:213:9: note: add new stmt: MEM <vector(4) long unsigned int> [(void *)&<retval> + 32B] = _84;
Dense.c:213:9: note: vectorizing stmts using SLP.
Dense.c:213:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:213:9: note: Vectorizing SLP tree:
Dense.c:213:9: note: node 0x7455a08 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: op template: <retval>.parent = 0B;
Dense.c:213:9: note: 	stmt 0 <retval>.parent = 0B;
Dense.c:213:9: note: 	stmt 1 <retval>.child = 0B;
Dense.c:213:9: note: 	children 0x7455a88
Dense.c:213:9: note: node (constant) 0x7455a88 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: 	{ 0B, 0B }
Dense.c:213:9: note: ------>vectorizing SLP node starting from: <retval>.parent = 0B;
Dense.c:213:9: note: vect_is_simple_use: operand 0B, type of def: constant
Dense.c:213:9: note: transform store. ncopies = 1
Dense.c:213:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.parent
Dense.c:213:9: note: created &<retval>.parent
Dense.c:213:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer * *)&<retval> + 72B] = { 0, 0 };
Dense.c:213:9: note: vectorizing stmts using SLP.
Dense.c:213:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:213:9: note: Vectorizing SLP tree:
Dense.c:213:9: note: node 0x7455b08 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_29(D);
Dense.c:213:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_29(D);
Dense.c:213:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&<retval> + 96B] = _4;
Dense.c:213:9: note: 	children 0x7455b88
Dense.c:213:9: note: node (external) 0x7455b88 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: 	{ MODEL_29(D), _4 }
Dense.c:213:9: note: ------>vectorizing SLP node starting from: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_29(D);
Dense.c:213:9: note: vect_is_simple_use: operand MODEL_29(D) + _3, type of def: internal
Dense.c:213:9: note: transform store. ncopies = 1
Dense.c:213:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B]
Dense.c:213:9: note: created &MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B]
Dense.c:213:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer *)&<retval> + 88B] = _89;
Dense.c:213:9: note: vectorizing stmts using SLP.
Dense.c:213:9: note: ***** The result for vector mode V32QI would be the same
Activations.c:17:19: optimized: loop vectorized using 32 byte vectors
Activations.c:17:19: optimized: loop vectorized using 16 byte vectors
Activations.c:17:19: optimized: loop vectorized using 32 byte vectors
Activations.c:17:19: optimized: loop vectorized using 16 byte vectors
Activations.c:14:13: note: vectorized 2 loops in function.
Activations.c:14:13: note: ***** Analysis failed with vector mode V8SF
Activations.c:14:13: note: ***** The result for vector mode V32QI would be the same
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V16QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V16QI
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V8QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V8QI
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V4QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V4QI
Activations.c:28:19: missed: couldn't vectorize loop
Activations.c:28:19: missed: not vectorized: unsupported control flow in loop.
Activations.c:28:19: optimized: loop vectorized using 32 byte vectors
Activations.c:28:19: optimized: loop vectorized using 16 byte vectors
Activations.c:25:13: note: vectorized 1 loops in function.
Activations.c:25:13: note: ***** Analysis failed with vector mode V8SF
Activations.c:25:13: note: ***** The result for vector mode V32QI would be the same
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V16QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V16QI
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V8QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V8QI
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V4QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V4QI
Activations.c:38:1: note: ***** Analysis failed with vector mode VOID
Activations.c:5:19: missed: couldn't vectorize loop
Activations.c:9:21: missed: statement clobbers memory: _36 = expf (_18);
Activations.c:5:19: missed: couldn't vectorize loop
Activations.c:9:21: missed: statement clobbers memory: _12 = expf (_11);
Activations.c:3:13: note: vectorized 0 loops in function.
Activations.c:9:21: missed: statement clobbers memory: _12 = expf (_11);
Activations.c:9:21: missed: statement clobbers memory: _36 = expf (_18);
Activations.c:11:1: note: ***** Analysis failed with vector mode V8SF
Activations.c:11:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SF
Activations.c:44:2: note: ***** Analysis failed with vector mode VOID
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:4:18: note: vectorized 0 loops in function.
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_4, inputs_15, _3);
PULSE.c:8:2: missed: statement clobbers memory: _5 (layer_17);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_12, pretmp_26, _9);
PULSE.c:13:28: note: ***** Analysis failed with vector mode V8SI
PULSE.c:13:28: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:22:2: missed: statement clobbers memory: _1 (layer_10(D));
PULSE.c:22:2: missed: statement clobbers memory: _14 (_2);
PULSE.c:22:2: missed: statement clobbers memory: _21 (_15);
PULSE.c:22:2: missed: statement clobbers memory: _28 (_22);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_29);
PULSE.c:26:3: missed: statement clobbers memory: memset (_34, 0, _32);
PULSE.c:26:3: missed: statement clobbers memory: memset (_27, 0, _25);
PULSE.c:26:3: missed: statement clobbers memory: memset (_20, 0, _18);
PULSE.c:26:3: missed: statement clobbers memory: memset (_7, 0, _5);
PULSE.c:28:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:28:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:39: missed: statement clobbers memory: _7 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:32:6: note: vectorized 1 loops in function.
PULSE.c:34:8: missed: statement clobbers memory: _1 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_2);
PULSE.c:39:39: missed: statement clobbers memory: _7 = rand ();
PULSE.c:40:40: missed: statement clobbers memory: _9 = rand ();
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8SI
PULSE.c:32:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:60:16: missed: couldn't vectorize loop
PULSE.c:60:16: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:63:17: missed: couldn't vectorize loop
PULSE.c:63:17: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:72:19: missed: couldn't vectorize loop
PULSE.c:72:19: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:39: missed: statement clobbers memory: _92 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:55:15: missed: couldn't vectorize loop
PULSE.c:56:14: missed: statement clobbers memory: _81 = _11 (_10, TMP_PTR_215);
PULSE.c:47:6: note: vectorized 1 loops in function.
PULSE.c:48:1: missed: statement clobbers memory: saved_stack.8_53 = __builtin_stack_save ();
PULSE.c:49:46: missed: statement clobbers memory: _1 = PULSE_GetLossFunctionPtr (loss_function_54(D));
PULSE.c:50:42: missed: statement clobbers memory: TMP_58 = aligned_alloc (64, _4);
PULSE.c:56:14: missed: statement clobbers memory: _81 = _11 (_10, TMP_PTR_215);
PULSE.c:59:15: missed: statement clobbers memory: random.5_64 = __builtin_alloca_with_align (_16, 32);
PULSE.c:34:8: missed: statement clobbers memory: _84 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_85);
PULSE.c:39:39: missed: statement clobbers memory: _92 = rand ();
PULSE.c:40:40: missed: statement clobbers memory: _95 = rand ();
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_121, inputs_116, _120);
PULSE.c:8:2: missed: statement clobbers memory: _122 (layer_117);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_128, _127, _126);
PULSE.c:66:11: missed: statement clobbers memory: loss_74 = PULSE_GetLoss_56 (_29, _28, _24, _23);
PULSE.c:22:2: missed: statement clobbers memory: _109 (output_60);
PULSE.c:22:2: missed: statement clobbers memory: _131 (_110);
PULSE.c:22:2: missed: statement clobbers memory: _138 (_132);
PULSE.c:22:2: missed: statement clobbers memory: _145 (_139);
PULSE.c:22:2: missed: statement clobbers memory: _152 (_146);
PULSE.c:22:2: missed: statement clobbers memory: _159 (_153);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_160);
PULSE.c:26:3: missed: statement clobbers memory: memset (_165, 0, _163);
PULSE.c:26:3: missed: statement clobbers memory: memset (_158, 0, _156);
PULSE.c:26:3: missed: statement clobbers memory: memset (_151, 0, _149);
PULSE.c:26:3: missed: statement clobbers memory: memset (_144, 0, _142);
PULSE.c:26:3: missed: statement clobbers memory: memset (_137, 0, _135);
PULSE.c:26:3: missed: statement clobbers memory: memset (_115, 0, _113);
PULSE.c:74:6: missed: statement clobbers memory: _33 (current_220, args);
PULSE.c:80:4: missed: statement clobbers memory: printf ("Epoch: %d | Item: %d | Loss: %.10f | Batch Loss: %.10f\r", i_217, j_218, _37, _36);
PULSE.c:84:2: missed: statement clobbers memory: free (TMP_58);
PULSE.c:47:6: missed: statement clobbers memory: __builtin_stack_restore (saved_stack.8_53);
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4DI
PULSE.c:47:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:91:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:91:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:125:20: missed: couldn't vectorize loop
PULSE.c:125:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:125:20: missed: couldn't vectorize loop
PULSE.c:125:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:104:20: missed: couldn't vectorize loop
PULSE.c:104:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:104:20: missed: couldn't vectorize loop
PULSE.c:104:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:93:13: note: vectorized 0 loops in function.
PULSE.c:96:39: missed: statement clobbers memory: layers_42 = malloc (_3);
PULSE.c:99:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:118:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:119:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:120:46: missed: statement clobbers memory: WEIGHTS_88 = aligned_alloc (64, 0);
PULSE.c:110:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.115_96];
PULSE.c:111:21: missed: statement clobbers memory: _77 = PULSE_GetDenseWeightsSize (args);
PULSE.c:112:16: missed: statement clobbers memory: _80 = PULSE_GetDenseIOSize (args);
PULSE.c:113:19: missed: statement clobbers memory: _83 = PULSE_GetDenseFixesSize (args);
PULSE.c:110:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})_226];
PULSE.c:111:21: missed: statement clobbers memory: _212 = PULSE_GetDenseWeightsSize (args);
PULSE.c:112:16: missed: statement clobbers memory: _215 = PULSE_GetDenseIOSize (args);
PULSE.c:113:19: missed: statement clobbers memory: _218 = PULSE_GetDenseFixesSize (args);
PULSE.c:118:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:119:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:120:46: missed: statement clobbers memory: WEIGHTS_47 = aligned_alloc (64, _5);
PULSE.c:121:41: missed: statement clobbers memory: IO_49 = aligned_alloc (64, _7);
PULSE.c:131:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.117_98];
PULSE.c:132:17: missed: statement clobbers memory: *_10 = PULSE_CreateDenseLayer (args, WEIGHTS_PTR_154, IO_PTR_155); [return slot optimization]
PULSE.c:133:20: missed: statement clobbers memory: _66 = PULSE_GetDenseWeightsSize (args);
PULSE.c:134:15: missed: statement clobbers memory: _69 = PULSE_GetDenseIOSize (args);
PULSE.c:131:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.117_126];
PULSE.c:132:17: missed: statement clobbers memory: *_108 = PULSE_CreateDenseLayer (args, WEIGHTS_PTR_150, IO_PTR_145); [return slot optimization]
PULSE.c:133:20: missed: statement clobbers memory: _97 = PULSE_GetDenseWeightsSize (args);
PULSE.c:134:15: missed: statement clobbers memory: _61 = PULSE_GetDenseIOSize (args);
PULSE.c:140:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:141:9: note: ***** Analysis succeeded with vector mode V8SI
PULSE.c:141:9: note: SLPing BB part
PULSE.c:141:9: note: Costing subgraph: 
PULSE.c:141:9: note: node 0x39f9d28 (max_nunits=4, refcnt=1) vector(4) unsigned int
PULSE.c:141:9: note: op template: <retval>.n_layers = _20;
PULSE.c:141:9: note: 	stmt 0 <retval>.n_layers = _20;
PULSE.c:141:9: note: 	stmt 1 <retval>.weights_size = WEIGHTS_SIZE_166;
PULSE.c:141:9: note: 	stmt 2 <retval>.io_size = IO_SIZE_169;
PULSE.c:141:9: note: 	stmt 3 <retval>.fixes_size = FIXES_SIZE_170;
PULSE.c:141:9: note: 	children 0x39f9e28
PULSE.c:141:9: note: node (external) 0x39f9e28 (max_nunits=1, refcnt=1) vector(4) unsigned int
PULSE.c:141:9: note: 	{ _20, WEIGHTS_SIZE_166, IO_SIZE_169, FIXES_SIZE_170 }
PULSE.c:141:9: note: Cost model analysis: 
PULSE.c:141:9: note: Cost model analysis for part in loop 0:
  Vector cost: 52
  Scalar cost: 64
PULSE.c:141:9: note: Costing subgraph: 
PULSE.c:141:9: note: node 0x39f9f28 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:141:9: note: op template: <retval>.layers = layers_42;
PULSE.c:141:9: note: 	stmt 0 <retval>.layers = layers_42;
PULSE.c:141:9: note: 	stmt 1 <retval>.weights = WEIGHTS_165;
PULSE.c:141:9: note: 	children 0x39fa028
PULSE.c:141:9: note: node (external) 0x39fa028 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:141:9: note: 	{ layers_42, WEIGHTS_165 }
PULSE.c:141:9: note: Cost model analysis: 
PULSE.c:141:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:141:9: note: Costing subgraph: 
PULSE.c:141:9: note: node 0x39fa0a8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:141:9: note: op template: <retval>.io = 0B;
PULSE.c:141:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:141:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:141:9: note: 	children 0x39fa128
PULSE.c:141:9: note: node (constant) 0x39fa128 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:141:9: note: 	{ 0B, 0B }
PULSE.c:141:9: note: Cost model analysis: 
PULSE.c:141:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:141:9: note: Basic block will be vectorized using SLP
PULSE.c:141:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:141:9: note: Vectorizing SLP tree:
PULSE.c:141:9: note: node 0x39f9d28 (max_nunits=4, refcnt=1) vector(4) unsigned int
PULSE.c:141:9: note: op template: <retval>.n_layers = _20;
PULSE.c:141:9: note: 	stmt 0 <retval>.n_layers = _20;
PULSE.c:141:9: note: 	stmt 1 <retval>.weights_size = WEIGHTS_SIZE_166;
PULSE.c:141:9: note: 	stmt 2 <retval>.io_size = IO_SIZE_169;
PULSE.c:141:9: note: 	stmt 3 <retval>.fixes_size = FIXES_SIZE_170;
PULSE.c:141:9: note: 	children 0x39f9e28
PULSE.c:141:9: note: node (external) 0x39f9e28 (max_nunits=1, refcnt=1) vector(4) unsigned int
PULSE.c:141:9: note: 	{ _20, WEIGHTS_SIZE_166, IO_SIZE_169, FIXES_SIZE_170 }
PULSE.c:141:9: note: ------>vectorizing SLP node starting from: <retval>.n_layers = _20;
PULSE.c:141:9: note: vect_is_simple_use: operand WEIGHTS_SIZE_166 = PHI <WEIGHTS_SIZE_163(36), 0(25)>, type of def: internal
PULSE.c:141:9: note: vect_is_simple_use: operand IO_SIZE_169 = PHI <IO_SIZE_162(36), 0(25)>, type of def: internal
PULSE.c:141:9: note: vect_is_simple_use: operand FIXES_SIZE_170 = PHI <FIXES_SIZE_161(36), 0(25)>, type of def: internal
PULSE.c:141:9: note: transform store. ncopies = 1
PULSE.c:141:9: note: create vector_type-pointer variable to type: vector(4) unsigned int  vectorizing a pointer ref: <retval>.n_layers
PULSE.c:141:9: note: created &<retval>.n_layers
PULSE.c:141:9: note: add new stmt: MEM <vector(4) unsigned int> [(unsigned int *)&<retval> + 32B] = _17;
PULSE.c:141:9: note: vectorizing stmts using SLP.
PULSE.c:141:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:141:9: note: Vectorizing SLP tree:
PULSE.c:141:9: note: node 0x39f9f28 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:141:9: note: op template: <retval>.layers = layers_42;
PULSE.c:141:9: note: 	stmt 0 <retval>.layers = layers_42;
PULSE.c:141:9: note: 	stmt 1 <retval>.weights = WEIGHTS_165;
PULSE.c:141:9: note: 	children 0x39fa028
PULSE.c:141:9: note: node (external) 0x39fa028 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:141:9: note: 	{ layers_42, WEIGHTS_165 }
PULSE.c:141:9: note: ------>vectorizing SLP node starting from: <retval>.layers = layers_42;
PULSE.c:141:9: note: vect_is_simple_use: operand WEIGHTS_165 = PHI <WEIGHTS_47(36), WEIGHTS_88(25)>, type of def: internal
PULSE.c:141:9: note: conflicting alias set types.
PULSE.c:141:9: note: transform store. ncopies = 1
PULSE.c:141:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.layers
PULSE.c:141:9: note: created &<retval>
PULSE.c:141:9: note: add new stmt: MEM <vector(2) long unsigned int> [(void *)&<retval>] = _87;
PULSE.c:141:9: note: vectorizing stmts using SLP.
PULSE.c:141:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:141:9: note: Vectorizing SLP tree:
PULSE.c:141:9: note: node 0x39fa0a8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:141:9: note: op template: <retval>.io = 0B;
PULSE.c:141:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:141:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:141:9: note: 	children 0x39fa128
PULSE.c:141:9: note: node (constant) 0x39fa128 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:141:9: note: 	{ 0B, 0B }
PULSE.c:141:9: note: ------>vectorizing SLP node starting from: <retval>.io = 0B;
PULSE.c:141:9: note: vect_is_simple_use: operand 0B, type of def: constant
PULSE.c:141:9: note: transform store. ncopies = 1
PULSE.c:141:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.io
PULSE.c:141:9: note: created &<retval>.io
PULSE.c:141:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval> + 16B] = { 0, 0 };
PULSE.c:141:9: note: vectorizing stmts using SLP.
PULSE.c:141:9: note: ***** The result for vector mode V32QI would be the same
PULSE.c:147:2: missed: statement clobbers memory: free (_1);
PULSE.c:148:2: missed: statement clobbers memory: free (_2);
PULSE.c:149:2: missed: statement clobbers memory: free (_3);
PULSE.c:150:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:150:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Loss.c:15:19: optimized: loop vectorized using 32 byte vectors
Loss.c:15:19: optimized:  loop versioned for vectorization because of possible aliasing
Loss.c:15:19: optimized: loop vectorized using 16 byte vectors
Loss.c:12:23: note: vectorized 1 loops in function.
Loss.c:12:23: note: ***** Analysis failed with vector mode V8SF
Loss.c:12:23: note: ***** The result for vector mode V32QI would be the same
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V16QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V16QI
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V8QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V8QI
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V4QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V4QI
Loss.c:7:19: missed: couldn't vectorize loop
Loss.c:8:38: missed: not vectorized: unsupported use in stmt.
Loss.c:4:23: note: vectorized 0 loops in function.
Loss.c:9:13: note: ***** Analysis failed with vector mode V8SF
Loss.c:9:13: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SF
Loss.c:23:2: note: ***** Analysis failed with vector mode VOID
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _28 (pretmp_91, _51, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:19: missed: not vectorized: no vectype for stmt: _19 = *_18;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:90:23: optimized: loop vectorized using 32 byte vectors
Dense.c:90:23: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:90:23: optimized: loop vectorized using 16 byte vectors
Dense.c:83:13: note: vectorized 1 loops in function.
Dense.c:86:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:83:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:83:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:83:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:83:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:83:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:83:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:83:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:83:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:167:56: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:167:56: note: SLPing BB part
Dense.c:167:56: note: Costing subgraph: 
Dense.c:167:56: note: node 0x8143098 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:167:56: note: op template: _4 = (long unsigned int) _3;
Dense.c:167:56: note: 	stmt 0 _4 = (long unsigned int) _3;
Dense.c:167:56: note: 	stmt 1 _7 = (sizetype) _2;
Dense.c:167:56: note: 	children 0x8143198
Dense.c:167:56: note: node (external) 0x8143198 (max_nunits=1, refcnt=1) vector(2) unsigned int
Dense.c:167:56: note: 	{ _3, _2 }
Dense.c:167:56: note: Cost model analysis: 
Dense.c:167:56: note: Cost model analysis for part in loop 0:
  Vector cost: 30
  Scalar cost: 8
Dense.c:167:56: missed: not vectorized: vectorization is not profitable.
Dense.c:167:56: note: ***** The result for vector mode V32QI would be the same
Dense.c:167:56: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:167:56: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:167:56: note: SLPing BB part
Dense.c:167:56: note: Costing subgraph: 
Dense.c:167:56: note: node 0x8143098 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:167:56: note: op template: _4 = (long unsigned int) _3;
Dense.c:167:56: note: 	stmt 0 _4 = (long unsigned int) _3;
Dense.c:167:56: note: 	stmt 1 _7 = (sizetype) _2;
Dense.c:167:56: note: 	children 0x8143118
Dense.c:167:56: note: node (external) 0x8143118 (max_nunits=1, refcnt=1) vector(2) unsigned int
Dense.c:167:56: note: 	{ _3, _2 }
Dense.c:167:56: note: Cost model analysis: 
Dense.c:167:56: note: Cost model analysis for part in loop 0:
  Vector cost: 30
  Scalar cost: 8
Dense.c:167:56: missed: not vectorized: vectorization is not profitable.
Dense.c:167:56: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:167:56: note: ***** Analysis failed with vector mode V8QI
Dense.c:167:56: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:167:56: note: ***** Analysis failed with vector mode V4QI
Dense.c:151:10: optimized: loop vectorized using 32 byte vectors
Dense.c:151:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:151:10: optimized: loop vectorized using 16 byte vectors
Dense.c:142:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
Dense.c:135:10: optimized: loop vectorized using 32 byte vectors
Dense.c:135:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:135:10: optimized: loop vectorized using 16 byte vectors
Dense.c:126:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
Dense.c:117:13: note: vectorized 2 loops in function.
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_28, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
Dense.c:117:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:117:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:117:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:117:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:117:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:117:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:117:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:117:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:62:15: missed: couldn't vectorize loop
Dense.c:62:15: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:75:11: optimized: loop vectorized using 32 byte vectors
Dense.c:75:11: optimized: loop vectorized using 16 byte vectors
Dense.c:66:11: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:881:10: missed: not vectorized: no vectype for stmt: _53 = MEM[(__m256 * {ref-all})w_ptr_82];
 scalar_type: __m256
Dense.c:54:13: note: vectorized 1 loops in function.
Dense.c:57:2: missed: statement clobbers memory: memcpy (_4, dense$baiases_63, _3);
Dense.c:79:2: missed: statement clobbers memory: _27 (pretmp_163, _68, 0);
Dense.c:54:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:54:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:160:107: note: ***** Analysis failed with vector mode V8SI
Dense.c:160:107: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
Dense.c:161:85: note: ***** Analysis failed with vector mode V8SI
Dense.c:161:85: note: ***** The result for vector mode V32QI would be the same
Dense.c:161:85: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:161:85: note: ***** Analysis failed with vector mode V16QI
Dense.c:161:85: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:161:85: note: ***** Analysis failed with vector mode V8QI
Dense.c:161:85: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:161:85: note: ***** Analysis failed with vector mode V4QI
Dense.c:162:107: note: ***** Analysis failed with vector mode V8SI
Dense.c:162:107: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
Dense.c:178:19: missed: couldn't vectorize loop
Dense.c:178:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:171:13: note: vectorized 0 loops in function.
Dense.c:179:38: missed: statement clobbers memory: _5 = rand ();
Dense.c:179:72: missed: statement clobbers memory: _65 = sqrt (_12);
Dense.c:192:19: missed: statement clobbers memory: _24 = PULSE_GetActivationFunctionPtr (_23);
Dense.c:208:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:209:4: missed: statement clobbers memory: exit (1);
Dense.c:213:9: note: ***** Analysis succeeded with vector mode V8SI
Dense.c:213:9: note: SLPing BB part
Dense.c:213:9: note: Costing subgraph: 
Dense.c:213:9: note: node 0x8163138 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:213:9: note: op template: <retval>.n_inputs = args$n_inputs_41;
Dense.c:213:9: note: 	stmt 0 <retval>.n_inputs = args$n_inputs_41;
Dense.c:213:9: note: 	stmt 1 <retval>.n_outputs = args$n_outputs_42;
Dense.c:213:9: note: 	children 0x81631b8
Dense.c:213:9: note: node 0x81631b8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:213:9: note: op template: args$n_inputs_41 = args.n_inputs;
Dense.c:213:9: note: 	stmt 0 args$n_inputs_41 = args.n_inputs;
Dense.c:213:9: note: 	stmt 1 args$n_outputs_42 = args.n_outputs;
Dense.c:213:9: note: Cost model analysis: 
Dense.c:213:9: note: Cost model analysis for part in loop 0:
  Vector cost: 40
  Scalar cost: 56
Dense.c:213:9: note: Costing subgraph: 
Dense.c:213:9: note: node 0x81632b8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: op template: <retval>.inputs = IO_31(D);
Dense.c:213:9: note: 	stmt 0 <retval>.inputs = IO_31(D);
Dense.c:213:9: note: 	stmt 1 <retval>.outputs = _22;
Dense.c:213:9: note: 	children 0x8163338
Dense.c:213:9: note: node (external) 0x8163338 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: 	{ IO_31(D), _22 }
Dense.c:213:9: note: Cost model analysis: 
Dense.c:213:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:213:9: note: Costing subgraph: 
Dense.c:213:9: note: node 0x8163438 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:213:9: note: op template: <retval>.feed = layer$feed_30;
Dense.c:213:9: note: 	stmt 0 <retval>.feed = layer$feed_30;
Dense.c:213:9: note: 	stmt 1 <retval>.back = layer$back_39;
Dense.c:213:9: note: 	stmt 2 <retval>.fix = layer$fix_40;
Dense.c:213:9: note: 	stmt 3 <retval>.activate = _25;
Dense.c:213:9: note: 	children 0x8163538
Dense.c:213:9: note: node (external) 0x8163538 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:213:9: note: 	{ layer$feed_30, layer$back_39, layer$fix_40, _25 }
Dense.c:213:9: note: Cost model analysis: 
Dense.c:213:9: note: Cost model analysis for part in loop 0:
  Vector cost: 64
  Scalar cost: 64
Dense.c:213:9: note: Costing subgraph: 
Dense.c:213:9: note: node 0x8163638 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: op template: <retval>.parent = 0B;
Dense.c:213:9: note: 	stmt 0 <retval>.parent = 0B;
Dense.c:213:9: note: 	stmt 1 <retval>.child = 0B;
Dense.c:213:9: note: 	children 0x81636b8
Dense.c:213:9: note: node (constant) 0x81636b8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: 	{ 0B, 0B }
Dense.c:213:9: note: Cost model analysis: 
Dense.c:213:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:213:9: note: Costing subgraph: 
Dense.c:213:9: note: node 0x8163738 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_29(D);
Dense.c:213:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_29(D);
Dense.c:213:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&<retval> + 96B] = _4;
Dense.c:213:9: note: 	children 0x81637b8
Dense.c:213:9: note: node (external) 0x81637b8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: 	{ MODEL_29(D), _4 }
Dense.c:213:9: note: Cost model analysis: 
Dense.c:213:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:213:9: note: Basic block will be vectorized using SLP
Dense.c:213:9: optimized: basic block part vectorized using 8 byte vectors
Dense.c:213:9: note: Vectorizing SLP tree:
Dense.c:213:9: note: node 0x8163138 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:213:9: note: op template: <retval>.n_inputs = args$n_inputs_41;
Dense.c:213:9: note: 	stmt 0 <retval>.n_inputs = args$n_inputs_41;
Dense.c:213:9: note: 	stmt 1 <retval>.n_outputs = args$n_outputs_42;
Dense.c:213:9: note: 	children 0x81631b8
Dense.c:213:9: note: node 0x81631b8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:213:9: note: op template: args$n_inputs_41 = args.n_inputs;
Dense.c:213:9: note: 	stmt 0 args$n_inputs_41 = args.n_inputs;
Dense.c:213:9: note: 	stmt 1 args$n_outputs_42 = args.n_outputs;
Dense.c:213:9: note: ------>vectorizing SLP node starting from: args$n_inputs_41 = args.n_inputs;
Dense.c:213:9: note: transform load. ncopies = 1
Dense.c:213:9: note: conflicting alias set types.
Dense.c:213:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: args.n_inputs
Dense.c:213:9: note: created &args
Dense.c:213:9: note: add new stmt: vect_args_n_inputs_41.508_26 = MEM <vector(2) unsigned int> [(void *)&args];
Dense.c:213:9: note: extracting lane for live stmt args$n_inputs_41 = args.n_inputs;
Dense.c:213:9: note: extracting lane for live stmt args$n_outputs_42 = args.n_outputs;
Dense.c:213:9: note: ------>vectorizing SLP node starting from: <retval>.n_inputs = args$n_inputs_41;
Dense.c:213:9: note: vect_is_simple_use: operand args.n_inputs, type of def: internal
Dense.c:213:9: note: vect_is_simple_use: operand args.n_outputs, type of def: internal
Dense.c:213:9: note: transform store. ncopies = 1
Dense.c:213:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: <retval>.n_inputs
Dense.c:213:9: note: created &<retval>.n_inputs
Dense.c:213:9: note: add new stmt: MEM <vector(2) unsigned int> [(unsigned int *)&<retval> + 128B] = vect_args_n_inputs_41.508_26;
Dense.c:213:9: note: vectorizing stmts using SLP.
Dense.c:213:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:213:9: note: Vectorizing SLP tree:
Dense.c:213:9: note: node 0x81632b8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: op template: <retval>.inputs = IO_31(D);
Dense.c:213:9: note: 	stmt 0 <retval>.inputs = IO_31(D);
Dense.c:213:9: note: 	stmt 1 <retval>.outputs = _22;
Dense.c:213:9: note: 	children 0x8163338
Dense.c:213:9: note: node (external) 0x8163338 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: 	{ IO_31(D), _22 }
Dense.c:213:9: note: ------>vectorizing SLP node starting from: <retval>.inputs = IO_31(D);
Dense.c:213:9: note: vect_is_simple_use: operand IO_31(D) + _21, type of def: internal
Dense.c:213:9: note: transform store. ncopies = 1
Dense.c:213:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.inputs
Dense.c:213:9: note: created &<retval>
Dense.c:213:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval>] = _78;
Dense.c:213:9: note: vectorizing stmts using SLP.
Dense.c:213:9: optimized: basic block part vectorized using 32 byte vectors
Dense.c:213:9: note: Vectorizing SLP tree:
Dense.c:213:9: note: node 0x8163438 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:213:9: note: op template: <retval>.feed = layer$feed_30;
Dense.c:213:9: note: 	stmt 0 <retval>.feed = layer$feed_30;
Dense.c:213:9: note: 	stmt 1 <retval>.back = layer$back_39;
Dense.c:213:9: note: 	stmt 2 <retval>.fix = layer$fix_40;
Dense.c:213:9: note: 	stmt 3 <retval>.activate = _25;
Dense.c:213:9: note: 	children 0x8163538
Dense.c:213:9: note: node (external) 0x8163538 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:213:9: note: 	{ layer$feed_30, layer$back_39, layer$fix_40, _25 }
Dense.c:213:9: note: ------>vectorizing SLP node starting from: <retval>.feed = layer$feed_30;
Dense.c:213:9: note: vect_is_simple_use: operand layer$back_39 = PHI <layer$back_45(D)(11), _BackDense(9), _SIMD_BackDense(15)>, type of def: internal
Dense.c:213:9: note: vect_is_simple_use: operand layer$fix_40 = PHI <layer$fix_46(D)(11), _FixDense(9), _SIMD_FixDense(15)>, type of def: internal
Dense.c:213:9: note: vect_is_simple_use: operand (void (*<T91a>) (float *, unsigned int, char)) _24, type of def: internal
Dense.c:213:9: note: conflicting alias set types.
Dense.c:213:9: note: transform store. ncopies = 1
Dense.c:213:9: note: create vector_type-pointer variable to type: vector(4) long unsigned int  vectorizing a pointer ref: <retval>.feed
Dense.c:213:9: note: created &<retval>.feed
Dense.c:213:9: note: add new stmt: MEM <vector(4) long unsigned int> [(void *)&<retval> + 32B] = _84;
Dense.c:213:9: note: vectorizing stmts using SLP.
Dense.c:213:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:213:9: note: Vectorizing SLP tree:
Dense.c:213:9: note: node 0x8163638 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: op template: <retval>.parent = 0B;
Dense.c:213:9: note: 	stmt 0 <retval>.parent = 0B;
Dense.c:213:9: note: 	stmt 1 <retval>.child = 0B;
Dense.c:213:9: note: 	children 0x81636b8
Dense.c:213:9: note: node (constant) 0x81636b8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: 	{ 0B, 0B }
Dense.c:213:9: note: ------>vectorizing SLP node starting from: <retval>.parent = 0B;
Dense.c:213:9: note: vect_is_simple_use: operand 0B, type of def: constant
Dense.c:213:9: note: transform store. ncopies = 1
Dense.c:213:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.parent
Dense.c:213:9: note: created &<retval>.parent
Dense.c:213:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer * *)&<retval> + 72B] = { 0, 0 };
Dense.c:213:9: note: vectorizing stmts using SLP.
Dense.c:213:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:213:9: note: Vectorizing SLP tree:
Dense.c:213:9: note: node 0x8163738 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_29(D);
Dense.c:213:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_29(D);
Dense.c:213:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&<retval> + 96B] = _4;
Dense.c:213:9: note: 	children 0x81637b8
Dense.c:213:9: note: node (external) 0x81637b8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: 	{ MODEL_29(D), _4 }
Dense.c:213:9: note: ------>vectorizing SLP node starting from: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_29(D);
Dense.c:213:9: note: vect_is_simple_use: operand MODEL_29(D) + _3, type of def: internal
Dense.c:213:9: note: transform store. ncopies = 1
Dense.c:213:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B]
Dense.c:213:9: note: created &MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B]
Dense.c:213:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer *)&<retval> + 88B] = _89;
Dense.c:213:9: note: vectorizing stmts using SLP.
Dense.c:213:9: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _28 (pretmp_91, _51, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:19: missed: not vectorized: no vectype for stmt: _19 = *_18;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:83:13: note: vectorized 0 loops in function.
Dense.c:86:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:115:1: note: ***** Analysis failed with vector mode V4DI
Dense.c:115:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Dense.c:167:56: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:167:56: note: SLPing BB part
Dense.c:167:56: note: Costing subgraph: 
Dense.c:167:56: note: node 0x8ba74d8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:167:56: note: op template: _4 = (long unsigned int) _3;
Dense.c:167:56: note: 	stmt 0 _4 = (long unsigned int) _3;
Dense.c:167:56: note: 	stmt 1 _7 = (sizetype) _2;
Dense.c:167:56: note: 	children 0x8ba75d8
Dense.c:167:56: note: node (external) 0x8ba75d8 (max_nunits=1, refcnt=1) vector(2) unsigned int
Dense.c:167:56: note: 	{ _3, _2 }
Dense.c:167:56: note: Cost model analysis: 
Dense.c:167:56: note: Cost model analysis for part in loop 0:
  Vector cost: 30
  Scalar cost: 8
Dense.c:167:56: missed: not vectorized: vectorization is not profitable.
Dense.c:167:56: note: ***** The result for vector mode V32QI would be the same
Dense.c:167:56: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:167:56: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:167:56: note: SLPing BB part
Dense.c:167:56: note: Costing subgraph: 
Dense.c:167:56: note: node 0x8ba74d8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:167:56: note: op template: _4 = (long unsigned int) _3;
Dense.c:167:56: note: 	stmt 0 _4 = (long unsigned int) _3;
Dense.c:167:56: note: 	stmt 1 _7 = (sizetype) _2;
Dense.c:167:56: note: 	children 0x8ba7558
Dense.c:167:56: note: node (external) 0x8ba7558 (max_nunits=1, refcnt=1) vector(2) unsigned int
Dense.c:167:56: note: 	{ _3, _2 }
Dense.c:167:56: note: Cost model analysis: 
Dense.c:167:56: note: Cost model analysis for part in loop 0:
  Vector cost: 30
  Scalar cost: 8
Dense.c:167:56: missed: not vectorized: vectorization is not profitable.
Dense.c:167:56: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:167:56: note: ***** Analysis failed with vector mode V8QI
Dense.c:167:56: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:167:56: note: ***** Analysis failed with vector mode V4QI
Dense.c:151:10: optimized: loop vectorized using 32 byte vectors
Dense.c:151:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:151:10: optimized: loop vectorized using 16 byte vectors
Dense.c:142:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
Dense.c:135:10: optimized: loop vectorized using 32 byte vectors
Dense.c:135:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:135:10: optimized: loop vectorized using 16 byte vectors
Dense.c:126:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
Dense.c:117:13: note: vectorized 2 loops in function.
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_28, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
Dense.c:117:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:117:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:117:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:117:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:117:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:117:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:117:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:117:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:62:15: missed: couldn't vectorize loop
Dense.c:62:15: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:75:11: optimized: loop vectorized using 32 byte vectors
Dense.c:75:11: optimized: loop vectorized using 16 byte vectors
Dense.c:66:11: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:881:10: missed: not vectorized: no vectype for stmt: _53 = MEM[(__m256 * {ref-all})w_ptr_82];
 scalar_type: __m256
Dense.c:54:13: note: vectorized 1 loops in function.
Dense.c:57:2: missed: statement clobbers memory: memcpy (_4, dense$baiases_63, _3);
Dense.c:79:2: missed: statement clobbers memory: _27 (pretmp_163, _68, 0);
Dense.c:54:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:54:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:160:107: note: ***** Analysis failed with vector mode V8SI
Dense.c:160:107: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
Dense.c:161:85: note: ***** Analysis failed with vector mode V8SI
Dense.c:161:85: note: ***** The result for vector mode V32QI would be the same
Dense.c:161:85: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:161:85: note: ***** Analysis failed with vector mode V16QI
Dense.c:161:85: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:161:85: note: ***** Analysis failed with vector mode V8QI
Dense.c:161:85: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:161:85: note: ***** Analysis failed with vector mode V4QI
Dense.c:162:107: note: ***** Analysis failed with vector mode V8SI
Dense.c:162:107: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
Dense.c:178:19: missed: couldn't vectorize loop
Dense.c:178:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:171:13: note: vectorized 0 loops in function.
Dense.c:179:38: missed: statement clobbers memory: _5 = rand ();
Dense.c:179:72: missed: statement clobbers memory: _65 = sqrt (_12);
Dense.c:192:19: missed: statement clobbers memory: _24 = PULSE_GetActivationFunctionPtr (_23);
Dense.c:208:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:209:4: missed: statement clobbers memory: exit (1);
Dense.c:213:9: note: ***** Analysis succeeded with vector mode V8SI
Dense.c:213:9: note: SLPing BB part
Dense.c:213:9: note: Costing subgraph: 
Dense.c:213:9: note: node 0x8b73a98 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:213:9: note: op template: <retval>.n_inputs = args$n_inputs_41;
Dense.c:213:9: note: 	stmt 0 <retval>.n_inputs = args$n_inputs_41;
Dense.c:213:9: note: 	stmt 1 <retval>.n_outputs = args$n_outputs_42;
Dense.c:213:9: note: 	children 0x8b73b18
Dense.c:213:9: note: node 0x8b73b18 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:213:9: note: op template: args$n_inputs_41 = args.n_inputs;
Dense.c:213:9: note: 	stmt 0 args$n_inputs_41 = args.n_inputs;
Dense.c:213:9: note: 	stmt 1 args$n_outputs_42 = args.n_outputs;
Dense.c:213:9: note: Cost model analysis: 
Dense.c:213:9: note: Cost model analysis for part in loop 0:
  Vector cost: 40
  Scalar cost: 56
Dense.c:213:9: note: Costing subgraph: 
Dense.c:213:9: note: node 0x8b73c18 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: op template: <retval>.inputs = IO_31(D);
Dense.c:213:9: note: 	stmt 0 <retval>.inputs = IO_31(D);
Dense.c:213:9: note: 	stmt 1 <retval>.outputs = _22;
Dense.c:213:9: note: 	children 0x8b73c98
Dense.c:213:9: note: node (external) 0x8b73c98 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: 	{ IO_31(D), _22 }
Dense.c:213:9: note: Cost model analysis: 
Dense.c:213:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:213:9: note: Costing subgraph: 
Dense.c:213:9: note: node 0x8b73d98 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:213:9: note: op template: <retval>.feed = layer$feed_30;
Dense.c:213:9: note: 	stmt 0 <retval>.feed = layer$feed_30;
Dense.c:213:9: note: 	stmt 1 <retval>.back = layer$back_39;
Dense.c:213:9: note: 	stmt 2 <retval>.fix = layer$fix_40;
Dense.c:213:9: note: 	stmt 3 <retval>.activate = _25;
Dense.c:213:9: note: 	children 0x8b73e98
Dense.c:213:9: note: node (external) 0x8b73e98 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:213:9: note: 	{ layer$feed_30, layer$back_39, layer$fix_40, _25 }
Dense.c:213:9: note: Cost model analysis: 
Dense.c:213:9: note: Cost model analysis for part in loop 0:
  Vector cost: 64
  Scalar cost: 64
Dense.c:213:9: note: Costing subgraph: 
Dense.c:213:9: note: node 0x8b73f98 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: op template: <retval>.parent = 0B;
Dense.c:213:9: note: 	stmt 0 <retval>.parent = 0B;
Dense.c:213:9: note: 	stmt 1 <retval>.child = 0B;
Dense.c:213:9: note: 	children 0x8b74018
Dense.c:213:9: note: node (constant) 0x8b74018 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: 	{ 0B, 0B }
Dense.c:213:9: note: Cost model analysis: 
Dense.c:213:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:213:9: note: Costing subgraph: 
Dense.c:213:9: note: node 0x8b74098 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_29(D);
Dense.c:213:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_29(D);
Dense.c:213:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&<retval> + 96B] = _4;
Dense.c:213:9: note: 	children 0x8b74118
Dense.c:213:9: note: node (external) 0x8b74118 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: 	{ MODEL_29(D), _4 }
Dense.c:213:9: note: Cost model analysis: 
Dense.c:213:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:213:9: note: Basic block will be vectorized using SLP
Dense.c:213:9: optimized: basic block part vectorized using 8 byte vectors
Dense.c:213:9: note: Vectorizing SLP tree:
Dense.c:213:9: note: node 0x8b73a98 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:213:9: note: op template: <retval>.n_inputs = args$n_inputs_41;
Dense.c:213:9: note: 	stmt 0 <retval>.n_inputs = args$n_inputs_41;
Dense.c:213:9: note: 	stmt 1 <retval>.n_outputs = args$n_outputs_42;
Dense.c:213:9: note: 	children 0x8b73b18
Dense.c:213:9: note: node 0x8b73b18 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:213:9: note: op template: args$n_inputs_41 = args.n_inputs;
Dense.c:213:9: note: 	stmt 0 args$n_inputs_41 = args.n_inputs;
Dense.c:213:9: note: 	stmt 1 args$n_outputs_42 = args.n_outputs;
Dense.c:213:9: note: ------>vectorizing SLP node starting from: args$n_inputs_41 = args.n_inputs;
Dense.c:213:9: note: transform load. ncopies = 1
Dense.c:213:9: note: conflicting alias set types.
Dense.c:213:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: args.n_inputs
Dense.c:213:9: note: created &args
Dense.c:213:9: note: add new stmt: vect_args_n_inputs_41.451_26 = MEM <vector(2) unsigned int> [(void *)&args];
Dense.c:213:9: note: extracting lane for live stmt args$n_inputs_41 = args.n_inputs;
Dense.c:213:9: note: extracting lane for live stmt args$n_outputs_42 = args.n_outputs;
Dense.c:213:9: note: ------>vectorizing SLP node starting from: <retval>.n_inputs = args$n_inputs_41;
Dense.c:213:9: note: vect_is_simple_use: operand args.n_inputs, type of def: internal
Dense.c:213:9: note: vect_is_simple_use: operand args.n_outputs, type of def: internal
Dense.c:213:9: note: transform store. ncopies = 1
Dense.c:213:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: <retval>.n_inputs
Dense.c:213:9: note: created &<retval>.n_inputs
Dense.c:213:9: note: add new stmt: MEM <vector(2) unsigned int> [(unsigned int *)&<retval> + 128B] = vect_args_n_inputs_41.451_26;
Dense.c:213:9: note: vectorizing stmts using SLP.
Dense.c:213:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:213:9: note: Vectorizing SLP tree:
Dense.c:213:9: note: node 0x8b73c18 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: op template: <retval>.inputs = IO_31(D);
Dense.c:213:9: note: 	stmt 0 <retval>.inputs = IO_31(D);
Dense.c:213:9: note: 	stmt 1 <retval>.outputs = _22;
Dense.c:213:9: note: 	children 0x8b73c98
Dense.c:213:9: note: node (external) 0x8b73c98 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: 	{ IO_31(D), _22 }
Dense.c:213:9: note: ------>vectorizing SLP node starting from: <retval>.inputs = IO_31(D);
Dense.c:213:9: note: vect_is_simple_use: operand IO_31(D) + _21, type of def: internal
Dense.c:213:9: note: transform store. ncopies = 1
Dense.c:213:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.inputs
Dense.c:213:9: note: created &<retval>
Dense.c:213:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval>] = _78;
Dense.c:213:9: note: vectorizing stmts using SLP.
Dense.c:213:9: optimized: basic block part vectorized using 32 byte vectors
Dense.c:213:9: note: Vectorizing SLP tree:
Dense.c:213:9: note: node 0x8b73d98 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:213:9: note: op template: <retval>.feed = layer$feed_30;
Dense.c:213:9: note: 	stmt 0 <retval>.feed = layer$feed_30;
Dense.c:213:9: note: 	stmt 1 <retval>.back = layer$back_39;
Dense.c:213:9: note: 	stmt 2 <retval>.fix = layer$fix_40;
Dense.c:213:9: note: 	stmt 3 <retval>.activate = _25;
Dense.c:213:9: note: 	children 0x8b73e98
Dense.c:213:9: note: node (external) 0x8b73e98 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:213:9: note: 	{ layer$feed_30, layer$back_39, layer$fix_40, _25 }
Dense.c:213:9: note: ------>vectorizing SLP node starting from: <retval>.feed = layer$feed_30;
Dense.c:213:9: note: vect_is_simple_use: operand layer$back_39 = PHI <layer$back_45(D)(11), _BackDense(9), _SIMD_BackDense(15)>, type of def: internal
Dense.c:213:9: note: vect_is_simple_use: operand layer$fix_40 = PHI <layer$fix_46(D)(11), _FixDense(9), _SIMD_FixDense(15)>, type of def: internal
Dense.c:213:9: note: vect_is_simple_use: operand (void (*<T91a>) (float *, unsigned int, char)) _24, type of def: internal
Dense.c:213:9: note: conflicting alias set types.
Dense.c:213:9: note: transform store. ncopies = 1
Dense.c:213:9: note: create vector_type-pointer variable to type: vector(4) long unsigned int  vectorizing a pointer ref: <retval>.feed
Dense.c:213:9: note: created &<retval>.feed
Dense.c:213:9: note: add new stmt: MEM <vector(4) long unsigned int> [(void *)&<retval> + 32B] = _84;
Dense.c:213:9: note: vectorizing stmts using SLP.
Dense.c:213:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:213:9: note: Vectorizing SLP tree:
Dense.c:213:9: note: node 0x8b73f98 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: op template: <retval>.parent = 0B;
Dense.c:213:9: note: 	stmt 0 <retval>.parent = 0B;
Dense.c:213:9: note: 	stmt 1 <retval>.child = 0B;
Dense.c:213:9: note: 	children 0x8b74018
Dense.c:213:9: note: node (constant) 0x8b74018 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: 	{ 0B, 0B }
Dense.c:213:9: note: ------>vectorizing SLP node starting from: <retval>.parent = 0B;
Dense.c:213:9: note: vect_is_simple_use: operand 0B, type of def: constant
Dense.c:213:9: note: transform store. ncopies = 1
Dense.c:213:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.parent
Dense.c:213:9: note: created &<retval>.parent
Dense.c:213:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer * *)&<retval> + 72B] = { 0, 0 };
Dense.c:213:9: note: vectorizing stmts using SLP.
Dense.c:213:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:213:9: note: Vectorizing SLP tree:
Dense.c:213:9: note: node 0x8b74098 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_29(D);
Dense.c:213:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_29(D);
Dense.c:213:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&<retval> + 96B] = _4;
Dense.c:213:9: note: 	children 0x8b74118
Dense.c:213:9: note: node (external) 0x8b74118 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: 	{ MODEL_29(D), _4 }
Dense.c:213:9: note: ------>vectorizing SLP node starting from: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_29(D);
Dense.c:213:9: note: vect_is_simple_use: operand MODEL_29(D) + _3, type of def: internal
Dense.c:213:9: note: transform store. ncopies = 1
Dense.c:213:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B]
Dense.c:213:9: note: created &MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B]
Dense.c:213:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer *)&<retval> + 88B] = _89;
Dense.c:213:9: note: vectorizing stmts using SLP.
Dense.c:213:9: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _28 (pretmp_91, _51, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:19: missed: not vectorized: no vectype for stmt: _19 = *_18;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:167:56: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:167:56: note: SLPing BB part
Dense.c:167:56: note: Costing subgraph: 
Dense.c:167:56: note: node 0x8034108 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:167:56: note: op template: _4 = (long unsigned int) _3;
Dense.c:167:56: note: 	stmt 0 _4 = (long unsigned int) _3;
Dense.c:167:56: note: 	stmt 1 _7 = (sizetype) _2;
Dense.c:167:56: note: 	children 0x8034208
Dense.c:167:56: note: node (external) 0x8034208 (max_nunits=1, refcnt=1) vector(2) unsigned int
Dense.c:167:56: note: 	{ _3, _2 }
Dense.c:167:56: note: Cost model analysis: 
Dense.c:167:56: note: Cost model analysis for part in loop 0:
  Vector cost: 30
  Scalar cost: 8
Dense.c:167:56: missed: not vectorized: vectorization is not profitable.
Dense.c:167:56: note: ***** The result for vector mode V32QI would be the same
Dense.c:167:56: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:167:56: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:167:56: note: SLPing BB part
Dense.c:167:56: note: Costing subgraph: 
Dense.c:167:56: note: node 0x8034108 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:167:56: note: op template: _4 = (long unsigned int) _3;
Dense.c:167:56: note: 	stmt 0 _4 = (long unsigned int) _3;
Dense.c:167:56: note: 	stmt 1 _7 = (sizetype) _2;
Dense.c:167:56: note: 	children 0x8034188
Dense.c:167:56: note: node (external) 0x8034188 (max_nunits=1, refcnt=1) vector(2) unsigned int
Dense.c:167:56: note: 	{ _3, _2 }
Dense.c:167:56: note: Cost model analysis: 
Dense.c:167:56: note: Cost model analysis for part in loop 0:
  Vector cost: 30
  Scalar cost: 8
Dense.c:167:56: missed: not vectorized: vectorization is not profitable.
Dense.c:167:56: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:167:56: note: ***** Analysis failed with vector mode V8QI
Dense.c:167:56: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:167:56: note: ***** Analysis failed with vector mode V4QI
Dense.c:151:10: optimized: loop vectorized using 32 byte vectors
Dense.c:151:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:151:10: optimized: loop vectorized using 16 byte vectors
Dense.c:142:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
Dense.c:135:10: optimized: loop vectorized using 32 byte vectors
Dense.c:135:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:135:10: optimized: loop vectorized using 16 byte vectors
Dense.c:126:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
Dense.c:117:13: note: vectorized 2 loops in function.
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_28, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
Dense.c:117:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:117:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:117:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:117:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:117:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:117:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:117:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:117:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:90:23: missed: couldn't vectorize loop
Dense.c:90:23: missed: not vectorized: unsupported outerloop form.
Dense.c:108:11: missed: couldn't vectorize loop
Dense.c:112:17: missed: not vectorized: no vectype for stmt: _123 = _130->errors;
 scalar_type: float *
Dense.c:90:23: missed: couldn't vectorize loop
Dense.c:90:23: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:108:11: missed: couldn't vectorize loop
Dense.c:112:17: missed: not vectorized: no vectype for stmt: _34 = _33->errors;
 scalar_type: float *
Dense.c:95:16: missed: couldn't vectorize loop
Dense.c:95:16: missed: not vectorized: unsupported control flow in loop.
Dense.c:83:13: note: vectorized 0 loops in function.
Dense.c:86:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:97:16: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:97:16: note: SLPing BB part
Dense.c:97:16: note: Costing subgraph: 
Dense.c:97:16: note: node 0x8034408 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:97:16: note: op template: _16 = (sizetype) j_97;
Dense.c:97:16: note: 	stmt 0 _16 = (sizetype) j_97;
Dense.c:97:16: note: 	stmt 1 _141 = (sizetype) wi_100;
Dense.c:97:16: note: 	children 0x8034508
Dense.c:97:16: note: node (external) 0x8034508 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:97:16: note: 	{ j_97, wi_100 }
Dense.c:97:16: note: Cost model analysis: 
Dense.c:97:16: note: Scalar 1 and vector 2 loop part do not match up, skipping scalar part
Dense.c:97:16: note: Cost model analysis for part in loop 2:
  Vector cost: 36
  Scalar cost: 8
Dense.c:97:16: missed: not vectorized: vectorization is not profitable.
Dense.c:97:16: note: ***** The result for vector mode V32QI would be the same
Dense.c:97:16: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:97:16: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:97:16: note: SLPing BB part
Dense.c:97:16: note: Costing subgraph: 
Dense.c:97:16: note: node 0x8034308 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:97:16: note: op template: _16 = (sizetype) j_97;
Dense.c:97:16: note: 	stmt 0 _16 = (sizetype) j_97;
Dense.c:97:16: note: 	stmt 1 _141 = (sizetype) wi_100;
Dense.c:97:16: note: 	children 0x8034708
Dense.c:97:16: note: node (external) 0x8034708 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:97:16: note: 	{ j_97, wi_100 }
Dense.c:97:16: note: Cost model analysis: 
Dense.c:97:16: note: Scalar 1 and vector 2 loop part do not match up, skipping scalar part
Dense.c:97:16: note: Cost model analysis for part in loop 2:
  Vector cost: 36
  Scalar cost: 8
Dense.c:97:16: missed: not vectorized: vectorization is not profitable.
Dense.c:97:16: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:112:29: note: ***** Analysis failed with vector mode V8QI
Dense.c:112:29: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:90:50: note: ***** Analysis failed with vector mode V4QI
Dense.c:62:15: missed: couldn't vectorize loop
Dense.c:62:15: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:75:11: optimized: loop vectorized using 32 byte vectors
Dense.c:75:11: optimized: loop vectorized using 16 byte vectors
Dense.c:66:11: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:881:10: missed: not vectorized: no vectype for stmt: _53 = MEM[(__m256 * {ref-all})w_ptr_82];
 scalar_type: __m256
Dense.c:54:13: note: vectorized 1 loops in function.
Dense.c:57:2: missed: statement clobbers memory: memcpy (_4, dense$baiases_63, _3);
Dense.c:79:2: missed: statement clobbers memory: _27 (pretmp_163, _68, 0);
Dense.c:54:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:54:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:160:107: note: ***** Analysis failed with vector mode V8SI
Dense.c:160:107: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
Dense.c:161:85: note: ***** Analysis failed with vector mode V8SI
Dense.c:161:85: note: ***** The result for vector mode V32QI would be the same
Dense.c:161:85: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:161:85: note: ***** Analysis failed with vector mode V16QI
Dense.c:161:85: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:161:85: note: ***** Analysis failed with vector mode V8QI
Dense.c:161:85: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:161:85: note: ***** Analysis failed with vector mode V4QI
Dense.c:162:107: note: ***** Analysis failed with vector mode V8SI
Dense.c:162:107: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
Dense.c:178:19: missed: couldn't vectorize loop
Dense.c:178:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:171:13: note: vectorized 0 loops in function.
Dense.c:179:38: missed: statement clobbers memory: _5 = rand ();
Dense.c:179:72: missed: statement clobbers memory: _65 = sqrt (_12);
Dense.c:192:19: missed: statement clobbers memory: _24 = PULSE_GetActivationFunctionPtr (_23);
Dense.c:208:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:209:4: missed: statement clobbers memory: exit (1);
Dense.c:213:9: note: ***** Analysis succeeded with vector mode V8SI
Dense.c:213:9: note: SLPing BB part
Dense.c:213:9: note: Costing subgraph: 
Dense.c:213:9: note: node 0x7e55378 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:213:9: note: op template: <retval>.n_inputs = args$n_inputs_41;
Dense.c:213:9: note: 	stmt 0 <retval>.n_inputs = args$n_inputs_41;
Dense.c:213:9: note: 	stmt 1 <retval>.n_outputs = args$n_outputs_42;
Dense.c:213:9: note: 	children 0x7e553f8
Dense.c:213:9: note: node 0x7e553f8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:213:9: note: op template: args$n_inputs_41 = args.n_inputs;
Dense.c:213:9: note: 	stmt 0 args$n_inputs_41 = args.n_inputs;
Dense.c:213:9: note: 	stmt 1 args$n_outputs_42 = args.n_outputs;
Dense.c:213:9: note: Cost model analysis: 
Dense.c:213:9: note: Cost model analysis for part in loop 0:
  Vector cost: 40
  Scalar cost: 56
Dense.c:213:9: note: Costing subgraph: 
Dense.c:213:9: note: node 0x7e554f8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: op template: <retval>.inputs = IO_31(D);
Dense.c:213:9: note: 	stmt 0 <retval>.inputs = IO_31(D);
Dense.c:213:9: note: 	stmt 1 <retval>.outputs = _22;
Dense.c:213:9: note: 	children 0x7e55578
Dense.c:213:9: note: node (external) 0x7e55578 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: 	{ IO_31(D), _22 }
Dense.c:213:9: note: Cost model analysis: 
Dense.c:213:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:213:9: note: Costing subgraph: 
Dense.c:213:9: note: node 0x7e55678 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:213:9: note: op template: <retval>.feed = layer$feed_30;
Dense.c:213:9: note: 	stmt 0 <retval>.feed = layer$feed_30;
Dense.c:213:9: note: 	stmt 1 <retval>.back = layer$back_39;
Dense.c:213:9: note: 	stmt 2 <retval>.fix = layer$fix_40;
Dense.c:213:9: note: 	stmt 3 <retval>.activate = _25;
Dense.c:213:9: note: 	children 0x7e55778
Dense.c:213:9: note: node (external) 0x7e55778 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:213:9: note: 	{ layer$feed_30, layer$back_39, layer$fix_40, _25 }
Dense.c:213:9: note: Cost model analysis: 
Dense.c:213:9: note: Cost model analysis for part in loop 0:
  Vector cost: 64
  Scalar cost: 64
Dense.c:213:9: note: Costing subgraph: 
Dense.c:213:9: note: node 0x7e55878 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: op template: <retval>.parent = 0B;
Dense.c:213:9: note: 	stmt 0 <retval>.parent = 0B;
Dense.c:213:9: note: 	stmt 1 <retval>.child = 0B;
Dense.c:213:9: note: 	children 0x7e558f8
Dense.c:213:9: note: node (constant) 0x7e558f8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: 	{ 0B, 0B }
Dense.c:213:9: note: Cost model analysis: 
Dense.c:213:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:213:9: note: Costing subgraph: 
Dense.c:213:9: note: node 0x7e55978 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_29(D);
Dense.c:213:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_29(D);
Dense.c:213:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&<retval> + 96B] = _4;
Dense.c:213:9: note: 	children 0x7e559f8
Dense.c:213:9: note: node (external) 0x7e559f8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: 	{ MODEL_29(D), _4 }
Dense.c:213:9: note: Cost model analysis: 
Dense.c:213:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:213:9: note: Basic block will be vectorized using SLP
Dense.c:213:9: optimized: basic block part vectorized using 8 byte vectors
Dense.c:213:9: note: Vectorizing SLP tree:
Dense.c:213:9: note: node 0x7e55378 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:213:9: note: op template: <retval>.n_inputs = args$n_inputs_41;
Dense.c:213:9: note: 	stmt 0 <retval>.n_inputs = args$n_inputs_41;
Dense.c:213:9: note: 	stmt 1 <retval>.n_outputs = args$n_outputs_42;
Dense.c:213:9: note: 	children 0x7e553f8
Dense.c:213:9: note: node 0x7e553f8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:213:9: note: op template: args$n_inputs_41 = args.n_inputs;
Dense.c:213:9: note: 	stmt 0 args$n_inputs_41 = args.n_inputs;
Dense.c:213:9: note: 	stmt 1 args$n_outputs_42 = args.n_outputs;
Dense.c:213:9: note: ------>vectorizing SLP node starting from: args$n_inputs_41 = args.n_inputs;
Dense.c:213:9: note: transform load. ncopies = 1
Dense.c:213:9: note: conflicting alias set types.
Dense.c:213:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: args.n_inputs
Dense.c:213:9: note: created &args
Dense.c:213:9: note: add new stmt: vect_args_n_inputs_41.500_26 = MEM <vector(2) unsigned int> [(void *)&args];
Dense.c:213:9: note: extracting lane for live stmt args$n_inputs_41 = args.n_inputs;
Dense.c:213:9: note: extracting lane for live stmt args$n_outputs_42 = args.n_outputs;
Dense.c:213:9: note: ------>vectorizing SLP node starting from: <retval>.n_inputs = args$n_inputs_41;
Dense.c:213:9: note: vect_is_simple_use: operand args.n_inputs, type of def: internal
Dense.c:213:9: note: vect_is_simple_use: operand args.n_outputs, type of def: internal
Dense.c:213:9: note: transform store. ncopies = 1
Dense.c:213:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: <retval>.n_inputs
Dense.c:213:9: note: created &<retval>.n_inputs
Dense.c:213:9: note: add new stmt: MEM <vector(2) unsigned int> [(unsigned int *)&<retval> + 128B] = vect_args_n_inputs_41.500_26;
Dense.c:213:9: note: vectorizing stmts using SLP.
Dense.c:213:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:213:9: note: Vectorizing SLP tree:
Dense.c:213:9: note: node 0x7e554f8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: op template: <retval>.inputs = IO_31(D);
Dense.c:213:9: note: 	stmt 0 <retval>.inputs = IO_31(D);
Dense.c:213:9: note: 	stmt 1 <retval>.outputs = _22;
Dense.c:213:9: note: 	children 0x7e55578
Dense.c:213:9: note: node (external) 0x7e55578 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: 	{ IO_31(D), _22 }
Dense.c:213:9: note: ------>vectorizing SLP node starting from: <retval>.inputs = IO_31(D);
Dense.c:213:9: note: vect_is_simple_use: operand IO_31(D) + _21, type of def: internal
Dense.c:213:9: note: transform store. ncopies = 1
Dense.c:213:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.inputs
Dense.c:213:9: note: created &<retval>
Dense.c:213:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval>] = _78;
Dense.c:213:9: note: vectorizing stmts using SLP.
Dense.c:213:9: optimized: basic block part vectorized using 32 byte vectors
Dense.c:213:9: note: Vectorizing SLP tree:
Dense.c:213:9: note: node 0x7e55678 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:213:9: note: op template: <retval>.feed = layer$feed_30;
Dense.c:213:9: note: 	stmt 0 <retval>.feed = layer$feed_30;
Dense.c:213:9: note: 	stmt 1 <retval>.back = layer$back_39;
Dense.c:213:9: note: 	stmt 2 <retval>.fix = layer$fix_40;
Dense.c:213:9: note: 	stmt 3 <retval>.activate = _25;
Dense.c:213:9: note: 	children 0x7e55778
Dense.c:213:9: note: node (external) 0x7e55778 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:213:9: note: 	{ layer$feed_30, layer$back_39, layer$fix_40, _25 }
Dense.c:213:9: note: ------>vectorizing SLP node starting from: <retval>.feed = layer$feed_30;
Dense.c:213:9: note: vect_is_simple_use: operand layer$back_39 = PHI <layer$back_45(D)(11), _BackDense(9), _SIMD_BackDense(15)>, type of def: internal
Dense.c:213:9: note: vect_is_simple_use: operand layer$fix_40 = PHI <layer$fix_46(D)(11), _FixDense(9), _SIMD_FixDense(15)>, type of def: internal
Dense.c:213:9: note: vect_is_simple_use: operand (void (*<T91a>) (float *, unsigned int, char)) _24, type of def: internal
Dense.c:213:9: note: conflicting alias set types.
Dense.c:213:9: note: transform store. ncopies = 1
Dense.c:213:9: note: create vector_type-pointer variable to type: vector(4) long unsigned int  vectorizing a pointer ref: <retval>.feed
Dense.c:213:9: note: created &<retval>.feed
Dense.c:213:9: note: add new stmt: MEM <vector(4) long unsigned int> [(void *)&<retval> + 32B] = _84;
Dense.c:213:9: note: vectorizing stmts using SLP.
Dense.c:213:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:213:9: note: Vectorizing SLP tree:
Dense.c:213:9: note: node 0x7e55878 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: op template: <retval>.parent = 0B;
Dense.c:213:9: note: 	stmt 0 <retval>.parent = 0B;
Dense.c:213:9: note: 	stmt 1 <retval>.child = 0B;
Dense.c:213:9: note: 	children 0x7e558f8
Dense.c:213:9: note: node (constant) 0x7e558f8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: 	{ 0B, 0B }
Dense.c:213:9: note: ------>vectorizing SLP node starting from: <retval>.parent = 0B;
Dense.c:213:9: note: vect_is_simple_use: operand 0B, type of def: constant
Dense.c:213:9: note: transform store. ncopies = 1
Dense.c:213:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.parent
Dense.c:213:9: note: created &<retval>.parent
Dense.c:213:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer * *)&<retval> + 72B] = { 0, 0 };
Dense.c:213:9: note: vectorizing stmts using SLP.
Dense.c:213:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:213:9: note: Vectorizing SLP tree:
Dense.c:213:9: note: node 0x7e55978 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_29(D);
Dense.c:213:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_29(D);
Dense.c:213:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&<retval> + 96B] = _4;
Dense.c:213:9: note: 	children 0x7e559f8
Dense.c:213:9: note: node (external) 0x7e559f8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: 	{ MODEL_29(D), _4 }
Dense.c:213:9: note: ------>vectorizing SLP node starting from: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_29(D);
Dense.c:213:9: note: vect_is_simple_use: operand MODEL_29(D) + _3, type of def: internal
Dense.c:213:9: note: transform store. ncopies = 1
Dense.c:213:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B]
Dense.c:213:9: note: created &MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B]
Dense.c:213:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer *)&<retval> + 88B] = _89;
Dense.c:213:9: note: vectorizing stmts using SLP.
Dense.c:213:9: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _28 (pretmp_91, _51, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:19: missed: not vectorized: no vectype for stmt: _19 = *_18;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:167:56: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:167:56: note: SLPing BB part
Dense.c:167:56: note: Costing subgraph: 
Dense.c:167:56: note: node 0x80bae98 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:167:56: note: op template: _4 = (long unsigned int) _3;
Dense.c:167:56: note: 	stmt 0 _4 = (long unsigned int) _3;
Dense.c:167:56: note: 	stmt 1 _7 = (sizetype) _2;
Dense.c:167:56: note: 	children 0x80baf98
Dense.c:167:56: note: node (external) 0x80baf98 (max_nunits=1, refcnt=1) vector(2) unsigned int
Dense.c:167:56: note: 	{ _3, _2 }
Dense.c:167:56: note: Cost model analysis: 
Dense.c:167:56: note: Cost model analysis for part in loop 0:
  Vector cost: 30
  Scalar cost: 8
Dense.c:167:56: missed: not vectorized: vectorization is not profitable.
Dense.c:167:56: note: ***** The result for vector mode V32QI would be the same
Dense.c:167:56: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:167:56: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:167:56: note: SLPing BB part
Dense.c:167:56: note: Costing subgraph: 
Dense.c:167:56: note: node 0x80bae98 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:167:56: note: op template: _4 = (long unsigned int) _3;
Dense.c:167:56: note: 	stmt 0 _4 = (long unsigned int) _3;
Dense.c:167:56: note: 	stmt 1 _7 = (sizetype) _2;
Dense.c:167:56: note: 	children 0x80baf18
Dense.c:167:56: note: node (external) 0x80baf18 (max_nunits=1, refcnt=1) vector(2) unsigned int
Dense.c:167:56: note: 	{ _3, _2 }
Dense.c:167:56: note: Cost model analysis: 
Dense.c:167:56: note: Cost model analysis for part in loop 0:
  Vector cost: 30
  Scalar cost: 8
Dense.c:167:56: missed: not vectorized: vectorization is not profitable.
Dense.c:167:56: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:167:56: note: ***** Analysis failed with vector mode V8QI
Dense.c:167:56: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:167:56: note: ***** Analysis failed with vector mode V4QI
Dense.c:151:10: optimized: loop vectorized using 32 byte vectors
Dense.c:151:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:151:10: optimized: loop vectorized using 16 byte vectors
Dense.c:142:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
Dense.c:135:10: optimized: loop vectorized using 32 byte vectors
Dense.c:135:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:135:10: optimized: loop vectorized using 16 byte vectors
Dense.c:126:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
Dense.c:117:13: note: vectorized 2 loops in function.
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_28, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
Dense.c:117:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:117:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:117:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:117:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:117:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:117:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:117:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:117:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:90:23: missed: couldn't vectorize loop
Dense.c:92:31: missed: not vectorized: no vectype for stmt: _121 = this_41(D)->outputs;
 scalar_type: float *
Dense.c:90:23: missed: couldn't vectorize loop
Dense.c:90:23: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:95:16: missed: couldn't vectorize loop
Dense.c:95:16: missed: not vectorized: unsupported control flow in loop.
Dense.c:83:13: note: vectorized 0 loops in function.
Dense.c:86:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:97:16: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:97:16: note: SLPing BB part
Dense.c:97:16: note: Costing subgraph: 
Dense.c:97:16: note: node 0x80bae98 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:97:16: note: op template: _16 = (sizetype) j_75;
Dense.c:97:16: note: 	stmt 0 _16 = (sizetype) j_75;
Dense.c:97:16: note: 	stmt 1 _113 = (sizetype) wi_76;
Dense.c:97:16: note: 	children 0x80baf98
Dense.c:97:16: note: node (external) 0x80baf98 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:97:16: note: 	{ j_75, wi_76 }
Dense.c:97:16: note: Cost model analysis: 
Dense.c:97:16: note: Scalar 1 and vector 2 loop part do not match up, skipping scalar part
Dense.c:97:16: note: Cost model analysis for part in loop 2:
  Vector cost: 36
  Scalar cost: 8
Dense.c:97:16: missed: not vectorized: vectorization is not profitable.
Dense.c:97:16: note: ***** The result for vector mode V32QI would be the same
Dense.c:97:16: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:97:16: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:97:16: note: SLPing BB part
Dense.c:97:16: note: Costing subgraph: 
Dense.c:97:16: note: node 0x80bae98 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:97:16: note: op template: _16 = (sizetype) j_75;
Dense.c:97:16: note: 	stmt 0 _16 = (sizetype) j_75;
Dense.c:97:16: note: 	stmt 1 _113 = (sizetype) wi_76;
Dense.c:97:16: note: 	children 0x80baf18
Dense.c:97:16: note: node (external) 0x80baf18 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:97:16: note: 	{ j_75, wi_76 }
Dense.c:97:16: note: Cost model analysis: 
Dense.c:97:16: note: Scalar 1 and vector 2 loop part do not match up, skipping scalar part
Dense.c:97:16: note: Cost model analysis for part in loop 2:
  Vector cost: 36
  Scalar cost: 8
Dense.c:97:16: missed: not vectorized: vectorization is not profitable.
Dense.c:97:16: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:90:50: note: ***** Analysis failed with vector mode V8QI
Dense.c:90:50: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:90:50: note: ***** Analysis failed with vector mode V4QI
Dense.c:62:15: missed: couldn't vectorize loop
Dense.c:62:15: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:75:11: optimized: loop vectorized using 32 byte vectors
Dense.c:75:11: optimized: loop vectorized using 16 byte vectors
Dense.c:66:11: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:881:10: missed: not vectorized: no vectype for stmt: _53 = MEM[(__m256 * {ref-all})w_ptr_82];
 scalar_type: __m256
Dense.c:54:13: note: vectorized 1 loops in function.
Dense.c:57:2: missed: statement clobbers memory: memcpy (_4, dense$baiases_63, _3);
Dense.c:79:2: missed: statement clobbers memory: _27 (pretmp_163, _68, 0);
Dense.c:54:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:54:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:160:107: note: ***** Analysis failed with vector mode V8SI
Dense.c:160:107: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
Dense.c:161:85: note: ***** Analysis failed with vector mode V8SI
Dense.c:161:85: note: ***** The result for vector mode V32QI would be the same
Dense.c:161:85: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:161:85: note: ***** Analysis failed with vector mode V16QI
Dense.c:161:85: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:161:85: note: ***** Analysis failed with vector mode V8QI
Dense.c:161:85: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:161:85: note: ***** Analysis failed with vector mode V4QI
Dense.c:162:107: note: ***** Analysis failed with vector mode V8SI
Dense.c:162:107: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
Dense.c:178:19: missed: couldn't vectorize loop
Dense.c:178:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:171:13: note: vectorized 0 loops in function.
Dense.c:179:38: missed: statement clobbers memory: _5 = rand ();
Dense.c:179:72: missed: statement clobbers memory: _65 = sqrt (_12);
Dense.c:192:19: missed: statement clobbers memory: _24 = PULSE_GetActivationFunctionPtr (_23);
Dense.c:208:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:209:4: missed: statement clobbers memory: exit (1);
Dense.c:213:9: note: ***** Analysis succeeded with vector mode V8SI
Dense.c:213:9: note: SLPing BB part
Dense.c:213:9: note: Costing subgraph: 
Dense.c:213:9: note: node 0x81044a8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:213:9: note: op template: <retval>.n_inputs = args$n_inputs_41;
Dense.c:213:9: note: 	stmt 0 <retval>.n_inputs = args$n_inputs_41;
Dense.c:213:9: note: 	stmt 1 <retval>.n_outputs = args$n_outputs_42;
Dense.c:213:9: note: 	children 0x8104528
Dense.c:213:9: note: node 0x8104528 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:213:9: note: op template: args$n_inputs_41 = args.n_inputs;
Dense.c:213:9: note: 	stmt 0 args$n_inputs_41 = args.n_inputs;
Dense.c:213:9: note: 	stmt 1 args$n_outputs_42 = args.n_outputs;
Dense.c:213:9: note: Cost model analysis: 
Dense.c:213:9: note: Cost model analysis for part in loop 0:
  Vector cost: 40
  Scalar cost: 56
Dense.c:213:9: note: Costing subgraph: 
Dense.c:213:9: note: node 0x8104628 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: op template: <retval>.inputs = IO_31(D);
Dense.c:213:9: note: 	stmt 0 <retval>.inputs = IO_31(D);
Dense.c:213:9: note: 	stmt 1 <retval>.outputs = _22;
Dense.c:213:9: note: 	children 0x81046a8
Dense.c:213:9: note: node (external) 0x81046a8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: 	{ IO_31(D), _22 }
Dense.c:213:9: note: Cost model analysis: 
Dense.c:213:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:213:9: note: Costing subgraph: 
Dense.c:213:9: note: node 0x81047a8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:213:9: note: op template: <retval>.feed = layer$feed_30;
Dense.c:213:9: note: 	stmt 0 <retval>.feed = layer$feed_30;
Dense.c:213:9: note: 	stmt 1 <retval>.back = layer$back_39;
Dense.c:213:9: note: 	stmt 2 <retval>.fix = layer$fix_40;
Dense.c:213:9: note: 	stmt 3 <retval>.activate = _25;
Dense.c:213:9: note: 	children 0x81048a8
Dense.c:213:9: note: node (external) 0x81048a8 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:213:9: note: 	{ layer$feed_30, layer$back_39, layer$fix_40, _25 }
Dense.c:213:9: note: Cost model analysis: 
Dense.c:213:9: note: Cost model analysis for part in loop 0:
  Vector cost: 64
  Scalar cost: 64
Dense.c:213:9: note: Costing subgraph: 
Dense.c:213:9: note: node 0x81049a8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: op template: <retval>.parent = 0B;
Dense.c:213:9: note: 	stmt 0 <retval>.parent = 0B;
Dense.c:213:9: note: 	stmt 1 <retval>.child = 0B;
Dense.c:213:9: note: 	children 0x8104a28
Dense.c:213:9: note: node (constant) 0x8104a28 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: 	{ 0B, 0B }
Dense.c:213:9: note: Cost model analysis: 
Dense.c:213:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:213:9: note: Costing subgraph: 
Dense.c:213:9: note: node 0x8104aa8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_29(D);
Dense.c:213:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_29(D);
Dense.c:213:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&<retval> + 96B] = _4;
Dense.c:213:9: note: 	children 0x8104b28
Dense.c:213:9: note: node (external) 0x8104b28 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: 	{ MODEL_29(D), _4 }
Dense.c:213:9: note: Cost model analysis: 
Dense.c:213:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:213:9: note: Basic block will be vectorized using SLP
Dense.c:213:9: optimized: basic block part vectorized using 8 byte vectors
Dense.c:213:9: note: Vectorizing SLP tree:
Dense.c:213:9: note: node 0x81044a8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:213:9: note: op template: <retval>.n_inputs = args$n_inputs_41;
Dense.c:213:9: note: 	stmt 0 <retval>.n_inputs = args$n_inputs_41;
Dense.c:213:9: note: 	stmt 1 <retval>.n_outputs = args$n_outputs_42;
Dense.c:213:9: note: 	children 0x8104528
Dense.c:213:9: note: node 0x8104528 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:213:9: note: op template: args$n_inputs_41 = args.n_inputs;
Dense.c:213:9: note: 	stmt 0 args$n_inputs_41 = args.n_inputs;
Dense.c:213:9: note: 	stmt 1 args$n_outputs_42 = args.n_outputs;
Dense.c:213:9: note: ------>vectorizing SLP node starting from: args$n_inputs_41 = args.n_inputs;
Dense.c:213:9: note: transform load. ncopies = 1
Dense.c:213:9: note: conflicting alias set types.
Dense.c:213:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: args.n_inputs
Dense.c:213:9: note: created &args
Dense.c:213:9: note: add new stmt: vect_args_n_inputs_41.484_26 = MEM <vector(2) unsigned int> [(void *)&args];
Dense.c:213:9: note: extracting lane for live stmt args$n_inputs_41 = args.n_inputs;
Dense.c:213:9: note: extracting lane for live stmt args$n_outputs_42 = args.n_outputs;
Dense.c:213:9: note: ------>vectorizing SLP node starting from: <retval>.n_inputs = args$n_inputs_41;
Dense.c:213:9: note: vect_is_simple_use: operand args.n_inputs, type of def: internal
Dense.c:213:9: note: vect_is_simple_use: operand args.n_outputs, type of def: internal
Dense.c:213:9: note: transform store. ncopies = 1
Dense.c:213:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: <retval>.n_inputs
Dense.c:213:9: note: created &<retval>.n_inputs
Dense.c:213:9: note: add new stmt: MEM <vector(2) unsigned int> [(unsigned int *)&<retval> + 128B] = vect_args_n_inputs_41.484_26;
Dense.c:213:9: note: vectorizing stmts using SLP.
Dense.c:213:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:213:9: note: Vectorizing SLP tree:
Dense.c:213:9: note: node 0x8104628 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: op template: <retval>.inputs = IO_31(D);
Dense.c:213:9: note: 	stmt 0 <retval>.inputs = IO_31(D);
Dense.c:213:9: note: 	stmt 1 <retval>.outputs = _22;
Dense.c:213:9: note: 	children 0x81046a8
Dense.c:213:9: note: node (external) 0x81046a8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: 	{ IO_31(D), _22 }
Dense.c:213:9: note: ------>vectorizing SLP node starting from: <retval>.inputs = IO_31(D);
Dense.c:213:9: note: vect_is_simple_use: operand IO_31(D) + _21, type of def: internal
Dense.c:213:9: note: transform store. ncopies = 1
Dense.c:213:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.inputs
Dense.c:213:9: note: created &<retval>
Dense.c:213:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval>] = _78;
Dense.c:213:9: note: vectorizing stmts using SLP.
Dense.c:213:9: optimized: basic block part vectorized using 32 byte vectors
Dense.c:213:9: note: Vectorizing SLP tree:
Dense.c:213:9: note: node 0x81047a8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:213:9: note: op template: <retval>.feed = layer$feed_30;
Dense.c:213:9: note: 	stmt 0 <retval>.feed = layer$feed_30;
Dense.c:213:9: note: 	stmt 1 <retval>.back = layer$back_39;
Dense.c:213:9: note: 	stmt 2 <retval>.fix = layer$fix_40;
Dense.c:213:9: note: 	stmt 3 <retval>.activate = _25;
Dense.c:213:9: note: 	children 0x81048a8
Dense.c:213:9: note: node (external) 0x81048a8 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:213:9: note: 	{ layer$feed_30, layer$back_39, layer$fix_40, _25 }
Dense.c:213:9: note: ------>vectorizing SLP node starting from: <retval>.feed = layer$feed_30;
Dense.c:213:9: note: vect_is_simple_use: operand layer$back_39 = PHI <layer$back_45(D)(11), _BackDense(9), _SIMD_BackDense(15)>, type of def: internal
Dense.c:213:9: note: vect_is_simple_use: operand layer$fix_40 = PHI <layer$fix_46(D)(11), _FixDense(9), _SIMD_FixDense(15)>, type of def: internal
Dense.c:213:9: note: vect_is_simple_use: operand (void (*<T91a>) (float *, unsigned int, char)) _24, type of def: internal
Dense.c:213:9: note: conflicting alias set types.
Dense.c:213:9: note: transform store. ncopies = 1
Dense.c:213:9: note: create vector_type-pointer variable to type: vector(4) long unsigned int  vectorizing a pointer ref: <retval>.feed
Dense.c:213:9: note: created &<retval>.feed
Dense.c:213:9: note: add new stmt: MEM <vector(4) long unsigned int> [(void *)&<retval> + 32B] = _84;
Dense.c:213:9: note: vectorizing stmts using SLP.
Dense.c:213:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:213:9: note: Vectorizing SLP tree:
Dense.c:213:9: note: node 0x81049a8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: op template: <retval>.parent = 0B;
Dense.c:213:9: note: 	stmt 0 <retval>.parent = 0B;
Dense.c:213:9: note: 	stmt 1 <retval>.child = 0B;
Dense.c:213:9: note: 	children 0x8104a28
Dense.c:213:9: note: node (constant) 0x8104a28 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: 	{ 0B, 0B }
Dense.c:213:9: note: ------>vectorizing SLP node starting from: <retval>.parent = 0B;
Dense.c:213:9: note: vect_is_simple_use: operand 0B, type of def: constant
Dense.c:213:9: note: transform store. ncopies = 1
Dense.c:213:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.parent
Dense.c:213:9: note: created &<retval>.parent
Dense.c:213:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer * *)&<retval> + 72B] = { 0, 0 };
Dense.c:213:9: note: vectorizing stmts using SLP.
Dense.c:213:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:213:9: note: Vectorizing SLP tree:
Dense.c:213:9: note: node 0x8104aa8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_29(D);
Dense.c:213:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_29(D);
Dense.c:213:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&<retval> + 96B] = _4;
Dense.c:213:9: note: 	children 0x8104b28
Dense.c:213:9: note: node (external) 0x8104b28 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: 	{ MODEL_29(D), _4 }
Dense.c:213:9: note: ------>vectorizing SLP node starting from: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_29(D);
Dense.c:213:9: note: vect_is_simple_use: operand MODEL_29(D) + _3, type of def: internal
Dense.c:213:9: note: transform store. ncopies = 1
Dense.c:213:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B]
Dense.c:213:9: note: created &MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B]
Dense.c:213:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer *)&<retval> + 88B] = _89;
Dense.c:213:9: note: vectorizing stmts using SLP.
Dense.c:213:9: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _28 (pretmp_91, _51, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:19: missed: not vectorized: no vectype for stmt: _19 = *_18;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:167:56: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:167:56: note: SLPing BB part
Dense.c:167:56: note: Costing subgraph: 
Dense.c:167:56: note: node 0x80d20f8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:167:56: note: op template: _4 = (long unsigned int) _3;
Dense.c:167:56: note: 	stmt 0 _4 = (long unsigned int) _3;
Dense.c:167:56: note: 	stmt 1 _7 = (sizetype) _2;
Dense.c:167:56: note: 	children 0x80d21f8
Dense.c:167:56: note: node (external) 0x80d21f8 (max_nunits=1, refcnt=1) vector(2) unsigned int
Dense.c:167:56: note: 	{ _3, _2 }
Dense.c:167:56: note: Cost model analysis: 
Dense.c:167:56: note: Cost model analysis for part in loop 0:
  Vector cost: 30
  Scalar cost: 8
Dense.c:167:56: missed: not vectorized: vectorization is not profitable.
Dense.c:167:56: note: ***** The result for vector mode V32QI would be the same
Dense.c:167:56: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:167:56: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:167:56: note: SLPing BB part
Dense.c:167:56: note: Costing subgraph: 
Dense.c:167:56: note: node 0x80d20f8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:167:56: note: op template: _4 = (long unsigned int) _3;
Dense.c:167:56: note: 	stmt 0 _4 = (long unsigned int) _3;
Dense.c:167:56: note: 	stmt 1 _7 = (sizetype) _2;
Dense.c:167:56: note: 	children 0x80d2178
Dense.c:167:56: note: node (external) 0x80d2178 (max_nunits=1, refcnt=1) vector(2) unsigned int
Dense.c:167:56: note: 	{ _3, _2 }
Dense.c:167:56: note: Cost model analysis: 
Dense.c:167:56: note: Cost model analysis for part in loop 0:
  Vector cost: 30
  Scalar cost: 8
Dense.c:167:56: missed: not vectorized: vectorization is not profitable.
Dense.c:167:56: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:167:56: note: ***** Analysis failed with vector mode V8QI
Dense.c:167:56: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:167:56: note: ***** Analysis failed with vector mode V4QI
Dense.c:151:10: optimized: loop vectorized using 32 byte vectors
Dense.c:151:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:151:10: optimized: loop vectorized using 16 byte vectors
Dense.c:142:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
Dense.c:135:10: optimized: loop vectorized using 32 byte vectors
Dense.c:135:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:135:10: optimized: loop vectorized using 16 byte vectors
Dense.c:126:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
Dense.c:117:13: note: vectorized 2 loops in function.
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_28, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
Dense.c:117:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:117:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:117:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:117:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:117:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:117:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:117:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:117:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:90:23: missed: couldn't vectorize loop
Dense.c:90:23: missed: not vectorized: unsupported outerloop form.
Dense.c:108:11: missed: couldn't vectorize loop
Dense.c:112:17: missed: not vectorized: no vectype for stmt: _123 = _130->errors;
 scalar_type: float *
Dense.c:90:23: missed: couldn't vectorize loop
Dense.c:90:23: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:108:11: missed: couldn't vectorize loop
Dense.c:112:17: missed: not vectorized: no vectype for stmt: _34 = _33->errors;
 scalar_type: float *
Dense.c:95:16: missed: couldn't vectorize loop
Dense.c:95:16: missed: not vectorized: unsupported control flow in loop.
Dense.c:83:13: note: vectorized 0 loops in function.
Dense.c:86:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:97:16: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:97:16: note: SLPing BB part
Dense.c:97:16: note: Costing subgraph: 
Dense.c:97:16: note: node 0x80d23f8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:97:16: note: op template: _16 = (sizetype) j_97;
Dense.c:97:16: note: 	stmt 0 _16 = (sizetype) j_97;
Dense.c:97:16: note: 	stmt 1 _141 = (sizetype) wi_100;
Dense.c:97:16: note: 	children 0x80d24f8
Dense.c:97:16: note: node (external) 0x80d24f8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:97:16: note: 	{ j_97, wi_100 }
Dense.c:97:16: note: Cost model analysis: 
Dense.c:97:16: note: Scalar 1 and vector 2 loop part do not match up, skipping scalar part
Dense.c:97:16: note: Cost model analysis for part in loop 2:
  Vector cost: 36
  Scalar cost: 8
Dense.c:97:16: missed: not vectorized: vectorization is not profitable.
Dense.c:97:16: note: ***** The result for vector mode V32QI would be the same
Dense.c:97:16: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:97:16: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:97:16: note: SLPing BB part
Dense.c:97:16: note: Costing subgraph: 
Dense.c:97:16: note: node 0x80d22f8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:97:16: note: op template: _16 = (sizetype) j_97;
Dense.c:97:16: note: 	stmt 0 _16 = (sizetype) j_97;
Dense.c:97:16: note: 	stmt 1 _141 = (sizetype) wi_100;
Dense.c:97:16: note: 	children 0x80d26f8
Dense.c:97:16: note: node (external) 0x80d26f8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:97:16: note: 	{ j_97, wi_100 }
Dense.c:97:16: note: Cost model analysis: 
Dense.c:97:16: note: Scalar 1 and vector 2 loop part do not match up, skipping scalar part
Dense.c:97:16: note: Cost model analysis for part in loop 2:
  Vector cost: 36
  Scalar cost: 8
Dense.c:97:16: missed: not vectorized: vectorization is not profitable.
Dense.c:97:16: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:112:29: note: ***** Analysis failed with vector mode V8QI
Dense.c:112:29: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:90:50: note: ***** Analysis failed with vector mode V4QI
Dense.c:62:15: missed: couldn't vectorize loop
Dense.c:62:15: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:75:11: optimized: loop vectorized using 32 byte vectors
Dense.c:75:11: optimized: loop vectorized using 16 byte vectors
Dense.c:66:11: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:881:10: missed: not vectorized: no vectype for stmt: _53 = MEM[(__m256 * {ref-all})w_ptr_82];
 scalar_type: __m256
Dense.c:54:13: note: vectorized 1 loops in function.
Dense.c:57:2: missed: statement clobbers memory: memcpy (_4, dense$baiases_63, _3);
Dense.c:79:2: missed: statement clobbers memory: _27 (pretmp_163, _68, 0);
Dense.c:54:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:54:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:160:107: note: ***** Analysis failed with vector mode V8SI
Dense.c:160:107: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
Dense.c:161:85: note: ***** Analysis failed with vector mode V8SI
Dense.c:161:85: note: ***** The result for vector mode V32QI would be the same
Dense.c:161:85: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:161:85: note: ***** Analysis failed with vector mode V16QI
Dense.c:161:85: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:161:85: note: ***** Analysis failed with vector mode V8QI
Dense.c:161:85: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:161:85: note: ***** Analysis failed with vector mode V4QI
Dense.c:162:107: note: ***** Analysis failed with vector mode V8SI
Dense.c:162:107: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
Dense.c:178:19: missed: couldn't vectorize loop
Dense.c:178:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:171:13: note: vectorized 0 loops in function.
Dense.c:179:38: missed: statement clobbers memory: _5 = rand ();
Dense.c:179:72: missed: statement clobbers memory: _65 = sqrt (_12);
Dense.c:192:19: missed: statement clobbers memory: _24 = PULSE_GetActivationFunctionPtr (_23);
Dense.c:208:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:209:4: missed: statement clobbers memory: exit (1);
Dense.c:213:9: note: ***** Analysis succeeded with vector mode V8SI
Dense.c:213:9: note: SLPing BB part
Dense.c:213:9: note: Costing subgraph: 
Dense.c:213:9: note: node 0x7fec8d8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:213:9: note: op template: <retval>.n_inputs = args$n_inputs_41;
Dense.c:213:9: note: 	stmt 0 <retval>.n_inputs = args$n_inputs_41;
Dense.c:213:9: note: 	stmt 1 <retval>.n_outputs = args$n_outputs_42;
Dense.c:213:9: note: 	children 0x7fec958
Dense.c:213:9: note: node 0x7fec958 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:213:9: note: op template: args$n_inputs_41 = args.n_inputs;
Dense.c:213:9: note: 	stmt 0 args$n_inputs_41 = args.n_inputs;
Dense.c:213:9: note: 	stmt 1 args$n_outputs_42 = args.n_outputs;
Dense.c:213:9: note: Cost model analysis: 
Dense.c:213:9: note: Cost model analysis for part in loop 0:
  Vector cost: 40
  Scalar cost: 56
Dense.c:213:9: note: Costing subgraph: 
Dense.c:213:9: note: node 0x7feca58 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: op template: <retval>.inputs = IO_31(D);
Dense.c:213:9: note: 	stmt 0 <retval>.inputs = IO_31(D);
Dense.c:213:9: note: 	stmt 1 <retval>.outputs = _22;
Dense.c:213:9: note: 	children 0x7fecad8
Dense.c:213:9: note: node (external) 0x7fecad8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: 	{ IO_31(D), _22 }
Dense.c:213:9: note: Cost model analysis: 
Dense.c:213:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:213:9: note: Costing subgraph: 
Dense.c:213:9: note: node 0x7fecbd8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:213:9: note: op template: <retval>.feed = layer$feed_30;
Dense.c:213:9: note: 	stmt 0 <retval>.feed = layer$feed_30;
Dense.c:213:9: note: 	stmt 1 <retval>.back = layer$back_39;
Dense.c:213:9: note: 	stmt 2 <retval>.fix = layer$fix_40;
Dense.c:213:9: note: 	stmt 3 <retval>.activate = _25;
Dense.c:213:9: note: 	children 0x7feccd8
Dense.c:213:9: note: node (external) 0x7feccd8 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:213:9: note: 	{ layer$feed_30, layer$back_39, layer$fix_40, _25 }
Dense.c:213:9: note: Cost model analysis: 
Dense.c:213:9: note: Cost model analysis for part in loop 0:
  Vector cost: 64
  Scalar cost: 64
Dense.c:213:9: note: Costing subgraph: 
Dense.c:213:9: note: node 0x7fecdd8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: op template: <retval>.parent = 0B;
Dense.c:213:9: note: 	stmt 0 <retval>.parent = 0B;
Dense.c:213:9: note: 	stmt 1 <retval>.child = 0B;
Dense.c:213:9: note: 	children 0x7fece58
Dense.c:213:9: note: node (constant) 0x7fece58 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: 	{ 0B, 0B }
Dense.c:213:9: note: Cost model analysis: 
Dense.c:213:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:213:9: note: Costing subgraph: 
Dense.c:213:9: note: node 0x7feced8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_29(D);
Dense.c:213:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_29(D);
Dense.c:213:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&<retval> + 96B] = _4;
Dense.c:213:9: note: 	children 0x7fecf58
Dense.c:213:9: note: node (external) 0x7fecf58 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: 	{ MODEL_29(D), _4 }
Dense.c:213:9: note: Cost model analysis: 
Dense.c:213:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:213:9: note: Basic block will be vectorized using SLP
Dense.c:213:9: optimized: basic block part vectorized using 8 byte vectors
Dense.c:213:9: note: Vectorizing SLP tree:
Dense.c:213:9: note: node 0x7fec8d8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:213:9: note: op template: <retval>.n_inputs = args$n_inputs_41;
Dense.c:213:9: note: 	stmt 0 <retval>.n_inputs = args$n_inputs_41;
Dense.c:213:9: note: 	stmt 1 <retval>.n_outputs = args$n_outputs_42;
Dense.c:213:9: note: 	children 0x7fec958
Dense.c:213:9: note: node 0x7fec958 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:213:9: note: op template: args$n_inputs_41 = args.n_inputs;
Dense.c:213:9: note: 	stmt 0 args$n_inputs_41 = args.n_inputs;
Dense.c:213:9: note: 	stmt 1 args$n_outputs_42 = args.n_outputs;
Dense.c:213:9: note: ------>vectorizing SLP node starting from: args$n_inputs_41 = args.n_inputs;
Dense.c:213:9: note: transform load. ncopies = 1
Dense.c:213:9: note: conflicting alias set types.
Dense.c:213:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: args.n_inputs
Dense.c:213:9: note: created &args
Dense.c:213:9: note: add new stmt: vect_args_n_inputs_41.500_26 = MEM <vector(2) unsigned int> [(void *)&args];
Dense.c:213:9: note: extracting lane for live stmt args$n_inputs_41 = args.n_inputs;
Dense.c:213:9: note: extracting lane for live stmt args$n_outputs_42 = args.n_outputs;
Dense.c:213:9: note: ------>vectorizing SLP node starting from: <retval>.n_inputs = args$n_inputs_41;
Dense.c:213:9: note: vect_is_simple_use: operand args.n_inputs, type of def: internal
Dense.c:213:9: note: vect_is_simple_use: operand args.n_outputs, type of def: internal
Dense.c:213:9: note: transform store. ncopies = 1
Dense.c:213:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: <retval>.n_inputs
Dense.c:213:9: note: created &<retval>.n_inputs
Dense.c:213:9: note: add new stmt: MEM <vector(2) unsigned int> [(unsigned int *)&<retval> + 128B] = vect_args_n_inputs_41.500_26;
Dense.c:213:9: note: vectorizing stmts using SLP.
Dense.c:213:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:213:9: note: Vectorizing SLP tree:
Dense.c:213:9: note: node 0x7feca58 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: op template: <retval>.inputs = IO_31(D);
Dense.c:213:9: note: 	stmt 0 <retval>.inputs = IO_31(D);
Dense.c:213:9: note: 	stmt 1 <retval>.outputs = _22;
Dense.c:213:9: note: 	children 0x7fecad8
Dense.c:213:9: note: node (external) 0x7fecad8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: 	{ IO_31(D), _22 }
Dense.c:213:9: note: ------>vectorizing SLP node starting from: <retval>.inputs = IO_31(D);
Dense.c:213:9: note: vect_is_simple_use: operand IO_31(D) + _21, type of def: internal
Dense.c:213:9: note: transform store. ncopies = 1
Dense.c:213:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.inputs
Dense.c:213:9: note: created &<retval>
Dense.c:213:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval>] = _78;
Dense.c:213:9: note: vectorizing stmts using SLP.
Dense.c:213:9: optimized: basic block part vectorized using 32 byte vectors
Dense.c:213:9: note: Vectorizing SLP tree:
Dense.c:213:9: note: node 0x7fecbd8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:213:9: note: op template: <retval>.feed = layer$feed_30;
Dense.c:213:9: note: 	stmt 0 <retval>.feed = layer$feed_30;
Dense.c:213:9: note: 	stmt 1 <retval>.back = layer$back_39;
Dense.c:213:9: note: 	stmt 2 <retval>.fix = layer$fix_40;
Dense.c:213:9: note: 	stmt 3 <retval>.activate = _25;
Dense.c:213:9: note: 	children 0x7feccd8
Dense.c:213:9: note: node (external) 0x7feccd8 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:213:9: note: 	{ layer$feed_30, layer$back_39, layer$fix_40, _25 }
Dense.c:213:9: note: ------>vectorizing SLP node starting from: <retval>.feed = layer$feed_30;
Dense.c:213:9: note: vect_is_simple_use: operand layer$back_39 = PHI <layer$back_45(D)(11), _BackDense(9), _SIMD_BackDense(15)>, type of def: internal
Dense.c:213:9: note: vect_is_simple_use: operand layer$fix_40 = PHI <layer$fix_46(D)(11), _FixDense(9), _SIMD_FixDense(15)>, type of def: internal
Dense.c:213:9: note: vect_is_simple_use: operand (void (*<T91a>) (float *, unsigned int, char)) _24, type of def: internal
Dense.c:213:9: note: conflicting alias set types.
Dense.c:213:9: note: transform store. ncopies = 1
Dense.c:213:9: note: create vector_type-pointer variable to type: vector(4) long unsigned int  vectorizing a pointer ref: <retval>.feed
Dense.c:213:9: note: created &<retval>.feed
Dense.c:213:9: note: add new stmt: MEM <vector(4) long unsigned int> [(void *)&<retval> + 32B] = _84;
Dense.c:213:9: note: vectorizing stmts using SLP.
Dense.c:213:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:213:9: note: Vectorizing SLP tree:
Dense.c:213:9: note: node 0x7fecdd8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: op template: <retval>.parent = 0B;
Dense.c:213:9: note: 	stmt 0 <retval>.parent = 0B;
Dense.c:213:9: note: 	stmt 1 <retval>.child = 0B;
Dense.c:213:9: note: 	children 0x7fece58
Dense.c:213:9: note: node (constant) 0x7fece58 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: 	{ 0B, 0B }
Dense.c:213:9: note: ------>vectorizing SLP node starting from: <retval>.parent = 0B;
Dense.c:213:9: note: vect_is_simple_use: operand 0B, type of def: constant
Dense.c:213:9: note: transform store. ncopies = 1
Dense.c:213:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.parent
Dense.c:213:9: note: created &<retval>.parent
Dense.c:213:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer * *)&<retval> + 72B] = { 0, 0 };
Dense.c:213:9: note: vectorizing stmts using SLP.
Dense.c:213:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:213:9: note: Vectorizing SLP tree:
Dense.c:213:9: note: node 0x7feced8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_29(D);
Dense.c:213:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_29(D);
Dense.c:213:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&<retval> + 96B] = _4;
Dense.c:213:9: note: 	children 0x7fecf58
Dense.c:213:9: note: node (external) 0x7fecf58 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: 	{ MODEL_29(D), _4 }
Dense.c:213:9: note: ------>vectorizing SLP node starting from: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_29(D);
Dense.c:213:9: note: vect_is_simple_use: operand MODEL_29(D) + _3, type of def: internal
Dense.c:213:9: note: transform store. ncopies = 1
Dense.c:213:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B]
Dense.c:213:9: note: created &MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B]
Dense.c:213:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer *)&<retval> + 88B] = _89;
Dense.c:213:9: note: vectorizing stmts using SLP.
Dense.c:213:9: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _28 (pretmp_91, _51, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:19: missed: not vectorized: no vectype for stmt: _19 = *_18;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:167:56: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:167:56: note: SLPing BB part
Dense.c:167:56: note: Costing subgraph: 
Dense.c:167:56: note: node 0x81df4b8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:167:56: note: op template: _4 = (long unsigned int) _3;
Dense.c:167:56: note: 	stmt 0 _4 = (long unsigned int) _3;
Dense.c:167:56: note: 	stmt 1 _7 = (sizetype) _2;
Dense.c:167:56: note: 	children 0x81df5b8
Dense.c:167:56: note: node (external) 0x81df5b8 (max_nunits=1, refcnt=1) vector(2) unsigned int
Dense.c:167:56: note: 	{ _3, _2 }
Dense.c:167:56: note: Cost model analysis: 
Dense.c:167:56: note: Cost model analysis for part in loop 0:
  Vector cost: 30
  Scalar cost: 8
Dense.c:167:56: missed: not vectorized: vectorization is not profitable.
Dense.c:167:56: note: ***** The result for vector mode V32QI would be the same
Dense.c:167:56: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:167:56: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:167:56: note: SLPing BB part
Dense.c:167:56: note: Costing subgraph: 
Dense.c:167:56: note: node 0x81df4b8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:167:56: note: op template: _4 = (long unsigned int) _3;
Dense.c:167:56: note: 	stmt 0 _4 = (long unsigned int) _3;
Dense.c:167:56: note: 	stmt 1 _7 = (sizetype) _2;
Dense.c:167:56: note: 	children 0x81df538
Dense.c:167:56: note: node (external) 0x81df538 (max_nunits=1, refcnt=1) vector(2) unsigned int
Dense.c:167:56: note: 	{ _3, _2 }
Dense.c:167:56: note: Cost model analysis: 
Dense.c:167:56: note: Cost model analysis for part in loop 0:
  Vector cost: 30
  Scalar cost: 8
Dense.c:167:56: missed: not vectorized: vectorization is not profitable.
Dense.c:167:56: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:167:56: note: ***** Analysis failed with vector mode V8QI
Dense.c:167:56: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:167:56: note: ***** Analysis failed with vector mode V4QI
Dense.c:151:10: optimized: loop vectorized using 32 byte vectors
Dense.c:151:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:151:10: optimized: loop vectorized using 16 byte vectors
Dense.c:142:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
Dense.c:135:10: optimized: loop vectorized using 32 byte vectors
Dense.c:135:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:135:10: optimized: loop vectorized using 16 byte vectors
Dense.c:126:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
Dense.c:117:13: note: vectorized 2 loops in function.
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_28, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
Dense.c:117:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:117:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:117:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:117:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:117:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:117:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:117:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:117:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:90:23: missed: couldn't vectorize loop
Dense.c:90:23: missed: not vectorized: unsupported outerloop form.
Dense.c:108:11: missed: couldn't vectorize loop
Dense.c:112:17: missed: not vectorized: no vectype for stmt: _123 = _130->errors;
 scalar_type: float *
Dense.c:90:23: missed: couldn't vectorize loop
Dense.c:90:23: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:108:11: missed: couldn't vectorize loop
Dense.c:112:17: missed: not vectorized: no vectype for stmt: _34 = _33->errors;
 scalar_type: float *
Dense.c:95:16: missed: couldn't vectorize loop
Dense.c:95:16: missed: not vectorized: unsupported control flow in loop.
Dense.c:83:13: note: vectorized 0 loops in function.
Dense.c:86:2: missed: statement clobbers memory: _1 (_3, _2, 1);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_19, _71);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_26, _68);
Dense.c:97:16: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:97:16: note: SLPing BB part
Dense.c:97:16: note: Costing subgraph: 
Dense.c:97:16: note: node 0x81df7b8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:97:16: note: op template: _16 = (sizetype) j_97;
Dense.c:97:16: note: 	stmt 0 _16 = (sizetype) j_97;
Dense.c:97:16: note: 	stmt 1 _141 = (sizetype) wi_100;
Dense.c:97:16: note: 	children 0x81df8b8
Dense.c:97:16: note: node (external) 0x81df8b8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:97:16: note: 	{ j_97, wi_100 }
Dense.c:97:16: note: Cost model analysis: 
Dense.c:97:16: note: Scalar 1 and vector 2 loop part do not match up, skipping scalar part
Dense.c:97:16: note: Cost model analysis for part in loop 2:
  Vector cost: 36
  Scalar cost: 8
Dense.c:97:16: missed: not vectorized: vectorization is not profitable.
Dense.c:97:16: note: ***** The result for vector mode V32QI would be the same
Dense.c:97:16: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:97:16: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:97:16: note: SLPing BB part
Dense.c:97:16: note: Costing subgraph: 
Dense.c:97:16: note: node 0x81df6b8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:97:16: note: op template: _16 = (sizetype) j_97;
Dense.c:97:16: note: 	stmt 0 _16 = (sizetype) j_97;
Dense.c:97:16: note: 	stmt 1 _141 = (sizetype) wi_100;
Dense.c:97:16: note: 	children 0x81dfab8
Dense.c:97:16: note: node (external) 0x81dfab8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:97:16: note: 	{ j_97, wi_100 }
Dense.c:97:16: note: Cost model analysis: 
Dense.c:97:16: note: Scalar 1 and vector 2 loop part do not match up, skipping scalar part
Dense.c:97:16: note: Cost model analysis for part in loop 2:
  Vector cost: 36
  Scalar cost: 8
Dense.c:97:16: missed: not vectorized: vectorization is not profitable.
Dense.c:97:16: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:112:29: note: ***** Analysis failed with vector mode V8QI
Dense.c:112:29: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:90:50: note: ***** Analysis failed with vector mode V4QI
Dense.c:62:15: missed: couldn't vectorize loop
Dense.c:62:15: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:75:11: optimized: loop vectorized using 32 byte vectors
Dense.c:75:11: optimized: loop vectorized using 16 byte vectors
Dense.c:66:11: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:881:10: missed: not vectorized: no vectype for stmt: _53 = MEM[(__m256 * {ref-all})w_ptr_82];
 scalar_type: __m256
Dense.c:54:13: note: vectorized 1 loops in function.
Dense.c:57:2: missed: statement clobbers memory: memcpy (_4, dense$baiases_63, _3);
Dense.c:79:2: missed: statement clobbers memory: _27 (pretmp_163, _68, 0);
Dense.c:54:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:54:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:160:107: note: ***** Analysis failed with vector mode V8SI
Dense.c:160:107: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
Dense.c:161:85: note: ***** Analysis failed with vector mode V8SI
Dense.c:161:85: note: ***** The result for vector mode V32QI would be the same
Dense.c:161:85: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:161:85: note: ***** Analysis failed with vector mode V16QI
Dense.c:161:85: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:161:85: note: ***** Analysis failed with vector mode V8QI
Dense.c:161:85: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:161:85: note: ***** Analysis failed with vector mode V4QI
Dense.c:162:107: note: ***** Analysis failed with vector mode V8SI
Dense.c:162:107: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
Dense.c:178:19: missed: couldn't vectorize loop
Dense.c:178:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:171:13: note: vectorized 0 loops in function.
Dense.c:179:38: missed: statement clobbers memory: _5 = rand ();
Dense.c:179:72: missed: statement clobbers memory: _65 = sqrt (_12);
Dense.c:192:19: missed: statement clobbers memory: _24 = PULSE_GetActivationFunctionPtr (_23);
Dense.c:208:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:209:4: missed: statement clobbers memory: exit (1);
Dense.c:213:9: note: ***** Analysis succeeded with vector mode V8SI
Dense.c:213:9: note: SLPing BB part
Dense.c:213:9: note: Costing subgraph: 
Dense.c:213:9: note: node 0x80e8558 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:213:9: note: op template: <retval>.n_inputs = args$n_inputs_41;
Dense.c:213:9: note: 	stmt 0 <retval>.n_inputs = args$n_inputs_41;
Dense.c:213:9: note: 	stmt 1 <retval>.n_outputs = args$n_outputs_42;
Dense.c:213:9: note: 	children 0x80e85d8
Dense.c:213:9: note: node 0x80e85d8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:213:9: note: op template: args$n_inputs_41 = args.n_inputs;
Dense.c:213:9: note: 	stmt 0 args$n_inputs_41 = args.n_inputs;
Dense.c:213:9: note: 	stmt 1 args$n_outputs_42 = args.n_outputs;
Dense.c:213:9: note: Cost model analysis: 
Dense.c:213:9: note: Cost model analysis for part in loop 0:
  Vector cost: 40
  Scalar cost: 56
Dense.c:213:9: note: Costing subgraph: 
Dense.c:213:9: note: node 0x80e86d8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: op template: <retval>.inputs = IO_31(D);
Dense.c:213:9: note: 	stmt 0 <retval>.inputs = IO_31(D);
Dense.c:213:9: note: 	stmt 1 <retval>.outputs = _22;
Dense.c:213:9: note: 	children 0x80e8758
Dense.c:213:9: note: node (external) 0x80e8758 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: 	{ IO_31(D), _22 }
Dense.c:213:9: note: Cost model analysis: 
Dense.c:213:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:213:9: note: Costing subgraph: 
Dense.c:213:9: note: node 0x80e8858 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:213:9: note: op template: <retval>.feed = layer$feed_30;
Dense.c:213:9: note: 	stmt 0 <retval>.feed = layer$feed_30;
Dense.c:213:9: note: 	stmt 1 <retval>.back = layer$back_39;
Dense.c:213:9: note: 	stmt 2 <retval>.fix = layer$fix_40;
Dense.c:213:9: note: 	stmt 3 <retval>.activate = _25;
Dense.c:213:9: note: 	children 0x80e8958
Dense.c:213:9: note: node (external) 0x80e8958 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:213:9: note: 	{ layer$feed_30, layer$back_39, layer$fix_40, _25 }
Dense.c:213:9: note: Cost model analysis: 
Dense.c:213:9: note: Cost model analysis for part in loop 0:
  Vector cost: 64
  Scalar cost: 64
Dense.c:213:9: note: Costing subgraph: 
Dense.c:213:9: note: node 0x80e8a58 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: op template: <retval>.parent = 0B;
Dense.c:213:9: note: 	stmt 0 <retval>.parent = 0B;
Dense.c:213:9: note: 	stmt 1 <retval>.child = 0B;
Dense.c:213:9: note: 	children 0x80e8ad8
Dense.c:213:9: note: node (constant) 0x80e8ad8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: 	{ 0B, 0B }
Dense.c:213:9: note: Cost model analysis: 
Dense.c:213:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:213:9: note: Costing subgraph: 
Dense.c:213:9: note: node 0x80e8b58 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_29(D);
Dense.c:213:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_29(D);
Dense.c:213:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&<retval> + 96B] = _4;
Dense.c:213:9: note: 	children 0x80e8bd8
Dense.c:213:9: note: node (external) 0x80e8bd8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: 	{ MODEL_29(D), _4 }
Dense.c:213:9: note: Cost model analysis: 
Dense.c:213:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:213:9: note: Basic block will be vectorized using SLP
Dense.c:213:9: optimized: basic block part vectorized using 8 byte vectors
Dense.c:213:9: note: Vectorizing SLP tree:
Dense.c:213:9: note: node 0x80e8558 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:213:9: note: op template: <retval>.n_inputs = args$n_inputs_41;
Dense.c:213:9: note: 	stmt 0 <retval>.n_inputs = args$n_inputs_41;
Dense.c:213:9: note: 	stmt 1 <retval>.n_outputs = args$n_outputs_42;
Dense.c:213:9: note: 	children 0x80e85d8
Dense.c:213:9: note: node 0x80e85d8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:213:9: note: op template: args$n_inputs_41 = args.n_inputs;
Dense.c:213:9: note: 	stmt 0 args$n_inputs_41 = args.n_inputs;
Dense.c:213:9: note: 	stmt 1 args$n_outputs_42 = args.n_outputs;
Dense.c:213:9: note: ------>vectorizing SLP node starting from: args$n_inputs_41 = args.n_inputs;
Dense.c:213:9: note: transform load. ncopies = 1
Dense.c:213:9: note: conflicting alias set types.
Dense.c:213:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: args.n_inputs
Dense.c:213:9: note: created &args
Dense.c:213:9: note: add new stmt: vect_args_n_inputs_41.498_26 = MEM <vector(2) unsigned int> [(void *)&args];
Dense.c:213:9: note: extracting lane for live stmt args$n_inputs_41 = args.n_inputs;
Dense.c:213:9: note: extracting lane for live stmt args$n_outputs_42 = args.n_outputs;
Dense.c:213:9: note: ------>vectorizing SLP node starting from: <retval>.n_inputs = args$n_inputs_41;
Dense.c:213:9: note: vect_is_simple_use: operand args.n_inputs, type of def: internal
Dense.c:213:9: note: vect_is_simple_use: operand args.n_outputs, type of def: internal
Dense.c:213:9: note: transform store. ncopies = 1
Dense.c:213:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: <retval>.n_inputs
Dense.c:213:9: note: created &<retval>.n_inputs
Dense.c:213:9: note: add new stmt: MEM <vector(2) unsigned int> [(unsigned int *)&<retval> + 128B] = vect_args_n_inputs_41.498_26;
Dense.c:213:9: note: vectorizing stmts using SLP.
Dense.c:213:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:213:9: note: Vectorizing SLP tree:
Dense.c:213:9: note: node 0x80e86d8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: op template: <retval>.inputs = IO_31(D);
Dense.c:213:9: note: 	stmt 0 <retval>.inputs = IO_31(D);
Dense.c:213:9: note: 	stmt 1 <retval>.outputs = _22;
Dense.c:213:9: note: 	children 0x80e8758
Dense.c:213:9: note: node (external) 0x80e8758 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: 	{ IO_31(D), _22 }
Dense.c:213:9: note: ------>vectorizing SLP node starting from: <retval>.inputs = IO_31(D);
Dense.c:213:9: note: vect_is_simple_use: operand IO_31(D) + _21, type of def: internal
Dense.c:213:9: note: transform store. ncopies = 1
Dense.c:213:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.inputs
Dense.c:213:9: note: created &<retval>
Dense.c:213:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval>] = _78;
Dense.c:213:9: note: vectorizing stmts using SLP.
Dense.c:213:9: optimized: basic block part vectorized using 32 byte vectors
Dense.c:213:9: note: Vectorizing SLP tree:
Dense.c:213:9: note: node 0x80e8858 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:213:9: note: op template: <retval>.feed = layer$feed_30;
Dense.c:213:9: note: 	stmt 0 <retval>.feed = layer$feed_30;
Dense.c:213:9: note: 	stmt 1 <retval>.back = layer$back_39;
Dense.c:213:9: note: 	stmt 2 <retval>.fix = layer$fix_40;
Dense.c:213:9: note: 	stmt 3 <retval>.activate = _25;
Dense.c:213:9: note: 	children 0x80e8958
Dense.c:213:9: note: node (external) 0x80e8958 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:213:9: note: 	{ layer$feed_30, layer$back_39, layer$fix_40, _25 }
Dense.c:213:9: note: ------>vectorizing SLP node starting from: <retval>.feed = layer$feed_30;
Dense.c:213:9: note: vect_is_simple_use: operand layer$back_39 = PHI <layer$back_45(D)(11), _BackDense(9), _SIMD_BackDense(15)>, type of def: internal
Dense.c:213:9: note: vect_is_simple_use: operand layer$fix_40 = PHI <layer$fix_46(D)(11), _FixDense(9), _SIMD_FixDense(15)>, type of def: internal
Dense.c:213:9: note: vect_is_simple_use: operand (void (*<T91a>) (float *, unsigned int, char)) _24, type of def: internal
Dense.c:213:9: note: conflicting alias set types.
Dense.c:213:9: note: transform store. ncopies = 1
Dense.c:213:9: note: create vector_type-pointer variable to type: vector(4) long unsigned int  vectorizing a pointer ref: <retval>.feed
Dense.c:213:9: note: created &<retval>.feed
Dense.c:213:9: note: add new stmt: MEM <vector(4) long unsigned int> [(void *)&<retval> + 32B] = _84;
Dense.c:213:9: note: vectorizing stmts using SLP.
Dense.c:213:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:213:9: note: Vectorizing SLP tree:
Dense.c:213:9: note: node 0x80e8a58 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: op template: <retval>.parent = 0B;
Dense.c:213:9: note: 	stmt 0 <retval>.parent = 0B;
Dense.c:213:9: note: 	stmt 1 <retval>.child = 0B;
Dense.c:213:9: note: 	children 0x80e8ad8
Dense.c:213:9: note: node (constant) 0x80e8ad8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: 	{ 0B, 0B }
Dense.c:213:9: note: ------>vectorizing SLP node starting from: <retval>.parent = 0B;
Dense.c:213:9: note: vect_is_simple_use: operand 0B, type of def: constant
Dense.c:213:9: note: transform store. ncopies = 1
Dense.c:213:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.parent
Dense.c:213:9: note: created &<retval>.parent
Dense.c:213:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer * *)&<retval> + 72B] = { 0, 0 };
Dense.c:213:9: note: vectorizing stmts using SLP.
Dense.c:213:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:213:9: note: Vectorizing SLP tree:
Dense.c:213:9: note: node 0x80e8b58 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_29(D);
Dense.c:213:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_29(D);
Dense.c:213:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&<retval> + 96B] = _4;
Dense.c:213:9: note: 	children 0x80e8bd8
Dense.c:213:9: note: node (external) 0x80e8bd8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: 	{ MODEL_29(D), _4 }
Dense.c:213:9: note: ------>vectorizing SLP node starting from: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_29(D);
Dense.c:213:9: note: vect_is_simple_use: operand MODEL_29(D) + _3, type of def: internal
Dense.c:213:9: note: transform store. ncopies = 1
Dense.c:213:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B]
Dense.c:213:9: note: created &MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B]
Dense.c:213:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer *)&<retval> + 88B] = _89;
Dense.c:213:9: note: vectorizing stmts using SLP.
Dense.c:213:9: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _28 (pretmp_91, _51, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:19: missed: not vectorized: no vectype for stmt: _19 = *_18;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:167:56: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:167:56: note: SLPing BB part
Dense.c:167:56: note: Costing subgraph: 
Dense.c:167:56: note: node 0x7be9b08 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:167:56: note: op template: _4 = (long unsigned int) _3;
Dense.c:167:56: note: 	stmt 0 _4 = (long unsigned int) _3;
Dense.c:167:56: note: 	stmt 1 _7 = (sizetype) _2;
Dense.c:167:56: note: 	children 0x7be9c08
Dense.c:167:56: note: node (external) 0x7be9c08 (max_nunits=1, refcnt=1) vector(2) unsigned int
Dense.c:167:56: note: 	{ _3, _2 }
Dense.c:167:56: note: Cost model analysis: 
Dense.c:167:56: note: Cost model analysis for part in loop 0:
  Vector cost: 30
  Scalar cost: 8
Dense.c:167:56: missed: not vectorized: vectorization is not profitable.
Dense.c:167:56: note: ***** The result for vector mode V32QI would be the same
Dense.c:167:56: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:167:56: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:167:56: note: SLPing BB part
Dense.c:167:56: note: Costing subgraph: 
Dense.c:167:56: note: node 0x7be9b08 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:167:56: note: op template: _4 = (long unsigned int) _3;
Dense.c:167:56: note: 	stmt 0 _4 = (long unsigned int) _3;
Dense.c:167:56: note: 	stmt 1 _7 = (sizetype) _2;
Dense.c:167:56: note: 	children 0x7be9b88
Dense.c:167:56: note: node (external) 0x7be9b88 (max_nunits=1, refcnt=1) vector(2) unsigned int
Dense.c:167:56: note: 	{ _3, _2 }
Dense.c:167:56: note: Cost model analysis: 
Dense.c:167:56: note: Cost model analysis for part in loop 0:
  Vector cost: 30
  Scalar cost: 8
Dense.c:167:56: missed: not vectorized: vectorization is not profitable.
Dense.c:167:56: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:167:56: note: ***** Analysis failed with vector mode V8QI
Dense.c:167:56: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:167:56: note: ***** Analysis failed with vector mode V4QI
Dense.c:151:10: optimized: loop vectorized using 32 byte vectors
Dense.c:151:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:151:10: optimized: loop vectorized using 16 byte vectors
Dense.c:142:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
Dense.c:135:10: optimized: loop vectorized using 32 byte vectors
Dense.c:135:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:135:10: optimized: loop vectorized using 16 byte vectors
Dense.c:126:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
Dense.c:117:13: note: vectorized 2 loops in function.
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_28, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
Dense.c:117:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:117:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:117:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:117:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:117:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:117:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:117:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:117:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:90:23: missed: couldn't vectorize loop
Dense.c:90:23: missed: not vectorized: unsupported outerloop form.
Dense.c:108:11: missed: couldn't vectorize loop
Dense.c:112:17: missed: not vectorized: no vectype for stmt: _123 = _130->errors;
 scalar_type: float *
Dense.c:90:23: missed: couldn't vectorize loop
Dense.c:90:23: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:108:11: missed: couldn't vectorize loop
Dense.c:112:17: missed: not vectorized: no vectype for stmt: _34 = _33->errors;
 scalar_type: float *
Dense.c:95:16: missed: couldn't vectorize loop
Dense.c:95:16: missed: not vectorized: unsupported control flow in loop.
Dense.c:83:13: note: vectorized 0 loops in function.
Dense.c:86:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:97:16: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:97:16: note: SLPing BB part
Dense.c:97:16: note: Costing subgraph: 
Dense.c:97:16: note: node 0x7be9e08 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:97:16: note: op template: _16 = (sizetype) j_97;
Dense.c:97:16: note: 	stmt 0 _16 = (sizetype) j_97;
Dense.c:97:16: note: 	stmt 1 _141 = (sizetype) wi_100;
Dense.c:97:16: note: 	children 0x7be9f08
Dense.c:97:16: note: node (external) 0x7be9f08 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:97:16: note: 	{ j_97, wi_100 }
Dense.c:97:16: note: Cost model analysis: 
Dense.c:97:16: note: Scalar 1 and vector 2 loop part do not match up, skipping scalar part
Dense.c:97:16: note: Cost model analysis for part in loop 2:
  Vector cost: 36
  Scalar cost: 8
Dense.c:97:16: missed: not vectorized: vectorization is not profitable.
Dense.c:97:16: note: ***** The result for vector mode V32QI would be the same
Dense.c:97:16: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:97:16: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:97:16: note: SLPing BB part
Dense.c:97:16: note: Costing subgraph: 
Dense.c:97:16: note: node 0x7be9d08 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:97:16: note: op template: _16 = (sizetype) j_97;
Dense.c:97:16: note: 	stmt 0 _16 = (sizetype) j_97;
Dense.c:97:16: note: 	stmt 1 _141 = (sizetype) wi_100;
Dense.c:97:16: note: 	children 0x7bea108
Dense.c:97:16: note: node (external) 0x7bea108 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:97:16: note: 	{ j_97, wi_100 }
Dense.c:97:16: note: Cost model analysis: 
Dense.c:97:16: note: Scalar 1 and vector 2 loop part do not match up, skipping scalar part
Dense.c:97:16: note: Cost model analysis for part in loop 2:
  Vector cost: 36
  Scalar cost: 8
Dense.c:97:16: missed: not vectorized: vectorization is not profitable.
Dense.c:97:16: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:112:29: note: ***** Analysis failed with vector mode V8QI
Dense.c:112:29: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:90:50: note: ***** Analysis failed with vector mode V4QI
Dense.c:62:15: missed: couldn't vectorize loop
Dense.c:62:15: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:75:11: optimized: loop vectorized using 32 byte vectors
Dense.c:75:11: optimized: loop vectorized using 16 byte vectors
Dense.c:66:11: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:881:10: missed: not vectorized: no vectype for stmt: _53 = MEM[(__m256 * {ref-all})w_ptr_82];
 scalar_type: __m256
Dense.c:54:13: note: vectorized 1 loops in function.
Dense.c:57:2: missed: statement clobbers memory: memcpy (_4, dense$baiases_63, _3);
Dense.c:79:2: missed: statement clobbers memory: _27 (pretmp_163, _68, 0);
Dense.c:54:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:54:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:160:107: note: ***** Analysis failed with vector mode V8SI
Dense.c:160:107: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
Dense.c:161:85: note: ***** Analysis failed with vector mode V8SI
Dense.c:161:85: note: ***** The result for vector mode V32QI would be the same
Dense.c:161:85: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:161:85: note: ***** Analysis failed with vector mode V16QI
Dense.c:161:85: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:161:85: note: ***** Analysis failed with vector mode V8QI
Dense.c:161:85: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:161:85: note: ***** Analysis failed with vector mode V4QI
Dense.c:162:107: note: ***** Analysis failed with vector mode V8SI
Dense.c:162:107: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
Dense.c:178:19: missed: couldn't vectorize loop
Dense.c:178:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:171:13: note: vectorized 0 loops in function.
Dense.c:179:38: missed: statement clobbers memory: _5 = rand ();
Dense.c:179:72: missed: statement clobbers memory: _65 = sqrt (_12);
Dense.c:192:19: missed: statement clobbers memory: _24 = PULSE_GetActivationFunctionPtr (_23);
Dense.c:208:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:209:4: missed: statement clobbers memory: exit (1);
Dense.c:213:9: note: ***** Analysis succeeded with vector mode V8SI
Dense.c:213:9: note: SLPing BB part
Dense.c:213:9: note: Costing subgraph: 
Dense.c:213:9: note: node 0x7a2ab68 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:213:9: note: op template: <retval>.n_inputs = args$n_inputs_41;
Dense.c:213:9: note: 	stmt 0 <retval>.n_inputs = args$n_inputs_41;
Dense.c:213:9: note: 	stmt 1 <retval>.n_outputs = args$n_outputs_42;
Dense.c:213:9: note: 	children 0x7a2abe8
Dense.c:213:9: note: node 0x7a2abe8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:213:9: note: op template: args$n_inputs_41 = args.n_inputs;
Dense.c:213:9: note: 	stmt 0 args$n_inputs_41 = args.n_inputs;
Dense.c:213:9: note: 	stmt 1 args$n_outputs_42 = args.n_outputs;
Dense.c:213:9: note: Cost model analysis: 
Dense.c:213:9: note: Cost model analysis for part in loop 0:
  Vector cost: 40
  Scalar cost: 56
Dense.c:213:9: note: Costing subgraph: 
Dense.c:213:9: note: node 0x7a2ace8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: op template: <retval>.inputs = IO_31(D);
Dense.c:213:9: note: 	stmt 0 <retval>.inputs = IO_31(D);
Dense.c:213:9: note: 	stmt 1 <retval>.outputs = _22;
Dense.c:213:9: note: 	children 0x7a2ad68
Dense.c:213:9: note: node (external) 0x7a2ad68 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: 	{ IO_31(D), _22 }
Dense.c:213:9: note: Cost model analysis: 
Dense.c:213:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:213:9: note: Costing subgraph: 
Dense.c:213:9: note: node 0x7a2ae68 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:213:9: note: op template: <retval>.feed = layer$feed_30;
Dense.c:213:9: note: 	stmt 0 <retval>.feed = layer$feed_30;
Dense.c:213:9: note: 	stmt 1 <retval>.back = layer$back_39;
Dense.c:213:9: note: 	stmt 2 <retval>.fix = layer$fix_40;
Dense.c:213:9: note: 	stmt 3 <retval>.activate = _25;
Dense.c:213:9: note: 	children 0x7a2af68
Dense.c:213:9: note: node (external) 0x7a2af68 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:213:9: note: 	{ layer$feed_30, layer$back_39, layer$fix_40, _25 }
Dense.c:213:9: note: Cost model analysis: 
Dense.c:213:9: note: Cost model analysis for part in loop 0:
  Vector cost: 64
  Scalar cost: 64
Dense.c:213:9: note: Costing subgraph: 
Dense.c:213:9: note: node 0x7a2b068 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: op template: <retval>.parent = 0B;
Dense.c:213:9: note: 	stmt 0 <retval>.parent = 0B;
Dense.c:213:9: note: 	stmt 1 <retval>.child = 0B;
Dense.c:213:9: note: 	children 0x7a2b0e8
Dense.c:213:9: note: node (constant) 0x7a2b0e8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: 	{ 0B, 0B }
Dense.c:213:9: note: Cost model analysis: 
Dense.c:213:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:213:9: note: Costing subgraph: 
Dense.c:213:9: note: node 0x7a2b168 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_29(D);
Dense.c:213:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_29(D);
Dense.c:213:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&<retval> + 96B] = _4;
Dense.c:213:9: note: 	children 0x7a2b1e8
Dense.c:213:9: note: node (external) 0x7a2b1e8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: 	{ MODEL_29(D), _4 }
Dense.c:213:9: note: Cost model analysis: 
Dense.c:213:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:213:9: note: Basic block will be vectorized using SLP
Dense.c:213:9: optimized: basic block part vectorized using 8 byte vectors
Dense.c:213:9: note: Vectorizing SLP tree:
Dense.c:213:9: note: node 0x7a2ab68 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:213:9: note: op template: <retval>.n_inputs = args$n_inputs_41;
Dense.c:213:9: note: 	stmt 0 <retval>.n_inputs = args$n_inputs_41;
Dense.c:213:9: note: 	stmt 1 <retval>.n_outputs = args$n_outputs_42;
Dense.c:213:9: note: 	children 0x7a2abe8
Dense.c:213:9: note: node 0x7a2abe8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:213:9: note: op template: args$n_inputs_41 = args.n_inputs;
Dense.c:213:9: note: 	stmt 0 args$n_inputs_41 = args.n_inputs;
Dense.c:213:9: note: 	stmt 1 args$n_outputs_42 = args.n_outputs;
Dense.c:213:9: note: ------>vectorizing SLP node starting from: args$n_inputs_41 = args.n_inputs;
Dense.c:213:9: note: transform load. ncopies = 1
Dense.c:213:9: note: conflicting alias set types.
Dense.c:213:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: args.n_inputs
Dense.c:213:9: note: created &args
Dense.c:213:9: note: add new stmt: vect_args_n_inputs_41.499_26 = MEM <vector(2) unsigned int> [(void *)&args];
Dense.c:213:9: note: extracting lane for live stmt args$n_inputs_41 = args.n_inputs;
Dense.c:213:9: note: extracting lane for live stmt args$n_outputs_42 = args.n_outputs;
Dense.c:213:9: note: ------>vectorizing SLP node starting from: <retval>.n_inputs = args$n_inputs_41;
Dense.c:213:9: note: vect_is_simple_use: operand args.n_inputs, type of def: internal
Dense.c:213:9: note: vect_is_simple_use: operand args.n_outputs, type of def: internal
Dense.c:213:9: note: transform store. ncopies = 1
Dense.c:213:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: <retval>.n_inputs
Dense.c:213:9: note: created &<retval>.n_inputs
Dense.c:213:9: note: add new stmt: MEM <vector(2) unsigned int> [(unsigned int *)&<retval> + 128B] = vect_args_n_inputs_41.499_26;
Dense.c:213:9: note: vectorizing stmts using SLP.
Dense.c:213:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:213:9: note: Vectorizing SLP tree:
Dense.c:213:9: note: node 0x7a2ace8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: op template: <retval>.inputs = IO_31(D);
Dense.c:213:9: note: 	stmt 0 <retval>.inputs = IO_31(D);
Dense.c:213:9: note: 	stmt 1 <retval>.outputs = _22;
Dense.c:213:9: note: 	children 0x7a2ad68
Dense.c:213:9: note: node (external) 0x7a2ad68 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: 	{ IO_31(D), _22 }
Dense.c:213:9: note: ------>vectorizing SLP node starting from: <retval>.inputs = IO_31(D);
Dense.c:213:9: note: vect_is_simple_use: operand IO_31(D) + _21, type of def: internal
Dense.c:213:9: note: transform store. ncopies = 1
Dense.c:213:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.inputs
Dense.c:213:9: note: created &<retval>
Dense.c:213:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval>] = _78;
Dense.c:213:9: note: vectorizing stmts using SLP.
Dense.c:213:9: optimized: basic block part vectorized using 32 byte vectors
Dense.c:213:9: note: Vectorizing SLP tree:
Dense.c:213:9: note: node 0x7a2ae68 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:213:9: note: op template: <retval>.feed = layer$feed_30;
Dense.c:213:9: note: 	stmt 0 <retval>.feed = layer$feed_30;
Dense.c:213:9: note: 	stmt 1 <retval>.back = layer$back_39;
Dense.c:213:9: note: 	stmt 2 <retval>.fix = layer$fix_40;
Dense.c:213:9: note: 	stmt 3 <retval>.activate = _25;
Dense.c:213:9: note: 	children 0x7a2af68
Dense.c:213:9: note: node (external) 0x7a2af68 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:213:9: note: 	{ layer$feed_30, layer$back_39, layer$fix_40, _25 }
Dense.c:213:9: note: ------>vectorizing SLP node starting from: <retval>.feed = layer$feed_30;
Dense.c:213:9: note: vect_is_simple_use: operand layer$back_39 = PHI <layer$back_45(D)(11), _BackDense(9), _SIMD_BackDense(15)>, type of def: internal
Dense.c:213:9: note: vect_is_simple_use: operand layer$fix_40 = PHI <layer$fix_46(D)(11), _FixDense(9), _SIMD_FixDense(15)>, type of def: internal
Dense.c:213:9: note: vect_is_simple_use: operand (void (*<T91a>) (float *, unsigned int, char)) _24, type of def: internal
Dense.c:213:9: note: conflicting alias set types.
Dense.c:213:9: note: transform store. ncopies = 1
Dense.c:213:9: note: create vector_type-pointer variable to type: vector(4) long unsigned int  vectorizing a pointer ref: <retval>.feed
Dense.c:213:9: note: created &<retval>.feed
Dense.c:213:9: note: add new stmt: MEM <vector(4) long unsigned int> [(void *)&<retval> + 32B] = _84;
Dense.c:213:9: note: vectorizing stmts using SLP.
Dense.c:213:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:213:9: note: Vectorizing SLP tree:
Dense.c:213:9: note: node 0x7a2b068 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: op template: <retval>.parent = 0B;
Dense.c:213:9: note: 	stmt 0 <retval>.parent = 0B;
Dense.c:213:9: note: 	stmt 1 <retval>.child = 0B;
Dense.c:213:9: note: 	children 0x7a2b0e8
Dense.c:213:9: note: node (constant) 0x7a2b0e8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: 	{ 0B, 0B }
Dense.c:213:9: note: ------>vectorizing SLP node starting from: <retval>.parent = 0B;
Dense.c:213:9: note: vect_is_simple_use: operand 0B, type of def: constant
Dense.c:213:9: note: transform store. ncopies = 1
Dense.c:213:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.parent
Dense.c:213:9: note: created &<retval>.parent
Dense.c:213:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer * *)&<retval> + 72B] = { 0, 0 };
Dense.c:213:9: note: vectorizing stmts using SLP.
Dense.c:213:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:213:9: note: Vectorizing SLP tree:
Dense.c:213:9: note: node 0x7a2b168 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_29(D);
Dense.c:213:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_29(D);
Dense.c:213:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&<retval> + 96B] = _4;
Dense.c:213:9: note: 	children 0x7a2b1e8
Dense.c:213:9: note: node (external) 0x7a2b1e8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:213:9: note: 	{ MODEL_29(D), _4 }
Dense.c:213:9: note: ------>vectorizing SLP node starting from: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_29(D);
Dense.c:213:9: note: vect_is_simple_use: operand MODEL_29(D) + _3, type of def: internal
Dense.c:213:9: note: transform store. ncopies = 1
Dense.c:213:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B]
Dense.c:213:9: note: created &MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B]
Dense.c:213:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer *)&<retval> + 88B] = _89;
Dense.c:213:9: note: vectorizing stmts using SLP.
Dense.c:213:9: note: ***** The result for vector mode V32QI would be the same
Activations.c:17:19: optimized: loop vectorized using 32 byte vectors
Activations.c:17:19: optimized: loop vectorized using 16 byte vectors
Activations.c:17:19: optimized: loop vectorized using 32 byte vectors
Activations.c:17:19: optimized: loop vectorized using 16 byte vectors
Activations.c:14:13: note: vectorized 2 loops in function.
Activations.c:14:13: note: ***** Analysis failed with vector mode V8SF
Activations.c:14:13: note: ***** The result for vector mode V32QI would be the same
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V16QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V16QI
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V8QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V8QI
Activations.c:14:13: note: ***** Re-trying analysis with vector mode V4QI
Activations.c:14:13: note: ***** Analysis failed with vector mode V4QI
Activations.c:28:19: missed: couldn't vectorize loop
Activations.c:28:19: missed: not vectorized: unsupported control flow in loop.
Activations.c:28:19: optimized: loop vectorized using 32 byte vectors
Activations.c:28:19: optimized: loop vectorized using 16 byte vectors
Activations.c:25:13: note: vectorized 1 loops in function.
Activations.c:25:13: note: ***** Analysis failed with vector mode V8SF
Activations.c:25:13: note: ***** The result for vector mode V32QI would be the same
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V16QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V16QI
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V8QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V8QI
Activations.c:25:13: note: ***** Re-trying analysis with vector mode V4QI
Activations.c:25:13: note: ***** Analysis failed with vector mode V4QI
Activations.c:38:1: note: ***** Analysis failed with vector mode VOID
Activations.c:5:19: missed: couldn't vectorize loop
Activations.c:9:21: missed: statement clobbers memory: _36 = expf (_18);
Activations.c:5:19: missed: couldn't vectorize loop
Activations.c:9:21: missed: statement clobbers memory: _12 = expf (_11);
Activations.c:3:13: note: vectorized 0 loops in function.
Activations.c:9:21: missed: statement clobbers memory: _12 = expf (_11);
Activations.c:9:21: missed: statement clobbers memory: _36 = expf (_18);
Activations.c:11:1: note: ***** Analysis failed with vector mode V8SF
Activations.c:11:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SF
Activations.c:44:2: note: ***** Analysis failed with vector mode VOID
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:4:18: note: vectorized 0 loops in function.
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_4, inputs_15, _3);
PULSE.c:8:2: missed: statement clobbers memory: _5 (layer_17);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_12, pretmp_26, _9);
PULSE.c:13:28: note: ***** Analysis failed with vector mode V8SI
PULSE.c:13:28: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
PULSE.c:22:2: missed: statement clobbers memory: _1 (layer_10(D));
PULSE.c:22:2: missed: statement clobbers memory: _14 (_2);
PULSE.c:22:2: missed: statement clobbers memory: _21 (_15);
PULSE.c:22:2: missed: statement clobbers memory: _28 (_22);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_29);
PULSE.c:26:3: missed: statement clobbers memory: memset (_34, 0, _32);
PULSE.c:26:3: missed: statement clobbers memory: memset (_27, 0, _25);
PULSE.c:26:3: missed: statement clobbers memory: memset (_20, 0, _18);
PULSE.c:26:3: missed: statement clobbers memory: memset (_7, 0, _5);
PULSE.c:28:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:28:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:39: missed: statement clobbers memory: _7 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:32:6: note: vectorized 1 loops in function.
PULSE.c:34:8: missed: statement clobbers memory: _1 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_2);
PULSE.c:39:39: missed: statement clobbers memory: _7 = rand ();
PULSE.c:40:40: missed: statement clobbers memory: _9 = rand ();
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8SI
PULSE.c:32:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:32:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:32:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:60:16: missed: couldn't vectorize loop
PULSE.c:60:16: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:63:17: missed: couldn't vectorize loop
PULSE.c:63:17: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
PULSE.c:72:19: missed: couldn't vectorize loop
PULSE.c:72:19: missed: not vectorized: number of iterations cannot be computed.
PULSE.c:10:4: missed: couldn't vectorize loop
PULSE.c:10:4: missed: not vectorized: unsupported control flow in loop.
PULSE.c:37:20: missed: couldn't vectorize loop
PULSE.c:39:39: missed: statement clobbers memory: _92 = rand ();
PULSE.c:35:20: optimized: loop vectorized using 32 byte vectors
PULSE.c:35:20: optimized: loop vectorized using 16 byte vectors
PULSE.c:55:15: missed: couldn't vectorize loop
PULSE.c:56:14: missed: statement clobbers memory: _81 = _11 (_10, TMP_PTR_215);
PULSE.c:47:6: note: vectorized 1 loops in function.
PULSE.c:48:1: missed: statement clobbers memory: saved_stack.8_53 = __builtin_stack_save ();
PULSE.c:49:46: missed: statement clobbers memory: _1 = PULSE_GetLossFunctionPtr (loss_function_54(D));
PULSE.c:50:42: missed: statement clobbers memory: TMP_58 = aligned_alloc (64, _4);
PULSE.c:56:14: missed: statement clobbers memory: _81 = _11 (_10, TMP_PTR_215);
PULSE.c:59:15: missed: statement clobbers memory: random.5_64 = __builtin_alloca_with_align (_16, 32);
PULSE.c:34:8: missed: statement clobbers memory: _84 = time (0B);
PULSE.c:34:2: missed: statement clobbers memory: srand (_85);
PULSE.c:39:39: missed: statement clobbers memory: _92 = rand ();
PULSE.c:40:40: missed: statement clobbers memory: _95 = rand ();
PULSE.c:7:3: missed: statement clobbers memory: memcpy (_121, inputs_116, _120);
PULSE.c:8:2: missed: statement clobbers memory: _122 (layer_117);
PULSE.c:12:3: missed: statement clobbers memory: memcpy (_128, _127, _126);
PULSE.c:66:11: missed: statement clobbers memory: loss_74 = PULSE_GetLoss_56 (_29, _28, _24, _23);
PULSE.c:22:2: missed: statement clobbers memory: _109 (output_60);
PULSE.c:22:2: missed: statement clobbers memory: _131 (_110);
PULSE.c:22:2: missed: statement clobbers memory: _138 (_132);
PULSE.c:22:2: missed: statement clobbers memory: _145 (_139);
PULSE.c:22:2: missed: statement clobbers memory: _152 (_146);
PULSE.c:22:2: missed: statement clobbers memory: _159 (_153);
PULSE.c:25:3: missed: statement clobbers memory: PULSE_Back (_160);
PULSE.c:26:3: missed: statement clobbers memory: memset (_165, 0, _163);
PULSE.c:26:3: missed: statement clobbers memory: memset (_158, 0, _156);
PULSE.c:26:3: missed: statement clobbers memory: memset (_151, 0, _149);
PULSE.c:26:3: missed: statement clobbers memory: memset (_144, 0, _142);
PULSE.c:26:3: missed: statement clobbers memory: memset (_137, 0, _135);
PULSE.c:26:3: missed: statement clobbers memory: memset (_115, 0, _113);
PULSE.c:74:6: missed: statement clobbers memory: _33 (current_220, args);
PULSE.c:80:4: missed: statement clobbers memory: printf ("Epoch: %d | Item: %d | Loss: %.10f | Batch Loss: %.10f\r", i_217, j_218, _37, _36);
PULSE.c:84:2: missed: statement clobbers memory: free (TMP_58);
PULSE.c:47:6: missed: statement clobbers memory: __builtin_stack_restore (saved_stack.8_53);
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4DI
PULSE.c:47:6: note: ***** The result for vector mode V32QI would be the same
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V16QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V16QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V8QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V8QI
PULSE.c:47:6: note: ***** Re-trying analysis with vector mode V4QI
PULSE.c:47:6: note: ***** Analysis failed with vector mode V4QI
PULSE.c:91:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:91:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
PULSE.c:125:20: missed: couldn't vectorize loop
PULSE.c:125:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:125:20: missed: couldn't vectorize loop
PULSE.c:125:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:104:20: missed: couldn't vectorize loop
PULSE.c:104:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:104:20: missed: couldn't vectorize loop
PULSE.c:104:20: missed: not vectorized: unsupported control flow in loop.
PULSE.c:93:13: note: vectorized 0 loops in function.
PULSE.c:96:39: missed: statement clobbers memory: layers_42 = malloc (_3);
PULSE.c:99:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:118:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:119:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:120:46: missed: statement clobbers memory: WEIGHTS_88 = aligned_alloc (64, 0);
PULSE.c:110:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.115_96];
PULSE.c:111:21: missed: statement clobbers memory: _77 = PULSE_GetDenseWeightsSize (args);
PULSE.c:112:16: missed: statement clobbers memory: _80 = PULSE_GetDenseIOSize (args);
PULSE.c:113:19: missed: statement clobbers memory: _83 = PULSE_GetDenseFixesSize (args);
PULSE.c:110:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})_226];
PULSE.c:111:21: missed: statement clobbers memory: _212 = PULSE_GetDenseWeightsSize (args);
PULSE.c:112:16: missed: statement clobbers memory: _215 = PULSE_GetDenseIOSize (args);
PULSE.c:113:19: missed: statement clobbers memory: _218 = PULSE_GetDenseFixesSize (args);
PULSE.c:118:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:119:2: missed: statement clobbers memory: __builtin_va_start (&layers_info, 0);
PULSE.c:120:46: missed: statement clobbers memory: WEIGHTS_47 = aligned_alloc (64, _5);
PULSE.c:121:41: missed: statement clobbers memory: IO_49 = aligned_alloc (64, _7);
PULSE.c:131:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.117_98];
PULSE.c:132:17: missed: statement clobbers memory: *_10 = PULSE_CreateDenseLayer (args, WEIGHTS_PTR_154, IO_PTR_155); [return slot optimization]
PULSE.c:133:20: missed: statement clobbers memory: _66 = PULSE_GetDenseWeightsSize (args);
PULSE.c:134:15: missed: statement clobbers memory: _69 = PULSE_GetDenseIOSize (args);
PULSE.c:131:26: missed: not vectorized: more than one data ref in stmt: args = MEM[(struct PULSE_DenseLayerArgs * {ref-all})addr.117_126];
PULSE.c:132:17: missed: statement clobbers memory: *_108 = PULSE_CreateDenseLayer (args, WEIGHTS_PTR_150, IO_PTR_145); [return slot optimization]
PULSE.c:133:20: missed: statement clobbers memory: _97 = PULSE_GetDenseWeightsSize (args);
PULSE.c:134:15: missed: statement clobbers memory: _61 = PULSE_GetDenseIOSize (args);
PULSE.c:140:2: missed: statement clobbers memory: __builtin_va_end (&layers_info);
PULSE.c:141:9: note: ***** Analysis succeeded with vector mode V8SI
PULSE.c:141:9: note: SLPing BB part
PULSE.c:141:9: note: Costing subgraph: 
PULSE.c:141:9: note: node 0x46b6d18 (max_nunits=4, refcnt=1) vector(4) unsigned int
PULSE.c:141:9: note: op template: <retval>.n_layers = _20;
PULSE.c:141:9: note: 	stmt 0 <retval>.n_layers = _20;
PULSE.c:141:9: note: 	stmt 1 <retval>.weights_size = WEIGHTS_SIZE_166;
PULSE.c:141:9: note: 	stmt 2 <retval>.io_size = IO_SIZE_169;
PULSE.c:141:9: note: 	stmt 3 <retval>.fixes_size = FIXES_SIZE_170;
PULSE.c:141:9: note: 	children 0x46b6e18
PULSE.c:141:9: note: node (external) 0x46b6e18 (max_nunits=1, refcnt=1) vector(4) unsigned int
PULSE.c:141:9: note: 	{ _20, WEIGHTS_SIZE_166, IO_SIZE_169, FIXES_SIZE_170 }
PULSE.c:141:9: note: Cost model analysis: 
PULSE.c:141:9: note: Cost model analysis for part in loop 0:
  Vector cost: 52
  Scalar cost: 64
PULSE.c:141:9: note: Costing subgraph: 
PULSE.c:141:9: note: node 0x46b6f18 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:141:9: note: op template: <retval>.layers = layers_42;
PULSE.c:141:9: note: 	stmt 0 <retval>.layers = layers_42;
PULSE.c:141:9: note: 	stmt 1 <retval>.weights = WEIGHTS_165;
PULSE.c:141:9: note: 	children 0x46b7018
PULSE.c:141:9: note: node (external) 0x46b7018 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:141:9: note: 	{ layers_42, WEIGHTS_165 }
PULSE.c:141:9: note: Cost model analysis: 
PULSE.c:141:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:141:9: note: Costing subgraph: 
PULSE.c:141:9: note: node 0x46b7098 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:141:9: note: op template: <retval>.io = 0B;
PULSE.c:141:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:141:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:141:9: note: 	children 0x46b7118
PULSE.c:141:9: note: node (constant) 0x46b7118 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:141:9: note: 	{ 0B, 0B }
PULSE.c:141:9: note: Cost model analysis: 
PULSE.c:141:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
PULSE.c:141:9: note: Basic block will be vectorized using SLP
PULSE.c:141:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:141:9: note: Vectorizing SLP tree:
PULSE.c:141:9: note: node 0x46b6d18 (max_nunits=4, refcnt=1) vector(4) unsigned int
PULSE.c:141:9: note: op template: <retval>.n_layers = _20;
PULSE.c:141:9: note: 	stmt 0 <retval>.n_layers = _20;
PULSE.c:141:9: note: 	stmt 1 <retval>.weights_size = WEIGHTS_SIZE_166;
PULSE.c:141:9: note: 	stmt 2 <retval>.io_size = IO_SIZE_169;
PULSE.c:141:9: note: 	stmt 3 <retval>.fixes_size = FIXES_SIZE_170;
PULSE.c:141:9: note: 	children 0x46b6e18
PULSE.c:141:9: note: node (external) 0x46b6e18 (max_nunits=1, refcnt=1) vector(4) unsigned int
PULSE.c:141:9: note: 	{ _20, WEIGHTS_SIZE_166, IO_SIZE_169, FIXES_SIZE_170 }
PULSE.c:141:9: note: ------>vectorizing SLP node starting from: <retval>.n_layers = _20;
PULSE.c:141:9: note: vect_is_simple_use: operand WEIGHTS_SIZE_166 = PHI <WEIGHTS_SIZE_163(36), 0(25)>, type of def: internal
PULSE.c:141:9: note: vect_is_simple_use: operand IO_SIZE_169 = PHI <IO_SIZE_162(36), 0(25)>, type of def: internal
PULSE.c:141:9: note: vect_is_simple_use: operand FIXES_SIZE_170 = PHI <FIXES_SIZE_161(36), 0(25)>, type of def: internal
PULSE.c:141:9: note: transform store. ncopies = 1
PULSE.c:141:9: note: create vector_type-pointer variable to type: vector(4) unsigned int  vectorizing a pointer ref: <retval>.n_layers
PULSE.c:141:9: note: created &<retval>.n_layers
PULSE.c:141:9: note: add new stmt: MEM <vector(4) unsigned int> [(unsigned int *)&<retval> + 32B] = _17;
PULSE.c:141:9: note: vectorizing stmts using SLP.
PULSE.c:141:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:141:9: note: Vectorizing SLP tree:
PULSE.c:141:9: note: node 0x46b6f18 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:141:9: note: op template: <retval>.layers = layers_42;
PULSE.c:141:9: note: 	stmt 0 <retval>.layers = layers_42;
PULSE.c:141:9: note: 	stmt 1 <retval>.weights = WEIGHTS_165;
PULSE.c:141:9: note: 	children 0x46b7018
PULSE.c:141:9: note: node (external) 0x46b7018 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:141:9: note: 	{ layers_42, WEIGHTS_165 }
PULSE.c:141:9: note: ------>vectorizing SLP node starting from: <retval>.layers = layers_42;
PULSE.c:141:9: note: vect_is_simple_use: operand WEIGHTS_165 = PHI <WEIGHTS_47(36), WEIGHTS_88(25)>, type of def: internal
PULSE.c:141:9: note: conflicting alias set types.
PULSE.c:141:9: note: transform store. ncopies = 1
PULSE.c:141:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.layers
PULSE.c:141:9: note: created &<retval>
PULSE.c:141:9: note: add new stmt: MEM <vector(2) long unsigned int> [(void *)&<retval>] = _87;
PULSE.c:141:9: note: vectorizing stmts using SLP.
PULSE.c:141:9: optimized: basic block part vectorized using 16 byte vectors
PULSE.c:141:9: note: Vectorizing SLP tree:
PULSE.c:141:9: note: node 0x46b7098 (max_nunits=2, refcnt=1) vector(2) long unsigned int
PULSE.c:141:9: note: op template: <retval>.io = 0B;
PULSE.c:141:9: note: 	stmt 0 <retval>.io = 0B;
PULSE.c:141:9: note: 	stmt 1 <retval>.fixes = 0B;
PULSE.c:141:9: note: 	children 0x46b7118
PULSE.c:141:9: note: node (constant) 0x46b7118 (max_nunits=1, refcnt=1) vector(2) long unsigned int
PULSE.c:141:9: note: 	{ 0B, 0B }
PULSE.c:141:9: note: ------>vectorizing SLP node starting from: <retval>.io = 0B;
PULSE.c:141:9: note: vect_is_simple_use: operand 0B, type of def: constant
PULSE.c:141:9: note: transform store. ncopies = 1
PULSE.c:141:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.io
PULSE.c:141:9: note: created &<retval>.io
PULSE.c:141:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval> + 16B] = { 0, 0 };
PULSE.c:141:9: note: vectorizing stmts using SLP.
PULSE.c:141:9: note: ***** The result for vector mode V32QI would be the same
PULSE.c:147:2: missed: statement clobbers memory: free (_1);
PULSE.c:148:2: missed: statement clobbers memory: free (_2);
PULSE.c:149:2: missed: statement clobbers memory: free (_3);
PULSE.c:150:1: note: ***** Analysis failed with vector mode V4DI
PULSE.c:150:1: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V4DI
Loss.c:15:19: optimized: loop vectorized using 32 byte vectors
Loss.c:15:19: optimized:  loop versioned for vectorization because of possible aliasing
Loss.c:15:19: optimized: loop vectorized using 16 byte vectors
Loss.c:12:23: note: vectorized 1 loops in function.
Loss.c:12:23: note: ***** Analysis failed with vector mode V8SF
Loss.c:12:23: note: ***** The result for vector mode V32QI would be the same
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V16QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V16QI
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V8QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V8QI
Loss.c:12:23: note: ***** Re-trying analysis with vector mode V4QI
Loss.c:12:23: note: ***** Analysis failed with vector mode V4QI
Loss.c:7:19: missed: couldn't vectorize loop
Loss.c:8:38: missed: not vectorized: unsupported use in stmt.
Loss.c:4:23: note: vectorized 0 loops in function.
Loss.c:9:13: note: ***** Analysis failed with vector mode V8SF
Loss.c:9:13: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SF
Loss.c:23:2: note: ***** Analysis failed with vector mode VOID
Dense.c:9:27: missed: couldn't vectorize loop
Dense.c:9:27: missed: not vectorized: unsupported outerloop form.
Dense.c:12:20: missed: couldn't vectorize loop
Dense.c:13:21: missed: not vectorized: complicated access pattern.
Dense.c:6:13: note: vectorized 0 loops in function.
Dense.c:16:2: missed: statement clobbers memory: _28 (pretmp_91, _51, 0);
Dense.c:9:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:9:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:9:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:9:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:24:27: missed: couldn't vectorize loop
Dense.c:24:27: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:28:20: optimized: loop vectorized using 32 byte vectors
Dense.c:28:20: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:28:20: optimized: loop vectorized using 16 byte vectors
Dense.c:28:20: missed: couldn't vectorize loop
Dense.c:30:19: missed: not vectorized: no vectype for stmt: _19 = *_18;
 scalar_type: float
Dense.c:20:13: note: vectorized 1 loops in function.
Dense.c:23:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:24:54: note: ***** Analysis failed with vector mode V4DI
Dense.c:24:54: note: ***** The result for vector mode V32QI would be the same
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V16QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V8QI
Dense.c:24:54: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:24:54: note: ***** Analysis failed with vector mode V4QI
Dense.c:41:28: missed: couldn't vectorize loop
Dense.c:41:28: missed: not vectorized: unsupported outerloop form.
Dense.c:45:21: optimized: loop vectorized using 32 byte vectors
Dense.c:45:21: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:45:21: optimized: loop vectorized using 16 byte vectors
Dense.c:37:13: note: vectorized 1 loops in function.
Dense.c:37:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:37:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:37:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:37:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:41:55: note: ***** Analysis failed with vector mode V4QI
Dense.c:182:56: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:182:56: note: SLPing BB part
Dense.c:182:56: note: Costing subgraph: 
Dense.c:182:56: note: node 0x7170e38 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:182:56: note: op template: _4 = (long unsigned int) _3;
Dense.c:182:56: note: 	stmt 0 _4 = (long unsigned int) _3;
Dense.c:182:56: note: 	stmt 1 _7 = (sizetype) _2;
Dense.c:182:56: note: 	children 0x7170f38
Dense.c:182:56: note: node (external) 0x7170f38 (max_nunits=1, refcnt=1) vector(2) unsigned int
Dense.c:182:56: note: 	{ _3, _2 }
Dense.c:182:56: note: Cost model analysis: 
Dense.c:182:56: note: Cost model analysis for part in loop 0:
  Vector cost: 30
  Scalar cost: 8
Dense.c:182:56: missed: not vectorized: vectorization is not profitable.
Dense.c:182:56: note: ***** The result for vector mode V32QI would be the same
Dense.c:182:56: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:182:56: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:182:56: note: SLPing BB part
Dense.c:182:56: note: Costing subgraph: 
Dense.c:182:56: note: node 0x7170e38 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:182:56: note: op template: _4 = (long unsigned int) _3;
Dense.c:182:56: note: 	stmt 0 _4 = (long unsigned int) _3;
Dense.c:182:56: note: 	stmt 1 _7 = (sizetype) _2;
Dense.c:182:56: note: 	children 0x7170eb8
Dense.c:182:56: note: node (external) 0x7170eb8 (max_nunits=1, refcnt=1) vector(2) unsigned int
Dense.c:182:56: note: 	{ _3, _2 }
Dense.c:182:56: note: Cost model analysis: 
Dense.c:182:56: note: Cost model analysis for part in loop 0:
  Vector cost: 30
  Scalar cost: 8
Dense.c:182:56: missed: not vectorized: vectorization is not profitable.
Dense.c:182:56: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:182:56: note: ***** Analysis failed with vector mode V8QI
Dense.c:182:56: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:182:56: note: ***** Analysis failed with vector mode V4QI
Dense.c:166:10: optimized: loop vectorized using 32 byte vectors
Dense.c:166:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:166:10: optimized: loop vectorized using 16 byte vectors
Dense.c:157:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
Dense.c:150:10: optimized: loop vectorized using 32 byte vectors
Dense.c:150:10: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:150:10: optimized: loop vectorized using 16 byte vectors
Dense.c:141:10: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
Dense.c:132:13: note: vectorized 2 loops in function.
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_14, _56);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_15, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_27, _64);
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:1029:3: missed: statement clobbers memory: __builtin_ia32_movntps256 (_28, { 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0 });
Dense.c:132:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:132:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:132:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:132:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:132:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:132:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:132:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:132:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:92:24: missed: couldn't vectorize loop
Dense.c:110:25: missed: failed: evolution of base is not affine.
Dense.c:107:12: optimized: loop vectorized using 32 byte vectors
Dense.c:107:12: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:92:24: missed: couldn't vectorize loop
Dense.c:92:24: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:107:12: optimized: loop vectorized using 32 byte vectors
Dense.c:107:12: optimized:  loop versioned for vectorization because of possible aliasing
Dense.c:97:17: missed: couldn't vectorize loop
Dense.c:97:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:114:24: missed: couldn't vectorize loop
Dense.c:116:32: missed: not vectorized: no vectype for stmt: _188 = this_89(D)->outputs;
 scalar_type: float *
Dense.c:114:24: missed: couldn't vectorize loop
Dense.c:114:24: missed: not vectorized: could not determine main exit from loop with multiple exits.
Dense.c:119:17: missed: couldn't vectorize loop
Dense.c:119:17: missed: not vectorized: number of iterations cannot be computed.
Dense.c:83:13: note: vectorized 2 loops in function.
Dense.c:86:2: missed: statement clobbers memory: _1 (_3, _2, 1);
Dense.c:83:13: note: ***** Analysis succeeded with vector mode V4DI
Dense.c:83:13: note: SLPing BB part
Dense.c:121:17: note: Costing subgraph: 
Dense.c:121:17: note: node 0x7171138 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:121:17: note: op template: _61 = (sizetype) j_153;
Dense.c:121:17: note: 	stmt 0 _61 = (sizetype) j_153;
Dense.c:121:17: note: 	stmt 1 _247 = (sizetype) wi_155;
Dense.c:121:17: note: 	children 0x7171238
Dense.c:121:17: note: node (external) 0x7171238 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:121:17: note: 	{ j_153, wi_155 }
Dense.c:121:17: note: Cost model analysis: 
Dense.c:121:17: note: Scalar 1 and vector 2 loop part do not match up, skipping scalar part
Dense.c:121:17: note: Cost model analysis for part in loop 2:
  Vector cost: 36
  Scalar cost: 8
Dense.c:121:17: missed: not vectorized: vectorization is not profitable.
Dense.c:83:13: note: Costing subgraph: 
Dense.c:83:13: note: node 0x7171338 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:83:13: note: op template: _17 = (sizetype) j_150;
Dense.c:83:13: note: 	stmt 0 _17 = (sizetype) j_150;
Dense.c:83:13: note: 	stmt 1 _311 = (sizetype) wi_154;
Dense.c:83:13: note: 	children 0x7171438
Dense.c:83:13: note: node (external) 0x7171438 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:83:13: note: 	{ j_150, wi_154 }
Dense.c:83:13: note: node 0x7171538 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:83:13: note: op template: _249 = (sizetype) j_142;
Dense.c:83:13: note: 	stmt 0 _249 = (sizetype) j_142;
Dense.c:83:13: note: 	stmt 1 _311 = (sizetype) wi_154;
Dense.c:83:13: note: 	children 0x7171638
Dense.c:83:13: note: node (external) 0x7171638 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:83:13: note: 	{ j_142, wi_154 }
Dense.c:83:13: note: Cost model analysis: 
Dense.c:83:13: note: Cost model analysis for part in loop 4:
  Vector cost: 36
  Scalar cost: 12
Dense.c:83:13: missed: not vectorized: vectorization is not profitable.
Dense.c:83:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:83:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:83:13: note: ***** Analysis succeeded with vector mode V16QI
Dense.c:83:13: note: SLPing BB part
Dense.c:121:17: note: Costing subgraph: 
Dense.c:121:17: note: node 0x7170e38 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:121:17: note: op template: _61 = (sizetype) j_153;
Dense.c:121:17: note: 	stmt 0 _61 = (sizetype) j_153;
Dense.c:121:17: note: 	stmt 1 _247 = (sizetype) wi_155;
Dense.c:121:17: note: 	children 0x7170fb8
Dense.c:121:17: note: node (external) 0x7170fb8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:121:17: note: 	{ j_153, wi_155 }
Dense.c:121:17: note: Cost model analysis: 
Dense.c:121:17: note: Scalar 1 and vector 2 loop part do not match up, skipping scalar part
Dense.c:121:17: note: Cost model analysis for part in loop 2:
  Vector cost: 36
  Scalar cost: 8
Dense.c:121:17: missed: not vectorized: vectorization is not profitable.
Dense.c:83:13: note: Costing subgraph: 
Dense.c:83:13: note: node 0x7171038 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:83:13: note: op template: _17 = (sizetype) j_150;
Dense.c:83:13: note: 	stmt 0 _17 = (sizetype) j_150;
Dense.c:83:13: note: 	stmt 1 _311 = (sizetype) wi_154;
Dense.c:83:13: note: 	children 0x71716b8
Dense.c:83:13: note: node (external) 0x71716b8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:83:13: note: 	{ j_150, wi_154 }
Dense.c:83:13: note: node 0x71717b8 (max_nunits=2, refcnt=1) vector(2) sizetype
Dense.c:83:13: note: op template: _249 = (sizetype) j_142;
Dense.c:83:13: note: 	stmt 0 _249 = (sizetype) j_142;
Dense.c:83:13: note: 	stmt 1 _311 = (sizetype) wi_154;
Dense.c:83:13: note: 	children 0x71713b8
Dense.c:83:13: note: node (external) 0x71713b8 (max_nunits=1, refcnt=1) vector(2) int
Dense.c:83:13: note: 	{ j_142, wi_154 }
Dense.c:83:13: note: Cost model analysis: 
Dense.c:83:13: note: Cost model analysis for part in loop 4:
  Vector cost: 36
  Scalar cost: 12
Dense.c:83:13: missed: not vectorized: vectorization is not profitable.
Dense.c:83:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:110:29: note: ***** Analysis failed with vector mode V8QI
Dense.c:110:29: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:92:51: note: ***** Analysis failed with vector mode V4QI
Dense.c:62:15: missed: couldn't vectorize loop
Dense.c:62:15: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
Dense.c:75:11: optimized: loop vectorized using 32 byte vectors
Dense.c:75:11: optimized: loop vectorized using 16 byte vectors
Dense.c:66:11: missed: couldn't vectorize loop
/usr/lib/gcc/x86_64-pc-linux-gnu/14.2.1/include/avxintrin.h:881:10: missed: not vectorized: no vectype for stmt: _53 = MEM[(__m256 * {ref-all})w_ptr_82];
 scalar_type: __m256
Dense.c:54:13: note: vectorized 1 loops in function.
Dense.c:57:2: missed: statement clobbers memory: memcpy (_4, dense$baiases_63, _3);
Dense.c:79:2: missed: statement clobbers memory: _27 (pretmp_163, _68, 0);
Dense.c:54:13: note: ***** Analysis failed with vector mode V4DI
Dense.c:54:13: note: ***** The result for vector mode V32QI would be the same
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V16QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V8QI
Dense.c:54:13: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:54:13: note: ***** Analysis failed with vector mode V4QI
Dense.c:175:107: note: ***** Analysis failed with vector mode V8SI
Dense.c:175:107: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
Dense.c:176:85: note: ***** Analysis failed with vector mode V8SI
Dense.c:176:85: note: ***** The result for vector mode V32QI would be the same
Dense.c:176:85: note: ***** Re-trying analysis with vector mode V16QI
Dense.c:176:85: note: ***** Analysis failed with vector mode V16QI
Dense.c:176:85: note: ***** Re-trying analysis with vector mode V8QI
Dense.c:176:85: note: ***** Analysis failed with vector mode V8QI
Dense.c:176:85: note: ***** Re-trying analysis with vector mode V4QI
Dense.c:176:85: note: ***** Analysis failed with vector mode V4QI
Dense.c:177:107: note: ***** Analysis failed with vector mode V8SI
Dense.c:177:107: note: ***** Skipping vector mode V32QI, which would repeat the analysis for V8SI
Dense.c:193:19: missed: couldn't vectorize loop
Dense.c:193:19: missed: not vectorized: unsupported control flow in loop.
Dense.c:186:13: note: vectorized 0 loops in function.
Dense.c:194:38: missed: statement clobbers memory: _5 = rand ();
Dense.c:194:72: missed: statement clobbers memory: _65 = sqrt (_12);
Dense.c:207:19: missed: statement clobbers memory: _24 = PULSE_GetActivationFunctionPtr (_23);
Dense.c:223:4: missed: statement clobbers memory: printf ("ERROR: PULSE Layer GPU are not supported on this device");
Dense.c:224:4: missed: statement clobbers memory: exit (1);
Dense.c:228:9: note: ***** Analysis succeeded with vector mode V8SI
Dense.c:228:9: note: SLPing BB part
Dense.c:228:9: note: Costing subgraph: 
Dense.c:228:9: note: node 0x6f92dc8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:228:9: note: op template: <retval>.n_inputs = args$n_inputs_41;
Dense.c:228:9: note: 	stmt 0 <retval>.n_inputs = args$n_inputs_41;
Dense.c:228:9: note: 	stmt 1 <retval>.n_outputs = args$n_outputs_42;
Dense.c:228:9: note: 	children 0x6f92e48
Dense.c:228:9: note: node 0x6f92e48 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:228:9: note: op template: args$n_inputs_41 = args.n_inputs;
Dense.c:228:9: note: 	stmt 0 args$n_inputs_41 = args.n_inputs;
Dense.c:228:9: note: 	stmt 1 args$n_outputs_42 = args.n_outputs;
Dense.c:228:9: note: Cost model analysis: 
Dense.c:228:9: note: Cost model analysis for part in loop 0:
  Vector cost: 40
  Scalar cost: 56
Dense.c:228:9: note: Costing subgraph: 
Dense.c:228:9: note: node 0x6f92f48 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:228:9: note: op template: <retval>.inputs = IO_31(D);
Dense.c:228:9: note: 	stmt 0 <retval>.inputs = IO_31(D);
Dense.c:228:9: note: 	stmt 1 <retval>.outputs = _22;
Dense.c:228:9: note: 	children 0x6f92fc8
Dense.c:228:9: note: node (external) 0x6f92fc8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:228:9: note: 	{ IO_31(D), _22 }
Dense.c:228:9: note: Cost model analysis: 
Dense.c:228:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:228:9: note: Costing subgraph: 
Dense.c:228:9: note: node 0x6f930c8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:228:9: note: op template: <retval>.feed = layer$feed_30;
Dense.c:228:9: note: 	stmt 0 <retval>.feed = layer$feed_30;
Dense.c:228:9: note: 	stmt 1 <retval>.back = layer$back_39;
Dense.c:228:9: note: 	stmt 2 <retval>.fix = layer$fix_40;
Dense.c:228:9: note: 	stmt 3 <retval>.activate = _25;
Dense.c:228:9: note: 	children 0x6f931c8
Dense.c:228:9: note: node (external) 0x6f931c8 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:228:9: note: 	{ layer$feed_30, layer$back_39, layer$fix_40, _25 }
Dense.c:228:9: note: Cost model analysis: 
Dense.c:228:9: note: Cost model analysis for part in loop 0:
  Vector cost: 64
  Scalar cost: 64
Dense.c:228:9: note: Costing subgraph: 
Dense.c:228:9: note: node 0x6f932c8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:228:9: note: op template: <retval>.parent = 0B;
Dense.c:228:9: note: 	stmt 0 <retval>.parent = 0B;
Dense.c:228:9: note: 	stmt 1 <retval>.child = 0B;
Dense.c:228:9: note: 	children 0x6f93348
Dense.c:228:9: note: node (constant) 0x6f93348 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:228:9: note: 	{ 0B, 0B }
Dense.c:228:9: note: Cost model analysis: 
Dense.c:228:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:228:9: note: Costing subgraph: 
Dense.c:228:9: note: node 0x6f933c8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:228:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_29(D);
Dense.c:228:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_29(D);
Dense.c:228:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&<retval> + 96B] = _4;
Dense.c:228:9: note: 	children 0x6f93448
Dense.c:228:9: note: node (external) 0x6f93448 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:228:9: note: 	{ MODEL_29(D), _4 }
Dense.c:228:9: note: Cost model analysis: 
Dense.c:228:9: note: Cost model analysis for part in loop 0:
  Vector cost: 32
  Scalar cost: 32
Dense.c:228:9: note: Basic block will be vectorized using SLP
Dense.c:228:9: optimized: basic block part vectorized using 8 byte vectors
Dense.c:228:9: note: Vectorizing SLP tree:
Dense.c:228:9: note: node 0x6f92dc8 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:228:9: note: op template: <retval>.n_inputs = args$n_inputs_41;
Dense.c:228:9: note: 	stmt 0 <retval>.n_inputs = args$n_inputs_41;
Dense.c:228:9: note: 	stmt 1 <retval>.n_outputs = args$n_outputs_42;
Dense.c:228:9: note: 	children 0x6f92e48
Dense.c:228:9: note: node 0x6f92e48 (max_nunits=2, refcnt=1) vector(2) unsigned int
Dense.c:228:9: note: op template: args$n_inputs_41 = args.n_inputs;
Dense.c:228:9: note: 	stmt 0 args$n_inputs_41 = args.n_inputs;
Dense.c:228:9: note: 	stmt 1 args$n_outputs_42 = args.n_outputs;
Dense.c:228:9: note: ------>vectorizing SLP node starting from: args$n_inputs_41 = args.n_inputs;
Dense.c:228:9: note: transform load. ncopies = 1
Dense.c:228:9: note: conflicting alias set types.
Dense.c:228:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: args.n_inputs
Dense.c:228:9: note: created &args
Dense.c:228:9: note: add new stmt: vect_args_n_inputs_41.553_26 = MEM <vector(2) unsigned int> [(void *)&args];
Dense.c:228:9: note: extracting lane for live stmt args$n_inputs_41 = args.n_inputs;
Dense.c:228:9: note: extracting lane for live stmt args$n_outputs_42 = args.n_outputs;
Dense.c:228:9: note: ------>vectorizing SLP node starting from: <retval>.n_inputs = args$n_inputs_41;
Dense.c:228:9: note: vect_is_simple_use: operand args.n_inputs, type of def: internal
Dense.c:228:9: note: vect_is_simple_use: operand args.n_outputs, type of def: internal
Dense.c:228:9: note: transform store. ncopies = 1
Dense.c:228:9: note: create vector_type-pointer variable to type: vector(2) unsigned int  vectorizing a pointer ref: <retval>.n_inputs
Dense.c:228:9: note: created &<retval>.n_inputs
Dense.c:228:9: note: add new stmt: MEM <vector(2) unsigned int> [(unsigned int *)&<retval> + 128B] = vect_args_n_inputs_41.553_26;
Dense.c:228:9: note: vectorizing stmts using SLP.
Dense.c:228:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:228:9: note: Vectorizing SLP tree:
Dense.c:228:9: note: node 0x6f92f48 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:228:9: note: op template: <retval>.inputs = IO_31(D);
Dense.c:228:9: note: 	stmt 0 <retval>.inputs = IO_31(D);
Dense.c:228:9: note: 	stmt 1 <retval>.outputs = _22;
Dense.c:228:9: note: 	children 0x6f92fc8
Dense.c:228:9: note: node (external) 0x6f92fc8 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:228:9: note: 	{ IO_31(D), _22 }
Dense.c:228:9: note: ------>vectorizing SLP node starting from: <retval>.inputs = IO_31(D);
Dense.c:228:9: note: vect_is_simple_use: operand IO_31(D) + _21, type of def: internal
Dense.c:228:9: note: transform store. ncopies = 1
Dense.c:228:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.inputs
Dense.c:228:9: note: created &<retval>
Dense.c:228:9: note: add new stmt: MEM <vector(2) long unsigned int> [(float * *)&<retval>] = _78;
Dense.c:228:9: note: vectorizing stmts using SLP.
Dense.c:228:9: optimized: basic block part vectorized using 32 byte vectors
Dense.c:228:9: note: Vectorizing SLP tree:
Dense.c:228:9: note: node 0x6f930c8 (max_nunits=4, refcnt=1) vector(4) long unsigned int
Dense.c:228:9: note: op template: <retval>.feed = layer$feed_30;
Dense.c:228:9: note: 	stmt 0 <retval>.feed = layer$feed_30;
Dense.c:228:9: note: 	stmt 1 <retval>.back = layer$back_39;
Dense.c:228:9: note: 	stmt 2 <retval>.fix = layer$fix_40;
Dense.c:228:9: note: 	stmt 3 <retval>.activate = _25;
Dense.c:228:9: note: 	children 0x6f931c8
Dense.c:228:9: note: node (external) 0x6f931c8 (max_nunits=1, refcnt=1) vector(4) long unsigned int
Dense.c:228:9: note: 	{ layer$feed_30, layer$back_39, layer$fix_40, _25 }
Dense.c:228:9: note: ------>vectorizing SLP node starting from: <retval>.feed = layer$feed_30;
Dense.c:228:9: note: vect_is_simple_use: operand layer$back_39 = PHI <layer$back_45(D)(11), _BackDense(9), _SIMD_BackDense(15)>, type of def: internal
Dense.c:228:9: note: vect_is_simple_use: operand layer$fix_40 = PHI <layer$fix_46(D)(11), _FixDense(9), _SIMD_FixDense(15)>, type of def: internal
Dense.c:228:9: note: vect_is_simple_use: operand (void (*<T91a>) (float *, unsigned int, char)) _24, type of def: internal
Dense.c:228:9: note: conflicting alias set types.
Dense.c:228:9: note: transform store. ncopies = 1
Dense.c:228:9: note: create vector_type-pointer variable to type: vector(4) long unsigned int  vectorizing a pointer ref: <retval>.feed
Dense.c:228:9: note: created &<retval>.feed
Dense.c:228:9: note: add new stmt: MEM <vector(4) long unsigned int> [(void *)&<retval> + 32B] = _84;
Dense.c:228:9: note: vectorizing stmts using SLP.
Dense.c:228:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:228:9: note: Vectorizing SLP tree:
Dense.c:228:9: note: node 0x6f932c8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:228:9: note: op template: <retval>.parent = 0B;
Dense.c:228:9: note: 	stmt 0 <retval>.parent = 0B;
Dense.c:228:9: note: 	stmt 1 <retval>.child = 0B;
Dense.c:228:9: note: 	children 0x6f93348
Dense.c:228:9: note: node (constant) 0x6f93348 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:228:9: note: 	{ 0B, 0B }
Dense.c:228:9: note: ------>vectorizing SLP node starting from: <retval>.parent = 0B;
Dense.c:228:9: note: vect_is_simple_use: operand 0B, type of def: constant
Dense.c:228:9: note: transform store. ncopies = 1
Dense.c:228:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: <retval>.parent
Dense.c:228:9: note: created &<retval>.parent
Dense.c:228:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer * *)&<retval> + 72B] = { 0, 0 };
Dense.c:228:9: note: vectorizing stmts using SLP.
Dense.c:228:9: optimized: basic block part vectorized using 16 byte vectors
Dense.c:228:9: note: Vectorizing SLP tree:
Dense.c:228:9: note: node 0x6f933c8 (max_nunits=2, refcnt=1) vector(2) long unsigned int
Dense.c:228:9: note: op template: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_29(D);
Dense.c:228:9: note: 	stmt 0 MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_29(D);
Dense.c:228:9: note: 	stmt 1 MEM <float *> [(struct PULSE_Layer *)&<retval> + 96B] = _4;
Dense.c:228:9: note: 	children 0x6f93448
Dense.c:228:9: note: node (external) 0x6f93448 (max_nunits=1, refcnt=1) vector(2) long unsigned int
Dense.c:228:9: note: 	{ MODEL_29(D), _4 }
Dense.c:228:9: note: ------>vectorizing SLP node starting from: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B] = MODEL_29(D);
Dense.c:228:9: note: vect_is_simple_use: operand MODEL_29(D) + _3, type of def: internal
Dense.c:228:9: note: transform store. ncopies = 1
Dense.c:228:9: note: create vector_type-pointer variable to type: vector(2) long unsigned int  vectorizing a pointer ref: MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B]
Dense.c:228:9: note: created &MEM <float *> [(struct PULSE_Layer *)&<retval> + 88B]
Dense.c:228:9: note: add new stmt: MEM <vector(2) long unsigned int> [(struct PULSE_Layer *)&<retval> + 88B] = _89;
Dense.c:228:9: note: vectorizing stmts using SLP.
Dense.c:228:9: note: ***** The result for vector mode V32QI would be the same
